[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Cl√©ment Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v1",
                "updated": "2024-09-22T08:30:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS 2024 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "Fran√ßois Treussart"
                    }
                ],
                "author_detail": {
                    "name": "Fran√ßois Treussart"
                },
                "author": "Fran√ßois Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas H√∂llein"
                    },
                    {
                        "name": "Alja≈æ Bo≈æiƒç"
                    },
                    {
                        "name": "Michael Zollh√∂fer"
                    },
                    {
                        "name": "Matthias Nie√üner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nie√üner"
                },
                "author": "Matthias Nie√üner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thie√üen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian T√∂nnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Luk√°≈° K√Ωvala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "Andr√© Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v2",
                "updated": "2024-09-25T06:46:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    46,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Beno√Æt Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.14513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14513v2",
                "updated": "2024-09-24T17:48:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    48,
                    58,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T16:18:14Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    18,
                    14,
                    6,
                    266,
                    0
                ],
                "title": "Order of Magnitude Speedups for LLM Membership Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order of Magnitude Speedups for LLM Membership Inference"
                },
                "summary": "Large Language Models (LLMs) have the promise to revolutionize computing\nbroadly, but their complexity and extensive training data also expose\nsignificant privacy vulnerabilities. One of the simplest privacy risks\nassociated with LLMs is their susceptibility to membership inference attacks\n(MIAs), wherein an adversary aims to determine whether a specific data point\nwas part of the model's training set. Although this is a known risk, state of\nthe art methodologies for MIAs rely on training multiple computationally costly\nshadow models, making risk evaluation prohibitive for large models. Here we\nadapt a recent line of work which uses quantile regression to mount membership\ninference attacks; we extend this work by proposing a low-cost MIA that\nleverages an ensemble of small quantile regression models to determine if a\ndocument belongs to the model's training set or not. We demonstrate the\neffectiveness of this approach on fine-tuned LLMs of varying families (OPT,\nPythia, Llama) and across multiple datasets. Across all scenarios we obtain\ncomparable or improved accuracy compared to state of the art shadow model\napproaches, with as little as 6% of their computation budget. We demonstrate\nincreased effectiveness across multi-epoch trained target models, and\narchitecture miss-specification robustness, that is, we can mount an effective\nattack against a model using a different tokenizer and architecture, without\nrequiring knowledge on the target model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the promise to revolutionize computing\nbroadly, but their complexity and extensive training data also expose\nsignificant privacy vulnerabilities. One of the simplest privacy risks\nassociated with LLMs is their susceptibility to membership inference attacks\n(MIAs), wherein an adversary aims to determine whether a specific data point\nwas part of the model's training set. Although this is a known risk, state of\nthe art methodologies for MIAs rely on training multiple computationally costly\nshadow models, making risk evaluation prohibitive for large models. Here we\nadapt a recent line of work which uses quantile regression to mount membership\ninference attacks; we extend this work by proposing a low-cost MIA that\nleverages an ensemble of small quantile regression models to determine if a\ndocument belongs to the model's training set or not. We demonstrate the\neffectiveness of this approach on fine-tuned LLMs of varying families (OPT,\nPythia, Llama) and across multiple datasets. Across all scenarios we obtain\ncomparable or improved accuracy compared to state of the art shadow model\napproaches, with as little as 6% of their computation budget. We demonstrate\nincreased effectiveness across multi-epoch trained target models, and\narchitecture miss-specification robustness, that is, we can mount an effective\nattack against a model using a different tokenizer and architecture, without\nrequiring knowledge on the target model."
                },
                "authors": [
                    {
                        "name": "Rongting Zhang"
                    },
                    {
                        "name": "Martin Bertran"
                    },
                    {
                        "name": "Aaron Roth"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Roth"
                },
                "author": "Aaron Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16275v1",
                "updated": "2024-09-24T17:47:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    47,
                    34,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:47:34Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    47,
                    34,
                    1,
                    268,
                    0
                ],
                "title": "Generative Factor Chaining: Coordinated Manipulation with\n  Diffusion-based Factor Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Factor Chaining: Coordinated Manipulation with\n  Diffusion-based Factor Graph"
                },
                "summary": "Learning to plan for multi-step, multi-manipulator tasks is notoriously\ndifficult because of the large search space and the complex constraint\nsatisfaction problems. We present Generative Factor Chaining~(GFC), a\ncomposable generative model for planning. GFC represents a planning problem as\na spatial-temporal factor graph, where nodes represent objects and robots in\nthe scene, spatial factors capture the distributions of valid relationships\namong nodes, and temporal factors represent the distributions of skill\ntransitions. Each factor is implemented as a modular diffusion model, which are\ncomposed during inference to generate feasible long-horizon plans through\nbi-directional message passing. We show that GFC can solve complex bimanual\nmanipulation tasks and exhibits strong generalization to unseen planning tasks\nwith novel combinations of objects and constraints. More details can be found\nat: https://generative-fc.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to plan for multi-step, multi-manipulator tasks is notoriously\ndifficult because of the large search space and the complex constraint\nsatisfaction problems. We present Generative Factor Chaining~(GFC), a\ncomposable generative model for planning. GFC represents a planning problem as\na spatial-temporal factor graph, where nodes represent objects and robots in\nthe scene, spatial factors capture the distributions of valid relationships\namong nodes, and temporal factors represent the distributions of skill\ntransitions. Each factor is implemented as a modular diffusion model, which are\ncomposed during inference to generate feasible long-horizon plans through\nbi-directional message passing. We show that GFC can solve complex bimanual\nmanipulation tasks and exhibits strong generalization to unseen planning tasks\nwith novel combinations of objects and constraints. More details can be found\nat: https://generative-fc.github.io/"
                },
                "authors": [
                    {
                        "name": "Utkarsh A. Mishra"
                    },
                    {
                        "name": "Yongxin Chen"
                    },
                    {
                        "name": "Danfei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Danfei Xu"
                },
                "author": "Danfei Xu",
                "arxiv_comment": "28 pages, 17 figures, 2024 Conference on Robot Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16271v1",
                "updated": "2024-09-24T17:44:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    44,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:44:24Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    44,
                    24,
                    1,
                    268,
                    0
                ],
                "title": "AIM 2024 Challenge on UHD Blind Photo Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM 2024 Challenge on UHD Blind Photo Quality Assessment"
                },
                "summary": "We introduce the AIM 2024 UHD-IQA Challenge, a competition to advance the\nNo-Reference Image Quality Assessment (NR-IQA) task for modern, high-resolution\nphotos. The challenge is based on the recently released UHD-IQA Benchmark\nDatabase, which comprises 6,073 UHD-1 (4K) images annotated with perceptual\nquality ratings from expert raters. Unlike previous NR-IQA datasets, UHD-IQA\nfocuses on highly aesthetic photos of superior technical quality, reflecting\nthe ever-increasing standards of digital photography. This challenge aims to\ndevelop efficient and effective NR-IQA models. Participants are tasked with\ncreating novel architectures and training strategies to achieve high predictive\nperformance on UHD-1 images within a computational budget of 50G MACs. This\nenables model deployment on edge devices and scalable processing of extensive\nimage collections. Winners are determined based on a combination of performance\nmetrics, including correlation measures (SRCC, PLCC, KRCC), absolute error\nmetrics (MAE, RMSE), and computational efficiency (G MACs). To excel in this\nchallenge, participants leverage techniques like knowledge distillation,\nlow-precision inference, and multi-scale training. By pushing the boundaries of\nNR-IQA for high-resolution photos, the UHD-IQA Challenge aims to stimulate the\ndevelopment of practical models that can keep pace with the rapidly evolving\nlandscape of digital photography. The innovative solutions emerging from this\ncompetition will have implications for various applications, from photo\ncuration and enhancement to image compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the AIM 2024 UHD-IQA Challenge, a competition to advance the\nNo-Reference Image Quality Assessment (NR-IQA) task for modern, high-resolution\nphotos. The challenge is based on the recently released UHD-IQA Benchmark\nDatabase, which comprises 6,073 UHD-1 (4K) images annotated with perceptual\nquality ratings from expert raters. Unlike previous NR-IQA datasets, UHD-IQA\nfocuses on highly aesthetic photos of superior technical quality, reflecting\nthe ever-increasing standards of digital photography. This challenge aims to\ndevelop efficient and effective NR-IQA models. Participants are tasked with\ncreating novel architectures and training strategies to achieve high predictive\nperformance on UHD-1 images within a computational budget of 50G MACs. This\nenables model deployment on edge devices and scalable processing of extensive\nimage collections. Winners are determined based on a combination of performance\nmetrics, including correlation measures (SRCC, PLCC, KRCC), absolute error\nmetrics (MAE, RMSE), and computational efficiency (G MACs). To excel in this\nchallenge, participants leverage techniques like knowledge distillation,\nlow-precision inference, and multi-scale training. By pushing the boundaries of\nNR-IQA for high-resolution photos, the UHD-IQA Challenge aims to stimulate the\ndevelopment of practical models that can keep pace with the rapidly evolving\nlandscape of digital photography. The innovative solutions emerging from this\ncompetition will have implications for various applications, from photo\ncuration and enhancement to image compression."
                },
                "authors": [
                    {
                        "name": "Vlad Hosu"
                    },
                    {
                        "name": "Marcos V. Conde"
                    },
                    {
                        "name": "Lorenzo Agnolucci"
                    },
                    {
                        "name": "Nabajeet Barman"
                    },
                    {
                        "name": "Saman Zadtootaghaj"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "ECCV 2024 - Advances in Image Manipulation (AIM). arXiv admin note:\n  text overlap with arXiv:2401.10511 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16266v1",
                "updated": "2024-09-24T17:37:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    37,
                    54,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:37:54Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    37,
                    54,
                    1,
                    268,
                    0
                ],
                "title": "REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial\n  Task Allocation in Multi-Human Multi-Robot Teams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial\n  Task Allocation in Multi-Human Multi-Robot Teams"
                },
                "summary": "Multi-human multi-robot teams combine the complementary strengths of humans\nand robots to tackle complex tasks across diverse applications. However, the\ninherent heterogeneity of these teams presents significant challenges in\ninitial task allocation (ITA), which involves assigning the most suitable tasks\nto each team member based on their individual capabilities before task\nexecution. While current learning-based methods have shown promising results,\nthey are often computationally expensive to train, and lack the flexibility to\nincorporate user preferences in multi-objective optimization and adapt to\nlast-minute changes in real-world dynamic environments. To address these\nissues, we propose REBEL, an LLM-based ITA framework that integrates rule-based\nand experience-enhanced learning. By leveraging Retrieval-Augmented Generation,\nREBEL dynamically retrieves relevant rules and past experiences, enhancing\nreasoning efficiency. Additionally, REBEL can complement pre-trained RL-based\nITA policies, improving situational awareness and overall team performance.\nExtensive experiments validate the effectiveness of our approach across various\nsettings. More details are available at https://sites.google.com/view/ita-rebel .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-human multi-robot teams combine the complementary strengths of humans\nand robots to tackle complex tasks across diverse applications. However, the\ninherent heterogeneity of these teams presents significant challenges in\ninitial task allocation (ITA), which involves assigning the most suitable tasks\nto each team member based on their individual capabilities before task\nexecution. While current learning-based methods have shown promising results,\nthey are often computationally expensive to train, and lack the flexibility to\nincorporate user preferences in multi-objective optimization and adapt to\nlast-minute changes in real-world dynamic environments. To address these\nissues, we propose REBEL, an LLM-based ITA framework that integrates rule-based\nand experience-enhanced learning. By leveraging Retrieval-Augmented Generation,\nREBEL dynamically retrieves relevant rules and past experiences, enhancing\nreasoning efficiency. Additionally, REBEL can complement pre-trained RL-based\nITA policies, improving situational awareness and overall team performance.\nExtensive experiments validate the effectiveness of our approach across various\nsettings. More details are available at https://sites.google.com/view/ita-rebel ."
                },
                "authors": [
                    {
                        "name": "Arjun Gupte"
                    },
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Vishnunandan L. N. Venkatesh"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Dezhong Zhao"
                    },
                    {
                        "name": "Byung-Cheol Min"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Cheol Min"
                },
                "author": "Byung-Cheol Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07638v2",
                "updated": "2024-09-24T17:34:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    34,
                    7,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-11T21:48:33Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    48,
                    33,
                    2,
                    255,
                    0
                ],
                "title": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities"
                },
                "summary": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance."
                },
                "authors": [
                    {
                        "name": "Thomas Ball"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Cormac Herley"
                    }
                ],
                "author_detail": {
                    "name": "Cormac Herley"
                },
                "author": "Cormac Herley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16253v1",
                "updated": "2024-09-24T17:21:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    21,
                    25,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:21:25Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    21,
                    25,
                    1,
                    268,
                    0
                ],
                "title": "Learning To Help: Training Models to Assist Legacy Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning To Help: Training Models to Assist Legacy Devices"
                },
                "summary": "Machine learning models implemented in hardware on physical devices may be\ndeployed for a long time. The computational abilities of the device may be\nlimited and become outdated with respect to newer improvements. Because of the\nsize of ML models, offloading some computation (e.g. to an edge cloud) can help\nsuch legacy devices. We cast this problem in the framework of learning with\nabstention (LWA) in which the expert (edge) must be trained to assist the\nclient (device). Prior work on LWA trains the client assuming the edge is\neither an oracle or a human expert. In this work, we formalize the reverse\nproblem of training the expert for a fixed (legacy) client. As in LWA, the\nclient uses a rejection rule to decide when to offload inference to the expert\n(at a cost). We find the Bayes-optimal rule, prove a generalization bound, and\nfind a consistent surrogate loss function. Empirical results show that our\nframework outperforms confidence-based rejection rules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models implemented in hardware on physical devices may be\ndeployed for a long time. The computational abilities of the device may be\nlimited and become outdated with respect to newer improvements. Because of the\nsize of ML models, offloading some computation (e.g. to an edge cloud) can help\nsuch legacy devices. We cast this problem in the framework of learning with\nabstention (LWA) in which the expert (edge) must be trained to assist the\nclient (device). Prior work on LWA trains the client assuming the edge is\neither an oracle or a human expert. In this work, we formalize the reverse\nproblem of training the expert for a fixed (legacy) client. As in LWA, the\nclient uses a rejection rule to decide when to offload inference to the expert\n(at a cost). We find the Bayes-optimal rule, prove a generalization bound, and\nfind a consistent surrogate loss function. Empirical results show that our\nframework outperforms confidence-based rejection rules."
                },
                "authors": [
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Anand Sarwate"
                    }
                ],
                "author_detail": {
                    "name": "Anand Sarwate"
                },
                "author": "Anand Sarwate",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15228v2",
                "updated": "2024-09-24T17:13:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    13,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T17:22:09Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    22,
                    9,
                    0,
                    267,
                    0
                ],
                "title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs."
                },
                "authors": [
                    {
                        "name": "Yixi Wu"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Zehao Wang"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun"
                    },
                    {
                        "name": "Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen"
                },
                "arxiv_affiliation": "Peter",
                "author": "Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13187v2",
                "updated": "2024-09-24T17:13:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    13,
                    7,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-20T03:28:48Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    28,
                    48,
                    4,
                    264,
                    0
                ],
                "title": "Cooperative Resilience in Artificial Intelligence Multiagent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Resilience in Artificial Intelligence Multiagent Systems"
                },
                "summary": "Resilience refers to the ability of systems to withstand, adapt to, and\nrecover from disruptive events. While studies on resilience have attracted\nsignificant attention across various research domains, the precise definition\nof this concept within the field of cooperative artificial intelligence remains\nunclear. This paper addresses this gap by proposing a clear definition of\n`cooperative resilience' and outlining a methodology for its quantitative\nmeasurement. The methodology is validated in an environment with RL-based and\nLLM-augmented autonomous agents, subjected to environmental changes and the\nintroduction of agents with unsustainable behaviors. These events are\nparameterized to create various scenarios for measuring cooperative resilience.\nThe results highlight the crucial role of resilience metrics in analyzing how\nthe collective system prepares for, resists, recovers from, sustains\nwell-being, and transforms in the face of disruptions. These findings provide\nfoundational insights into the definition, measurement, and preliminary\nanalysis of cooperative resilience, offering significant implications for the\nbroader field of AI. Moreover, the methodology and metrics developed here can\nbe adapted to a wide range of AI applications, enhancing the reliability and\neffectiveness of AI in dynamic and unpredictable environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resilience refers to the ability of systems to withstand, adapt to, and\nrecover from disruptive events. While studies on resilience have attracted\nsignificant attention across various research domains, the precise definition\nof this concept within the field of cooperative artificial intelligence remains\nunclear. This paper addresses this gap by proposing a clear definition of\n`cooperative resilience' and outlining a methodology for its quantitative\nmeasurement. The methodology is validated in an environment with RL-based and\nLLM-augmented autonomous agents, subjected to environmental changes and the\nintroduction of agents with unsustainable behaviors. These events are\nparameterized to create various scenarios for measuring cooperative resilience.\nThe results highlight the crucial role of resilience metrics in analyzing how\nthe collective system prepares for, resists, recovers from, sustains\nwell-being, and transforms in the face of disruptions. These findings provide\nfoundational insights into the definition, measurement, and preliminary\nanalysis of cooperative resilience, offering significant implications for the\nbroader field of AI. Moreover, the methodology and metrics developed here can\nbe adapted to a wide range of AI applications, enhancing the reliability and\neffectiveness of AI in dynamic and unpredictable environments."
                },
                "authors": [
                    {
                        "name": "Manuela Chacon-Chamorro"
                    },
                    {
                        "name": "Luis Felipe Giraldo"
                    },
                    {
                        "name": "Nicanor Quijano"
                    },
                    {
                        "name": "Vicente Vargas-Panesso"
                    },
                    {
                        "name": "C√©sar Gonz√°lez"
                    },
                    {
                        "name": "Juan Sebasti√°n Pinz√≥n"
                    },
                    {
                        "name": "Rub√©n Manrique"
                    },
                    {
                        "name": "Manuel R√≠os"
                    },
                    {
                        "name": "Yesid Fonseca"
                    },
                    {
                        "name": "Daniel G√≥mez-Barrera"
                    },
                    {
                        "name": "M√≥nica Perdomo-P√©rez"
                    }
                ],
                "author_detail": {
                    "name": "M√≥nica Perdomo-P√©rez"
                },
                "author": "M√≥nica Perdomo-P√©rez",
                "arxiv_comment": "Supplementary material in\n  https://github.com/mavivi95/resilience/blob/main/Supplementary_File.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16243v1",
                "updated": "2024-09-24T17:07:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    7,
                    45,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:07:45Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    7,
                    45,
                    1,
                    268,
                    0
                ],
                "title": "A fast and sound tagging method for discontinuous named-entity\n  recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast and sound tagging method for discontinuous named-entity\n  recognition"
                },
                "summary": "We introduce a novel tagging scheme for discontinuous named entity\nrecognition based on an explicit description of the inner structure of\ndiscontinuous mentions. We rely on a weighted finite state automaton for both\nmarginal and maximum a posteriori inference. As such, our method is sound in\nthe sense that (1) well-formedness of predicted tag sequences is ensured via\nthe automaton structure and (2) there is an unambiguous mapping between\nwell-formed sequences of tags and (discontinuous) mentions. We evaluate our\napproach on three English datasets in the biomedical domain, and report\ncomparable results to state-of-the-art while having a way simpler and faster\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel tagging scheme for discontinuous named entity\nrecognition based on an explicit description of the inner structure of\ndiscontinuous mentions. We rely on a weighted finite state automaton for both\nmarginal and maximum a posteriori inference. As such, our method is sound in\nthe sense that (1) well-formedness of predicted tag sequences is ensured via\nthe automaton structure and (2) there is an unambiguous mapping between\nwell-formed sequences of tags and (discontinuous) mentions. We evaluate our\napproach on three English datasets in the biomedical domain, and report\ncomparable results to state-of-the-art while having a way simpler and faster\nmodel."
                },
                "authors": [
                    {
                        "name": "Caio Corro"
                    }
                ],
                "author_detail": {
                    "name": "Caio Corro"
                },
                "author": "Caio Corro",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16241v1",
                "updated": "2024-09-24T17:04:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    4,
                    12,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:04:12Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    4,
                    12,
                    1,
                    268,
                    0
                ],
                "title": "LLM Echo Chamber: personalized and automated disinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Echo Chamber: personalized and automated disinformation"
                },
                "summary": "Recent advancements have showcased the capabilities of Large Language Models\nlike GPT4 and Llama2 in tasks such as summarization, translation, and content\nreview. However, their widespread use raises concerns, particularly around the\npotential for LLMs to spread persuasive, humanlike misinformation at scale,\nwhich could significantly influence public opinion. This study examines these\nrisks, focusing on LLMs ability to propagate misinformation as factual. To\ninvestigate this, we built the LLM Echo Chamber, a controlled digital\nenvironment simulating social media chatrooms, where misinformation often\nspreads. Echo chambers, where individuals only interact with like minded\npeople, further entrench beliefs. By studying malicious bots spreading\nmisinformation in this environment, we can better understand this phenomenon.\nWe reviewed current LLMs, explored misinformation risks, and applied sota\nfinetuning techniques. Using Microsoft phi2 model, finetuned with our custom\ndataset, we generated harmful content to create the Echo Chamber. This setup,\nevaluated by GPT4 for persuasiveness and harmfulness, sheds light on the\nethical concerns surrounding LLMs and emphasizes the need for stronger\nsafeguards against misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have showcased the capabilities of Large Language Models\nlike GPT4 and Llama2 in tasks such as summarization, translation, and content\nreview. However, their widespread use raises concerns, particularly around the\npotential for LLMs to spread persuasive, humanlike misinformation at scale,\nwhich could significantly influence public opinion. This study examines these\nrisks, focusing on LLMs ability to propagate misinformation as factual. To\ninvestigate this, we built the LLM Echo Chamber, a controlled digital\nenvironment simulating social media chatrooms, where misinformation often\nspreads. Echo chambers, where individuals only interact with like minded\npeople, further entrench beliefs. By studying malicious bots spreading\nmisinformation in this environment, we can better understand this phenomenon.\nWe reviewed current LLMs, explored misinformation risks, and applied sota\nfinetuning techniques. Using Microsoft phi2 model, finetuned with our custom\ndataset, we generated harmful content to create the Echo Chamber. This setup,\nevaluated by GPT4 for persuasiveness and harmfulness, sheds light on the\nethical concerns surrounding LLMs and emphasizes the need for stronger\nsafeguards against misinformation."
                },
                "authors": [
                    {
                        "name": "Tony Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tony Ma"
                },
                "author": "Tony Ma",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16235v1",
                "updated": "2024-09-24T16:51:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    51,
                    36,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:51:36Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    51,
                    36,
                    1,
                    268,
                    0
                ],
                "title": "EuroLLM: Multilingual Language Models for Europe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EuroLLM: Multilingual Language Models for Europe"
                },
                "summary": "The quality of open-weight LLMs has seen significant improvement, yet they\nremain predominantly focused on English. In this paper, we introduce the\nEuroLLM project, aimed at developing a suite of open-weight multilingual LLMs\ncapable of understanding and generating text in all official European Union\nlanguages, as well as several additional relevant languages. We outline the\nprogress made to date, detailing our data collection and filtering process, the\ndevelopment of scaling laws, the creation of our multilingual tokenizer, and\nthe data mix and modeling configurations. Additionally, we release our initial\nmodels: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on\nmultilingual general benchmarks and machine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of open-weight LLMs has seen significant improvement, yet they\nremain predominantly focused on English. In this paper, we introduce the\nEuroLLM project, aimed at developing a suite of open-weight multilingual LLMs\ncapable of understanding and generating text in all official European Union\nlanguages, as well as several additional relevant languages. We outline the\nprogress made to date, detailing our data collection and filtering process, the\ndevelopment of scaling laws, the creation of our multilingual tokenizer, and\nthe data mix and modeling configurations. Additionally, we release our initial\nmodels: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on\nmultilingual general benchmarks and machine translation."
                },
                "authors": [
                    {
                        "name": "Pedro Henrique Martins"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Jo√£o Alves"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Duarte M. Alves"
                    },
                    {
                        "name": "Jos√© Pombal"
                    },
                    {
                        "name": "Amin Farajian"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Mateusz Klimaszewski"
                    },
                    {
                        "name": "Pierre Colombo"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Jos√© G. C. de Souza"
                    },
                    {
                        "name": "Alexandra Birch"
                    },
                    {
                        "name": "Andr√© F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr√© F. T. Martins"
                },
                "author": "Andr√© F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01721v2",
                "updated": "2024-09-24T16:40:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    40,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-03T18:27:44Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    27,
                    44,
                    0,
                    155,
                    0
                ],
                "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs"
                },
                "summary": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address\n$\\textit{Normal Outliers}$, which are activations across all tokens with\nrelatively large magnitudes. However, these methods struggle with smoothing\n$\\textit{Massive Outliers}$ that display significantly larger values, which\nleads to significant performance degradation in low-bit quantization. In this\npaper, we introduce DuQuant, a novel approach that utilizes rotation and\npermutation transformations to more effectively mitigate both massive and\nnormal outliers. First, DuQuant starts by constructing rotation matrices, using\nspecific outlier dimensions as prior knowledge, to redistribute outliers to\nadjacent channels by block-wise rotation. Second, We further employ a zigzag\npermutation to balance the distribution of outliers across blocks, thereby\nreducing block-wise variance. A subsequent rotation further smooths the\nactivation landscape, enhancing model performance. DuQuant simplifies the\nquantization process and excels in managing outliers, outperforming the\nstate-of-the-art baselines across various sizes and types of LLMs on multiple\ntasks, even with 4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address\n$\\textit{Normal Outliers}$, which are activations across all tokens with\nrelatively large magnitudes. However, these methods struggle with smoothing\n$\\textit{Massive Outliers}$ that display significantly larger values, which\nleads to significant performance degradation in low-bit quantization. In this\npaper, we introduce DuQuant, a novel approach that utilizes rotation and\npermutation transformations to more effectively mitigate both massive and\nnormal outliers. First, DuQuant starts by constructing rotation matrices, using\nspecific outlier dimensions as prior knowledge, to redistribute outliers to\nadjacent channels by block-wise rotation. Second, We further employ a zigzag\npermutation to balance the distribution of outliers across blocks, thereby\nreducing block-wise variance. A subsequent rotation further smooths the\nactivation landscape, enhancing model performance. DuQuant simplifies the\nquantization process and excels in managing outliers, outperforming the\nstate-of-the-art baselines across various sizes and types of LLMs on multiple\ntasks, even with 4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Jingzhi Cui"
                    },
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Linzhan Mou"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "arxiv_comment": "26 pages, 13 figures, Website at https://duquant.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16220v1",
                "updated": "2024-09-24T16:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    31,
                    33,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    31,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "Towards Enhancing Linked Data Retrieval in Conversational UIs using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Enhancing Linked Data Retrieval in Conversational UIs using\n  Large Language Models"
                },
                "summary": "Despite the recent broad adoption of Large Language Models (LLMs) across\nvarious domains, their potential for enriching information systems in\nextracting and exploring Linked Data (LD) and Resource Description Framework\n(RDF) triplestores has not been extensively explored. This paper examines the\nintegration of LLMs within existing systems, emphasising the enhancement of\nconversational user interfaces (UIs) and their capabilities for data extraction\nby producing more accurate SPARQL queries without the requirement for model\nretraining. Typically, conversational UI models necessitate retraining with the\nintroduction of new datasets or updates, limiting their functionality as\ngeneral-purpose extraction tools. Our approach addresses this limitation by\nincorporating LLMs into the conversational UI workflow, significantly enhancing\ntheir ability to comprehend and process user queries effectively. By leveraging\nthe advanced natural language understanding capabilities of LLMs, our method\nimproves RDF entity extraction within web systems employing conventional\nchatbots. This integration facilitates a more nuanced and context-aware\ninteraction model, critical for handling the complex query patterns often\nencountered in RDF datasets and Linked Open Data (LOD) endpoints. The\nevaluation of this methodology shows a marked enhancement in system\nexpressivity and the accuracy of responses to user queries, indicating a\npromising direction for future research in this area. This investigation not\nonly underscores the versatility of LLMs in enhancing existing information\nsystems but also sets the stage for further explorations into their potential\napplications within more specialised domains of web information systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent broad adoption of Large Language Models (LLMs) across\nvarious domains, their potential for enriching information systems in\nextracting and exploring Linked Data (LD) and Resource Description Framework\n(RDF) triplestores has not been extensively explored. This paper examines the\nintegration of LLMs within existing systems, emphasising the enhancement of\nconversational user interfaces (UIs) and their capabilities for data extraction\nby producing more accurate SPARQL queries without the requirement for model\nretraining. Typically, conversational UI models necessitate retraining with the\nintroduction of new datasets or updates, limiting their functionality as\ngeneral-purpose extraction tools. Our approach addresses this limitation by\nincorporating LLMs into the conversational UI workflow, significantly enhancing\ntheir ability to comprehend and process user queries effectively. By leveraging\nthe advanced natural language understanding capabilities of LLMs, our method\nimproves RDF entity extraction within web systems employing conventional\nchatbots. This integration facilitates a more nuanced and context-aware\ninteraction model, critical for handling the complex query patterns often\nencountered in RDF datasets and Linked Open Data (LOD) endpoints. The\nevaluation of this methodology shows a marked enhancement in system\nexpressivity and the accuracy of responses to user queries, indicating a\npromising direction for future research in this area. This investigation not\nonly underscores the versatility of LLMs in enhancing existing information\nsystems but also sets the stage for further explorations into their potential\napplications within more specialised domains of web information systems."
                },
                "authors": [
                    {
                        "name": "Omar Mussa"
                    },
                    {
                        "name": "Omer Rana"
                    },
                    {
                        "name": "Beno√Æt Goossens"
                    },
                    {
                        "name": "Pablo Orozco-Terwengel"
                    },
                    {
                        "name": "Charith Perera"
                    }
                ],
                "author_detail": {
                    "name": "Charith Perera"
                },
                "author": "Charith Perera",
                "arxiv_comment": "This paper has been accepted at the 25th International Web\n  Information Systems Engineering Conference (WISE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16218v1",
                "updated": "2024-09-24T16:25:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    25,
                    53,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:25:53Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    25,
                    53,
                    1,
                    268,
                    0
                ],
                "title": "Problem-oriented AutoML in Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-oriented AutoML in Clustering"
                },
                "summary": "The Problem-oriented AutoML in Clustering (PoAC) framework introduces a\nnovel, flexible approach to automating clustering tasks by addressing the\nshortcomings of traditional AutoML solutions. Conventional methods often rely\non predefined internal Clustering Validity Indexes (CVIs) and static\nmeta-features, limiting their adaptability and effectiveness across diverse\nclustering tasks. In contrast, PoAC establishes a dynamic connection between\nthe clustering problem, CVIs, and meta-features, allowing users to customize\nthese components based on the specific context and goals of their task. At its\ncore, PoAC employs a surrogate model trained on a large meta-knowledge base of\nprevious clustering datasets and solutions, enabling it to infer the quality of\nnew clustering pipelines and synthesize optimal solutions for unseen datasets.\nUnlike many AutoML frameworks that are constrained by fixed evaluation metrics\nand algorithm sets, PoAC is algorithm-agnostic, adapting seamlessly to\ndifferent clustering problems without requiring additional data or retraining.\nExperimental results demonstrate that PoAC not only outperforms\nstate-of-the-art frameworks on a variety of datasets but also excels in\nspecific tasks such as data visualization, and highlight its ability to\ndynamically adjust pipeline configurations based on dataset complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Problem-oriented AutoML in Clustering (PoAC) framework introduces a\nnovel, flexible approach to automating clustering tasks by addressing the\nshortcomings of traditional AutoML solutions. Conventional methods often rely\non predefined internal Clustering Validity Indexes (CVIs) and static\nmeta-features, limiting their adaptability and effectiveness across diverse\nclustering tasks. In contrast, PoAC establishes a dynamic connection between\nthe clustering problem, CVIs, and meta-features, allowing users to customize\nthese components based on the specific context and goals of their task. At its\ncore, PoAC employs a surrogate model trained on a large meta-knowledge base of\nprevious clustering datasets and solutions, enabling it to infer the quality of\nnew clustering pipelines and synthesize optimal solutions for unseen datasets.\nUnlike many AutoML frameworks that are constrained by fixed evaluation metrics\nand algorithm sets, PoAC is algorithm-agnostic, adapting seamlessly to\ndifferent clustering problems without requiring additional data or retraining.\nExperimental results demonstrate that PoAC not only outperforms\nstate-of-the-art frameworks on a variety of datasets but also excels in\nspecific tasks such as data visualization, and highlight its ability to\ndynamically adjust pipeline configurations based on dataset complexity."
                },
                "authors": [
                    {
                        "name": "Matheus Camilo da Silva"
                    },
                    {
                        "name": "Gabriel Marques Tavares"
                    },
                    {
                        "name": "Eric Medvet"
                    },
                    {
                        "name": "Sylvio Barbon Junior"
                    }
                ],
                "author_detail": {
                    "name": "Sylvio Barbon Junior"
                },
                "author": "Sylvio Barbon Junior",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16213v1",
                "updated": "2024-09-24T16:16:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    16,
                    19,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:16:19Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    16,
                    19,
                    1,
                    268,
                    0
                ],
                "title": "Deep Learning for Precision Agriculture: Post-Spraying Evaluation and\n  Deposition Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning for Precision Agriculture: Post-Spraying Evaluation and\n  Deposition Estimation"
                },
                "summary": "Precision spraying evaluation requires automation primarily in post-spraying\nimagery. In this paper we propose an eXplainable Artificial Intelligence (XAI)\ncomputer vision pipeline to evaluate a precision spraying system post-spraying\nwithout the need for traditional agricultural methods. The developed system can\nsemantically segment potential targets such as lettuce, chickweed, and\nmeadowgrass and correctly identify if targets have been sprayed. Furthermore,\nthis pipeline evaluates using a domain-specific Weakly Supervised Deposition\nEstimation task, allowing for class-specific quantification of spray deposit\nweights in {\\mu}L. Estimation of coverage rates of spray deposition in a\nclass-wise manner allows for further understanding of effectiveness of\nprecision spraying systems. Our study evaluates different Class Activation\nMapping techniques, namely AblationCAM and ScoreCAM, to determine which is more\neffective and interpretable for these tasks. In the pipeline, inference-only\nfeature fusion is used to allow for further interpretability and to enable the\nautomation of precision spraying evaluation post-spray. Our findings indicate\nthat a Fully Convolutional Network with an EfficientNet-B0 backbone and\ninference-only feature fusion achieves an average absolute difference in\ndeposition values of 156.8 {\\mu}L across three classes in our test set. The\ndataset curated in this paper is publicly available at\nhttps://github.com/Harry-Rogers/PSIE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision spraying evaluation requires automation primarily in post-spraying\nimagery. In this paper we propose an eXplainable Artificial Intelligence (XAI)\ncomputer vision pipeline to evaluate a precision spraying system post-spraying\nwithout the need for traditional agricultural methods. The developed system can\nsemantically segment potential targets such as lettuce, chickweed, and\nmeadowgrass and correctly identify if targets have been sprayed. Furthermore,\nthis pipeline evaluates using a domain-specific Weakly Supervised Deposition\nEstimation task, allowing for class-specific quantification of spray deposit\nweights in {\\mu}L. Estimation of coverage rates of spray deposition in a\nclass-wise manner allows for further understanding of effectiveness of\nprecision spraying systems. Our study evaluates different Class Activation\nMapping techniques, namely AblationCAM and ScoreCAM, to determine which is more\neffective and interpretable for these tasks. In the pipeline, inference-only\nfeature fusion is used to allow for further interpretability and to enable the\nautomation of precision spraying evaluation post-spray. Our findings indicate\nthat a Fully Convolutional Network with an EfficientNet-B0 backbone and\ninference-only feature fusion achieves an average absolute difference in\ndeposition values of 156.8 {\\mu}L across three classes in our test set. The\ndataset curated in this paper is publicly available at\nhttps://github.com/Harry-Rogers/PSIE"
                },
                "authors": [
                    {
                        "name": "Harry Rogers"
                    },
                    {
                        "name": "Tahmina Zebin"
                    },
                    {
                        "name": "Grzegorz Cielniak"
                    },
                    {
                        "name": "Beatriz De La Iglesia"
                    },
                    {
                        "name": "Ben Magri"
                    }
                ],
                "author_detail": {
                    "name": "Ben Magri"
                },
                "author": "Ben Magri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16209v1",
                "updated": "2024-09-24T16:09:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    9,
                    29,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:09:29Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    9,
                    29,
                    1,
                    268,
                    0
                ],
                "title": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM"
                },
                "summary": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Shengyi Ding"
                    },
                    {
                        "name": "Deen Ma"
                    },
                    {
                        "name": "Yixuan Wu"
                    },
                    {
                        "name": "Hongjie Liao"
                    },
                    {
                        "name": "Kaiyuan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyuan Hu"
                },
                "author": "Kaiyuan Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v2",
                "updated": "2024-09-24T16:07:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    7,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tom√°≈° Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16202v2",
                "updated": "2024-09-25T03:35:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    35,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T16:00:28Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    0,
                    28,
                    1,
                    268,
                    0
                ],
                "title": "CJEval: A Benchmark for Assessing Large Language Models Using Chinese\n  Junior High School Exam Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CJEval: A Benchmark for Assessing Large Language Models Using Chinese\n  Junior High School Exam Data"
                },
                "summary": "Online education platforms have significantly transformed the dissemination\nof educational resources by providing a dynamic and digital infrastructure.\nWith the further enhancement of this transformation, the advent of Large\nLanguage Models (LLMs) has elevated the intelligence levels of these platforms.\nHowever, current academic benchmarks provide limited guidance for real-world\nindustry scenarios. This limitation arises because educational applications\nrequire more than mere test question responses. To bridge this gap, we\nintroduce CJEval, a benchmark based on Chinese Junior High School Exam\nEvaluations. CJEval consists of 26,136 samples across four application-level\neducational tasks covering ten subjects. These samples include not only\nquestions and answers but also detailed annotations such as question types,\ndifficulty levels, knowledge concepts, and answer explanations. By utilizing\nthis benchmark, we assessed LLMs' potential applications and conducted a\ncomprehensive analysis of their performance by fine-tuning on various\neducational tasks. Extensive experiments and discussions have highlighted the\nopportunities and challenges of applying LLMs in the field of education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online education platforms have significantly transformed the dissemination\nof educational resources by providing a dynamic and digital infrastructure.\nWith the further enhancement of this transformation, the advent of Large\nLanguage Models (LLMs) has elevated the intelligence levels of these platforms.\nHowever, current academic benchmarks provide limited guidance for real-world\nindustry scenarios. This limitation arises because educational applications\nrequire more than mere test question responses. To bridge this gap, we\nintroduce CJEval, a benchmark based on Chinese Junior High School Exam\nEvaluations. CJEval consists of 26,136 samples across four application-level\neducational tasks covering ten subjects. These samples include not only\nquestions and answers but also detailed annotations such as question types,\ndifficulty levels, knowledge concepts, and answer explanations. By utilizing\nthis benchmark, we assessed LLMs' potential applications and conducted a\ncomprehensive analysis of their performance by fine-tuning on various\neducational tasks. Extensive experiments and discussions have highlighted the\nopportunities and challenges of applying LLMs in the field of education."
                },
                "authors": [
                    {
                        "name": "Qian-Wen Zhang"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Lingfeng Qiao"
                    },
                    {
                        "name": "Liangcai Gao"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16191v1",
                "updated": "2024-09-24T15:38:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    38,
                    11,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:38:11Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    38,
                    11,
                    1,
                    268,
                    0
                ],
                "title": "HelloBench: Evaluating Long Text Generation Capabilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HelloBench: Evaluating Long Text Generation Capabilities of Large\n  Language Models"
                },
                "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in various tasks (e.g., long-context understanding), and many\nbenchmarks have been proposed. However, we observe that long text generation\ncapabilities are not well investigated. Therefore, we introduce the\nHierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,\nin-the-wild, and open-ended benchmark to evaluate LLMs' performance in\ngenerating long text. Based on Bloom's Taxonomy, HelloBench categorizes long\ntext generation tasks into five subtasks: open-ended QA, summarization, chat,\ntext completion, and heuristic text generation. Besides, we propose\nHierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation\nmethod that significantly reduces the time and effort required for human\nevaluation while maintaining a high correlation with human evaluation. We have\nconducted extensive experiments across around 30 mainstream LLMs and observed\nthat the current LLMs lack long text generation capabilities. Specifically,\nfirst, regardless of whether the instructions include explicit or implicit\nlength constraints, we observe that most LLMs cannot generate text that is\nlonger than 4000 words. Second, we observe that while some LLMs can generate\nlonger text, many issues exist (e.g., severe repetition and quality\ndegradation). Third, to demonstrate the effectiveness of HelloEval, we compare\nHelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge\nmethods, which show that HelloEval has the highest correlation with human\nevaluation. We release our code in https://github.com/Quehry/HelloBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in various tasks (e.g., long-context understanding), and many\nbenchmarks have been proposed. However, we observe that long text generation\ncapabilities are not well investigated. Therefore, we introduce the\nHierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,\nin-the-wild, and open-ended benchmark to evaluate LLMs' performance in\ngenerating long text. Based on Bloom's Taxonomy, HelloBench categorizes long\ntext generation tasks into five subtasks: open-ended QA, summarization, chat,\ntext completion, and heuristic text generation. Besides, we propose\nHierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation\nmethod that significantly reduces the time and effort required for human\nevaluation while maintaining a high correlation with human evaluation. We have\nconducted extensive experiments across around 30 mainstream LLMs and observed\nthat the current LLMs lack long text generation capabilities. Specifically,\nfirst, regardless of whether the instructions include explicit or implicit\nlength constraints, we observe that most LLMs cannot generate text that is\nlonger than 4000 words. Second, we observe that while some LLMs can generate\nlonger text, many issues exist (e.g., severe repetition and quality\ndegradation). Third, to demonstrate the effectiveness of HelloEval, we compare\nHelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge\nmethods, which show that HelloEval has the highest correlation with human\nevaluation. We release our code in https://github.com/Quehry/HelloBench."
                },
                "authors": [
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Feiyu Duan"
                    },
                    {
                        "name": "Liqun He"
                    },
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Wenge Rong"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16689v2",
                "updated": "2024-09-24T15:26:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    26,
                    1,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-23T17:54:39Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    54,
                    39,
                    1,
                    205,
                    0
                ],
                "title": "Robust Preference for Dynamical Dark Energy in DESI BAO and SN\n  Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Preference for Dynamical Dark Energy in DESI BAO and SN\n  Measurements"
                },
                "summary": "Recent Baryon Acoustic Oscillation (BAO) measurements released by DESI, when\ncombined with Cosmic Microwave Background (CMB) data from Planck and two\ndifferent samples of Type Ia supernovae (Pantheon-Plus and DESY5) reveal a\npreference for Dynamical Dark Energy (DDE) characterized by a present-day\nquintessence-like equation of state that crossed into the phantom regime in the\npast. A core ansatz for this result is assuming a linear\nChevallier-Polarski-Linder (CPL) parameterization $w(a) = w_0 + w_a (1-a)$ to\ndescribe the evolution of the DE equation of state (EoS). In this paper, we\ntest if and to what extent this assumption impacts the results. To prevent\nbroadening uncertainties in cosmological parameter inference and facilitate\ndirect comparison with the baseline CPL case, we focus on 4 alternative\nwell-known models that, just like CPL, consist of only two free parameters: the\npresent-day DE EoS ($w_0$) and a parameter quantifying its dynamical evolution\n($w_a$). We demonstrate that the preference for DDE remains robust regardless\nof the parameterization: $w_0$ consistently remains in the quintessence regime,\nwhile $w_a$ consistently indicates a preference for a dynamical evolution\ntowards the phantom regime. This tendency is significantly strengthened by\nDESY5 SN measurements. By comparing the best-fit $\\chi^2$ obtained within each\nDDE model, we notice that the linear CPL parameterization is not the\nbest-fitting case. Among the models considered, the EoS proposed by Barboza and\nAlcaniz consistently leads to the most significant improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Baryon Acoustic Oscillation (BAO) measurements released by DESI, when\ncombined with Cosmic Microwave Background (CMB) data from Planck and two\ndifferent samples of Type Ia supernovae (Pantheon-Plus and DESY5) reveal a\npreference for Dynamical Dark Energy (DDE) characterized by a present-day\nquintessence-like equation of state that crossed into the phantom regime in the\npast. A core ansatz for this result is assuming a linear\nChevallier-Polarski-Linder (CPL) parameterization $w(a) = w_0 + w_a (1-a)$ to\ndescribe the evolution of the DE equation of state (EoS). In this paper, we\ntest if and to what extent this assumption impacts the results. To prevent\nbroadening uncertainties in cosmological parameter inference and facilitate\ndirect comparison with the baseline CPL case, we focus on 4 alternative\nwell-known models that, just like CPL, consist of only two free parameters: the\npresent-day DE EoS ($w_0$) and a parameter quantifying its dynamical evolution\n($w_a$). We demonstrate that the preference for DDE remains robust regardless\nof the parameterization: $w_0$ consistently remains in the quintessence regime,\nwhile $w_a$ consistently indicates a preference for a dynamical evolution\ntowards the phantom regime. This tendency is significantly strengthened by\nDESY5 SN measurements. By comparing the best-fit $\\chi^2$ obtained within each\nDDE model, we notice that the linear CPL parameterization is not the\nbest-fitting case. Among the models considered, the EoS proposed by Barboza and\nAlcaniz consistently leads to the most significant improvement."
                },
                "authors": [
                    {
                        "name": "William Giar√®"
                    },
                    {
                        "name": "Mahdi Najafi"
                    },
                    {
                        "name": "Supriya Pan"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    },
                    {
                        "name": "Javad T. Firouzjaee"
                    }
                ],
                "author_detail": {
                    "name": "Javad T. Firouzjaee"
                },
                "author": "Javad T. Firouzjaee",
                "arxiv_comment": "44 pages, 7 figures, 7 tables. V2: additional references, more\n  details and discussion on the results and their interpretation, added a new\n  Appendix presenting additional constraints on interesting properties of the\n  DE EoS across the different models analyzed. Version accepted for publication\n  in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16178v1",
                "updated": "2024-09-24T15:22:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    22,
                    4,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:22:04Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    22,
                    4,
                    1,
                    268,
                    0
                ],
                "title": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single\n  Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single\n  Image"
                },
                "summary": "We focus on recovering 3D object pose and shape from single images. This is\nhighly challenging due to strong (self-)occlusions, depth ambiguities, the\nenormous shape variance, and lack of 3D ground truth for natural images. Recent\nwork relies mostly on learning from finite datasets, so it struggles\ngeneralizing, while it focuses mostly on the shape itself, largely ignoring the\nalignment with pixels. Moreover, it performs feed-forward inference, so it\ncannot refine estimates. We tackle these limitations with a novel framework,\ncalled SDFit. To this end, we make three key observations: (1) Learned\nsigned-distance-function (SDF) models act as a strong morphable shape prior.\n(2) Foundational models embed 2D images and 3D shapes in a joint space, and (3)\nalso infer rich features from images. SDFit exploits these as follows. First,\nit uses a category-level morphable SDF (mSDF) model, called DIT, to generate 3D\nshape hypotheses. This mSDF is initialized by querying OpenShape's latent space\nconditioned on the input image. Then, it computes 2D-to-3D correspondences, by\nextracting and matching features from the image and mSDF. Last, it fits the\nmSDF to the image in an render-and-compare fashion, to iteratively refine\nestimates. We evaluate SDFit on the Pix3D and Pascal3D+ datasets of real-world\nimages. SDFit performs roughly on par with state-of-the-art learned methods,\nbut, uniquely, requires no re-training. Thus, SDFit is promising for\ngeneralizing in the wild, paving the way for future research. Code will be\nreleased",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We focus on recovering 3D object pose and shape from single images. This is\nhighly challenging due to strong (self-)occlusions, depth ambiguities, the\nenormous shape variance, and lack of 3D ground truth for natural images. Recent\nwork relies mostly on learning from finite datasets, so it struggles\ngeneralizing, while it focuses mostly on the shape itself, largely ignoring the\nalignment with pixels. Moreover, it performs feed-forward inference, so it\ncannot refine estimates. We tackle these limitations with a novel framework,\ncalled SDFit. To this end, we make three key observations: (1) Learned\nsigned-distance-function (SDF) models act as a strong morphable shape prior.\n(2) Foundational models embed 2D images and 3D shapes in a joint space, and (3)\nalso infer rich features from images. SDFit exploits these as follows. First,\nit uses a category-level morphable SDF (mSDF) model, called DIT, to generate 3D\nshape hypotheses. This mSDF is initialized by querying OpenShape's latent space\nconditioned on the input image. Then, it computes 2D-to-3D correspondences, by\nextracting and matching features from the image and mSDF. Last, it fits the\nmSDF to the image in an render-and-compare fashion, to iteratively refine\nestimates. We evaluate SDFit on the Pix3D and Pascal3D+ datasets of real-world\nimages. SDFit performs roughly on par with state-of-the-art learned methods,\nbut, uniquely, requires no re-training. Thus, SDFit is promising for\ngeneralizing in the wild, paving the way for future research. Code will be\nreleased"
                },
                "authors": [
                    {
                        "name": "Dimitrije Antiƒá"
                    },
                    {
                        "name": "Sai Kumar Dwivedi"
                    },
                    {
                        "name": "Shashank Tripathi"
                    },
                    {
                        "name": "Theo Gevers"
                    },
                    {
                        "name": "Dimitrios Tzionas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Tzionas"
                },
                "author": "Dimitrios Tzionas",
                "arxiv_comment": "11 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16176v1",
                "updated": "2024-09-24T15:20:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    20,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:20:39Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    20,
                    39,
                    1,
                    268,
                    0
                ],
                "title": "Cyber Knowledge Completion Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber Knowledge Completion Using Large Language Models"
                },
                "summary": "The integration of the Internet of Things (IoT) into Cyber-Physical Systems\n(CPSs) has expanded their cyber-attack surface, introducing new and\nsophisticated threats with potential to exploit emerging vulnerabilities.\nAssessing the risks of CPSs is increasingly difficult due to incomplete and\noutdated cybersecurity knowledge. This highlights the urgent need for\nbetter-informed risk assessments and mitigation strategies. While previous\nefforts have relied on rule-based natural language processing (NLP) tools to\nmap vulnerabilities, weaknesses, and attack patterns, recent advancements in\nLarge Language Models (LLMs) present a unique opportunity to enhance\ncyber-attack knowledge completion through improved reasoning, inference, and\nsummarization capabilities. We apply embedding models to encapsulate\ninformation on attack patterns and adversarial techniques, generating mappings\nbetween them using vector embeddings. Additionally, we propose a\nRetrieval-Augmented Generation (RAG)-based approach that leverages pre-trained\nmodels to create structured mappings between different taxonomies of threat\npatterns. Further, we use a small hand-labeled dataset to compare the proposed\nRAG-based approach to a baseline standard binary classification model. Thus,\nthe proposed approach provides a comprehensive framework to address the\nchallenge of cyber-attack knowledge graph completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of the Internet of Things (IoT) into Cyber-Physical Systems\n(CPSs) has expanded their cyber-attack surface, introducing new and\nsophisticated threats with potential to exploit emerging vulnerabilities.\nAssessing the risks of CPSs is increasingly difficult due to incomplete and\noutdated cybersecurity knowledge. This highlights the urgent need for\nbetter-informed risk assessments and mitigation strategies. While previous\nefforts have relied on rule-based natural language processing (NLP) tools to\nmap vulnerabilities, weaknesses, and attack patterns, recent advancements in\nLarge Language Models (LLMs) present a unique opportunity to enhance\ncyber-attack knowledge completion through improved reasoning, inference, and\nsummarization capabilities. We apply embedding models to encapsulate\ninformation on attack patterns and adversarial techniques, generating mappings\nbetween them using vector embeddings. Additionally, we propose a\nRetrieval-Augmented Generation (RAG)-based approach that leverages pre-trained\nmodels to create structured mappings between different taxonomies of threat\npatterns. Further, we use a small hand-labeled dataset to compare the proposed\nRAG-based approach to a baseline standard binary classification model. Thus,\nthe proposed approach provides a comprehensive framework to address the\nchallenge of cyber-attack knowledge graph completion."
                },
                "authors": [
                    {
                        "name": "Braden K Webb"
                    },
                    {
                        "name": "Sumit Purohit"
                    },
                    {
                        "name": "Rounak Meyur"
                    }
                ],
                "author_detail": {
                    "name": "Rounak Meyur"
                },
                "author": "Rounak Meyur",
                "arxiv_comment": "7 pages, 2 figures. Submitted to 2024 IEEE International Conference\n  on Big Data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16167v1",
                "updated": "2024-09-24T15:08:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    41,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:08:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging."
                },
                "authors": [
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Didi Zhu"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Xuwu Wang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13107v2",
                "updated": "2024-09-24T15:08:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    3,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-19T22:24:46Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    22,
                    24,
                    46,
                    3,
                    263,
                    0
                ],
                "title": "Towards Robust Automation of Surgical Systems via Digital Twin-based\n  Scene Representations from Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Automation of Surgical Systems via Digital Twin-based\n  Scene Representations from Foundation Models"
                },
                "summary": "Large language model-based (LLM) agents are emerging as a powerful enabler of\nrobust embodied intelligence due to their capability of planning complex action\nsequences. Sound planning ability is necessary for robust automation in many\ntask domains, but especially in surgical automation. These agents rely on a\nhighly detailed natural language representation of the scene. Thus, to leverage\nthe emergent capabilities of LLM agents for surgical task planning, developing\nsimilarly powerful and robust perception algorithms is necessary to derive a\ndetailed scene representation of the environment from visual input. Previous\nresearch has focused primarily on enabling LLM-based task planning while\nadopting simple yet severely limited perception solutions to meet the needs for\nbench-top experiments but lack the critical flexibility to scale to less\nconstrained settings. In this work, we propose an alternate perception approach\n-- a digital twin-based machine perception approach that capitalizes on the\nconvincing performance and out-of-the-box generalization of recent vision\nfoundation models. Integrating our digital twin-based scene representation and\nLLM agent for planning with the dVRK platform, we develop an embodied\nintelligence system and evaluate its robustness in performing peg transfer and\ngauze retrieval tasks. Our approach shows strong task performance and\ngeneralizability to varied environment settings. Despite convincing\nperformance, this work is merely a first step towards the integration of\ndigital twin-based scene representations. Future studies are necessary for the\nrealization of a comprehensive digital twin framework to improve the\ninterpretability and generalizability of embodied intelligence in surgery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based (LLM) agents are emerging as a powerful enabler of\nrobust embodied intelligence due to their capability of planning complex action\nsequences. Sound planning ability is necessary for robust automation in many\ntask domains, but especially in surgical automation. These agents rely on a\nhighly detailed natural language representation of the scene. Thus, to leverage\nthe emergent capabilities of LLM agents for surgical task planning, developing\nsimilarly powerful and robust perception algorithms is necessary to derive a\ndetailed scene representation of the environment from visual input. Previous\nresearch has focused primarily on enabling LLM-based task planning while\nadopting simple yet severely limited perception solutions to meet the needs for\nbench-top experiments but lack the critical flexibility to scale to less\nconstrained settings. In this work, we propose an alternate perception approach\n-- a digital twin-based machine perception approach that capitalizes on the\nconvincing performance and out-of-the-box generalization of recent vision\nfoundation models. Integrating our digital twin-based scene representation and\nLLM agent for planning with the dVRK platform, we develop an embodied\nintelligence system and evaluate its robustness in performing peg transfer and\ngauze retrieval tasks. Our approach shows strong task performance and\ngeneralizability to varied environment settings. Despite convincing\nperformance, this work is merely a first step towards the integration of\ndigital twin-based scene representations. Future studies are necessary for the\nrealization of a comprehensive digital twin framework to improve the\ninterpretability and generalizability of embodied intelligence in surgery."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Lalithkumar Seenivasan"
                    },
                    {
                        "name": "Hongchao Shu"
                    },
                    {
                        "name": "Grayson Byrd"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Pu Xiao"
                    },
                    {
                        "name": "Juan Antonio Barragan"
                    },
                    {
                        "name": "Russell H. Taylor"
                    },
                    {
                        "name": "Peter Kazanzides"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16354v2",
                "updated": "2024-09-24T15:07:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    7,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-25T01:12:57Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    1,
                    12,
                    57,
                    0,
                    85,
                    0
                ],
                "title": "ChatDBG: An AI-Powered Debugging Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatDBG: An AI-Powered Debugging Assistant"
                },
                "summary": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like `why is x null?'. To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded\nroughly 50,000 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like `why is x null?'. To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded\nroughly 50,000 times."
                },
                "authors": [
                    {
                        "name": "Kyla Levin"
                    },
                    {
                        "name": "Nicolas van Kempen"
                    },
                    {
                        "name": "Emery D. Berger"
                    },
                    {
                        "name": "Stephen N. Freund"
                    }
                ],
                "author_detail": {
                    "name": "Stephen N. Freund"
                },
                "author": "Stephen N. Freund",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15513v2",
                "updated": "2024-09-24T14:59:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    59,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-05-24T12:57:29Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    12,
                    57,
                    29,
                    4,
                    145,
                    0
                ],
                "title": "Seismic fragility curves fitting revisited: ordinal regression models\n  and their generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seismic fragility curves fitting revisited: ordinal regression models\n  and their generalization"
                },
                "summary": "This study revisits the modeling of seismic fragility curves by applying\nordinal regression models, offering an alternative to the commonly used\nlog-normal distribution function. It compares various ordinal regression\napproaches, including Cumulative, Sequential, and Adjacent Category models,\nalong with extensions that account for category-specific effects and variance\nheterogeneity. The methodologies are applied to bridge damage data from the\n2008 Wenchuan earthquake, using both frequentist and Bayesian inference\nmethods, with model diagnostics conducted using surrogate residuals. The\nanalysis examines eleven models, from basic forms to those incorporating\nheteroscedastic extensions and category-specific effects. Based on\nleave-one-out cross-validation, the Sequential model with category-specific\neffects performs well compared to traditional Cumulative probit models. The\nresults indicate differences in damage probability predictions between the\nmodels, suggesting the potential for more flexible fragility curve modeling\ntechniques to improve seismic risk assessments. This study highlights the\nimportance of continued evaluation of existing methods to enhance the\npredictive accuracy and applicability of seismic fragility models in\nperformance-based earthquake engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study revisits the modeling of seismic fragility curves by applying\nordinal regression models, offering an alternative to the commonly used\nlog-normal distribution function. It compares various ordinal regression\napproaches, including Cumulative, Sequential, and Adjacent Category models,\nalong with extensions that account for category-specific effects and variance\nheterogeneity. The methodologies are applied to bridge damage data from the\n2008 Wenchuan earthquake, using both frequentist and Bayesian inference\nmethods, with model diagnostics conducted using surrogate residuals. The\nanalysis examines eleven models, from basic forms to those incorporating\nheteroscedastic extensions and category-specific effects. Based on\nleave-one-out cross-validation, the Sequential model with category-specific\neffects performs well compared to traditional Cumulative probit models. The\nresults indicate differences in damage probability predictions between the\nmodels, suggesting the potential for more flexible fragility curve modeling\ntechniques to improve seismic risk assessments. This study highlights the\nimportance of continued evaluation of existing methods to enhance the\npredictive accuracy and applicability of seismic fragility models in\nperformance-based earthquake engineering."
                },
                "authors": [
                    {
                        "name": "Libo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Libo Chen"
                },
                "author": "Libo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v2",
                "updated": "2024-09-24T14:59:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    59,
                    30,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof tokens that exceed the maximum context length of LLMs, and they experience\nreduced visual clarity due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and introduce Visual Context\nLatent Summarization, which condenses visual contexts into highly compact\nforms. Extensive experiments demonstrate that our model achieves promising\nresults on popular long video understanding benchmarks, despite being trained\non limited image data. Moreover, Video-XL strikes a promising balance between\nefficiency and effectiveness, processing 1024 frames on a single 80GB GPU while\nachieving nearly 100\\% accuracy in the Needle-in-a-Haystack evaluation. We\nenvision Video-XL becoming a valuable tool for long video applications such as\nvideo summarization, surveillance anomaly detection, and Ad placement\nidentification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof tokens that exceed the maximum context length of LLMs, and they experience\nreduced visual clarity due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and introduce Visual Context\nLatent Summarization, which condenses visual contexts into highly compact\nforms. Extensive experiments demonstrate that our model achieves promising\nresults on popular long video understanding benchmarks, despite being trained\non limited image data. Moreover, Video-XL strikes a promising balance between\nefficiency and effectiveness, processing 1024 frames on a single 80GB GPU while\nachieving nearly 100\\% accuracy in the Needle-in-a-Haystack evaluation. We\nenvision Video-XL becoming a valuable tool for long video applications such as\nvideo summarization, surveillance anomaly detection, and Ad placement\nidentification."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16154v2",
                "updated": "2024-09-25T09:00:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    0,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T14:58:27Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    58,
                    27,
                    1,
                    268,
                    0
                ],
                "title": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed"
                },
                "summary": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources."
                },
                "authors": [
                    {
                        "name": "Alexander Prutsch"
                    },
                    {
                        "name": "Horst Bischof"
                    },
                    {
                        "name": "Horst Possegger"
                    }
                ],
                "author_detail": {
                    "name": "Horst Possegger"
                },
                "author": "Horst Possegger",
                "arxiv_comment": "Accepted to IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16136v1",
                "updated": "2024-09-24T14:43:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    43,
                    14,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:43:14Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    43,
                    14,
                    1,
                    268,
                    0
                ],
                "title": "HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear\n  Composition for Open-Vocabulary Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear\n  Composition for Open-Vocabulary Object Detection"
                },
                "summary": "Open-vocabulary object detection (OVD) models are considered to be Large\nMulti-modal Models (LMM), due to their extensive training data and a large\nnumber of parameters. Mainstream OVD models prioritize object coarse-grained\ncategory rather than focus on their fine-grained attributes, e.g., colors or\nmaterials, thus failed to identify objects specified with certain attributes.\nHowever, OVD models are pretrained on large-scale image-text pairs with rich\nattribute words, whose latent feature space can represent the global text\nfeature as a linear composition of fine-grained attribute tokens without\nhighlighting them. Therefore, we propose in this paper a universal and explicit\napproach for frozen mainstream OVD models that boosts their attribute-level\ndetection capabilities by highlighting fine-grained attributes in explicit\nlinear space. Firstly, a LLM is leveraged to highlight attribute words within\nthe input text as a zero-shot prompted task. Secondly, by strategically\nadjusting the token masks, the text encoders of OVD models extract both global\ntext and attribute-specific features, which are then explicitly composited as\ntwo vectors in linear space to form the new attribute-highlighted feature for\ndetection tasks, where corresponding scalars are hand-crafted or learned to\nreweight both two vectors. Notably, these scalars can be seamlessly transferred\namong different OVD models, which proves that such an explicit linear\ncomposition is universal. Empirical evaluation on the FG-OVD dataset\ndemonstrates that our proposed method uniformly improves fine-grained\nattribute-level OVD of various mainstream models and achieves new\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary object detection (OVD) models are considered to be Large\nMulti-modal Models (LMM), due to their extensive training data and a large\nnumber of parameters. Mainstream OVD models prioritize object coarse-grained\ncategory rather than focus on their fine-grained attributes, e.g., colors or\nmaterials, thus failed to identify objects specified with certain attributes.\nHowever, OVD models are pretrained on large-scale image-text pairs with rich\nattribute words, whose latent feature space can represent the global text\nfeature as a linear composition of fine-grained attribute tokens without\nhighlighting them. Therefore, we propose in this paper a universal and explicit\napproach for frozen mainstream OVD models that boosts their attribute-level\ndetection capabilities by highlighting fine-grained attributes in explicit\nlinear space. Firstly, a LLM is leveraged to highlight attribute words within\nthe input text as a zero-shot prompted task. Secondly, by strategically\nadjusting the token masks, the text encoders of OVD models extract both global\ntext and attribute-specific features, which are then explicitly composited as\ntwo vectors in linear space to form the new attribute-highlighted feature for\ndetection tasks, where corresponding scalars are hand-crafted or learned to\nreweight both two vectors. Notably, these scalars can be seamlessly transferred\namong different OVD models, which proves that such an explicit linear\ncomposition is universal. Empirical evaluation on the FG-OVD dataset\ndemonstrates that our proposed method uniformly improves fine-grained\nattribute-level OVD of various mainstream models and achieves new\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Yuqi Ma"
                    },
                    {
                        "name": "Mengyin Liu"
                    },
                    {
                        "name": "Chao Zhu"
                    },
                    {
                        "name": "Xu-Cheng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xu-Cheng Yin"
                },
                "author": "Xu-Cheng Yin",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18490v2",
                "updated": "2024-09-24T14:32:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    32,
                    6,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-26T16:54:19Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    16,
                    54,
                    19,
                    2,
                    178,
                    0
                ],
                "title": "Parameter selection and optimization of a computational network model of\n  blood flow in single-ventricle patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter selection and optimization of a computational network model of\n  blood flow in single-ventricle patients"
                },
                "summary": "Hypoplastic left heart syndrome (HLHS) is a congenital heart disease\nresponsible for 23% of infant cardiac deaths each year. HLHS patients are born\nwith an underdeveloped left heart, requiring several surgeries to reconstruct\nthe aorta and create a single ventricle circuit known as the Fontan\ncirculation. While survival into early adulthood is becoming more common,\nFontan patients suffer from reduced cardiac output, putting them at risk for a\nmultitude of complications. These patients are monitored using chest and neck\nMRI imaging, but these scans do not capture energy loss, pressure, wave\nintensity, or hemodynamics beyond the imaged region. This study develops a\nframework for predicting these missing features by combining imaging data and\ncomputational fluid dynamics (CFD) models. Predicted features from models of\nHLHS patients are compared to those from control patients with a double outlet\nright ventricle (DORV). We use parameter inference to render the model\npatient-specific. In the calibrated model, we predict pressure, flow,\nwave-intensity (WI), and wall shear stress (WSS). Results reveal that HLHS\npatients have higher vascular stiffness and lower compliance than DORV\npatients, resulting in lower WSS and higher WI in the ascending aorta and\nincreased WSS and decreased WI in the descending aorta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypoplastic left heart syndrome (HLHS) is a congenital heart disease\nresponsible for 23% of infant cardiac deaths each year. HLHS patients are born\nwith an underdeveloped left heart, requiring several surgeries to reconstruct\nthe aorta and create a single ventricle circuit known as the Fontan\ncirculation. While survival into early adulthood is becoming more common,\nFontan patients suffer from reduced cardiac output, putting them at risk for a\nmultitude of complications. These patients are monitored using chest and neck\nMRI imaging, but these scans do not capture energy loss, pressure, wave\nintensity, or hemodynamics beyond the imaged region. This study develops a\nframework for predicting these missing features by combining imaging data and\ncomputational fluid dynamics (CFD) models. Predicted features from models of\nHLHS patients are compared to those from control patients with a double outlet\nright ventricle (DORV). We use parameter inference to render the model\npatient-specific. In the calibrated model, we predict pressure, flow,\nwave-intensity (WI), and wall shear stress (WSS). Results reveal that HLHS\npatients have higher vascular stiffness and lower compliance than DORV\npatients, resulting in lower WSS and higher WI in the ascending aorta and\nincreased WSS and decreased WI in the descending aorta."
                },
                "authors": [
                    {
                        "name": "Alyssa M. Taylor-LaPole"
                    },
                    {
                        "name": "L. Mihaela Paun"
                    },
                    {
                        "name": "Dan Lior"
                    },
                    {
                        "name": "Justin D Weigand"
                    },
                    {
                        "name": "Charles Puelz"
                    },
                    {
                        "name": "Mette S. Olufsen"
                    }
                ],
                "author_detail": {
                    "name": "Mette S. Olufsen"
                },
                "author": "Mette S. Olufsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16120v1",
                "updated": "2024-09-24T14:30:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    30,
                    21,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:30:21Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    30,
                    21,
                    1,
                    268,
                    0
                ],
                "title": "MOSS: Enabling Code-Driven Evolution and Context Management for AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSS: Enabling Code-Driven Evolution and Context Management for AI\n  Agents"
                },
                "summary": "Developing AI agents powered by large language models (LLMs) faces\nsignificant challenges in achieving true Turing completeness and adaptive,\ncode-driven evolution. Current approaches often generate code independently of\nits runtime context, relying heavily on the LLM's memory, which results in\ninefficiencies and limits adaptability. Manual protocol development in sandbox\nenvironments further constrains the agent's autonomous adaptability. Crucially,\nachieving consistency in code and context across multi-turn interactions and\nensuring isolation of local variables within each interaction remains an\nunsolved problem.\n  We introduce MOSS (llM-oriented Operating System Simulation), a novel\nframework that addresses these challenges by integrating code generation with a\ndynamic context management system. MOSS ensures consistency and adaptability by\nusing a mechanism that maintains the Python context across interactions,\nincluding isolation of local variables and preservation of runtime integrity.\nAt its core, the framework employs an Inversion of Control (IoC) container in\nconjunction with decorators to enforce the least knowledge principle, allowing\nagents to focus on abstract interfaces rather than concrete implementations.\nThis facilitates seamless integration of new tools and libraries, enables\nruntime instance replacement, and reduces prompt complexity, providing a \"what\nyou see is what you get\" environment for the agent.\n  Through a series of case studies, we show how this framework can enhance the\nefficiency and capabilities of agent development and highlight its advantages\nin moving towards Turing-complete agents capable of evolving through code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing AI agents powered by large language models (LLMs) faces\nsignificant challenges in achieving true Turing completeness and adaptive,\ncode-driven evolution. Current approaches often generate code independently of\nits runtime context, relying heavily on the LLM's memory, which results in\ninefficiencies and limits adaptability. Manual protocol development in sandbox\nenvironments further constrains the agent's autonomous adaptability. Crucially,\nachieving consistency in code and context across multi-turn interactions and\nensuring isolation of local variables within each interaction remains an\nunsolved problem.\n  We introduce MOSS (llM-oriented Operating System Simulation), a novel\nframework that addresses these challenges by integrating code generation with a\ndynamic context management system. MOSS ensures consistency and adaptability by\nusing a mechanism that maintains the Python context across interactions,\nincluding isolation of local variables and preservation of runtime integrity.\nAt its core, the framework employs an Inversion of Control (IoC) container in\nconjunction with decorators to enforce the least knowledge principle, allowing\nagents to focus on abstract interfaces rather than concrete implementations.\nThis facilitates seamless integration of new tools and libraries, enables\nruntime instance replacement, and reduces prompt complexity, providing a \"what\nyou see is what you get\" environment for the agent.\n  Through a series of case studies, we show how this framework can enhance the\nefficiency and capabilities of agent development and highlight its advantages\nin moving towards Turing-complete agents capable of evolving through code."
                },
                "authors": [
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16221v2",
                "updated": "2024-09-24T14:25:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    25,
                    58,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-23T06:56:54Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    6,
                    56,
                    54,
                    1,
                    205,
                    0
                ],
                "title": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models"
                },
                "summary": "Abstention Ability (AA) is a critical aspect of Large Language Model (LLM)\nreliability, referring to an LLM's capability to withhold responses when\nuncertain or lacking a definitive answer, without compromising performance.\nAlthough previous studies have attempted to improve AA, they lack a\nstandardised evaluation method and remain unsuitable for black-box models where\ntoken prediction probabilities are inaccessible. This makes comparative\nanalysis challenging, especially for state-of-the-art closed-source commercial\nLLMs. This paper bridges this gap by introducing a black-box evaluation\napproach and a new dataset, Abstain-QA, crafted to rigorously assess AA across\nvaried question types (answerable and unanswerable), domains (well-represented\nand under-represented), and task types (fact centric and reasoning). We also\npropose a new confusion matrix, the ''Answerable-Unanswerable Confusion\nMatrix'' (AUCM) which serves as the basis for evaluating AA, by offering a\nstructured and precise approach for assessment. Finally, we explore the impact\nof three prompting strategies-Strict Prompting, Verbal Confidence Thresholding,\nand Chain-of-Thought (CoT)-on improving AA. Our results indicate that even\npowerful models like GPT-4, Mixtral 8x22b encounter difficulties with\nabstention; however, strategic approaches such as Strict prompting and CoT can\nenhance this capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstention Ability (AA) is a critical aspect of Large Language Model (LLM)\nreliability, referring to an LLM's capability to withhold responses when\nuncertain or lacking a definitive answer, without compromising performance.\nAlthough previous studies have attempted to improve AA, they lack a\nstandardised evaluation method and remain unsuitable for black-box models where\ntoken prediction probabilities are inaccessible. This makes comparative\nanalysis challenging, especially for state-of-the-art closed-source commercial\nLLMs. This paper bridges this gap by introducing a black-box evaluation\napproach and a new dataset, Abstain-QA, crafted to rigorously assess AA across\nvaried question types (answerable and unanswerable), domains (well-represented\nand under-represented), and task types (fact centric and reasoning). We also\npropose a new confusion matrix, the ''Answerable-Unanswerable Confusion\nMatrix'' (AUCM) which serves as the basis for evaluating AA, by offering a\nstructured and precise approach for assessment. Finally, we explore the impact\nof three prompting strategies-Strict Prompting, Verbal Confidence Thresholding,\nand Chain-of-Thought (CoT)-on improving AA. Our results indicate that even\npowerful models like GPT-4, Mixtral 8x22b encounter difficulties with\nabstention; however, strategic approaches such as Strict prompting and CoT can\nenhance this capability."
                },
                "authors": [
                    {
                        "name": "Nishanth Madhusudhan"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Masoud Hashemi"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Hashemi"
                },
                "author": "Masoud Hashemi",
                "arxiv_comment": "8 pages (excluding limitations, references and appendix) and 5\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05610v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05610v3",
                "updated": "2024-09-24T14:20:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    20,
                    45,
                    1,
                    268,
                    0
                ],
                "published": "2024-04-08T15:35:03Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    15,
                    35,
                    3,
                    0,
                    99,
                    0
                ],
                "title": "KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI"
                },
                "summary": "The Message-Passing Interface (MPI) and C++ form the backbone of\nhigh-performance computing, but MPI only provides C and Fortran bindings. While\nthis offers great language interoperability, high-level programming languages\nlike C++ make software development quicker and less error-prone.\n  We propose novel C++ language bindings that cover all abstraction levels from\nlow-level MPI calls to convenient STL-style bindings, where most parameters are\ninferred from a small subset of parameters, by bringing named parameters to\nC++. This enables rapid prototyping and fine-tuning runtime behavior and memory\nmanagement. A flexible type system and additional safety guarantees help to\nprevent programming errors.\n  By exploiting C++'s template metaprogramming capabilities, this has (near)\nzero overhead, as only required code paths are generated at compile time.\n  We demonstrate that our library is a strong foundation for a future\ndistributed standard library using multiple application benchmarks, ranging\nfrom text-book sorting algorithms to phylogenetic interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Message-Passing Interface (MPI) and C++ form the backbone of\nhigh-performance computing, but MPI only provides C and Fortran bindings. While\nthis offers great language interoperability, high-level programming languages\nlike C++ make software development quicker and less error-prone.\n  We propose novel C++ language bindings that cover all abstraction levels from\nlow-level MPI calls to convenient STL-style bindings, where most parameters are\ninferred from a small subset of parameters, by bringing named parameters to\nC++. This enables rapid prototyping and fine-tuning runtime behavior and memory\nmanagement. A flexible type system and additional safety guarantees help to\nprevent programming errors.\n  By exploiting C++'s template metaprogramming capabilities, this has (near)\nzero overhead, as only required code paths are generated at compile time.\n  We demonstrate that our library is a strong foundation for a future\ndistributed standard library using multiple application benchmarks, ranging\nfrom text-book sorting algorithms to phylogenetic interference."
                },
                "authors": [
                    {
                        "name": "Tim Niklas Uhl"
                    },
                    {
                        "name": "Matthias Schimek"
                    },
                    {
                        "name": "Lukas H√ºbner"
                    },
                    {
                        "name": "Demian Hespe"
                    },
                    {
                        "name": "Florian Kurpicz"
                    },
                    {
                        "name": "Christoph Stelz"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "To appear at SC24, November 17-22, 2024, Atlanta, Georgia, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05610v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05610v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10137v2",
                "updated": "2024-09-24T14:15:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    15,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-04-15T21:03:38Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    21,
                    3,
                    38,
                    0,
                    106,
                    0
                ],
                "title": "Parameterizations for Large-Scale Variational System Identification\n  Using Unconstrained Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterizations for Large-Scale Variational System Identification\n  Using Unconstrained Optimization"
                },
                "summary": "This paper details how to parameterize the posterior distribution of\nstate-space systems to generate improved optimization problems for system\nidentification using variational inference. Three different parameterizations\nof the assumed state-path posterior distribution are proposed based on this\nrepresentation: time-varying, steady-state, and convolution smoother; each\nresulting in a different parameter estimator. In contrast to existing methods\nfor variational system identification, the proposed estimators can be\nimplemented with unconstrained optimization methods. Furthermore, when applied\nto mini-batches in conjunction with stochastic optimization, the\nconvolution-smoother formulation enables identification of large linear and\nnonlinear state-space systems from very large datasets. For linear systems, the\nmethod achieves the same performance as the inherently sequential\nprediction-error methods using an embarrassingly parallel algorithm that\nbenefits from large speedups when computed in modern graphical processing units\n(GPUs). The ability of the proposed estimators to identify large models, work\nwith large datasets split into mini-batches, and work in parallel on GPUs make\nthem well-suited for identifying deep models for applications in systems and\ncontrol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper details how to parameterize the posterior distribution of\nstate-space systems to generate improved optimization problems for system\nidentification using variational inference. Three different parameterizations\nof the assumed state-path posterior distribution are proposed based on this\nrepresentation: time-varying, steady-state, and convolution smoother; each\nresulting in a different parameter estimator. In contrast to existing methods\nfor variational system identification, the proposed estimators can be\nimplemented with unconstrained optimization methods. Furthermore, when applied\nto mini-batches in conjunction with stochastic optimization, the\nconvolution-smoother formulation enables identification of large linear and\nnonlinear state-space systems from very large datasets. For linear systems, the\nmethod achieves the same performance as the inherently sequential\nprediction-error methods using an embarrassingly parallel algorithm that\nbenefits from large speedups when computed in modern graphical processing units\n(GPUs). The ability of the proposed estimators to identify large models, work\nwith large datasets split into mini-batches, and work in parallel on GPUs make\nthem well-suited for identifying deep models for applications in systems and\ncontrol."
                },
                "authors": [
                    {
                        "name": "Dimas Abreu Archanjo Dutra"
                    }
                ],
                "author_detail": {
                    "name": "Dimas Abreu Archanjo Dutra"
                },
                "author": "Dimas Abreu Archanjo Dutra",
                "arxiv_comment": "Accepted for publication in Automatica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02731v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02731v4",
                "updated": "2024-09-24T14:14:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    14,
                    40,
                    1,
                    268,
                    0
                ],
                "published": "2024-01-05T09:58:09Z",
                "published_parsed": [
                    2024,
                    1,
                    5,
                    9,
                    58,
                    9,
                    4,
                    5,
                    0
                ],
                "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts\n  for Instruction Tuning on General Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts\n  for Instruction Tuning on General Tasks"
                },
                "summary": "Large language models (LLMs) have demonstrated considerable proficiency in\ngeneral natural language processing (NLP) tasks. Instruction tuning, a\nsuccessful paradigm, enhances the ability of LLMs to follow natural language\ninstructions and exhibit robust generalization across general tasks. However,\nthese models often encounter performance limitations across multiple tasks due\nto constrained model capacity. Expanding this capacity during the instruction\ntuning phase poses significant challenges. To address this issue, we introduce\nparameter-efficient sparsity crafting (PESC), which crafts dense models into\nsparse models using the mixture-of-experts (MoE) architecture. PESC integrates\nadapters into the MoE layers of sparse models, differentiating experts without\naltering the individual weights within these layers. This method significantly\nreduces computational costs and GPU memory requirements, facilitating model\ncapacity expansion through a minimal parameter increase when guaranteeing the\nquality of approximation in function space compared to original sparse\nupcycling. Our empirical evaluation demonstrates the effectiveness of the PESC\nmethod. Using PESC during instruction tuning, our best sparse model outperforms\nother sparse and dense models and exhibits superior general capabilities\ncompared to GPT-3.5. Our code is available at\nhttps://github.com/wuhy68/Parameter-Efficient-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated considerable proficiency in\ngeneral natural language processing (NLP) tasks. Instruction tuning, a\nsuccessful paradigm, enhances the ability of LLMs to follow natural language\ninstructions and exhibit robust generalization across general tasks. However,\nthese models often encounter performance limitations across multiple tasks due\nto constrained model capacity. Expanding this capacity during the instruction\ntuning phase poses significant challenges. To address this issue, we introduce\nparameter-efficient sparsity crafting (PESC), which crafts dense models into\nsparse models using the mixture-of-experts (MoE) architecture. PESC integrates\nadapters into the MoE layers of sparse models, differentiating experts without\naltering the individual weights within these layers. This method significantly\nreduces computational costs and GPU memory requirements, facilitating model\ncapacity expansion through a minimal parameter increase when guaranteeing the\nquality of approximation in function space compared to original sparse\nupcycling. Our empirical evaluation demonstrates the effectiveness of the PESC\nmethod. Using PESC during instruction tuning, our best sparse model outperforms\nother sparse and dense models and exhibits superior general capabilities\ncompared to GPT-3.5. Our code is available at\nhttps://github.com/wuhy68/Parameter-Efficient-MoE."
                },
                "authors": [
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Haisheng Zheng"
                    },
                    {
                        "name": "Zhuolun He"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02731v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02731v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02652v2",
                "updated": "2024-09-24T14:10:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    10,
                    13,
                    1,
                    268,
                    0
                ],
                "published": "2023-05-04T08:45:51Z",
                "published_parsed": [
                    2023,
                    5,
                    4,
                    8,
                    45,
                    51,
                    3,
                    124,
                    0
                ],
                "title": "Angular power spectrum of gravitational-wave transient sources as a\n  probe of the large-scale structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Angular power spectrum of gravitational-wave transient sources as a\n  probe of the large-scale structure"
                },
                "summary": "We present a new, simulation-based inference method to compute the angular\npower spectrum of the distribution of foreground gravitational-wave transient\nevents. As a first application of this method, we use the binary black hole\nmergers observed during the LIGO, Virgo, and KAGRA third observation run to\ntest the spatial distribution of these sources. We find no evidence for\nanisotropy in their angular distribution. We discuss further applications of\nthis method to investigate other gravitational-wave source populations and\ntheir correlations to the cosmological large-scale structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new, simulation-based inference method to compute the angular\npower spectrum of the distribution of foreground gravitational-wave transient\nevents. As a first application of this method, we use the binary black hole\nmergers observed during the LIGO, Virgo, and KAGRA third observation run to\ntest the spatial distribution of these sources. We find no evidence for\nanisotropy in their angular distribution. We discuss further applications of\nthis method to investigate other gravitational-wave source populations and\ntheir correlations to the cosmological large-scale structure."
                },
                "authors": [
                    {
                        "name": "Yanyan Zheng"
                    },
                    {
                        "name": "Nikolaos Kouvatsos"
                    },
                    {
                        "name": "Jacob Golomb"
                    },
                    {
                        "name": "Marco Cavagli√†"
                    },
                    {
                        "name": "Arianna I. Renzini"
                    },
                    {
                        "name": "Mairi Sakellariadou"
                    }
                ],
                "author_detail": {
                    "name": "Mairi Sakellariadou"
                },
                "author": "Mairi Sakellariadou",
                "arxiv_doi": "10.1103/PhysRevLett.131.171403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevLett.131.171403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.02652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16106v1",
                "updated": "2024-09-24T14:07:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    7,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:07:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    7,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy\n  Protection in the Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario of Use Scheme: Threat Model Specification for Speaker Privacy\n  Protection in the Medical Domain"
                },
                "summary": "Speech recordings are being more frequently used to detect and monitor\ndisease, leading to privacy concerns. Beyond cryptography, protection of speech\ncan be addressed by approaches, such as perturbation, disentanglement, and\nre-synthesis, that eliminate sensitive information of the speaker, leaving the\ninformation necessary for medical analysis purposes. In order for such privacy\nprotective approaches to be developed, clear and systematic specifications of\nassumptions concerning medical settings and the needs of medical professionals\nare necessary. In this paper, we propose a Scenario of Use Scheme that\nincorporates an Attacker Model, which characterizes the adversary against whom\nthe speaker's privacy must be defended, and a Protector Model, which specifies\nthe defense. We discuss the connection of the scheme with previous work on\nspeech privacy. Finally, we present a concrete example of a specified Scenario\nof Use and a set of experiments about protecting speaker data against gender\ninference attacks while maintaining utility for Parkinson's detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech recordings are being more frequently used to detect and monitor\ndisease, leading to privacy concerns. Beyond cryptography, protection of speech\ncan be addressed by approaches, such as perturbation, disentanglement, and\nre-synthesis, that eliminate sensitive information of the speaker, leaving the\ninformation necessary for medical analysis purposes. In order for such privacy\nprotective approaches to be developed, clear and systematic specifications of\nassumptions concerning medical settings and the needs of medical professionals\nare necessary. In this paper, we propose a Scenario of Use Scheme that\nincorporates an Attacker Model, which characterizes the adversary against whom\nthe speaker's privacy must be defended, and a Protector Model, which specifies\nthe defense. We discuss the connection of the scheme with previous work on\nspeech privacy. Finally, we present a concrete example of a specified Scenario\nof Use and a set of experiments about protecting speaker data against gender\ninference attacks while maintaining utility for Parkinson's detection."
                },
                "authors": [
                    {
                        "name": "Mehtab Ur Rahman"
                    },
                    {
                        "name": "Martha Larson"
                    },
                    {
                        "name": "Louis ten Bosch"
                    },
                    {
                        "name": "Cristian Tejedor-Garc√≠a"
                    }
                ],
                "author_detail": {
                    "name": "Cristian Tejedor-Garc√≠a"
                },
                "author": "Cristian Tejedor-Garc√≠a",
                "arxiv_comment": "Accepted and published at SPSC Symposium 2024 4th Symposium on\n  Security and Privacy in Speech Communication. Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02485v2",
                "updated": "2024-09-24T13:58:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    58,
                    37,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-04T07:23:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    23,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Adversarial Attacks on Machine Learning-Aided Visualizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks on Machine Learning-Aided Visualizations"
                },
                "summary": "Research in ML4VIS investigates how to use machine learning (ML) techniques\nto generate visualizations, and the field is rapidly growing with high societal\nimpact. However, as with any computational pipeline that employs ML processes,\nML4VIS approaches are susceptible to a range of ML-specific adversarial\nattacks. These attacks can manipulate visualization generations, causing\nanalysts to be tricked and their judgments to be impaired. Due to a lack of\nsynthesis from both visualization and ML perspectives, this security aspect is\nlargely overlooked by the current ML4VIS literature. To bridge this gap, we\ninvestigate the potential vulnerabilities of ML-aided visualizations from\nadversarial attacks using a holistic lens of both visualization and ML\nperspectives. We first identify the attack surface (i.e., attack entry points)\nthat is unique in ML-aided visualizations. We then exemplify five different\nadversarial attacks. These examples highlight the range of possible attacks\nwhen considering the attack surface and multiple different adversary\ncapabilities. Our results show that adversaries can induce various attacks,\nsuch as creating arbitrary and deceptive visualizations, by systematically\nidentifying input attributes that are influential in ML inferences. Based on\nour observations of the attack surface characteristics and the attack examples,\nwe underline the importance of comprehensive studies of security issues and\ndefense mechanisms as a call of urgency for the ML4VIS community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in ML4VIS investigates how to use machine learning (ML) techniques\nto generate visualizations, and the field is rapidly growing with high societal\nimpact. However, as with any computational pipeline that employs ML processes,\nML4VIS approaches are susceptible to a range of ML-specific adversarial\nattacks. These attacks can manipulate visualization generations, causing\nanalysts to be tricked and their judgments to be impaired. Due to a lack of\nsynthesis from both visualization and ML perspectives, this security aspect is\nlargely overlooked by the current ML4VIS literature. To bridge this gap, we\ninvestigate the potential vulnerabilities of ML-aided visualizations from\nadversarial attacks using a holistic lens of both visualization and ML\nperspectives. We first identify the attack surface (i.e., attack entry points)\nthat is unique in ML-aided visualizations. We then exemplify five different\nadversarial attacks. These examples highlight the range of possible attacks\nwhen considering the attack surface and multiple different adversary\ncapabilities. Our results show that adversaries can induce various attacks,\nsuch as creating arbitrary and deceptive visualizations, by systematically\nidentifying input attributes that are influential in ML inferences. Based on\nour observations of the attack surface characteristics and the attack examples,\nwe underline the importance of comprehensive studies of security issues and\ndefense mechanisms as a call of urgency for the ML4VIS community."
                },
                "authors": [
                    {
                        "name": "Takanori Fujiwara"
                    },
                    {
                        "name": "Kostiantyn Kucher"
                    },
                    {
                        "name": "Junpeng Wang"
                    },
                    {
                        "name": "Rafael M. Martins"
                    },
                    {
                        "name": "Andreas Kerren"
                    },
                    {
                        "name": "Anders Ynnerman"
                    }
                ],
                "author_detail": {
                    "name": "Anders Ynnerman"
                },
                "author": "Anders Ynnerman",
                "arxiv_doi": "10.1007/s12650-024-01029-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s12650-024-01029-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version of the article has been accepted for publication, after\n  peer review (when applicable) but is not the Version of Record and does not\n  reflect post-acceptance improvements, or any corrections. The Version of\n  Record is available online at: http://dx.doi.org/10.1007/s12650-024-01029-2",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01238v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01238v3",
                "updated": "2024-09-24T13:53:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    53,
                    59,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-03T11:56:07Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    11,
                    56,
                    7,
                    0,
                    155,
                    0
                ],
                "title": "EffiQA: Efficient Question-Answering with Strategic Multi-Model\n  Collaboration on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffiQA: Efficient Question-Answering with Strategic Multi-Model\n  Collaboration on Knowledge Graphs"
                },
                "summary": "While large language models (LLMs) have shown remarkable capabilities in\nnatural language processing, they struggle with complex, multi-step reasoning\ntasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs\nand KGs either underutilize the reasoning abilities of LLMs or suffer from\nprohibitive computational costs due to tight coupling. To address these\nlimitations, we propose a novel collaborative framework named EffiQA that can\nstrike a balance between performance and efficiency via an iterative paradigm.\nEffiQA consists of three stages: global planning, efficient KG exploration, and\nself-reflection. Specifically, EffiQA leverages the commonsense capability of\nLLMs to explore potential reasoning pathways through global planning. Then, it\noffloads semantic pruning to a small plug-in model for efficient KG\nexploration. Finally, the exploration results are fed to LLMs for\nself-reflection to further improve the global planning and efficient KG\nexploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's\neffectiveness, achieving an optimal balance between reasoning accuracy and\ncomputational costs. We hope the proposed new framework will pave the way for\nefficient, knowledge-intensive querying by redefining the integration of LLMs\nand KGs, fostering future research on knowledge-based question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown remarkable capabilities in\nnatural language processing, they struggle with complex, multi-step reasoning\ntasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs\nand KGs either underutilize the reasoning abilities of LLMs or suffer from\nprohibitive computational costs due to tight coupling. To address these\nlimitations, we propose a novel collaborative framework named EffiQA that can\nstrike a balance between performance and efficiency via an iterative paradigm.\nEffiQA consists of three stages: global planning, efficient KG exploration, and\nself-reflection. Specifically, EffiQA leverages the commonsense capability of\nLLMs to explore potential reasoning pathways through global planning. Then, it\noffloads semantic pruning to a small plug-in model for efficient KG\nexploration. Finally, the exploration results are fed to LLMs for\nself-reflection to further improve the global planning and efficient KG\nexploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's\neffectiveness, achieving an optimal balance between reasoning accuracy and\ncomputational costs. We hope the proposed new framework will pave the way for\nefficient, knowledge-intensive querying by redefining the integration of LLMs\nand KGs, fostering future research on knowledge-based question answering."
                },
                "authors": [
                    {
                        "name": "Zixuan Dong"
                    },
                    {
                        "name": "Baoyun Peng"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Jia Fu"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Yongxue Shan"
                    },
                    {
                        "name": "Xin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhou"
                },
                "author": "Xin Zhou",
                "arxiv_comment": "10 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01238v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01238v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01833v2",
                "updated": "2024-09-24T13:51:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    51,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-04-02T10:45:49Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    10,
                    45,
                    49,
                    1,
                    93,
                    0
                ],
                "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack"
                },
                "summary": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models."
                },
                "authors": [
                    {
                        "name": "Mark Russinovich"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Ronen Eldan"
                    }
                ],
                "author_detail": {
                    "name": "Ronen Eldan"
                },
                "author": "Ronen Eldan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16096v1",
                "updated": "2024-09-24T13:50:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    50,
                    32,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T13:50:32Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    50,
                    32,
                    1,
                    268,
                    0
                ],
                "title": "Exploring Hint Generation Approaches in Open-Domain Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Hint Generation Approaches in Open-Domain Question Answering"
                },
                "summary": "Automatic Question Answering (QA) systems rely on contextual information to\nprovide accurate answers. Commonly, contexts are prepared through either\nretrieval-based or generation-based methods. The former involves retrieving\nrelevant documents from a corpus like Wikipedia, whereas the latter uses\ngenerative models such as Large Language Models (LLMs) to generate the context.\nIn this paper, we introduce a novel context preparation approach called HINTQA,\nwhich employs Automatic Hint Generation (HG) techniques. Unlike traditional\nmethods, HINTQA prompts LLMs to produce hints about potential answers for the\nquestion rather than generating relevant context. We evaluate our approach\nacross three QA datasets including TriviaQA, NaturalQuestions, and Web\nQuestions, examining how the number and order of hints impact performance. Our\nfindings show that the HINTQA surpasses both retrieval-based and\ngeneration-based approaches. We demonstrate that hints enhance the accuracy of\nanswers more than retrieved and generated contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Question Answering (QA) systems rely on contextual information to\nprovide accurate answers. Commonly, contexts are prepared through either\nretrieval-based or generation-based methods. The former involves retrieving\nrelevant documents from a corpus like Wikipedia, whereas the latter uses\ngenerative models such as Large Language Models (LLMs) to generate the context.\nIn this paper, we introduce a novel context preparation approach called HINTQA,\nwhich employs Automatic Hint Generation (HG) techniques. Unlike traditional\nmethods, HINTQA prompts LLMs to produce hints about potential answers for the\nquestion rather than generating relevant context. We evaluate our approach\nacross three QA datasets including TriviaQA, NaturalQuestions, and Web\nQuestions, examining how the number and order of hints impact performance. Our\nfindings show that the HINTQA surpasses both retrieval-based and\ngeneration-based approaches. We demonstrate that hints enhance the accuracy of\nanswers more than retrieved and generated contexts."
                },
                "authors": [
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03175v2",
                "updated": "2024-09-24T13:30:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    30,
                    25,
                    1,
                    268,
                    0
                ],
                "published": "2024-02-05T16:42:10Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    16,
                    42,
                    10,
                    0,
                    36,
                    0
                ],
                "title": "Beyond the Black Box: A Statistical Model for LLM Reasoning and\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Black Box: A Statistical Model for LLM Reasoning and\n  Inference"
                },
                "summary": "This paper introduces a novel Bayesian learning model to explain the behavior\nof Large Language Models (LLMs), focusing on their core optimization metric of\nnext token prediction. We develop a theoretical framework based on an ideal\ngenerative text model represented by a multinomial transition probability\nmatrix with a prior, and examine how LLMs approximate this matrix. Key\ncontributions include: (i) a continuity theorem relating embeddings to\nmultinomial distributions, (ii) a demonstration that LLM text generation aligns\nwith Bayesian learning principles, (iii) an explanation for the emergence of\nin-context learning in larger models, (iv) empirical validation using\nvisualizations of next token probabilities from an instrumented Llama model Our\nfindings provide new insights into LLM functioning, offering a statistical\nfoundation for understanding their capabilities and limitations. This framework\nhas implications for LLM design, training, and application, potentially guiding\nfuture developments in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel Bayesian learning model to explain the behavior\nof Large Language Models (LLMs), focusing on their core optimization metric of\nnext token prediction. We develop a theoretical framework based on an ideal\ngenerative text model represented by a multinomial transition probability\nmatrix with a prior, and examine how LLMs approximate this matrix. Key\ncontributions include: (i) a continuity theorem relating embeddings to\nmultinomial distributions, (ii) a demonstration that LLM text generation aligns\nwith Bayesian learning principles, (iii) an explanation for the emergence of\nin-context learning in larger models, (iv) empirical validation using\nvisualizations of next token probabilities from an instrumented Llama model Our\nfindings provide new insights into LLM functioning, offering a statistical\nfoundation for understanding their capabilities and limitations. This framework\nhas implications for LLM design, training, and application, potentially guiding\nfuture developments in the field."
                },
                "authors": [
                    {
                        "name": "Siddhartha Dalal"
                    },
                    {
                        "name": "Vishal Misra"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Misra"
                },
                "author": "Vishal Misra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01121v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01121v3",
                "updated": "2024-09-24T13:26:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    26,
                    8,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-02T08:05:03Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    8,
                    5,
                    3,
                    5,
                    62,
                    0
                ],
                "title": "OpenGraph: Towards Open Graph Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenGraph: Towards Open Graph Foundation Models"
                },
                "summary": "Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains."
                },
                "authors": [
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Ben Kao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted by EMNLP'2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01121v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01121v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15491v2",
                "updated": "2024-09-24T13:25:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    25,
                    1,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-21T15:41:02Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    15,
                    41,
                    2,
                    3,
                    81,
                    0
                ],
                "title": "Open Conversational LLMs do not know most Spanish words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Conversational LLMs do not know most Spanish words"
                },
                "summary": "The growing interest in Large Language Models (LLMs) and in particular in\nconversational models with which users can interact has led to the development\nof a large number of open-source chat LLMs. These models are evaluated on a\nwide range of benchmarks to assess their capabilities in answering questions or\nsolving problems on almost any possible topic or to test their ability to\nreason or interpret texts. Instead, the evaluation of the knowledge that these\nmodels have of the languages has received much less attention. For example, the\nwords that they can recognize and use in different languages. In this paper, we\nevaluate the knowledge that open-source chat LLMs have of Spanish words by\ntesting a sample of words in a reference dictionary. The results show that\nopen-source chat LLMs produce incorrect meanings for an important fraction of\nthe words and are not able to use most of the words correctly to write\nsentences with context. These results show how Spanish is left behind in the\nopen-source LLM race and highlight the need to push for linguistic fairness in\nconversational LLMs ensuring that they provide similar performance across\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in Large Language Models (LLMs) and in particular in\nconversational models with which users can interact has led to the development\nof a large number of open-source chat LLMs. These models are evaluated on a\nwide range of benchmarks to assess their capabilities in answering questions or\nsolving problems on almost any possible topic or to test their ability to\nreason or interpret texts. Instead, the evaluation of the knowledge that these\nmodels have of the languages has received much less attention. For example, the\nwords that they can recognize and use in different languages. In this paper, we\nevaluate the knowledge that open-source chat LLMs have of Spanish words by\ntesting a sample of words in a reference dictionary. The results show that\nopen-source chat LLMs produce incorrect meanings for an important fraction of\nthe words and are not able to use most of the words correctly to write\nsentences with context. These results show how Spanish is left behind in the\nopen-source LLM race and highlight the need to push for linguistic fairness in\nconversational LLMs ensuring that they provide similar performance across\nlanguages."
                },
                "authors": [
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Miguel Gonz√°lez"
                    },
                    {
                        "name": "Nina Melero"
                    },
                    {
                        "name": "Raquel Ferrando"
                    },
                    {
                        "name": "Gonzalo Mart√≠nez"
                    },
                    {
                        "name": "Elena Merino-G√≥mez"
                    },
                    {
                        "name": "Jos√© Alberto Hern√°ndez"
                    },
                    {
                        "name": "Pedro Reviriego"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Reviriego"
                },
                "author": "Pedro Reviriego",
                "arxiv_doi": "10.26342/2024-73-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.26342/2024-73-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.15491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Procesamiento del Lenguaje Natural, 73, 95-108",
                "arxiv_journal_ref": "Procesamiento del Lenguaje Natural, n. 73, 2024.\n  http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6603",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16075v2",
                "updated": "2024-09-25T08:59:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    59,
                    26,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T13:21:21Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    21,
                    21,
                    1,
                    268,
                    0
                ],
                "title": "Ultra-low latency quantum-inspired machine learning predictors\n  implemented on FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-low latency quantum-inspired machine learning predictors\n  implemented on FPGA"
                },
                "summary": "Tensor Networks (TNs) are a computational paradigm used for representing\nquantum many-body systems. Recent works have shown how TNs can also be applied\nto perform Machine Learning (ML) tasks, yielding comparable results to standard\nsupervised learning techniques. In this work, we study the use of Tree Tensor\nNetworks (TTNs) in high-frequency real-time applications by exploiting the\nlow-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We\npresent different implementations of TTN classifiers, capable of performing\ninference on classical ML datasets as well as on complex physics data. A\npreparatory analysis of bond dimensions and weight quantization is realized in\nthe training phase, together with entanglement entropy and correlation\nmeasurements, that help setting the choice of the TTN architecture. The\ngenerated TTNs are then deployed on a hardware accelerator; using an FPGA\nintegrated into a server, the inference of the TTN is completely offloaded.\nEventually, a classifier for High Energy Physics (HEP) applications is\nimplemented and executed fully pipelined with sub-microsecond latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Networks (TNs) are a computational paradigm used for representing\nquantum many-body systems. Recent works have shown how TNs can also be applied\nto perform Machine Learning (ML) tasks, yielding comparable results to standard\nsupervised learning techniques. In this work, we study the use of Tree Tensor\nNetworks (TTNs) in high-frequency real-time applications by exploiting the\nlow-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We\npresent different implementations of TTN classifiers, capable of performing\ninference on classical ML datasets as well as on complex physics data. A\npreparatory analysis of bond dimensions and weight quantization is realized in\nthe training phase, together with entanglement entropy and correlation\nmeasurements, that help setting the choice of the TTN architecture. The\ngenerated TTNs are then deployed on a hardware accelerator; using an FPGA\nintegrated into a server, the inference of the TTN is completely offloaded.\nEventually, a classifier for High Energy Physics (HEP) applications is\nimplemented and executed fully pipelined with sub-microsecond latency."
                },
                "authors": [
                    {
                        "name": "Lorenzo Borella"
                    },
                    {
                        "name": "Alberto Coppi"
                    },
                    {
                        "name": "Jacopo Pazzini"
                    },
                    {
                        "name": "Andrea Stanco"
                    },
                    {
                        "name": "Marco Trenti"
                    },
                    {
                        "name": "Andrea Triossi"
                    },
                    {
                        "name": "Marco Zanetti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Zanetti"
                },
                "author": "Marco Zanetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03935v2",
                "updated": "2024-09-24T13:08:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    8,
                    14,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-05T23:01:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    23,
                    1,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "Galled Perfect Transfer Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galled Perfect Transfer Networks"
                },
                "summary": "Predicting horizontal gene transfers often requires comparative sequence\ndata, but recent work has shown that character-based approaches could also be\nuseful for this task. Notably, perfect transfer networks (PTN) explain the\ncharacter diversity of a set of taxa for traits that are gained once, rarely\nlost, but that can be transferred laterally. Characterizing the structure of\nsuch characters is an important step towards understanding more complex\ncharacters. Although efficient algorithms can infer such networks from\ncharacter data, they can sometimes predict overly complicated transfer\nhistories. With the goal of recovering the simplest possible scenarios in this\nmodel, we introduce galled perfect transfer networks, which are PTNs that are\ngalled trees. Such networks are useful for characters that are incompatible in\nterms of tree-like evolution, but that do fit in an almost-tree scenario. We\nprovide polynomial-time algorithms for two problems: deciding whether one can\nadd transfer edges to a tree to transform it into a galled PTN, and deciding\nwhether a set of characters are galled-compatible, that is, they can be\nexplained by some galled PTN. We also analyze a real dataset comprising of a\nbacterial species trees and KEGG functions as characters, and derive several\nconclusions on the difficulty of explaining characters in a galled tree, which\nprovide several directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting horizontal gene transfers often requires comparative sequence\ndata, but recent work has shown that character-based approaches could also be\nuseful for this task. Notably, perfect transfer networks (PTN) explain the\ncharacter diversity of a set of taxa for traits that are gained once, rarely\nlost, but that can be transferred laterally. Characterizing the structure of\nsuch characters is an important step towards understanding more complex\ncharacters. Although efficient algorithms can infer such networks from\ncharacter data, they can sometimes predict overly complicated transfer\nhistories. With the goal of recovering the simplest possible scenarios in this\nmodel, we introduce galled perfect transfer networks, which are PTNs that are\ngalled trees. Such networks are useful for characters that are incompatible in\nterms of tree-like evolution, but that do fit in an almost-tree scenario. We\nprovide polynomial-time algorithms for two problems: deciding whether one can\nadd transfer edges to a tree to transform it into a galled PTN, and deciding\nwhether a set of characters are galled-compatible, that is, they can be\nexplained by some galled PTN. We also analyze a real dataset comprising of a\nbacterial species trees and KEGG functions as characters, and derive several\nconclusions on the difficulty of explaining characters in a galled tree, which\nprovide several directions for future research."
                },
                "authors": [
                    {
                        "name": "Alitzel L√≥pez S√°nchez"
                    },
                    {
                        "name": "Manuel Lafond"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Lafond"
                },
                "author": "Manuel Lafond",
                "arxiv_comment": "extended article based on previously accepted manuscript at RECOMB-CG\n  2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15188v2",
                "updated": "2024-09-24T13:03:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    3,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T16:39:12Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    16,
                    39,
                    12,
                    0,
                    267,
                    0
                ],
                "title": "PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large\n  Language Models"
                },
                "summary": "Effective patient-provider communication is crucial in clinical care,\ndirectly impacting patient outcomes and quality of life. Traditional evaluation\nmethods, such as human ratings, patient feedback, and provider\nself-assessments, are often limited by high costs and scalability issues.\nAlthough existing natural language processing (NLP) techniques show promise,\nthey struggle with the nuances of clinical communication and require sensitive\nclinical data for training, reducing their effectiveness in real-world\napplications. Emerging large language models (LLMs) offer a new approach to\nassessing complex communication metrics, with the potential to advance the\nfield through integration into passive sensing and just-in-time intervention\nsystems. This study explores LLMs as evaluators of palliative care\ncommunication quality, leveraging their linguistic, in-context learning, and\nreasoning capabilities. Specifically, using simulated scripts crafted and\nlabeled by healthcare professionals, we test proprietary models (e.g., GPT-4)\nand fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset\ngenerated by GPT-4 to evaluate clinical conversations, to identify key metrics\nsuch as `understanding' and `empathy'. Our findings demonstrated LLMs' superior\nperformance in evaluating clinical communication, providing actionable feedback\nwith reasoning, and demonstrating the feasibility and practical viability of\ndeveloping in-house LLMs. This research highlights LLMs' potential to enhance\npatient-provider interactions and lays the groundwork for downstream steps in\ndeveloping LLM-empowered clinical health systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective patient-provider communication is crucial in clinical care,\ndirectly impacting patient outcomes and quality of life. Traditional evaluation\nmethods, such as human ratings, patient feedback, and provider\nself-assessments, are often limited by high costs and scalability issues.\nAlthough existing natural language processing (NLP) techniques show promise,\nthey struggle with the nuances of clinical communication and require sensitive\nclinical data for training, reducing their effectiveness in real-world\napplications. Emerging large language models (LLMs) offer a new approach to\nassessing complex communication metrics, with the potential to advance the\nfield through integration into passive sensing and just-in-time intervention\nsystems. This study explores LLMs as evaluators of palliative care\ncommunication quality, leveraging their linguistic, in-context learning, and\nreasoning capabilities. Specifically, using simulated scripts crafted and\nlabeled by healthcare professionals, we test proprietary models (e.g., GPT-4)\nand fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset\ngenerated by GPT-4 to evaluate clinical conversations, to identify key metrics\nsuch as `understanding' and `empathy'. Our findings demonstrated LLMs' superior\nperformance in evaluating clinical communication, providing actionable feedback\nwith reasoning, and demonstrating the feasibility and practical viability of\ndeveloping in-house LLMs. This research highlights LLMs' potential to enhance\npatient-provider interactions and lays the groundwork for downstream steps in\ndeveloping LLM-empowered clinical health systems."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Fangxu Yuan"
                    },
                    {
                        "name": "Virginia LeBaron"
                    },
                    {
                        "name": "Tabor Flickinger"
                    },
                    {
                        "name": "Laura E. Barnes"
                    }
                ],
                "author_detail": {
                    "name": "Laura E. Barnes"
                },
                "author": "Laura E. Barnes",
                "arxiv_comment": "Accepted by ACM Transactions on Computing for Healthcare, Special\n  Issue on Large Language Models, Conversational Systems, and Generative AI in\n  Health, pending minor revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16061v1",
                "updated": "2024-09-24T13:02:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    2,
                    58,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T13:02:58Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    2,
                    58,
                    1,
                    268,
                    0
                ],
                "title": "JWST Observations of Young protoStars (JOYS). HH 211: the textbook case\n  of a protostellar jet and outflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST Observations of Young protoStars (JOYS). HH 211: the textbook case\n  of a protostellar jet and outflow"
                },
                "summary": "We use the James Webb Space Telescope (JWST) and its Mid-Infrared Instrument\n(MIRI) (5-28 um), to study the embedded HH 211 flow. We map a 0.95'x0.22'\nregion, covering the full extent of the blue-shifted lobe, the central\nprotostellar region, and a small portion of the red-shifted lobe. The jet\ndriving source is not detected even at the longest mid-IR wavelengths. The\noverall morphology of the flow consists of a highly collimated jet, mostly\nmolecular (H2, HD) with an inner atomic ([FeI], [FeII], [SI], [NiII])\nstructure. The jet shocks the ambient medium, producing several large\nbow-shocks, rich in forbidden atomic and molecular lines, and is driving an H2\nmolecular outflow, mostly traced by low-J, v=0 transitions. Moreover, 0-0 S(1)\nuncollimated emission is also detected down to 2\"-3\" (~650-1000 au) from the\nsource, tracing a cold (T=200-400 K), less dense and poorly collimated\nmolecular wind. The atomic jet ([FeII] at 26 um) is detected down to ~130 au\nfrom source, whereas the lack of H2 emission close to the source is likely due\nto the large visual extinction. Dust continuum-emission is detected at the\nterminal bow-shocks, and in the blue- and red-shifted jet, being likely dust\nlifted from the disk. The jet shows an onion-like structure, with layers of\ndifferent size, velocity, temperature, and chemical composition. Moreover,\nmoving from the inner jet to the outer bow-shocks, different physical,\nkinematic and excitation conditions for both molecular and atomic gas are\nobserved. The jet mass-flux rate, momentum, and momentum flux of the warm H2\ncomponent are up to one order of magnitude higher than those inferred from the\natomic jet component. Our findings indicate that the warm H2 component is the\nprimary mover of the outflow, namely it is the most significant dynamical\ncomponent of the jet, in contrast to jets from more evolved YSOs, where the\natomic component is dominant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use the James Webb Space Telescope (JWST) and its Mid-Infrared Instrument\n(MIRI) (5-28 um), to study the embedded HH 211 flow. We map a 0.95'x0.22'\nregion, covering the full extent of the blue-shifted lobe, the central\nprotostellar region, and a small portion of the red-shifted lobe. The jet\ndriving source is not detected even at the longest mid-IR wavelengths. The\noverall morphology of the flow consists of a highly collimated jet, mostly\nmolecular (H2, HD) with an inner atomic ([FeI], [FeII], [SI], [NiII])\nstructure. The jet shocks the ambient medium, producing several large\nbow-shocks, rich in forbidden atomic and molecular lines, and is driving an H2\nmolecular outflow, mostly traced by low-J, v=0 transitions. Moreover, 0-0 S(1)\nuncollimated emission is also detected down to 2\"-3\" (~650-1000 au) from the\nsource, tracing a cold (T=200-400 K), less dense and poorly collimated\nmolecular wind. The atomic jet ([FeII] at 26 um) is detected down to ~130 au\nfrom source, whereas the lack of H2 emission close to the source is likely due\nto the large visual extinction. Dust continuum-emission is detected at the\nterminal bow-shocks, and in the blue- and red-shifted jet, being likely dust\nlifted from the disk. The jet shows an onion-like structure, with layers of\ndifferent size, velocity, temperature, and chemical composition. Moreover,\nmoving from the inner jet to the outer bow-shocks, different physical,\nkinematic and excitation conditions for both molecular and atomic gas are\nobserved. The jet mass-flux rate, momentum, and momentum flux of the warm H2\ncomponent are up to one order of magnitude higher than those inferred from the\natomic jet component. Our findings indicate that the warm H2 component is the\nprimary mover of the outflow, namely it is the most significant dynamical\ncomponent of the jet, in contrast to jets from more evolved YSOs, where the\natomic component is dominant."
                },
                "authors": [
                    {
                        "name": "A. Caratti o Garatti"
                    },
                    {
                        "name": "T. P. Ray"
                    },
                    {
                        "name": "P. J. Kavanagh"
                    },
                    {
                        "name": "M. J. McCaughrean"
                    },
                    {
                        "name": "C. Gieser"
                    },
                    {
                        "name": "T. Giannini"
                    },
                    {
                        "name": "E. F. van Dishoeck"
                    },
                    {
                        "name": "K. Justtanont"
                    },
                    {
                        "name": "M. L. van Gelder"
                    },
                    {
                        "name": "L. Francis"
                    },
                    {
                        "name": "H. Beuther"
                    },
                    {
                        "name": "≈Å. Tychoniec"
                    },
                    {
                        "name": "B. Nisini"
                    },
                    {
                        "name": "M. G. Navarro"
                    },
                    {
                        "name": "R. Devaraj"
                    },
                    {
                        "name": "S. Reyes"
                    },
                    {
                        "name": "P. Nazar"
                    },
                    {
                        "name": "P. Klaassen"
                    },
                    {
                        "name": "M. G√ºdel"
                    },
                    {
                        "name": "Th. Henning"
                    },
                    {
                        "name": "P. O. Lagage"
                    },
                    {
                        "name": "G. √ñstlin"
                    },
                    {
                        "name": "B. Vandenbussche"
                    },
                    {
                        "name": "C. Waelkens"
                    },
                    {
                        "name": "G. Wright"
                    }
                ],
                "author_detail": {
                    "name": "G. Wright"
                },
                "author": "G. Wright",
                "arxiv_comment": "Paper accepted in A&A for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07181v2",
                "updated": "2024-09-24T12:50:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    50,
                    17,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-11T10:50:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    50,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "A new analytic approach to infer the cosmic-ray ionization rate in hot\n  molecular cores from HCO$^+$, N$_2$H$^+$, and CO observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new analytic approach to infer the cosmic-ray ionization rate in hot\n  molecular cores from HCO$^+$, N$_2$H$^+$, and CO observations"
                },
                "summary": "The cosmic-ray ionization rate ($\\zeta_2$) is one of the key parameters in\nstar formation, since it regulates the chemical and dynamical evolution of\nmolecular clouds by ionizing molecules and determining the coupling between the\nmagnetic field and gas. However, measurements of $\\zeta_2$ in dense clouds\n(e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$) are difficult and sensitive to the\nmodel assumptions. The aim is to find a convenient analytic approach that can\nbe used in high-mass star-forming regions (HMSFRs), especially for warm gas\nenvironments such as hot molecular cores (HMCs). We propose a new analytic\napproach to calculate $\\zeta_2$ through HCO$^+$, N$_2$H$^+$, and CO\nmeasurements. Our method gives a good approximation, to within $50$\\%, of\n$\\zeta_2$ in dense and warm gas (e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$, $T =\n50, 100$ K) for $A_{\\rm V} \\geq 4$ mag and $t \\geq 2\\times10^4$ yr at Solar\nmetallicity. The analytic approach gives better results for higher densities.\nHowever, it starts to underestimate the CRIR at low metallicity ($Z =\n0.1Z_\\odot$) and high CRIR ($\\zeta_2 \\geq 3\\times10^{-15}$ s$^{-1}$). By\napplying our method to the OMC-2 FIR4 envelope and the L1157-B1 shock region,\nwe find $\\zeta_2$ values of $(1.0\\pm0.3)\\times10^{-14}$ s$^{-1}$ and\n$(2.2\\pm0.4)\\times10^{-16}$ s$^{-1}$, consistent with those previously\nreported. We calculate $\\zeta_2$ toward a total of 82 samples in HMSFRs,\nfinding that the average value of $\\zeta_2$ toward all HMC samples ($\\zeta_2$ =\n(7.4$\\pm$5.0)$\\times$10$^{-16}$ s$^{-1}$) is more than an order of magnitude\nhigher than the theoretical prediction of cosmic-ray attenuation models,\nfavoring the scenario that locally accelerated cosmic rays in embedded\nprotostars should be responsible for the observed high $\\zeta_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic-ray ionization rate ($\\zeta_2$) is one of the key parameters in\nstar formation, since it regulates the chemical and dynamical evolution of\nmolecular clouds by ionizing molecules and determining the coupling between the\nmagnetic field and gas. However, measurements of $\\zeta_2$ in dense clouds\n(e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$) are difficult and sensitive to the\nmodel assumptions. The aim is to find a convenient analytic approach that can\nbe used in high-mass star-forming regions (HMSFRs), especially for warm gas\nenvironments such as hot molecular cores (HMCs). We propose a new analytic\napproach to calculate $\\zeta_2$ through HCO$^+$, N$_2$H$^+$, and CO\nmeasurements. Our method gives a good approximation, to within $50$\\%, of\n$\\zeta_2$ in dense and warm gas (e.g., $n_{\\rm H} \\geq 10^4$ cm$^{-3}$, $T =\n50, 100$ K) for $A_{\\rm V} \\geq 4$ mag and $t \\geq 2\\times10^4$ yr at Solar\nmetallicity. The analytic approach gives better results for higher densities.\nHowever, it starts to underestimate the CRIR at low metallicity ($Z =\n0.1Z_\\odot$) and high CRIR ($\\zeta_2 \\geq 3\\times10^{-15}$ s$^{-1}$). By\napplying our method to the OMC-2 FIR4 envelope and the L1157-B1 shock region,\nwe find $\\zeta_2$ values of $(1.0\\pm0.3)\\times10^{-14}$ s$^{-1}$ and\n$(2.2\\pm0.4)\\times10^{-16}$ s$^{-1}$, consistent with those previously\nreported. We calculate $\\zeta_2$ toward a total of 82 samples in HMSFRs,\nfinding that the average value of $\\zeta_2$ toward all HMC samples ($\\zeta_2$ =\n(7.4$\\pm$5.0)$\\times$10$^{-16}$ s$^{-1}$) is more than an order of magnitude\nhigher than the theoretical prediction of cosmic-ray attenuation models,\nfavoring the scenario that locally accelerated cosmic rays in embedded\nprotostars should be responsible for the observed high $\\zeta_2$."
                },
                "authors": [
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "Thomas G. Bisbas"
                    },
                    {
                        "name": "Marco Padovani"
                    },
                    {
                        "name": "Brandt A. L. Gaches"
                    }
                ],
                "author_detail": {
                    "name": "Brandt A. L. Gaches"
                },
                "author": "Brandt A. L. Gaches",
                "arxiv_comment": "14 pages, 11 figures, accepted by A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16040v1",
                "updated": "2024-09-24T12:42:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    42,
                    18,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:42:18Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    42,
                    18,
                    1,
                    268,
                    0
                ],
                "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of\n  Experts"
                },
                "summary": "Deep learning for time series forecasting has seen significant advancements\nover the past decades. However, despite the success of large-scale pre-training\nin language and vision domains, pre-trained time series models remain limited\nin scale and operate at a high cost, hindering the development of larger\ncapable forecasting models in real-world applications. In response, we\nintroduce Time-MoE, a scalable and unified architecture designed to pre-train\nlarger, more capable forecasting foundation models while reducing inference\ncosts. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE\nenhances computational efficiency by activating only a subset of networks for\neach prediction, reducing computational load while maintaining high model\ncapacity. This allows Time-MoE to scale effectively without a corresponding\nincrease in inference costs. Time-MoE comprises a family of decoder-only\ntransformer models that operate in an auto-regressive manner and support\nflexible forecasting horizons with varying input context lengths. We\npre-trained these models on our newly introduced large-scale data Time-300B,\nwhich spans over 9 domains and encompassing over 300 billion time points. For\nthe first time, we scaled a time series foundation model up to 2.4 billion\nparameters, achieving significantly improved forecasting precision. Our results\nvalidate the applicability of scaling laws for training tokens and model size\nin the context of time series forecasting. Compared to dense models with the\nsame number of activated parameters or equivalent computation budgets, our\nmodels consistently outperform them by large margin. These advancements\nposition Time-MoE as a state-of-the-art solution for tackling real-world time\nseries forecasting challenges with superior capability, efficiency, and\nflexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning for time series forecasting has seen significant advancements\nover the past decades. However, despite the success of large-scale pre-training\nin language and vision domains, pre-trained time series models remain limited\nin scale and operate at a high cost, hindering the development of larger\ncapable forecasting models in real-world applications. In response, we\nintroduce Time-MoE, a scalable and unified architecture designed to pre-train\nlarger, more capable forecasting foundation models while reducing inference\ncosts. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE\nenhances computational efficiency by activating only a subset of networks for\neach prediction, reducing computational load while maintaining high model\ncapacity. This allows Time-MoE to scale effectively without a corresponding\nincrease in inference costs. Time-MoE comprises a family of decoder-only\ntransformer models that operate in an auto-regressive manner and support\nflexible forecasting horizons with varying input context lengths. We\npre-trained these models on our newly introduced large-scale data Time-300B,\nwhich spans over 9 domains and encompassing over 300 billion time points. For\nthe first time, we scaled a time series foundation model up to 2.4 billion\nparameters, achieving significantly improved forecasting precision. Our results\nvalidate the applicability of scaling laws for training tokens and model size\nin the context of time series forecasting. Compared to dense models with the\nsame number of activated parameters or equivalent computation budgets, our\nmodels consistently outperform them by large margin. These advancements\nposition Time-MoE as a state-of-the-art solution for tackling real-world time\nseries forecasting challenges with superior capability, efficiency, and\nflexibility."
                },
                "authors": [
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Yuqi Nie"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Zhou Ye"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Ming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jin"
                },
                "author": "Ming Jin",
                "arxiv_comment": "29 pages, 10 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00628v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00628v4",
                "updated": "2024-09-24T12:40:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    40,
                    19,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-01T16:03:37Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    16,
                    3,
                    37,
                    4,
                    61,
                    0
                ],
                "title": "Region-Adaptive Transform with Segmentation Prior for Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Transform with Segmentation Prior for Image Compression"
                },
                "summary": "Learned Image Compression (LIC) has shown remarkable progress in recent\nyears. Existing works commonly employ CNN-based or self-attention-based modules\nas transform methods for compression. However, there is no prior research on\nneural transform that focuses on specific regions. In response, we introduce\nthe class-agnostic segmentation masks (i.e. semantic masks without category\nlabels) for extracting region-adaptive contextual information. Our proposed\nmodule, Region-Adaptive Transform, applies adaptive convolutions on different\nregions guided by the masks. Additionally, we introduce a plug-and-play module\nnamed Scale Affine Layer to incorporate rich contexts from various regions.\nWhile there have been prior image compression efforts that involve segmentation\nmasks as additional intermediate inputs, our approach differs significantly\nfrom them. Our advantages lie in that, to avoid extra bitrate overhead, we\ntreat these masks as privilege information, which is accessible during the\nmodel training stage but not required during the inference phase. To the best\nof our knowledge, we are the first to employ class-agnostic masks as privilege\ninformation and achieve superior performance in pixel-fidelity metrics, such as\nPeak Signal to Noise Ratio (PSNR). The experimental results demonstrate our\nimprovement compared to previously well-performing methods, with about 8.2%\nbitrate saving compared to VTM-17.0. The source code is available at\nhttps://github.com/GityuxiLiu/SegPIC-for-Image-Compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Image Compression (LIC) has shown remarkable progress in recent\nyears. Existing works commonly employ CNN-based or self-attention-based modules\nas transform methods for compression. However, there is no prior research on\nneural transform that focuses on specific regions. In response, we introduce\nthe class-agnostic segmentation masks (i.e. semantic masks without category\nlabels) for extracting region-adaptive contextual information. Our proposed\nmodule, Region-Adaptive Transform, applies adaptive convolutions on different\nregions guided by the masks. Additionally, we introduce a plug-and-play module\nnamed Scale Affine Layer to incorporate rich contexts from various regions.\nWhile there have been prior image compression efforts that involve segmentation\nmasks as additional intermediate inputs, our approach differs significantly\nfrom them. Our advantages lie in that, to avoid extra bitrate overhead, we\ntreat these masks as privilege information, which is accessible during the\nmodel training stage but not required during the inference phase. To the best\nof our knowledge, we are the first to employ class-agnostic masks as privilege\ninformation and achieve superior performance in pixel-fidelity metrics, such as\nPeak Signal to Noise Ratio (PSNR). The experimental results demonstrate our\nimprovement compared to previously well-performing methods, with about 8.2%\nbitrate saving compared to VTM-17.0. The source code is available at\nhttps://github.com/GityuxiLiu/SegPIC-for-Image-Compression."
                },
                "authors": [
                    {
                        "name": "Yuxi Liu"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Huihui Bai"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Yao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yao Zhao"
                },
                "author": "Yao Zhao",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00628v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00628v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16030v1",
                "updated": "2024-09-24T12:29:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    29,
                    44,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:29:44Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    29,
                    44,
                    1,
                    268,
                    0
                ],
                "title": "MHRC: Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MHRC: Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration\n  with Large Language Models"
                },
                "summary": "The integration of large language models (LLMs) with robotics has\nsignificantly advanced robots' abilities in perception, cognition, and task\nplanning. The use of natural language interfaces offers a unified approach for\nexpressing the capability differences of heterogeneous robots, facilitating\ncommunication between them, and enabling seamless task allocation and\ncollaboration. Currently, the utilization of LLMs to achieve decentralized\nmulti-heterogeneous robot collaborative tasks remains an under-explored area of\nresearch. In this paper, we introduce a novel framework that utilizes LLMs to\nachieve decentralized collaboration among multiple heterogeneous robots. Our\nframework supports three robot categories, mobile robots, manipulation robots,\nand mobile manipulation robots, working together to complete tasks such as\nexploration, transportation, and organization. We developed a rich set of\ntextual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task\nplanning efficiency and overall system performance. The mobile manipulation\nrobot can adjust its base position flexibly, ensuring optimal conditions for\ngrasping tasks. The manipulation robot can comprehend task requirements, seek\nassistance when necessary, and handle objects appropriately. Meanwhile, the\nmobile robot can explore the environment extensively, map object locations, and\ncommunicate this information to the mobile manipulation robot, thus improving\ntask execution efficiency. We evaluated the framework using PyBullet, creating\nscenarios with three different room layouts and three distinct operational\ntasks. We tested various LLM models and conducted ablation studies to assess\nthe contributions of different modules. The experimental results confirm the\neffectiveness and necessity of our proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) with robotics has\nsignificantly advanced robots' abilities in perception, cognition, and task\nplanning. The use of natural language interfaces offers a unified approach for\nexpressing the capability differences of heterogeneous robots, facilitating\ncommunication between them, and enabling seamless task allocation and\ncollaboration. Currently, the utilization of LLMs to achieve decentralized\nmulti-heterogeneous robot collaborative tasks remains an under-explored area of\nresearch. In this paper, we introduce a novel framework that utilizes LLMs to\nachieve decentralized collaboration among multiple heterogeneous robots. Our\nframework supports three robot categories, mobile robots, manipulation robots,\nand mobile manipulation robots, working together to complete tasks such as\nexploration, transportation, and organization. We developed a rich set of\ntextual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task\nplanning efficiency and overall system performance. The mobile manipulation\nrobot can adjust its base position flexibly, ensuring optimal conditions for\ngrasping tasks. The manipulation robot can comprehend task requirements, seek\nassistance when necessary, and handle objects appropriately. Meanwhile, the\nmobile robot can explore the environment extensively, map object locations, and\ncommunicate this information to the mobile manipulation robot, thus improving\ntask execution efficiency. We evaluated the framework using PyBullet, creating\nscenarios with three different room layouts and three distinct operational\ntasks. We tested various LLM models and conducted ablation studies to assess\nthe contributions of different modules. The experimental results confirm the\neffectiveness and necessity of our proposed framework."
                },
                "authors": [
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Yueliang Ying"
                    },
                    {
                        "name": "Sai Li"
                    },
                    {
                        "name": "Jianmin Ji"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16025v1",
                "updated": "2024-09-24T12:24:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    24,
                    34,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:24:34Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    24,
                    34,
                    1,
                    268,
                    0
                ],
                "title": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question\n  Answering"
                },
                "summary": "Users post numerous product-related questions on e-commerce platforms,\naffecting their purchase decisions. Product-related question answering (PQA)\nentails utilizing product-related resources to provide precise responses to\nusers. We propose a novel task of Multilingual Cross-market Product-based\nQuestion Answering (MCPQA) and define the task as providing answers to\nproduct-related questions in a main marketplace by utilizing information from\nanother resource-rich auxiliary marketplace in a multilingual context. We\nintroduce a large-scale dataset comprising over 7 million questions from 17\nmarketplaces across 11 languages. We then perform automatic translation on the\nElectronics category of our dataset, naming it as McMarket. We focus on two\nsubtasks: review-based answer generation and product-related question ranking.\nFor each subtask, we label a subset of McMarket using an LLM and further\nevaluate the quality of the annotations via human assessment. We then conduct\nexperiments to benchmark our dataset, using models ranging from traditional\nlexical models to LLMs in both single-market and cross-market scenarios across\nMcMarket and the corresponding LLM subset. Results show that incorporating\ncross-market information significantly enhances performance in both tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users post numerous product-related questions on e-commerce platforms,\naffecting their purchase decisions. Product-related question answering (PQA)\nentails utilizing product-related resources to provide precise responses to\nusers. We propose a novel task of Multilingual Cross-market Product-based\nQuestion Answering (MCPQA) and define the task as providing answers to\nproduct-related questions in a main marketplace by utilizing information from\nanother resource-rich auxiliary marketplace in a multilingual context. We\nintroduce a large-scale dataset comprising over 7 million questions from 17\nmarketplaces across 11 languages. We then perform automatic translation on the\nElectronics category of our dataset, naming it as McMarket. We focus on two\nsubtasks: review-based answer generation and product-related question ranking.\nFor each subtask, we label a subset of McMarket using an LLM and further\nevaluate the quality of the annotations via human assessment. We then conduct\nexperiments to benchmark our dataset, using models ranging from traditional\nlexical models to LLMs in both single-market and cross-market scenarios across\nMcMarket and the corresponding LLM subset. Results show that incorporating\ncross-market information significantly enhances performance in both tasks."
                },
                "authors": [
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Anders S√∏gaard"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Aliannejadi"
                },
                "author": "Mohammad Aliannejadi",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16022v1",
                "updated": "2024-09-24T12:23:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    23,
                    15,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:23:15Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    23,
                    15,
                    1,
                    268,
                    0
                ],
                "title": "AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming\n  in LLM-Based Batch Relevance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming\n  in LLM-Based Batch Relevance Assessment"
                },
                "summary": "Cognitive biases are systematic deviations in thinking that lead to\nirrational judgments and problematic decision-making, extensively studied\nacross various fields. Recently, large language models (LLMs) have shown\nadvanced understanding capabilities but may inherit human biases from their\ntraining data. While social biases in LLMs have been well-studied, cognitive\nbiases have received less attention, with existing research focusing on\nspecific scenarios. The broader impact of cognitive biases on LLMs in various\ndecision-making contexts remains underexplored. We investigated whether LLMs\nare influenced by the threshold priming effect in relevance judgments, a core\ntask and widely-discussed research topic in the Information Retrieval (IR)\ncoummunity. The priming effect occurs when exposure to certain stimuli\nunconsciously affects subsequent behavior and decisions. Our experiment\nemployed 10 topics from the TREC 2019 Deep Learning passage track collection,\nand tested AI judgments under different document relevance scores, batch\nlengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.\nResults showed that LLMs tend to give lower scores to later documents if\nearlier ones have high relevance, and vice versa, regardless of the combination\nand model used. Our finding demonstrates that LLM%u2019s judgments, similar to\nhuman judgments, are also influenced by threshold priming biases, and suggests\nthat researchers and system engineers should take into account potential\nhuman-like cognitive biases in designing, evaluating, and auditing LLMs in IR\ntasks and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive biases are systematic deviations in thinking that lead to\nirrational judgments and problematic decision-making, extensively studied\nacross various fields. Recently, large language models (LLMs) have shown\nadvanced understanding capabilities but may inherit human biases from their\ntraining data. While social biases in LLMs have been well-studied, cognitive\nbiases have received less attention, with existing research focusing on\nspecific scenarios. The broader impact of cognitive biases on LLMs in various\ndecision-making contexts remains underexplored. We investigated whether LLMs\nare influenced by the threshold priming effect in relevance judgments, a core\ntask and widely-discussed research topic in the Information Retrieval (IR)\ncoummunity. The priming effect occurs when exposure to certain stimuli\nunconsciously affects subsequent behavior and decisions. Our experiment\nemployed 10 topics from the TREC 2019 Deep Learning passage track collection,\nand tested AI judgments under different document relevance scores, batch\nlengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.\nResults showed that LLMs tend to give lower scores to later documents if\nearlier ones have high relevance, and vice versa, regardless of the combination\nand model used. Our finding demonstrates that LLM%u2019s judgments, similar to\nhuman judgments, are also influenced by threshold priming biases, and suggests\nthat researchers and system engineers should take into account potential\nhuman-like cognitive biases in designing, evaluating, and auditing LLMs in IR\ntasks and beyond."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Jiqun Liu"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16016v1",
                "updated": "2024-09-24T12:19:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    19,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:19:31Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    19,
                    31,
                    1,
                    268,
                    0
                ],
                "title": "VascX Models: Model Ensembles for Retinal Vascular Analysis from Color\n  Fundus Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VascX Models: Model Ensembles for Retinal Vascular Analysis from Color\n  Fundus Images"
                },
                "summary": "We introduce VascX models, a comprehensive set of model ensembles for\nanalyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs\nwere aggregated from public datasets for vessel, artery-vein, and disc\nsegmentation; and fovea localization. Additional CFIs from the population-based\nRotterdam Study were, with arteries and veins annotated by graders at pixel\nlevel. Our models achieved robust performance across devices from different\nvendors, varying levels of image quality levels, and diverse pathologies. Our\nmodels demonstrated superior segmentation performance compared to existing\nsystems under a variety of conditions. Significant enhancements were observed\nin artery-vein and disc segmentation performance, particularly in segmentations\nof these structures on CFIs of intermediate quality, a common characteristic of\nlarge cohorts and clinical datasets. Our model outperformed human graders in\nsegmenting vessels with greater precision. With VascX models we provide a\nrobust, ready-to-use set of model ensembles and inference code aimed at\nsimplifying the implementation and enhancing the quality of automated retinal\nvasculature analyses. The precise vessel parameters generated by the model can\nserve as starting points for the identification of disease patterns in and\noutside of the eye.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce VascX models, a comprehensive set of model ensembles for\nanalyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs\nwere aggregated from public datasets for vessel, artery-vein, and disc\nsegmentation; and fovea localization. Additional CFIs from the population-based\nRotterdam Study were, with arteries and veins annotated by graders at pixel\nlevel. Our models achieved robust performance across devices from different\nvendors, varying levels of image quality levels, and diverse pathologies. Our\nmodels demonstrated superior segmentation performance compared to existing\nsystems under a variety of conditions. Significant enhancements were observed\nin artery-vein and disc segmentation performance, particularly in segmentations\nof these structures on CFIs of intermediate quality, a common characteristic of\nlarge cohorts and clinical datasets. Our model outperformed human graders in\nsegmenting vessels with greater precision. With VascX models we provide a\nrobust, ready-to-use set of model ensembles and inference code aimed at\nsimplifying the implementation and enhancing the quality of automated retinal\nvasculature analyses. The precise vessel parameters generated by the model can\nserve as starting points for the identification of disease patterns in and\noutside of the eye."
                },
                "authors": [
                    {
                        "name": "Jose Vargas Quiros"
                    },
                    {
                        "name": "Bart Liefers"
                    },
                    {
                        "name": "Karin van Garderen"
                    },
                    {
                        "name": "Jeroen Vermeulen"
                    },
                    {
                        "name": "Eyened Reading Center"
                    },
                    {
                        "name": "Sinergia Consortium"
                    },
                    {
                        "name": "Caroline Klaver"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Klaver"
                },
                "author": "Caroline Klaver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02850v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02850v7",
                "updated": "2024-09-24T12:11:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    11,
                    50,
                    1,
                    268,
                    0
                ],
                "published": "2024-05-05T08:43:07Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    8,
                    43,
                    7,
                    6,
                    126,
                    0
                ],
                "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Halfway Escape Optimization: A Quantum-Inspired Solution for General\n  Optimization Problems"
                },
                "summary": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems. The HEO mimics the effects between quantum such as tunneling,\nentanglement. After the introduction to the HEO mechansims, the study presents\na comprehensive evaluation of HEO's performance against extensively-used\noptimization algorithms, including Particle Swarm Optimization (PSO), Genetic\nAlgorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer\n(GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary\nanalysis encompasses 14 benchmark functions with dimension 30, demonstrating\nHEO's effectiveness and adaptability in navigating general optimization\nproblems. The test of HEO in Pressure Vessel Design and Tubular Column Design\nalso infers its feasibility and potential in real-time applications. Further\nvalidation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a\nhigher accuracy record.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems. The HEO mimics the effects between quantum such as tunneling,\nentanglement. After the introduction to the HEO mechansims, the study presents\na comprehensive evaluation of HEO's performance against extensively-used\noptimization algorithms, including Particle Swarm Optimization (PSO), Genetic\nAlgorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer\n(GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary\nanalysis encompasses 14 benchmark functions with dimension 30, demonstrating\nHEO's effectiveness and adaptability in navigating general optimization\nproblems. The test of HEO in Pressure Vessel Design and Tubular Column Design\nalso infers its feasibility and potential in real-time applications. Further\nvalidation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a\nhigher accuracy record."
                },
                "authors": [
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "Anwar PP Abdul Majeed"
                    },
                    {
                        "name": "Pascal Lefevre"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Lefevre"
                },
                "author": "Pascal Lefevre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02850v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02850v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16011v1",
                "updated": "2024-09-24T12:11:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    11,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:11:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    11,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "CrowdSurfer: Sampling Optimization Augmented with Vector-Quantized\n  Variational AutoEncoder for Dense Crowd Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdSurfer: Sampling Optimization Augmented with Vector-Quantized\n  Variational AutoEncoder for Dense Crowd Navigation"
                },
                "summary": "Navigation amongst densely packed crowds remains a challenge for mobile\nrobots. The complexity increases further if the environment layout changes,\nmaking the prior computed global plan infeasible. In this paper, we show that\nit is possible to dramatically enhance crowd navigation by just improving the\nlocal planner. Our approach combines generative modelling with inference time\noptimization to generate sophisticated long-horizon local plans at interactive\nrates. More specifically, we train a Vector Quantized Variational AutoEncoder\nto learn a prior over the expert trajectory distribution conditioned on the\nperception input. At run-time, this is used as an initialization for a\nsampling-based optimizer for further refinement. Our approach does not require\nany sophisticated prediction of dynamic obstacles and yet provides\nstate-of-the-art performance. In particular, we compare against the recent\nDRL-VO approach and show a 40% improvement in success rate and a 6% improvement\nin travel time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigation amongst densely packed crowds remains a challenge for mobile\nrobots. The complexity increases further if the environment layout changes,\nmaking the prior computed global plan infeasible. In this paper, we show that\nit is possible to dramatically enhance crowd navigation by just improving the\nlocal planner. Our approach combines generative modelling with inference time\noptimization to generate sophisticated long-horizon local plans at interactive\nrates. More specifically, we train a Vector Quantized Variational AutoEncoder\nto learn a prior over the expert trajectory distribution conditioned on the\nperception input. At run-time, this is used as an initialization for a\nsampling-based optimizer for further refinement. Our approach does not require\nany sophisticated prediction of dynamic obstacles and yet provides\nstate-of-the-art performance. In particular, we compare against the recent\nDRL-VO approach and show a 40% improvement in success rate and a 6% improvement\nin travel time."
                },
                "authors": [
                    {
                        "name": "Naman Kumar"
                    },
                    {
                        "name": "Antareep Singha"
                    },
                    {
                        "name": "Laksh Nanwani"
                    },
                    {
                        "name": "Dhruv Potdar"
                    },
                    {
                        "name": "Tarun R"
                    },
                    {
                        "name": "Fatemeh Rastgar"
                    },
                    {
                        "name": "Simon Idoko"
                    },
                    {
                        "name": "Arun Kumar Singh"
                    },
                    {
                        "name": "K. Madhava Krishna"
                    }
                ],
                "author_detail": {
                    "name": "K. Madhava Krishna"
                },
                "author": "K. Madhava Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16005v1",
                "updated": "2024-09-24T12:06:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    6,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:06:31Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    6,
                    31,
                    1,
                    268,
                    0
                ],
                "title": "Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character\n  Pre-training in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character\n  Pre-training in LLMs"
                },
                "summary": "The integration of large language models (LLMs) with pre-trained speech\nmodels has opened up new avenues in automatic speech recognition (ASR). While\nLLMs excel in multimodal understanding tasks, effectively leveraging their\ncapabilities for ASR remains a significant challenge. This paper presents a\nnovel training approach to enhance LLM performance in ASR tasks. We propose\npre-training LLMs on Pinyin embedding sequences, which represent pronunciation\nfeatures, to generate corresponding Chinese characters. This step enables the\nLLM to adapt to generating text from pronunciation features before encountering\nreal speech data. Furthermore, we fine-tune the LoRA parameters to enhance the\nLLM's understanding of speech modality information. In AISHELL-1 corpus, our\napproach yields a 9.5% relative improvement in ASR tasks compared to the\nbaseline without Pinyi-to-Character pre-training. Additionally, incorporating\nauxiliary text data for Pinyi-to-Character pre-training further boosts\nperformance, achieving a 19.0% relative improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) with pre-trained speech\nmodels has opened up new avenues in automatic speech recognition (ASR). While\nLLMs excel in multimodal understanding tasks, effectively leveraging their\ncapabilities for ASR remains a significant challenge. This paper presents a\nnovel training approach to enhance LLM performance in ASR tasks. We propose\npre-training LLMs on Pinyin embedding sequences, which represent pronunciation\nfeatures, to generate corresponding Chinese characters. This step enables the\nLLM to adapt to generating text from pronunciation features before encountering\nreal speech data. Furthermore, we fine-tune the LoRA parameters to enhance the\nLLM's understanding of speech modality information. In AISHELL-1 corpus, our\napproach yields a 9.5% relative improvement in ASR tasks compared to the\nbaseline without Pinyi-to-Character pre-training. Additionally, incorporating\nauxiliary text data for Pinyi-to-Character pre-training further boosts\nperformance, achieving a 19.0% relative improvement."
                },
                "authors": [
                    {
                        "name": "Yang Yuhang"
                    },
                    {
                        "name": "Peng Yizhou"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Xionghu Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Xionghu Zhong"
                },
                "author": "Xionghu Zhong",
                "arxiv_comment": "Accepted by ISCSLP2024-Special session-Speech Processing in LLM Era",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02898v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02898v2",
                "updated": "2024-09-24T12:04:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    4,
                    40,
                    1,
                    268,
                    0
                ],
                "published": "2024-02-05T11:10:27Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    11,
                    10,
                    27,
                    0,
                    36,
                    0
                ],
                "title": "Bayesian Federated Inference for regression models based on non-shared\n  multicenter data sets from heterogeneous populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Federated Inference for regression models based on non-shared\n  multicenter data sets from heterogeneous populations"
                },
                "summary": "To estimate accurately the parameters of a regression model, the sample size\nmust be large enough relative to the number of possible predictors for the\nmodel. In practice, sufficient data is often lacking, which can lead to\noverfitting of the model and, as a consequence, unreliable predictions of the\noutcome of new patients. Pooling data from different data sets collected in\ndifferent (medical) centers would alleviate this problem, but is often not\nfeasible due to privacy regulation or logistic problems. An alternative route\nwould be to analyze the local data in the centers separately and combine the\nstatistical inference results with the Bayesian Federated Inference (BFI)\nmethodology. The aim of this approach is to compute from the inference results\nin separate centers what would have been found if the statistical analysis was\nperformed on the combined data. We explain the methodology under homogeneity\nand heterogeneity across the populations in the separate centers, and give real\nlife examples for better understanding. Excellent performance of the proposed\nmethodology is shown. An R-package to do all the calculations has been\ndeveloped and is illustrated in this paper. The mathematical details are given\nin the Appendix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To estimate accurately the parameters of a regression model, the sample size\nmust be large enough relative to the number of possible predictors for the\nmodel. In practice, sufficient data is often lacking, which can lead to\noverfitting of the model and, as a consequence, unreliable predictions of the\noutcome of new patients. Pooling data from different data sets collected in\ndifferent (medical) centers would alleviate this problem, but is often not\nfeasible due to privacy regulation or logistic problems. An alternative route\nwould be to analyze the local data in the centers separately and combine the\nstatistical inference results with the Bayesian Federated Inference (BFI)\nmethodology. The aim of this approach is to compute from the inference results\nin separate centers what would have been found if the statistical analysis was\nperformed on the combined data. We explain the methodology under homogeneity\nand heterogeneity across the populations in the separate centers, and give real\nlife examples for better understanding. Excellent performance of the proposed\nmethodology is shown. An R-package to do all the calculations has been\ndeveloped and is illustrated in this paper. The mathematical details are given\nin the Appendix."
                },
                "authors": [
                    {
                        "name": "Marianne A Jonker"
                    },
                    {
                        "name": "Hassan Pazira"
                    },
                    {
                        "name": "Anthony CC Coolen"
                    }
                ],
                "author_detail": {
                    "name": "Anthony CC Coolen"
                },
                "author": "Anthony CC Coolen",
                "arxiv_comment": "33 pages, 1 figure, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02898v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02481v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02481v4",
                "updated": "2024-09-24T12:00:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    0,
                    29,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-04T16:49:06Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    16,
                    49,
                    6,
                    1,
                    156,
                    0
                ],
                "title": "Large Language Models as Carriers of Hidden Messages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Carriers of Hidden Messages"
                },
                "summary": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels)."
                },
                "authors": [
                    {
                        "name": "Jakub Hoscilowicz"
                    },
                    {
                        "name": "Pawel Popiolek"
                    },
                    {
                        "name": "Jan Rudkowski"
                    },
                    {
                        "name": "Jedrzej Bieniasz"
                    },
                    {
                        "name": "Artur Janicki"
                    }
                ],
                "author_detail": {
                    "name": "Artur Janicki"
                },
                "author": "Artur Janicki",
                "arxiv_comment": "Work in progress. Code is available at\n  https://github.com/j-hoscilowic/zurek-stegano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02481v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02481v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15995v1",
                "updated": "2024-09-24T11:52:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    52,
                    9,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T11:52:09Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    52,
                    9,
                    1,
                    268,
                    0
                ],
                "title": "Robust Inference for Non-Linear Regression Models with Applications in\n  Enzyme Kinetics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Inference for Non-Linear Regression Models with Applications in\n  Enzyme Kinetics"
                },
                "summary": "Despite linear regression being the most popular statistical modelling\ntechnique, in real-life we often need to deal with situations where the true\nrelationship between the response and the covariates is nonlinear in\nparameters. In such cases one needs to adopt appropriate non-linear regression\n(NLR) analysis, having wider applications in biochemical and medical studies\namong many others. In this paper we propose a new improved robust estimation\nand testing methodologies for general NLR models based on the minimum density\npower divergence approach and apply our proposal to analyze the widely popular\nMichaelis-Menten (MM) model in enzyme kinetics. We establish the asymptotic\nproperties of our proposed estimator and tests, along with their theoretical\nrobustness characteristics through influence function analysis. For the\nparticular MM model, we have further empirically justified the robustness and\nthe efficiency of our proposed estimator and the testing procedure through\nextensive simulation studies and several interesting real data examples of\nenzyme-catalyzed (biochemical) reactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite linear regression being the most popular statistical modelling\ntechnique, in real-life we often need to deal with situations where the true\nrelationship between the response and the covariates is nonlinear in\nparameters. In such cases one needs to adopt appropriate non-linear regression\n(NLR) analysis, having wider applications in biochemical and medical studies\namong many others. In this paper we propose a new improved robust estimation\nand testing methodologies for general NLR models based on the minimum density\npower divergence approach and apply our proposal to analyze the widely popular\nMichaelis-Menten (MM) model in enzyme kinetics. We establish the asymptotic\nproperties of our proposed estimator and tests, along with their theoretical\nrobustness characteristics through influence function analysis. For the\nparticular MM model, we have further empirically justified the robustness and\nthe efficiency of our proposed estimator and the testing procedure through\nextensive simulation studies and several interesting real data examples of\nenzyme-catalyzed (biochemical) reactions."
                },
                "authors": [
                    {
                        "name": "Suryasis Jana"
                    },
                    {
                        "name": "Abhik Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Ghosh"
                },
                "author": "Abhik Ghosh",
                "arxiv_comment": "Pre-print; Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08040v3",
                "updated": "2024-09-24T11:46:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    46,
                    9,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-12T19:23:13Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    19,
                    23,
                    13,
                    1,
                    72,
                    0
                ],
                "title": "Low-Energy On-Device Personalization for MCUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Energy On-Device Personalization for MCUs"
                },
                "summary": "Microcontroller Units (MCUs) are ideal platforms for edge applications due to\ntheir low cost and energy consumption, and are widely used in various\napplications, including personalized machine learning tasks, where customized\nmodels can enhance the task adaptation. However, existing approaches for local\non-device personalization mostly support simple ML architectures or require\ncomplex local pre-training/training, leading to high energy consumption and\nnegating the low-energy advantage of MCUs.\n  In this paper, we introduce $MicroT$, an efficient and low-energy MCU\npersonalization approach. $MicroT$ includes a robust, general, but tiny feature\nextractor, developed through self-supervised knowledge distillation, which\ntrains a task-specific head to enable independent on-device personalization\nwith minimal energy and computational requirements. MicroT implements an\nMCU-optimized early-exit inference mechanism called stage-decision to further\nreduce energy costs. This mechanism allows for user-configurable exit criteria\n(stage-decision ratio) to adaptively balance energy cost with model\nperformance. We evaluated MicroT using two models, three datasets, and two MCU\nboards. $MicroT$ outperforms traditional transfer learning (TTL) and two SOTA\napproaches by 2.12 - 11.60% across two models and three datasets. Targeting\nwidely used energy-aware edge devices, MicroT's on-device training requires no\nadditional complex operations, halving the energy cost compared to SOTA\napproaches by up to 2.28$\\times$ while keeping SRAM usage below 1MB. During\nlocal inference, MicroT reduces energy cost by 14.17% compared to TTL across\ntwo boards and two datasets, highlighting its suitability for long-term use on\nenergy-aware resource-constrained MCUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microcontroller Units (MCUs) are ideal platforms for edge applications due to\ntheir low cost and energy consumption, and are widely used in various\napplications, including personalized machine learning tasks, where customized\nmodels can enhance the task adaptation. However, existing approaches for local\non-device personalization mostly support simple ML architectures or require\ncomplex local pre-training/training, leading to high energy consumption and\nnegating the low-energy advantage of MCUs.\n  In this paper, we introduce $MicroT$, an efficient and low-energy MCU\npersonalization approach. $MicroT$ includes a robust, general, but tiny feature\nextractor, developed through self-supervised knowledge distillation, which\ntrains a task-specific head to enable independent on-device personalization\nwith minimal energy and computational requirements. MicroT implements an\nMCU-optimized early-exit inference mechanism called stage-decision to further\nreduce energy costs. This mechanism allows for user-configurable exit criteria\n(stage-decision ratio) to adaptively balance energy cost with model\nperformance. We evaluated MicroT using two models, three datasets, and two MCU\nboards. $MicroT$ outperforms traditional transfer learning (TTL) and two SOTA\napproaches by 2.12 - 11.60% across two models and three datasets. Targeting\nwidely used energy-aware edge devices, MicroT's on-device training requires no\nadditional complex operations, halving the energy cost compared to SOTA\napproaches by up to 2.28$\\times$ while keeping SRAM usage below 1MB. During\nlocal inference, MicroT reduces energy cost by 14.17% compared to TTL across\ntwo boards and two datasets, highlighting its suitability for long-term use on\nenergy-aware resource-constrained MCUs."
                },
                "authors": [
                    {
                        "name": "Yushan Huang"
                    },
                    {
                        "name": "Ranya Aloufi"
                    },
                    {
                        "name": "Xavier Cadet"
                    },
                    {
                        "name": "Yuchen Zhao"
                    },
                    {
                        "name": "Payam Barnaghi"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "arxiv_comment": "Accepted to The 9th ACM/IEEE Symposium on Edge Computing (SEC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15981v1",
                "updated": "2024-09-24T11:22:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    22,
                    55,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T11:22:55Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    22,
                    55,
                    1,
                    268,
                    0
                ],
                "title": "GPT-4 as a Homework Tutor can Improve Student Engagement and Learning\n  Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4 as a Homework Tutor can Improve Student Engagement and Learning\n  Outcomes"
                },
                "summary": "This work contributes to the scarce empirical literature on LLM-based\ninteractive homework in real-world educational settings and offers a practical,\nscalable solution for improving homework in schools. Homework is an important\npart of education in schools across the world, but in order to maximize\nbenefit, it needs to be accompanied with feedback and followup questions. We\ndeveloped a prompting strategy that enables GPT-4 to conduct interactive\nhomework sessions for high-school students learning English as a second\nlanguage. Our strategy requires minimal efforts in content preparation, one of\nthe key challenges of alternatives like home tutors or ITSs. We carried out a\nRandomized Controlled Trial (RCT) in four high-school classes, replacing\ntraditional homework with GPT-4 homework sessions for the treatment group. We\nobserved significant improvements in learning outcomes, specifically a greater\ngain in grammar, and student engagement. In addition, students reported high\nlevels of satisfaction with the system and wanted to continue using it after\nthe end of the RCT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work contributes to the scarce empirical literature on LLM-based\ninteractive homework in real-world educational settings and offers a practical,\nscalable solution for improving homework in schools. Homework is an important\npart of education in schools across the world, but in order to maximize\nbenefit, it needs to be accompanied with feedback and followup questions. We\ndeveloped a prompting strategy that enables GPT-4 to conduct interactive\nhomework sessions for high-school students learning English as a second\nlanguage. Our strategy requires minimal efforts in content preparation, one of\nthe key challenges of alternatives like home tutors or ITSs. We carried out a\nRandomized Controlled Trial (RCT) in four high-school classes, replacing\ntraditional homework with GPT-4 homework sessions for the treatment group. We\nobserved significant improvements in learning outcomes, specifically a greater\ngain in grammar, and student engagement. In addition, students reported high\nlevels of satisfaction with the system and wanted to continue using it after\nthe end of the RCT."
                },
                "authors": [
                    {
                        "name": "Alessandro Vanzo"
                    },
                    {
                        "name": "Sankalan Pal Chowdhury"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "Submitted to LAK25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15980v1",
                "updated": "2024-09-24T11:22:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    22,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T11:22:24Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    22,
                    24,
                    1,
                    268,
                    0
                ],
                "title": "Leveraging Unsupervised Learning for Cost-Effective Visual Anomaly\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Unsupervised Learning for Cost-Effective Visual Anomaly\n  Detection"
                },
                "summary": "Traditional machine learning-based visual inspection systems require\nextensive data collection and repetitive model training to improve accuracy.\nThese systems typically require expensive camera, computing equipment and\nsignificant machine learning expertise, which can substantially burden small\nand medium-sized enterprises. This study explores leveraging unsupervised\nlearning methods with pre-trained models and low-cost hardware to create a\ncost-effective visual anomaly detection system. The research aims to develop a\nlow-cost visual anomaly detection solution that uses minimal data for model\ntraining while maintaining generalizability and scalability. The system\nutilises unsupervised learning models from Anomalib and is deployed on\naffordable Raspberry Pi hardware through openVINO. The results show that this\ncost-effective system can complete anomaly defection training and inference on\na Raspberry Pi in just 90 seconds using only 10 normal product images,\nachieving an F1 macro score exceeding 0.95. While the system is slightly\nsensitive to environmental changes like lighting, product positioning, or\nbackground, it remains a swift and economical method for factory automation\ninspection for small and medium-sized manufacturers",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning-based visual inspection systems require\nextensive data collection and repetitive model training to improve accuracy.\nThese systems typically require expensive camera, computing equipment and\nsignificant machine learning expertise, which can substantially burden small\nand medium-sized enterprises. This study explores leveraging unsupervised\nlearning methods with pre-trained models and low-cost hardware to create a\ncost-effective visual anomaly detection system. The research aims to develop a\nlow-cost visual anomaly detection solution that uses minimal data for model\ntraining while maintaining generalizability and scalability. The system\nutilises unsupervised learning models from Anomalib and is deployed on\naffordable Raspberry Pi hardware through openVINO. The results show that this\ncost-effective system can complete anomaly defection training and inference on\na Raspberry Pi in just 90 seconds using only 10 normal product images,\nachieving an F1 macro score exceeding 0.95. While the system is slightly\nsensitive to environmental changes like lighting, product positioning, or\nbackground, it remains a swift and economical method for factory automation\ninspection for small and medium-sized manufacturers"
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Zhengyang Ling"
                    },
                    {
                        "name": "Sam Brook"
                    },
                    {
                        "name": "Duncan McFarlane"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14517v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14517v3",
                "updated": "2024-09-24T11:22:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    22,
                    4,
                    1,
                    268,
                    0
                ],
                "published": "2023-11-24T14:45:53Z",
                "published_parsed": [
                    2023,
                    11,
                    24,
                    14,
                    45,
                    53,
                    4,
                    328,
                    0
                ],
                "title": "tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models"
                },
                "summary": "Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in\nthe field of audio and speech processing. Its employment ranges from sound\nevent detection to text-to-audio generation. However, one of the main\nlimitations is the considerable amount of data required in the training process\nand the overall computational complexity during inference. This paper\ninvestigates how we can reduce the complexity of contrastive language-audio\npre-trained models, yielding an efficient model that we call tinyCLAP. We\nderive an unimodal distillation loss from first principles and explore how the\ndimensionality of the shared, multimodal latent space can be reduced via\npruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a\nminimal reduction (less than 5%) in zero-shot classification performance across\nthe three sound event detection datasets on which it was tested",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in\nthe field of audio and speech processing. Its employment ranges from sound\nevent detection to text-to-audio generation. However, one of the main\nlimitations is the considerable amount of data required in the training process\nand the overall computational complexity during inference. This paper\ninvestigates how we can reduce the complexity of contrastive language-audio\npre-trained models, yielding an efficient model that we call tinyCLAP. We\nderive an unimodal distillation loss from first principles and explore how the\ndimensionality of the shared, multimodal latent space can be reduced via\npruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a\nminimal reduction (less than 5%) in zero-shot classification performance across\nthe three sound event detection datasets on which it was tested"
                },
                "authors": [
                    {
                        "name": "Francesco Paissan"
                    },
                    {
                        "name": "Elisabetta Farella"
                    }
                ],
                "author_detail": {
                    "name": "Elisabetta Farella"
                },
                "author": "Elisabetta Farella",
                "arxiv_doi": "10.21437/Interspeech.2024-193",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21437/Interspeech.2024-193",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.14517v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14517v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of Interspeech. Please use the citation available at\n  https://www.isca-archive.org/interspeech_2024/paissan24_interspeech.html",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15979v1",
                "updated": "2024-09-24T11:21:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    21,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T11:21:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    21,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Finetuning LLMs for Comparative Assessment Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning LLMs for Comparative Assessment Tasks"
                },
                "summary": "Automated assessment in natural language generation is a challenging task.\nInstruction-tuned large language models (LLMs) have shown promise in\nreference-free evaluation, particularly through comparative assessment.\nHowever, the quadratic computational complexity of pairwise comparisons limits\nits scalability. To address this, efficient comparative assessment has been\nexplored by applying comparative strategies on zero-shot LLM probabilities. We\npropose a framework for finetuning LLMs for comparative assessment to align the\nmodel's output with the target distribution of comparative probabilities. By\ntraining on soft probabilities, our approach improves state-of-the-art\nperformance while maintaining high performance with an efficient subset of\ncomparisons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated assessment in natural language generation is a challenging task.\nInstruction-tuned large language models (LLMs) have shown promise in\nreference-free evaluation, particularly through comparative assessment.\nHowever, the quadratic computational complexity of pairwise comparisons limits\nits scalability. To address this, efficient comparative assessment has been\nexplored by applying comparative strategies on zero-shot LLM probabilities. We\npropose a framework for finetuning LLMs for comparative assessment to align the\nmodel's output with the target distribution of comparative probabilities. By\ntraining on soft probabilities, our approach improves state-of-the-art\nperformance while maintaining high performance with an efficient subset of\ncomparisons."
                },
                "authors": [
                    {
                        "name": "Vatsal Raina"
                    },
                    {
                        "name": "Adian Liusie"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "arxiv_comment": "8 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15973v1",
                "updated": "2024-09-24T11:07:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    7,
                    33,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T11:07:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    7,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "Edge-device Collaborative Computing for Multi-view Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-device Collaborative Computing for Multi-view Classification"
                },
                "summary": "Motivated by the proliferation of Internet-of-Thing (IoT) devices and the\nrapid advances in the field of deep learning, there is a growing interest in\npushing deep learning computations, conventionally handled by the cloud, to the\nedge of the network to deliver faster responses to end users, reduce bandwidth\nconsumption to the cloud, and address privacy concerns. However, to fully\nrealize deep learning at the edge, two main challenges still need to be\naddressed: (i) how to meet the high resource requirements of deep learning on\nresource-constrained devices, and (ii) how to leverage the availability of\nmultiple streams of spatially correlated data, to increase the effectiveness of\ndeep learning and improve application-level performance. To address the above\nchallenges, we explore collaborative inference at the edge, in which edge nodes\nand end devices share correlated data and the inference computational burden by\nleveraging different ways to split computation and fuse data. Besides\ntraditional centralized and distributed schemes for edge-end device\ncollaborative inference, we introduce selective schemes that decrease bandwidth\nresource consumption by effectively reducing data redundancy. As a reference\nscenario, we focus on multi-view classification in a networked system in which\nsensing nodes can capture overlapping fields of view. The proposed schemes are\ncompared in terms of accuracy, computational expenditure at the nodes,\ncommunication overhead, inference latency, robustness, and noise sensitivity.\nExperimental results highlight that selective collaborative schemes can achieve\ndifferent trade-offs between the above performance metrics, with some of them\nbringing substantial communication savings (from 18% to 74% of the transmitted\ndata with respect to centralized inference) while still keeping the inference\naccuracy well above 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the proliferation of Internet-of-Thing (IoT) devices and the\nrapid advances in the field of deep learning, there is a growing interest in\npushing deep learning computations, conventionally handled by the cloud, to the\nedge of the network to deliver faster responses to end users, reduce bandwidth\nconsumption to the cloud, and address privacy concerns. However, to fully\nrealize deep learning at the edge, two main challenges still need to be\naddressed: (i) how to meet the high resource requirements of deep learning on\nresource-constrained devices, and (ii) how to leverage the availability of\nmultiple streams of spatially correlated data, to increase the effectiveness of\ndeep learning and improve application-level performance. To address the above\nchallenges, we explore collaborative inference at the edge, in which edge nodes\nand end devices share correlated data and the inference computational burden by\nleveraging different ways to split computation and fuse data. Besides\ntraditional centralized and distributed schemes for edge-end device\ncollaborative inference, we introduce selective schemes that decrease bandwidth\nresource consumption by effectively reducing data redundancy. As a reference\nscenario, we focus on multi-view classification in a networked system in which\nsensing nodes can capture overlapping fields of view. The proposed schemes are\ncompared in terms of accuracy, computational expenditure at the nodes,\ncommunication overhead, inference latency, robustness, and noise sensitivity.\nExperimental results highlight that selective collaborative schemes can achieve\ndifferent trade-offs between the above performance metrics, with some of them\nbringing substantial communication savings (from 18% to 74% of the transmitted\ndata with respect to centralized inference) while still keeping the inference\naccuracy well above 90%."
                },
                "authors": [
                    {
                        "name": "Marco Palena"
                    },
                    {
                        "name": "Tania Cerquitelli"
                    },
                    {
                        "name": "Carla Fabiana Chiasserini"
                    }
                ],
                "author_detail": {
                    "name": "Carla Fabiana Chiasserini"
                },
                "author": "Carla Fabiana Chiasserini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10553v2",
                "updated": "2024-09-24T11:00:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    0,
                    38,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-02T13:27:36Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    27,
                    36,
                    0,
                    246,
                    0
                ],
                "title": "\"Flipped\" University: LLM-Assisted Lifelong Learning Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Flipped\" University: LLM-Assisted Lifelong Learning Environment"
                },
                "summary": "The rapid development of artificial intelligence technologies, particularly\nLarge Language Models (LLMs), has revolutionized the landscape of lifelong\nlearning. This paper introduces a conceptual framework for a self-constructed\nlifelong learning environment supported by LLMs. It highlights the inadequacies\nof traditional education systems in keeping pace with the rapid deactualization\nof knowledge and skills. The proposed framework emphasizes the transformation\nfrom institutionalized education to personalized, self-driven learning. It\nleverages the natural language capabilities of LLMs to provide dynamic and\nadaptive learning experiences, facilitating the creation of personal\nintellectual agents that assist in knowledge acquisition. The framework\nintegrates principles of lifelong learning, including the necessity of building\npersonal world models, the dual modes of learning (training and exploration),\nand the creation of reusable learning artifacts. Additionally, it underscores\nthe importance of curiosity-driven learning and reflective practices in\nmaintaining an effective learning trajectory. The paper envisions the evolution\nof educational institutions into \"flipped\" universities, focusing on supporting\nglobal knowledge consistency rather than merely structuring and transmitting\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of artificial intelligence technologies, particularly\nLarge Language Models (LLMs), has revolutionized the landscape of lifelong\nlearning. This paper introduces a conceptual framework for a self-constructed\nlifelong learning environment supported by LLMs. It highlights the inadequacies\nof traditional education systems in keeping pace with the rapid deactualization\nof knowledge and skills. The proposed framework emphasizes the transformation\nfrom institutionalized education to personalized, self-driven learning. It\nleverages the natural language capabilities of LLMs to provide dynamic and\nadaptive learning experiences, facilitating the creation of personal\nintellectual agents that assist in knowledge acquisition. The framework\nintegrates principles of lifelong learning, including the necessity of building\npersonal world models, the dual modes of learning (training and exploration),\nand the creation of reusable learning artifacts. Additionally, it underscores\nthe importance of curiosity-driven learning and reflective practices in\nmaintaining an effective learning trajectory. The paper envisions the evolution\nof educational institutions into \"flipped\" universities, focusing on supporting\nglobal knowledge consistency rather than merely structuring and transmitting\nknowledge."
                },
                "authors": [
                    {
                        "name": "Kirill Krinkin"
                    },
                    {
                        "name": "Tatiana Berlenko"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Berlenko"
                },
                "author": "Tatiana Berlenko",
                "arxiv_comment": "Pre-print version, accepted for 31st International Conference on\n  Neural Information Processing (ICONIP2024), 13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v2",
                "updated": "2024-09-24T10:57:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    57,
                    18,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15965v1",
                "updated": "2024-09-24T10:51:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    51,
                    15,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T10:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    51,
                    15,
                    1,
                    268,
                    0
                ],
                "title": "A sparsified Christoffel function for high-dimensional inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A sparsified Christoffel function for high-dimensional inference"
                },
                "summary": "Christoffel polynomials are classical tools from approximation theory. They\ncan be used to estimate the (compact) support of a measure $\\mu$ on\n$\\mathbb{R}^d$ based on its low-degree moments. Recently, they have been\napplied to problems in data science, including outlier detection and support\ninference. A major downside of Christoffel polynomials in such applications is\nthe fact that, in order to compute their coefficients, one must invert a matrix\nwhose size grows rapidly with the dimension $d$. In this paper, we propose a\nmodification of the Christoffel polynomial which is significantly cheaper to\ncompute, but retains many of its desirable properties. Our approach relies on\nsparsity of the underlying measure $\\mu$, described by a graphical model. The\ncomplexity of our modification depends on the treewidth of this model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Christoffel polynomials are classical tools from approximation theory. They\ncan be used to estimate the (compact) support of a measure $\\mu$ on\n$\\mathbb{R}^d$ based on its low-degree moments. Recently, they have been\napplied to problems in data science, including outlier detection and support\ninference. A major downside of Christoffel polynomials in such applications is\nthe fact that, in order to compute their coefficients, one must invert a matrix\nwhose size grows rapidly with the dimension $d$. In this paper, we propose a\nmodification of the Christoffel polynomial which is significantly cheaper to\ncompute, but retains many of its desirable properties. Our approach relies on\nsparsity of the underlying measure $\\mu$, described by a graphical model. The\ncomplexity of our modification depends on the treewidth of this model."
                },
                "authors": [
                    {
                        "name": "Jean-Bernard Lasserre"
                    },
                    {
                        "name": "Lucas Slot"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Slot"
                },
                "author": "Lucas Slot",
                "arxiv_comment": "21 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15963v1",
                "updated": "2024-09-24T10:48:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    48,
                    13,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T10:48:13Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    48,
                    13,
                    1,
                    268,
                    0
                ],
                "title": "Provably Efficient Exploration in Inverse Constrained Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Efficient Exploration in Inverse Constrained Reinforcement\n  Learning"
                },
                "summary": "To obtain the optimal constraints in complex environments, Inverse\nConstrained Reinforcement Learning (ICRL) seeks to recover these constraints\nfrom expert demonstrations in a data-driven manner. Existing ICRL algorithms\ncollect training samples from an interactive environment. However, the efficacy\nand efficiency of these sampling strategies remain unknown. To bridge this gap,\nwe introduce a strategic exploration framework with provable efficiency.\nSpecifically, we define a feasible constraint set for ICRL problems and\ninvestigate how expert policy and environmental dynamics influence the\noptimality of constraints. Motivated by our findings, we propose two\nexploratory algorithms to achieve efficient constraint inference via 1)\ndynamically reducing the bounded aggregate error of cost estimation and 2)\nstrategically constraining the exploration policy. Both algorithms are\ntheoretically grounded with tractable sample complexity. We empirically\ndemonstrate the performance of our algorithms under various environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To obtain the optimal constraints in complex environments, Inverse\nConstrained Reinforcement Learning (ICRL) seeks to recover these constraints\nfrom expert demonstrations in a data-driven manner. Existing ICRL algorithms\ncollect training samples from an interactive environment. However, the efficacy\nand efficiency of these sampling strategies remain unknown. To bridge this gap,\nwe introduce a strategic exploration framework with provable efficiency.\nSpecifically, we define a feasible constraint set for ICRL problems and\ninvestigate how expert policy and environmental dynamics influence the\noptimality of constraints. Motivated by our findings, we propose two\nexploratory algorithms to achieve efficient constraint inference via 1)\ndynamically reducing the bounded aggregate error of cost estimation and 2)\nstrategically constraining the exploration policy. Both algorithms are\ntheoretically grounded with tractable sample complexity. We empirically\ndemonstrate the performance of our algorithms under various environments."
                },
                "authors": [
                    {
                        "name": "Bo Yue"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Guiliang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guiliang Liu"
                },
                "author": "Guiliang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15957v1",
                "updated": "2024-09-24T10:42:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    42,
                    23,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T10:42:23Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    42,
                    23,
                    1,
                    268,
                    0
                ],
                "title": "ASD-Diffusion: Anomalous Sound Detection with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASD-Diffusion: Anomalous Sound Detection with Diffusion Models"
                },
                "summary": "Unsupervised Anomalous Sound Detection (ASD) aims to design a generalizable\nmethod that can be used to detect anomalies when only normal sounds are given.\nIn this paper, Anomalous Sound Detection based on Diffusion Models\n(ASD-Diffusion) is proposed for ASD in real-world factories. In our pipeline,\nthe anomalies in acoustic features are reconstructed from their noisy corrupted\nfeatures into their approximate normal pattern. Secondly, a post-processing\nanomalies filter algorithm is proposed to detect anomalies that exhibit\nsignificant deviation from the original input after reconstruction.\nFurthermore, denoising diffusion implicit model is introduced to accelerate the\ninference speed by a longer sampling interval of the denoising process. The\nproposed method is innovative in the application of diffusion models as a new\nscheme. Experimental results on the development set of DCASE 2023 challenge\ntask 2 outperform the baseline by 7.75%, demonstrating the effectiveness of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Anomalous Sound Detection (ASD) aims to design a generalizable\nmethod that can be used to detect anomalies when only normal sounds are given.\nIn this paper, Anomalous Sound Detection based on Diffusion Models\n(ASD-Diffusion) is proposed for ASD in real-world factories. In our pipeline,\nthe anomalies in acoustic features are reconstructed from their noisy corrupted\nfeatures into their approximate normal pattern. Secondly, a post-processing\nanomalies filter algorithm is proposed to detect anomalies that exhibit\nsignificant deviation from the original input after reconstruction.\nFurthermore, denoising diffusion implicit model is introduced to accelerate the\ninference speed by a longer sampling interval of the denoising process. The\nproposed method is innovative in the application of diffusion models as a new\nscheme. Experimental results on the development set of DCASE 2023 challenge\ntask 2 outperform the baseline by 7.75%, demonstrating the effectiveness of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Fengrun Zhang"
                    },
                    {
                        "name": "Xiang Xie"
                    },
                    {
                        "name": "Kai Guo"
                    }
                ],
                "author_detail": {
                    "name": "Kai Guo"
                },
                "author": "Kai Guo",
                "arxiv_comment": "This paper will appear at ICPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11176v2",
                "updated": "2024-09-24T10:01:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    1,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-17T03:29:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    3,
                    29,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process\n  Refinement"
                },
                "summary": "Large language model agents have exhibited exceptional performance across a\nrange of complex interactive tasks. Recent approaches have utilized tuning with\nexpert trajectories to enhance agent performance, yet they primarily\nconcentrate on outcome rewards, which may lead to errors or suboptimal actions\ndue to the absence of process supervision signals. In this paper, we introduce\nthe Iterative step-level Process Refinement (IPR) framework, which provides\ndetailed step-by-step guidance to enhance agent training. Specifically, we\nadopt the Monte Carlo method to estimate step-level rewards. During each\niteration, the agent explores along the expert trajectory and generates new\nactions. These actions are then evaluated against the corresponding step of\nexpert trajectory using step-level rewards. Such comparison helps identify\ndiscrepancies, yielding contrastive action pairs that serve as training data\nfor the agent. Our experiments on three complex agent tasks demonstrate that\nour framework outperforms a variety of strong baselines. Moreover, our\nanalytical findings highlight the effectiveness of IPR in augmenting action\nefficiency and its applicability to diverse models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model agents have exhibited exceptional performance across a\nrange of complex interactive tasks. Recent approaches have utilized tuning with\nexpert trajectories to enhance agent performance, yet they primarily\nconcentrate on outcome rewards, which may lead to errors or suboptimal actions\ndue to the absence of process supervision signals. In this paper, we introduce\nthe Iterative step-level Process Refinement (IPR) framework, which provides\ndetailed step-by-step guidance to enhance agent training. Specifically, we\nadopt the Monte Carlo method to estimate step-level rewards. During each\niteration, the agent explores along the expert trajectory and generates new\nactions. These actions are then evaluated against the corresponding step of\nexpert trajectory using step-level rewards. Such comparison helps identify\ndiscrepancies, yielding contrastive action pairs that serve as training data\nfor the agent. Our experiments on three complex agent tasks demonstrate that\nour framework outperforms a variety of strong baselines. Moreover, our\nanalytical findings highlight the effectiveness of IPR in augmenting action\nefficiency and its applicability to diverse models."
                },
                "authors": [
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Xiutian Zhao"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "Accepted to EMNLP 2024 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15934v1",
                "updated": "2024-09-24T09:57:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:57:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents"
                },
                "summary": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains."
                },
                "authors": [
                    {
                        "name": "Samuel Arcadinho"
                    },
                    {
                        "name": "David Aparicio"
                    },
                    {
                        "name": "Mariana Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Almeida"
                },
                "author": "Mariana Almeida",
                "arxiv_comment": "14 pages, 5 figures, Submitted to GenBench@EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15933v1",
                "updated": "2024-09-24T09:57:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    25,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:57:25Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    25,
                    1,
                    268,
                    0
                ],
                "title": "SLIMER-IT: Zero-Shot NER on Italian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIMER-IT: Zero-Shot NER on Italian Language"
                },
                "summary": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags."
                },
                "authors": [
                    {
                        "name": "Andrew Zamai"
                    },
                    {
                        "name": "Leonardo Rigutini"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Andrea Zugarini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zugarini"
                },
                "author": "Andrea Zugarini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12425v2",
                "updated": "2024-09-24T09:55:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    55,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-08-22T14:20:11Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    14,
                    20,
                    11,
                    3,
                    235,
                    0
                ],
                "title": "Dynamic Gated Recurrent Neural Network for Compute-efficient Speech\n  Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Gated Recurrent Neural Network for Compute-efficient Speech\n  Enhancement"
                },
                "summary": "This paper introduces a new Dynamic Gated Recurrent Neural Network (DG-RNN)\nfor compute-efficient speech enhancement models running on resource-constrained\nhardware platforms. It leverages the slow evolution characteristic of RNN\nhidden states over steps, and updates only a selected set of neurons at each\nstep by adding a newly proposed select gate to the RNN model. This select gate\nallows the computation cost of the conventional RNN to be reduced during\nnetwork inference. As a realization of the DG-RNN, we further propose the\nDynamic Gated Recurrent Unit (D-GRU) which does not require additional\nparameters. Test results obtained from several state-of-the-art\ncompute-efficient RNN-based speech enhancement architectures using the DNS\nchallenge dataset, show that the D-GRU based model variants maintain similar\nspeech intelligibility and quality metrics comparable to the baseline GRU based\nmodels even with an average 50% reduction in GRU computes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new Dynamic Gated Recurrent Neural Network (DG-RNN)\nfor compute-efficient speech enhancement models running on resource-constrained\nhardware platforms. It leverages the slow evolution characteristic of RNN\nhidden states over steps, and updates only a selected set of neurons at each\nstep by adding a newly proposed select gate to the RNN model. This select gate\nallows the computation cost of the conventional RNN to be reduced during\nnetwork inference. As a realization of the DG-RNN, we further propose the\nDynamic Gated Recurrent Unit (D-GRU) which does not require additional\nparameters. Test results obtained from several state-of-the-art\ncompute-efficient RNN-based speech enhancement architectures using the DNS\nchallenge dataset, show that the D-GRU based model variants maintain similar\nspeech intelligibility and quality metrics comparable to the baseline GRU based\nmodels even with an average 50% reduction in GRU computes."
                },
                "authors": [
                    {
                        "name": "Longbiao Cheng"
                    },
                    {
                        "name": "Ashutosh Pandey"
                    },
                    {
                        "name": "Buye Xu"
                    },
                    {
                        "name": "Tobi Delbruck"
                    },
                    {
                        "name": "Shih-Chii Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Chii Liu"
                },
                "author": "Shih-Chii Liu",
                "arxiv_doi": "10.21437/Interspeech.2024-958",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21437/Interspeech.2024-958",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of Interspeech 2024",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02089v2",
                "updated": "2024-09-24T09:50:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    50,
                    58,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-02T09:25:58Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    25,
                    58,
                    1,
                    184,
                    0
                ],
                "title": "GPTCast: a weather language model for precipitation nowcasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTCast: a weather language model for precipitation nowcasting"
                },
                "summary": "This work introduces GPTCast, a generative deep-learning method for ensemble\nnowcast of radar-based precipitation, inspired by advancements in large\nlanguage models (LLMs). We employ a GPT model as a forecaster to learn\nspatiotemporal precipitation dynamics using tokenized radar images. The\ntokenizer is based on a Quantized Variational Autoencoder featuring a novel\nreconstruction loss tailored for the skewed distribution of precipitation that\npromotes faithful reconstruction of high rainfall rates. The approach produces\nrealistic ensemble forecasts and provides probabilistic outputs with accurate\nuncertainty estimation. The model is trained without resorting to randomness,\nall variability is learned solely from the data and exposed by model at\ninference for ensemble generation. We train and test GPTCast using a 6-year\nradar dataset over the Emilia-Romagna region in Northern Italy, showing\nsuperior results compared to state-of-the-art ensemble extrapolation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces GPTCast, a generative deep-learning method for ensemble\nnowcast of radar-based precipitation, inspired by advancements in large\nlanguage models (LLMs). We employ a GPT model as a forecaster to learn\nspatiotemporal precipitation dynamics using tokenized radar images. The\ntokenizer is based on a Quantized Variational Autoencoder featuring a novel\nreconstruction loss tailored for the skewed distribution of precipitation that\npromotes faithful reconstruction of high rainfall rates. The approach produces\nrealistic ensemble forecasts and provides probabilistic outputs with accurate\nuncertainty estimation. The model is trained without resorting to randomness,\nall variability is learned solely from the data and exposed by model at\ninference for ensemble generation. We train and test GPTCast using a 6-year\nradar dataset over the Emilia-Romagna region in Northern Italy, showing\nsuperior results compared to state-of-the-art ensemble extrapolation methods."
                },
                "authors": [
                    {
                        "name": "Gabriele Franch"
                    },
                    {
                        "name": "Elena Tomasi"
                    },
                    {
                        "name": "Rishabh Wanjari"
                    },
                    {
                        "name": "Virginia Poli"
                    },
                    {
                        "name": "Chiara Cardinali"
                    },
                    {
                        "name": "Pier Paolo Alberoni"
                    },
                    {
                        "name": "Marco Cristoforetti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Cristoforetti"
                },
                "author": "Marco Cristoforetti",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09920v2",
                "updated": "2024-09-24T09:48:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    48,
                    36,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-14T11:02:21Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    11,
                    2,
                    21,
                    4,
                    166,
                    0
                ],
                "title": "Knowledge Editing in Language Models via Adapted Direct Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing in Language Models via Adapted Direct Preference\n  Optimization"
                },
                "summary": "Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods."
                },
                "authors": [
                    {
                        "name": "Amit Rozner"
                    },
                    {
                        "name": "Barak Battash"
                    },
                    {
                        "name": "Lior Wolf"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15915v1",
                "updated": "2024-09-24T09:33:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    33,
                    12,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:33:12Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    33,
                    12,
                    1,
                    268,
                    0
                ],
                "title": "Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts"
                },
                "summary": "Large Language Models (LLMs) have shown promise in solving natural\nlanguage-described planning tasks, but their direct use often leads to\ninconsistent reasoning and hallucination. While hybrid LLM-symbolic planning\npipelines have emerged as a more robust alternative, they typically require\nextensive expert intervention to refine and validate generated action schemas.\nIt not only limits scalability but also introduces a potential for biased\ninterpretation, as a single expert's interpretation of ambiguous natural\nlanguage descriptions might not align with the user's actual intent. To address\nthis, we propose a novel approach that constructs an action schema library to\ngenerate multiple candidates, accounting for the diverse possible\ninterpretations of natural language descriptions. We further introduce a\nsemantic validation and ranking module that automatically filter and rank the\ngenerated schemas and plans without expert-in-the-loop. The experiments showed\nour pipeline maintains superiority in planning over the direct LLM planning\napproach. These findings demonstrate the feasibility of a fully automated\nend-to-end LLM-symbolic planner that requires no expert intervention, opening\nup the possibility for a broader audience to engage with AI planning with less\nprerequisite of domain expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in solving natural\nlanguage-described planning tasks, but their direct use often leads to\ninconsistent reasoning and hallucination. While hybrid LLM-symbolic planning\npipelines have emerged as a more robust alternative, they typically require\nextensive expert intervention to refine and validate generated action schemas.\nIt not only limits scalability but also introduces a potential for biased\ninterpretation, as a single expert's interpretation of ambiguous natural\nlanguage descriptions might not align with the user's actual intent. To address\nthis, we propose a novel approach that constructs an action schema library to\ngenerate multiple candidates, accounting for the diverse possible\ninterpretations of natural language descriptions. We further introduce a\nsemantic validation and ranking module that automatically filter and rank the\ngenerated schemas and plans without expert-in-the-loop. The experiments showed\nour pipeline maintains superiority in planning over the direct LLM planning\napproach. These findings demonstrate the feasibility of a fully automated\nend-to-end LLM-symbolic planner that requires no expert intervention, opening\nup the possibility for a broader audience to engage with AI planning with less\nprerequisite of domain expertise."
                },
                "authors": [
                    {
                        "name": "Sukai Huang"
                    },
                    {
                        "name": "Nir Lipovetzky"
                    },
                    {
                        "name": "Trevor Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohn"
                },
                "author": "Trevor Cohn",
                "arxiv_comment": "8 main body pages, 10 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15907v1",
                "updated": "2024-09-24T09:24:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    24,
                    3,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:24:03Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    24,
                    3,
                    1,
                    268,
                    0
                ],
                "title": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection"
                },
                "summary": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper."
                },
                "authors": [
                    {
                        "name": "Xingyu Ma"
                    },
                    {
                        "name": "Xin Tian"
                    },
                    {
                        "name": "Lingxiang Wu"
                    },
                    {
                        "name": "Xuepeng Wang"
                    },
                    {
                        "name": "Xueming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "arxiv_comment": "This paper has been accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15905v1",
                "updated": "2024-09-24T09:20:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    20,
                    22,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:20:22Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    20,
                    22,
                    1,
                    268,
                    0
                ],
                "title": "Boosting Code-Switching ASR with Mixture of Experts Enhanced\n  Speech-Conditioned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Code-Switching ASR with Mixture of Experts Enhanced\n  Speech-Conditioned LLM"
                },
                "summary": "In this paper, we introduce a speech-conditioned Large Language Model (LLM)\nintegrated with a Mixture of Experts (MoE) based connector to address the\nchallenge of Code-Switching (CS) in Automatic Speech Recognition (ASR).\nSpecifically, we propose an Insertion and Deletion of Interruption Token (IDIT)\nmechanism for better transfer text generation ability of LLM to speech\nrecognition task. We also present a connecter with MoE architecture that\nmanages multiple languages efficiently. To further enhance the collaboration of\nmultiple experts and leverage the understanding capabilities of LLM, we propose\na two-stage progressive training strategy: 1) The connector is unfrozen and\ntrained with language-specialized experts to map speech representations to the\ntext space. 2) The connector and LLM LoRA adaptor are trained with the proposed\nIDIT mechanism and all experts are activated to learn general representations.\nExperimental results demonstrate that our method significantly outperforms\nstate-of-the-art models, including end-to-end and large-scale audio-language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a speech-conditioned Large Language Model (LLM)\nintegrated with a Mixture of Experts (MoE) based connector to address the\nchallenge of Code-Switching (CS) in Automatic Speech Recognition (ASR).\nSpecifically, we propose an Insertion and Deletion of Interruption Token (IDIT)\nmechanism for better transfer text generation ability of LLM to speech\nrecognition task. We also present a connecter with MoE architecture that\nmanages multiple languages efficiently. To further enhance the collaboration of\nmultiple experts and leverage the understanding capabilities of LLM, we propose\na two-stage progressive training strategy: 1) The connector is unfrozen and\ntrained with language-specialized experts to map speech representations to the\ntext space. 2) The connector and LLM LoRA adaptor are trained with the proposed\nIDIT mechanism and all experts are activated to learn general representations.\nExperimental results demonstrate that our method significantly outperforms\nstate-of-the-art models, including end-to-end and large-scale audio-language\nmodels."
                },
                "authors": [
                    {
                        "name": "Fengrun Zhang"
                    },
                    {
                        "name": "Wang Geng"
                    },
                    {
                        "name": "Hukai Huang"
                    },
                    {
                        "name": "Cheng Yi"
                    },
                    {
                        "name": "He Qu"
                    }
                ],
                "author_detail": {
                    "name": "He Qu"
                },
                "author": "He Qu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15895v1",
                "updated": "2024-09-24T09:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    15,
                    37,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    15,
                    37,
                    1,
                    268,
                    0
                ],
                "title": "Preference-Guided Refactored Tuning for Retrieval Augmented Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-Guided Refactored Tuning for Retrieval Augmented Code\n  Generation"
                },
                "summary": "Retrieval-augmented code generation utilizes Large Language Models as the\ngenerator and significantly expands their code generation capabilities by\nproviding relevant code, documentation, and more via the retriever. The current\napproach suffers from two primary limitations: 1) information redundancy. The\nindiscriminate inclusion of redundant information can result in resource\nwastage and may misguide generators, affecting their effectiveness and\nefficiency. 2) preference gap. Due to different optimization objectives, the\nretriever strives to procure code with higher ground truth similarity, yet this\neffort does not substantially benefit the generator. The retriever and the\ngenerator may prefer different golden code, and this gap in preference results\nin a suboptimal design. Additionally, differences in parameterization knowledge\nacquired during pre-training result in varying preferences among different\ngenerators.\n  To address these limitations, in this paper, we propose RRG (Retrieve,\nRefactor, Generate), a novel framework for effective and efficient code\ngeneration. This framework introduces a code refactorer module between the\nretriever and the generator to bridge them. The refactoring process transforms\nthe raw retrieved code into a more concise, efficient, and model-friendly\nversion. It eliminates redundant information and noise, reducing the input\nlength. Consequently, the generator receives higher-quality context, enabling\nit to produce more accurate results with lower inference costs. We conducted\ncomprehensive experiments on multiple datasets. In the experiments, we\nconfirmed the existence of a preference gap between the retriever and the\ngenerator, and RRG effectively bridges this gap. Specifically, RRG achieved\nsignificant performance improvements, with increases of up to 28% on EM, 13% on\nBLEU, and 6.8% on CodeBLEU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented code generation utilizes Large Language Models as the\ngenerator and significantly expands their code generation capabilities by\nproviding relevant code, documentation, and more via the retriever. The current\napproach suffers from two primary limitations: 1) information redundancy. The\nindiscriminate inclusion of redundant information can result in resource\nwastage and may misguide generators, affecting their effectiveness and\nefficiency. 2) preference gap. Due to different optimization objectives, the\nretriever strives to procure code with higher ground truth similarity, yet this\neffort does not substantially benefit the generator. The retriever and the\ngenerator may prefer different golden code, and this gap in preference results\nin a suboptimal design. Additionally, differences in parameterization knowledge\nacquired during pre-training result in varying preferences among different\ngenerators.\n  To address these limitations, in this paper, we propose RRG (Retrieve,\nRefactor, Generate), a novel framework for effective and efficient code\ngeneration. This framework introduces a code refactorer module between the\nretriever and the generator to bridge them. The refactoring process transforms\nthe raw retrieved code into a more concise, efficient, and model-friendly\nversion. It eliminates redundant information and noise, reducing the input\nlength. Consequently, the generator receives higher-quality context, enabling\nit to produce more accurate results with lower inference costs. We conducted\ncomprehensive experiments on multiple datasets. In the experiments, we\nconfirmed the existence of a preference gap between the retriever and the\ngenerator, and RRG effectively bridges this gap. Specifically, RRG achieved\nsignificant performance improvements, with increases of up to 28% on EM, 13% on\nBLEU, and 6.8% on CodeBLEU."
                },
                "authors": [
                    {
                        "name": "Xinyu Gao"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Deze Wang"
                    },
                    {
                        "name": "Zhenhan Guan"
                    },
                    {
                        "name": "Zejian Shi"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Shanshan Li"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Li"
                },
                "author": "Shanshan Li",
                "arxiv_comment": "ASE2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15890v1",
                "updated": "2024-09-24T09:02:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    2,
                    28,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:02:28Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    2,
                    28,
                    1,
                    268,
                    0
                ],
                "title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLB: Benchmarking LLMs' Humanlikeness in Language Use"
                },
                "summary": "As synthetic data becomes increasingly prevalent in training language models,\nparticularly through generated dialogue, concerns have emerged that these\nmodels may deviate from authentic human language patterns, potentially losing\nthe richness and creativity inherent in human communication. This highlights\nthe critical need to assess the humanlikeness of language models in real-world\nlanguage use. In this paper, we present a comprehensive humanlikeness benchmark\n(HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic\nexperiments designed to probe core linguistic aspects, including sound, word,\nsyntax, semantics, and discourse (see\nhttps://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these\ncomparisons, we collected responses from over 2,000 human participants and\ncompared them to outputs from the LLMs in these experiments.\n  For rigorous evaluation, we developed a coding algorithm that accurately\nidentified language use patterns, enabling the extraction of response\ndistributions for each task. By comparing the response distributions between\nhuman participants and LLMs, we quantified humanlikeness through distributional\nsimilarity. Our results reveal fine-grained differences in how well LLMs\nreplicate human responses across various linguistic levels. Importantly, we\nfound that improvements in other performance metrics did not necessarily lead\nto greater humanlikeness, and in some cases, even resulted in a decline. By\nintroducing psycholinguistic methods to model evaluation, this benchmark offers\nthe first framework for systematically assessing the humanlikeness of LLMs in\nlanguage use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As synthetic data becomes increasingly prevalent in training language models,\nparticularly through generated dialogue, concerns have emerged that these\nmodels may deviate from authentic human language patterns, potentially losing\nthe richness and creativity inherent in human communication. This highlights\nthe critical need to assess the humanlikeness of language models in real-world\nlanguage use. In this paper, we present a comprehensive humanlikeness benchmark\n(HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic\nexperiments designed to probe core linguistic aspects, including sound, word,\nsyntax, semantics, and discourse (see\nhttps://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these\ncomparisons, we collected responses from over 2,000 human participants and\ncompared them to outputs from the LLMs in these experiments.\n  For rigorous evaluation, we developed a coding algorithm that accurately\nidentified language use patterns, enabling the extraction of response\ndistributions for each task. By comparing the response distributions between\nhuman participants and LLMs, we quantified humanlikeness through distributional\nsimilarity. Our results reveal fine-grained differences in how well LLMs\nreplicate human responses across various linguistic levels. Importantly, we\nfound that improvements in other performance metrics did not necessarily lead\nto greater humanlikeness, and in some cases, even resulted in a decline. By\nintroducing psycholinguistic methods to model evaluation, this benchmark offers\nthe first framework for systematically assessing the humanlikeness of LLMs in\nlanguage use."
                },
                "authors": [
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Bei Xiao"
                    },
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15884v1",
                "updated": "2024-09-24T08:56:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    56,
                    25,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:56:25Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    56,
                    25,
                    1,
                    268,
                    0
                ],
                "title": "Interpolation filter design for sample rate independent audio effect\n  RNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpolation filter design for sample rate independent audio effect\n  RNNs"
                },
                "summary": "Recurrent neural networks (RNNs) are effective at emulating the non-linear,\nstateful behavior of analog guitar amplifiers and distortion effects. Unlike\nthe case of direct circuit simulation, RNNs have a fixed sample rate encoded in\ntheir model weights, making the sample rate non-adjustable during inference.\nRecent work has proposed increasing the sample rate of RNNs at inference\n(oversampling) by increasing the feedback delay length in samples, using a\nfractional delay filter for non-integer conversions. Here, we investigate the\ntask of lowering the sample rate at inference (undersampling), and propose\nusing an extrapolation filter to approximate the required fractional signal\nadvance. We consider two filter design methods and analyze the impact of filter\norder on audio quality. Our results show that the correct choice of filter can\ngive high quality results for both oversampling and undersampling; however, in\nsome cases the sample rate adjustment leads to unwanted artefacts in the output\nsignal. We analyse these failure cases through linearised stability analysis,\nshowing that they result from instability around a fixed point. This approach\nenables an informed prediction of suitable interpolation filters for a given\nRNN model before runtime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent neural networks (RNNs) are effective at emulating the non-linear,\nstateful behavior of analog guitar amplifiers and distortion effects. Unlike\nthe case of direct circuit simulation, RNNs have a fixed sample rate encoded in\ntheir model weights, making the sample rate non-adjustable during inference.\nRecent work has proposed increasing the sample rate of RNNs at inference\n(oversampling) by increasing the feedback delay length in samples, using a\nfractional delay filter for non-integer conversions. Here, we investigate the\ntask of lowering the sample rate at inference (undersampling), and propose\nusing an extrapolation filter to approximate the required fractional signal\nadvance. We consider two filter design methods and analyze the impact of filter\norder on audio quality. Our results show that the correct choice of filter can\ngive high quality results for both oversampling and undersampling; however, in\nsome cases the sample rate adjustment leads to unwanted artefacts in the output\nsignal. We analyse these failure cases through linearised stability analysis,\nshowing that they result from instability around a fixed point. This approach\nenables an informed prediction of suitable interpolation filters for a given\nRNN model before runtime."
                },
                "authors": [
                    {
                        "name": "Alistair Carson"
                    },
                    {
                        "name": "Alec Wright"
                    },
                    {
                        "name": "Stefan Bilbao"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bilbao"
                },
                "author": "Stefan Bilbao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15883v1",
                "updated": "2024-09-24T08:56:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    56,
                    10,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:56:10Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    56,
                    10,
                    1,
                    268,
                    0
                ],
                "title": "Unsupervised dMRI Artifact Detection via Angular Resolution Enhancement\n  and Cycle Consistency Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised dMRI Artifact Detection via Angular Resolution Enhancement\n  and Cycle Consistency Learning"
                },
                "summary": "Diffusion magnetic resonance imaging (dMRI) is a crucial technique in\nneuroimaging studies, allowing for the non-invasive probing of the underlying\nstructures of brain tissues. Clinical dMRI data is susceptible to various\nartifacts during acquisition, which can lead to unreliable subsequent analyses.\nTherefore, dMRI preprocessing is essential for improving image quality, and\nmanual inspection is often required to ensure that the preprocessed data is\nsufficiently corrected. However, manual inspection requires expertise and is\ntime-consuming, especially with large-scale dMRI datasets. Given these\nchallenges, an automated dMRI artifact detection tool is necessary to increase\nthe productivity and reliability of dMRI data analysis. To this end, we propose\na novel unsupervised deep learning framework called $\\textbf{U}$nsupervised\n$\\textbf{d}$MRI $\\textbf{A}$rtifact $\\textbf{D}$etection via $\\textbf{A}$ngular\nResolution Enhancement and $\\textbf{C}$ycle Consistency Learning (UdAD-AC).\nUdAD-AC leverages dMRI angular resolution enhancement and cycle consistency\nlearning to capture the effective representation of artifact-free dMRI data\nduring training, and it identifies data containing artifacts using designed\nconfidence score during inference. To assess the capability of UdAD-AC, several\ncommonly reported dMRI artifacts, including bias field, susceptibility\ndistortion, and corrupted volume, were added to the testing data. Experimental\nresults demonstrate that UdAD-AC achieves the best performance compared to\ncompetitive methods in unsupervised dMRI artifact detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion magnetic resonance imaging (dMRI) is a crucial technique in\nneuroimaging studies, allowing for the non-invasive probing of the underlying\nstructures of brain tissues. Clinical dMRI data is susceptible to various\nartifacts during acquisition, which can lead to unreliable subsequent analyses.\nTherefore, dMRI preprocessing is essential for improving image quality, and\nmanual inspection is often required to ensure that the preprocessed data is\nsufficiently corrected. However, manual inspection requires expertise and is\ntime-consuming, especially with large-scale dMRI datasets. Given these\nchallenges, an automated dMRI artifact detection tool is necessary to increase\nthe productivity and reliability of dMRI data analysis. To this end, we propose\na novel unsupervised deep learning framework called $\\textbf{U}$nsupervised\n$\\textbf{d}$MRI $\\textbf{A}$rtifact $\\textbf{D}$etection via $\\textbf{A}$ngular\nResolution Enhancement and $\\textbf{C}$ycle Consistency Learning (UdAD-AC).\nUdAD-AC leverages dMRI angular resolution enhancement and cycle consistency\nlearning to capture the effective representation of artifact-free dMRI data\nduring training, and it identifies data containing artifacts using designed\nconfidence score during inference. To assess the capability of UdAD-AC, several\ncommonly reported dMRI artifacts, including bias field, susceptibility\ndistortion, and corrupted volume, were added to the testing data. Experimental\nresults demonstrate that UdAD-AC achieves the best performance compared to\ncompetitive methods in unsupervised dMRI artifact detection."
                },
                "authors": [
                    {
                        "name": "Sheng Chen"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Weidong Cai"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Cai"
                },
                "author": "Weidong Cai",
                "arxiv_comment": "Accepted to AJCAI2024, dMRI, Unsupervised artifact detection, Angular\n  resolution enhancement, Cycle consistency",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15881v1",
                "updated": "2024-09-24T08:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    55,
                    7,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    55,
                    7,
                    1,
                    268,
                    0
                ],
                "title": "Automatic Bottom-Up Taxonomy Construction: A Software Application Domain\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Bottom-Up Taxonomy Construction: A Software Application Domain\n  Study"
                },
                "summary": "Previous research in software application domain classification has faced\nchallenges due to the lack of a proper taxonomy that explicitly models\nrelations between classes. As a result, current solutions are less effective\nfor real-world usage. This study aims to develop a comprehensive software\napplication domain taxonomy by integrating multiple datasources and leveraging\nensemble methods. The goal is to overcome the limitations of individual sources\nand configurations by creating a more robust, accurate, and reproducible\ntaxonomy. This study employs a quantitative research design involving three\ndifferent datasources: an existing Computer Science Ontology (CSO), Wikidata,\nand LLMs. The study utilises a combination of automated and human evaluations\nto assess the quality of a taxonomy. The outcome measures include the number of\nunlinked terms, self-loops, and overall connectivity of the taxonomy. The\nresults indicate that individual datasources have advantages and drawbacks: the\nCSO datasource showed minimal variance across different configurations, but a\nnotable issue of missing technical terms and a high number of self-loops. The\nWikipedia datasource required significant filtering during construction to\nimprove metric performance. LLM-generated taxonomies demonstrated better\nperformance when using context-rich prompts. An ensemble approach showed the\nmost promise, successfully reducing the number of unlinked terms and\nself-loops, thus creating a more connected and comprehensive taxonomy. The\nstudy addresses the construction of a software application domain taxonomy\nrelying on pre-existing resources. Our results indicate that an ensemble\napproach to taxonomy construction can effectively address the limitations of\nindividual datasources. Future work should focus on refining the ensemble\ntechniques and exploring additional datasources to enhance the taxonomy's\naccuracy and completeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research in software application domain classification has faced\nchallenges due to the lack of a proper taxonomy that explicitly models\nrelations between classes. As a result, current solutions are less effective\nfor real-world usage. This study aims to develop a comprehensive software\napplication domain taxonomy by integrating multiple datasources and leveraging\nensemble methods. The goal is to overcome the limitations of individual sources\nand configurations by creating a more robust, accurate, and reproducible\ntaxonomy. This study employs a quantitative research design involving three\ndifferent datasources: an existing Computer Science Ontology (CSO), Wikidata,\nand LLMs. The study utilises a combination of automated and human evaluations\nto assess the quality of a taxonomy. The outcome measures include the number of\nunlinked terms, self-loops, and overall connectivity of the taxonomy. The\nresults indicate that individual datasources have advantages and drawbacks: the\nCSO datasource showed minimal variance across different configurations, but a\nnotable issue of missing technical terms and a high number of self-loops. The\nWikipedia datasource required significant filtering during construction to\nimprove metric performance. LLM-generated taxonomies demonstrated better\nperformance when using context-rich prompts. An ensemble approach showed the\nmost promise, successfully reducing the number of unlinked terms and\nself-loops, thus creating a more connected and comprehensive taxonomy. The\nstudy addresses the construction of a software application domain taxonomy\nrelying on pre-existing resources. Our results indicate that an ensemble\napproach to taxonomy construction can effectively address the limitations of\nindividual datasources. Future work should focus on refining the ensemble\ntechniques and exploring additional datasources to enhance the taxonomy's\naccuracy and completeness."
                },
                "authors": [
                    {
                        "name": "Cezar Sas"
                    },
                    {
                        "name": "Andrea Capiluppi"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Capiluppi"
                },
                "author": "Andrea Capiluppi",
                "arxiv_comment": "17 pages, 8 tables, 6 figures, and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12623v2",
                "updated": "2024-09-24T08:49:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    49,
                    21,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-19T09:52:35Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    9,
                    52,
                    35,
                    3,
                    263,
                    0
                ],
                "title": "CamelEval: Advancing Culturally Aligned Arabic Language Models and\n  Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CamelEval: Advancing Culturally Aligned Arabic Language Models and\n  Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are the cornerstones of modern artificial\nintelligence systems. This paper introduces Juhaina, a Arabic-English bilingual\nLLM specifically designed to align with the values and preferences of Arabic\nspeakers. Juhaina inherently supports advanced functionalities such as\ninstruction following, open-ended question answering, information provisioning,\nand text processing. Our model contains 9.24 billion parameters and is trained\non a context window of up to 8,192 tokens. This paper details the creation\nprocess of Juhaina and provides an extensive empirical evaluation. Furthermore,\nwe identify the limitations of widely-adopted Open Arabic LLM Leaderboard\n(OALL) and propose a new evaluation benchmark, CamelEval. Our findings\ndemonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as\nthe Llama and Gemma families, in generating helpful responses in Arabic,\nproviding factually accurate information about the region, and understanding\nnuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI\ntechnologies, serving over 400 million Arabic speakers by offering LLMs that\nnot only communicate in their language but also comprehend their culture. We\npublicly release all models on Huggingface \\url{https://huggingface.co/elmrc}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are the cornerstones of modern artificial\nintelligence systems. This paper introduces Juhaina, a Arabic-English bilingual\nLLM specifically designed to align with the values and preferences of Arabic\nspeakers. Juhaina inherently supports advanced functionalities such as\ninstruction following, open-ended question answering, information provisioning,\nand text processing. Our model contains 9.24 billion parameters and is trained\non a context window of up to 8,192 tokens. This paper details the creation\nprocess of Juhaina and provides an extensive empirical evaluation. Furthermore,\nwe identify the limitations of widely-adopted Open Arabic LLM Leaderboard\n(OALL) and propose a new evaluation benchmark, CamelEval. Our findings\ndemonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as\nthe Llama and Gemma families, in generating helpful responses in Arabic,\nproviding factually accurate information about the region, and understanding\nnuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI\ntechnologies, serving over 400 million Arabic speakers by offering LLMs that\nnot only communicate in their language but also comprehend their culture. We\npublicly release all models on Huggingface \\url{https://huggingface.co/elmrc}."
                },
                "authors": [
                    {
                        "name": "Zhaozhi Qian"
                    },
                    {
                        "name": "Faroq Altam"
                    },
                    {
                        "name": "Muhammad Alqurishi"
                    },
                    {
                        "name": "Riad Souissi"
                    }
                ],
                "author_detail": {
                    "name": "Riad Souissi"
                },
                "author": "Riad Souissi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15869v1",
                "updated": "2024-09-24T08:42:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    42,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:42:31Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    42,
                    31,
                    1,
                    268,
                    0
                ],
                "title": "Whisper in Medusa's Ear: Multi-head Efficient Decoding for\n  Transformer-based ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whisper in Medusa's Ear: Multi-head Efficient Decoding for\n  Transformer-based ASR"
                },
                "summary": "Large transformer-based models have significant potential for speech\ntranscription and translation. Their self-attention mechanisms and parallel\nprocessing enable them to capture complex patterns and dependencies in audio\nsequences. However, this potential comes with challenges, as these large and\ncomputationally intensive models lead to slow inference speeds. Various\noptimization strategies have been proposed to improve performance, including\nefficient hardware utilization and algorithmic enhancements. In this paper, we\nintroduce Whisper-Medusa, a novel approach designed to enhance processing speed\nwith minimal impact on Word Error Rate (WER). The proposed model extends the\nOpenAI's Whisper architecture by predicting multiple tokens per iteration,\nresulting in a 50% reduction in latency. We showcase the effectiveness of\nWhisper-Medusa across different learning setups and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large transformer-based models have significant potential for speech\ntranscription and translation. Their self-attention mechanisms and parallel\nprocessing enable them to capture complex patterns and dependencies in audio\nsequences. However, this potential comes with challenges, as these large and\ncomputationally intensive models lead to slow inference speeds. Various\noptimization strategies have been proposed to improve performance, including\nefficient hardware utilization and algorithmic enhancements. In this paper, we\nintroduce Whisper-Medusa, a novel approach designed to enhance processing speed\nwith minimal impact on Word Error Rate (WER). The proposed model extends the\nOpenAI's Whisper architecture by predicting multiple tokens per iteration,\nresulting in a 50% reduction in latency. We showcase the effectiveness of\nWhisper-Medusa across different learning setups and datasets."
                },
                "authors": [
                    {
                        "name": "Yael Segal-Feldman"
                    },
                    {
                        "name": "Aviv Shamsian"
                    },
                    {
                        "name": "Aviv Navon"
                    },
                    {
                        "name": "Gill Hetz"
                    },
                    {
                        "name": "Joseph Keshet"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Keshet"
                },
                "author": "Joseph Keshet",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15868v2",
                "updated": "2024-09-25T07:03:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T08:41:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    41,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Privacy Evaluation Benchmarks for NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Evaluation Benchmarks for NLP Models"
                },
                "summary": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "arxiv_comment": "Needs further optimization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15865v1",
                "updated": "2024-09-24T08:37:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    37,
                    4,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:37:04Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    37,
                    4,
                    1,
                    268,
                    0
                ],
                "title": "BeSimulator: A Large Language Model Powered Text-based Behavior\n  Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeSimulator: A Large Language Model Powered Text-based Behavior\n  Simulator"
                },
                "summary": "Traditional robot simulators focus on physical process modeling and realistic\nrendering, often suffering from high computational costs, inefficiencies, and\nlimited adaptability. To handle this issue, we propose Behavior Simulation in\nrobotics to emphasize checking the behavior logic of robots and achieving\nsufficient alignment between the outcome of robot actions and real scenarios.\nIn this paper, we introduce BeSimulator, a modular and novel LLM-powered\nframework, as an attempt towards behavior simulation in the context of\ntext-based environments. By constructing text-based virtual environments and\nperforming semantic-level simulation, BeSimulator can generalize across\nscenarios and achieve long-horizon complex simulation. Inspired by human\ncognition processes, it employs a \"consider-decide-capture-transfer\"\nmethodology, termed Chain of Behavior Simulation, which excels at analyzing\naction feasibility and state transitions. Additionally, BeSimulator\nincorporates code-driven reasoning to enable arithmetic operations and enhance\nreliability, as well as integrates reflective feedback to refine simulation.\nBased on our manually constructed behavior-tree-based simulation benchmark\nBTSIMBENCH, our experiments show a significant performance improvement in\nbehavior simulation compared to baselines, ranging from 14.7% to 26.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional robot simulators focus on physical process modeling and realistic\nrendering, often suffering from high computational costs, inefficiencies, and\nlimited adaptability. To handle this issue, we propose Behavior Simulation in\nrobotics to emphasize checking the behavior logic of robots and achieving\nsufficient alignment between the outcome of robot actions and real scenarios.\nIn this paper, we introduce BeSimulator, a modular and novel LLM-powered\nframework, as an attempt towards behavior simulation in the context of\ntext-based environments. By constructing text-based virtual environments and\nperforming semantic-level simulation, BeSimulator can generalize across\nscenarios and achieve long-horizon complex simulation. Inspired by human\ncognition processes, it employs a \"consider-decide-capture-transfer\"\nmethodology, termed Chain of Behavior Simulation, which excels at analyzing\naction feasibility and state transitions. Additionally, BeSimulator\nincorporates code-driven reasoning to enable arithmetic operations and enhance\nreliability, as well as integrates reflective feedback to refine simulation.\nBased on our manually constructed behavior-tree-based simulation benchmark\nBTSIMBENCH, our experiments show a significant performance improvement in\nbehavior simulation compared to baselines, ranging from 14.7% to 26.6%."
                },
                "authors": [
                    {
                        "name": "Jianan Wang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Xueying Wang"
                    },
                    {
                        "name": "Fu Li"
                    },
                    {
                        "name": "Yunlong Wu"
                    },
                    {
                        "name": "Juan Chen"
                    },
                    {
                        "name": "Xiaodong Yi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Yi"
                },
                "author": "Xiaodong Yi",
                "arxiv_comment": "7 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15861v1",
                "updated": "2024-09-24T08:33:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    33,
                    41,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:33:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    33,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding"
                },
                "summary": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API."
                },
                "authors": [
                    {
                        "name": "Abdulfattah Safa"
                    },
                    {
                        "name": "G√∂zde G√ºl ≈ûahin"
                    }
                ],
                "author_detail": {
                    "name": "G√∂zde G√ºl ≈ûahin"
                },
                "author": "G√∂zde G√ºl ≈ûahin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.05004v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.05004v4",
                "updated": "2024-09-24T08:29:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    29,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2023-09-10T11:36:01Z",
                "published_parsed": [
                    2023,
                    9,
                    10,
                    11,
                    36,
                    1,
                    6,
                    253,
                    0
                ],
                "title": "Reconstructing the kinetic chemotaxis kernel using macroscopic data:\n  well-posedness and ill-posedness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the kinetic chemotaxis kernel using macroscopic data:\n  well-posedness and ill-posedness"
                },
                "summary": "Bacterial motion is steered by external stimuli (chemotaxis), and the motion\ndescribed on the mesoscopic scale is uniquely determined by a parameter $K$\nthat models velocity change response from the bacteria. This parameter is\ncalled chemotaxis kernel. In a practical setting, it is inferred by\nexperimental data. We deploy a PDE-constrained optimization framework to\nperform this reconstruction using velocity-averaged, localized data taken in\nthe interior of the domain. The problem can be well-posed or ill-posed\ndepending on the data preparation and the experimental setup. In particular, we\npropose one specific design that guarantees numerical reconstructability and\nlocal convergence. This design is adapted to the discretization of $K$ in space\nand decouples the reconstruction of local values of $K$ into smaller cell\nproblems, opening up parallelization opportunities. Numerical evidences support\nthe theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bacterial motion is steered by external stimuli (chemotaxis), and the motion\ndescribed on the mesoscopic scale is uniquely determined by a parameter $K$\nthat models velocity change response from the bacteria. This parameter is\ncalled chemotaxis kernel. In a practical setting, it is inferred by\nexperimental data. We deploy a PDE-constrained optimization framework to\nperform this reconstruction using velocity-averaged, localized data taken in\nthe interior of the domain. The problem can be well-posed or ill-posed\ndepending on the data preparation and the experimental setup. In particular, we\npropose one specific design that guarantees numerical reconstructability and\nlocal convergence. This design is adapted to the discretization of $K$ in space\nand decouples the reconstruction of local values of $K$ into smaller cell\nproblems, opening up parallelization opportunities. Numerical evidences support\nthe theoretical findings."
                },
                "authors": [
                    {
                        "name": "Kathrin Hellmuth"
                    },
                    {
                        "name": "Christian Klingenberg"
                    },
                    {
                        "name": "Qin Li"
                    },
                    {
                        "name": "Min Tang"
                    }
                ],
                "author_detail": {
                    "name": "Min Tang"
                },
                "author": "Min Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.05004v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.05004v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35R30, 65M32, 92C17, 49M41, 49K40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13731v2",
                "updated": "2024-09-24T08:24:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    24,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-10T02:00:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    0,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation"
                },
                "summary": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods."
                },
                "authors": [
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "Zhongshu Zhu"
                    },
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Ling Zhong"
                    },
                    {
                        "name": "Yuan Qu"
                    },
                    {
                        "name": "Peilong Zhao"
                    },
                    {
                        "name": "Zhongpu Bo"
                    },
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Huaidong Xiong"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Zaoyang Wang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wenguang Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15849v1",
                "updated": "2024-09-24T08:20:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    20,
                    56,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:20:56Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    20,
                    56,
                    1,
                    268,
                    0
                ],
                "title": "Twin Network Augmentation: A Novel Training Strategy for Improved\n  Spiking Neural Networks and Efficient Weight Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twin Network Augmentation: A Novel Training Strategy for Improved\n  Spiking Neural Networks and Efficient Weight Quantization"
                },
                "summary": "The proliferation of Artificial Neural Networks (ANNs) has led to increased\nenergy consumption, raising concerns about their sustainability. Spiking Neural\nNetworks (SNNs), which are inspired by biological neural systems and operate\nusing sparse, event-driven spikes to communicate information between neurons,\noffer a potential solution due to their lower energy requirements. An\nalternative technique for reducing a neural network's footprint is\nquantization, which compresses weight representations to decrease memory usage\nand energy consumption. In this study, we present Twin Network Augmentation\n(TNA), a novel training framework aimed at improving the performance of SNNs\nwhile also facilitating an enhanced compression through low-precision\nquantization of weights. TNA involves co-training an SNN with a twin network,\noptimizing both networks to minimize their cross-entropy losses and the mean\nsquared error between their output logits. We demonstrate that TNA\nsignificantly enhances classification performance across various vision\ndatasets and in addition is particularly effective when applied when reducing\nSNNs to ternary weight precision. Notably, during inference , only the ternary\nSNN is retained, significantly reducing the network in number of neurons,\nconnectivity and weight size representation. Our results show that TNA\noutperforms traditional knowledge distillation methods and achieves\nstate-of-the-art performance for the evaluated network architecture on\nbenchmark datasets, including CIFAR-10, CIFAR-100, and CIFAR-10-DVS. This paper\nunderscores the effectiveness of TNA in bridging the performance gap between\nSNNs and ANNs and suggests further exploration into the application of TNA in\ndifferent network architectures and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Artificial Neural Networks (ANNs) has led to increased\nenergy consumption, raising concerns about their sustainability. Spiking Neural\nNetworks (SNNs), which are inspired by biological neural systems and operate\nusing sparse, event-driven spikes to communicate information between neurons,\noffer a potential solution due to their lower energy requirements. An\nalternative technique for reducing a neural network's footprint is\nquantization, which compresses weight representations to decrease memory usage\nand energy consumption. In this study, we present Twin Network Augmentation\n(TNA), a novel training framework aimed at improving the performance of SNNs\nwhile also facilitating an enhanced compression through low-precision\nquantization of weights. TNA involves co-training an SNN with a twin network,\noptimizing both networks to minimize their cross-entropy losses and the mean\nsquared error between their output logits. We demonstrate that TNA\nsignificantly enhances classification performance across various vision\ndatasets and in addition is particularly effective when applied when reducing\nSNNs to ternary weight precision. Notably, during inference , only the ternary\nSNN is retained, significantly reducing the network in number of neurons,\nconnectivity and weight size representation. Our results show that TNA\noutperforms traditional knowledge distillation methods and achieves\nstate-of-the-art performance for the evaluated network architecture on\nbenchmark datasets, including CIFAR-10, CIFAR-100, and CIFAR-10-DVS. This paper\nunderscores the effectiveness of TNA in bridging the performance gap between\nSNNs and ANNs and suggests further exploration into the application of TNA in\ndifferent network architectures and datasets."
                },
                "authors": [
                    {
                        "name": "Lucas Deckers"
                    },
                    {
                        "name": "Benjamin Vandersmissen"
                    },
                    {
                        "name": "Ing Jyh Tsang"
                    },
                    {
                        "name": "Werner Van Leekwijck"
                    },
                    {
                        "name": "Steven Latr√©"
                    }
                ],
                "author_detail": {
                    "name": "Steven Latr√©"
                },
                "author": "Steven Latr√©",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15113v2",
                "updated": "2024-09-24T08:18:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    18,
                    34,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T15:20:27Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    20,
                    27,
                    0,
                    267,
                    0
                ],
                "title": "Inferring Scientific Cross-Document Coreference and Hierarchy with\n  Definition-Augmented Relational Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Scientific Cross-Document Coreference and Hierarchy with\n  Definition-Augmented Relational Reasoning"
                },
                "summary": "We address the fundamental task of inferring cross-document coreference and\nhierarchy in scientific texts, which has important applications in knowledge\ngraph construction, search, recommendation and discovery. LLMs can struggle\nwhen faced with many long-tail technical concepts with nuanced variations. We\npresent a novel method which generates context-dependent definitions of concept\nmentions by retrieving full-text literature, and uses the definitions to\nenhance detection of cross-document relations. We further generate relational\ndefinitions, which describe how two concept mentions are related or different,\nand design an efficient re-ranking approach to address the combinatorial\nexplosion involved in inferring links across papers. In both fine-tuning and\nin-context learning settings we achieve large gains in performance. We provide\nanalysis of generated definitions, shedding light on the relational reasoning\nability of LLMs over fine-grained scientific concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the fundamental task of inferring cross-document coreference and\nhierarchy in scientific texts, which has important applications in knowledge\ngraph construction, search, recommendation and discovery. LLMs can struggle\nwhen faced with many long-tail technical concepts with nuanced variations. We\npresent a novel method which generates context-dependent definitions of concept\nmentions by retrieving full-text literature, and uses the definitions to\nenhance detection of cross-document relations. We further generate relational\ndefinitions, which describe how two concept mentions are related or different,\nand design an efficient re-ranking approach to address the combinatorial\nexplosion involved in inferring links across papers. In both fine-tuning and\nin-context learning settings we achieve large gains in performance. We provide\nanalysis of generated definitions, shedding light on the relational reasoning\nability of LLMs over fine-grained scientific concepts."
                },
                "authors": [
                    {
                        "name": "Lior Forer"
                    },
                    {
                        "name": "Tom Hope"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hope"
                },
                "author": "Tom Hope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15846v1",
                "updated": "2024-09-24T08:17:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    17,
                    50,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:17:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    17,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "Potential Field as Scene Affordance for Behavior Change-Based Visual\n  Risk Object Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential Field as Scene Affordance for Behavior Change-Based Visual\n  Risk Object Identification"
                },
                "summary": "We study behavior change-based visual risk object identification\n(Visual-ROI), a critical framework designed to detect potential hazards for\nintelligent driving systems. Existing methods often show significant\nlimitations in spatial accuracy and temporal consistency, stemming from an\nincomplete understanding of scene affordance. For example, these methods\nfrequently misidentify vehicles that do not impact the ego vehicle as risk\nobjects. Furthermore, existing behavior change-based methods are inefficient\nbecause they implement causal inference in the perspective image space. We\npropose a new framework with a Bird's Eye View (BEV) representation to overcome\nthe above challenges. Specifically, we utilize potential fields as scene\naffordance, involving repulsive forces derived from road infrastructure and\ntraffic participants, along with attractive forces sourced from target\ndestinations. In this work, we compute potential fields by assigning different\nenergy levels according to the semantic labels obtained from BEV semantic\nsegmentation. We conduct thorough experiments and ablation studies, comparing\nthe proposed method with various state-of-the-art algorithms on both synthetic\nand real-world datasets. Our results show a notable increase in spatial and\ntemporal consistency, with enhancements of 20.3% and 11.6% on the RiskBench\ndataset, respectively. Additionally, we can improve computational efficiency by\n88%. We achieve improvements of 5.4% in spatial accuracy and 7.2% in temporal\nconsistency on the nuScenes dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study behavior change-based visual risk object identification\n(Visual-ROI), a critical framework designed to detect potential hazards for\nintelligent driving systems. Existing methods often show significant\nlimitations in spatial accuracy and temporal consistency, stemming from an\nincomplete understanding of scene affordance. For example, these methods\nfrequently misidentify vehicles that do not impact the ego vehicle as risk\nobjects. Furthermore, existing behavior change-based methods are inefficient\nbecause they implement causal inference in the perspective image space. We\npropose a new framework with a Bird's Eye View (BEV) representation to overcome\nthe above challenges. Specifically, we utilize potential fields as scene\naffordance, involving repulsive forces derived from road infrastructure and\ntraffic participants, along with attractive forces sourced from target\ndestinations. In this work, we compute potential fields by assigning different\nenergy levels according to the semantic labels obtained from BEV semantic\nsegmentation. We conduct thorough experiments and ablation studies, comparing\nthe proposed method with various state-of-the-art algorithms on both synthetic\nand real-world datasets. Our results show a notable increase in spatial and\ntemporal consistency, with enhancements of 20.3% and 11.6% on the RiskBench\ndataset, respectively. Additionally, we can improve computational efficiency by\n88%. We achieve improvements of 5.4% in spatial accuracy and 7.2% in temporal\nconsistency on the nuScenes dataset."
                },
                "authors": [
                    {
                        "name": "Pang-Yuan Pao"
                    },
                    {
                        "name": "Shu-Wei Lu"
                    },
                    {
                        "name": "Ze-Yan Lu"
                    },
                    {
                        "name": "Yi-Ting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Ting Chen"
                },
                "author": "Yi-Ting Chen",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14605v2",
                "updated": "2024-09-24T08:02:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    2,
                    21,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T21:52:58Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    21,
                    52,
                    58,
                    6,
                    266,
                    0
                ],
                "title": "First Field Trial of LLM-Powered AI Agent for Lifecycle Management of\n  Autonomous Driving Optical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Field Trial of LLM-Powered AI Agent for Lifecycle Management of\n  Autonomous Driving Optical Networks"
                },
                "summary": "We design and demonstrate the first field trial of LLM-powered AI Agent for\nADON. Three operation modes of the Agent are proposed for network lifecycle\nmanagement. The Agent efficiently processes wavelength add/drop and soft/hard\nfailures, and achieves comparable performance to human-designed algorithms for\npower optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design and demonstrate the first field trial of LLM-powered AI Agent for\nADON. Three operation modes of the Agent are proposed for network lifecycle\nmanagement. The Agent efficiently processes wavelength add/drop and soft/hard\nfailures, and achieves comparable performance to human-designed algorithms for\npower optimization."
                },
                "authors": [
                    {
                        "name": "Xiaomin Liu"
                    },
                    {
                        "name": "Qizhi Qiu"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Yuming Cheng"
                    },
                    {
                        "name": "Lilin Yi"
                    },
                    {
                        "name": "Weisheng Hu"
                    },
                    {
                        "name": "Qunbi Zhuge"
                    }
                ],
                "author_detail": {
                    "name": "Qunbi Zhuge"
                },
                "author": "Qunbi Zhuge",
                "arxiv_comment": "Version submitted to ECOC PDP 2024 on September 6th",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10918v2",
                "updated": "2024-09-24T07:44:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    44,
                    27,
                    1,
                    268,
                    0
                ],
                "published": "2024-08-20T15:03:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    3,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CHECKWHY: Causal Fact Verification via Argument Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHECKWHY: Causal Fact Verification via Argument Structure"
                },
                "summary": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Jiasheng Si"
                    },
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Haiyang Zhu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Accepted by ACL2024; Awarded as Outstanding Paper Award and Area\n  Chair Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08438v2",
                "updated": "2024-09-24T07:41:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    41,
                    19,
                    1,
                    268,
                    0
                ],
                "published": "2024-01-06T03:59:59Z",
                "published_parsed": [
                    2024,
                    1,
                    6,
                    3,
                    59,
                    59,
                    5,
                    6,
                    0
                ],
                "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language\n  Models"
                },
                "summary": "Cognitive dynamics are pivotal to advance human understanding of the world.\nRecent advancements in large language models (LLMs) reveal their potential for\ncognitive simulation. However, these LLM-based cognitive studies primarily\nfocus on static modeling, overlooking the dynamic nature of cognition. To\nbridge this gap, we propose the concept of the cognitive dynamics of LLMs and\npresent a corresponding task with the inspiration of longitudinal studies.\nTowards the task, we develop CogBench, a novel benchmark to assess the\ncognitive dynamics of LLMs and validate it through participant surveys. We also\ndesign two evaluation metrics for CogBench, including Authenticity and\nRationality. Recognizing the inherent static nature of LLMs, we introduce\nCogGPT for the task, which features an innovative iterative cognitive mechanism\naimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate\nthe superiority of CogGPT over existing methods, particularly in its ability to\nfacilitate role-specific cognitive dynamics under continuous information flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive dynamics are pivotal to advance human understanding of the world.\nRecent advancements in large language models (LLMs) reveal their potential for\ncognitive simulation. However, these LLM-based cognitive studies primarily\nfocus on static modeling, overlooking the dynamic nature of cognition. To\nbridge this gap, we propose the concept of the cognitive dynamics of LLMs and\npresent a corresponding task with the inspiration of longitudinal studies.\nTowards the task, we develop CogBench, a novel benchmark to assess the\ncognitive dynamics of LLMs and validate it through participant surveys. We also\ndesign two evaluation metrics for CogBench, including Authenticity and\nRationality. Recognizing the inherent static nature of LLMs, we introduce\nCogGPT for the task, which features an innovative iterative cognitive mechanism\naimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate\nthe superiority of CogGPT over existing methods, particularly in its ability to\nfacilitate role-specific cognitive dynamics under continuous information flows."
                },
                "authors": [
                    {
                        "name": "Yaojia Lv"
                    },
                    {
                        "name": "Haojie Pan"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jiafeng Liang"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Ruiji Fu"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted to EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15827v1",
                "updated": "2024-09-24T07:40:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    40,
                    33,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T07:40:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    40,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to\n  Model Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to\n  Model Interpretability"
                },
                "summary": "As large language models (LLMs) become advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms,\nwhich are well-suited for probing deeper cognitive aspects of language\nprocessing, to explore neuron-level representations in language model across\nthree tasks: sound-shape association, sound-gender association, and implicit\ncausality. Our findings indicate that while GPT-2-XL struggles with the\nsound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: when GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in transformer based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms,\nwhich are well-suited for probing deeper cognitive aspects of language\nprocessing, to explore neuron-level representations in language model across\nthree tasks: sound-shape association, sound-gender association, and implicit\ncausality. Our findings indicate that while GPT-2-XL struggles with the\nsound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: when GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in transformer based LLMs."
                },
                "authors": [
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Bei Xiao"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.14513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14513v2",
                "updated": "2024-09-24T17:48:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    48,
                    58,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T16:18:14Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    18,
                    14,
                    6,
                    266,
                    0
                ],
                "title": "Order of Magnitude Speedups for LLM Membership Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order of Magnitude Speedups for LLM Membership Inference"
                },
                "summary": "Large Language Models (LLMs) have the promise to revolutionize computing\nbroadly, but their complexity and extensive training data also expose\nsignificant privacy vulnerabilities. One of the simplest privacy risks\nassociated with LLMs is their susceptibility to membership inference attacks\n(MIAs), wherein an adversary aims to determine whether a specific data point\nwas part of the model's training set. Although this is a known risk, state of\nthe art methodologies for MIAs rely on training multiple computationally costly\nshadow models, making risk evaluation prohibitive for large models. Here we\nadapt a recent line of work which uses quantile regression to mount membership\ninference attacks; we extend this work by proposing a low-cost MIA that\nleverages an ensemble of small quantile regression models to determine if a\ndocument belongs to the model's training set or not. We demonstrate the\neffectiveness of this approach on fine-tuned LLMs of varying families (OPT,\nPythia, Llama) and across multiple datasets. Across all scenarios we obtain\ncomparable or improved accuracy compared to state of the art shadow model\napproaches, with as little as 6% of their computation budget. We demonstrate\nincreased effectiveness across multi-epoch trained target models, and\narchitecture miss-specification robustness, that is, we can mount an effective\nattack against a model using a different tokenizer and architecture, without\nrequiring knowledge on the target model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the promise to revolutionize computing\nbroadly, but their complexity and extensive training data also expose\nsignificant privacy vulnerabilities. One of the simplest privacy risks\nassociated with LLMs is their susceptibility to membership inference attacks\n(MIAs), wherein an adversary aims to determine whether a specific data point\nwas part of the model's training set. Although this is a known risk, state of\nthe art methodologies for MIAs rely on training multiple computationally costly\nshadow models, making risk evaluation prohibitive for large models. Here we\nadapt a recent line of work which uses quantile regression to mount membership\ninference attacks; we extend this work by proposing a low-cost MIA that\nleverages an ensemble of small quantile regression models to determine if a\ndocument belongs to the model's training set or not. We demonstrate the\neffectiveness of this approach on fine-tuned LLMs of varying families (OPT,\nPythia, Llama) and across multiple datasets. Across all scenarios we obtain\ncomparable or improved accuracy compared to state of the art shadow model\napproaches, with as little as 6% of their computation budget. We demonstrate\nincreased effectiveness across multi-epoch trained target models, and\narchitecture miss-specification robustness, that is, we can mount an effective\nattack against a model using a different tokenizer and architecture, without\nrequiring knowledge on the target model."
                },
                "authors": [
                    {
                        "name": "Rongting Zhang"
                    },
                    {
                        "name": "Martin Bertran"
                    },
                    {
                        "name": "Aaron Roth"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Roth"
                },
                "author": "Aaron Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16271v1",
                "updated": "2024-09-24T17:44:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    44,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:44:24Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    44,
                    24,
                    1,
                    268,
                    0
                ],
                "title": "AIM 2024 Challenge on UHD Blind Photo Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM 2024 Challenge on UHD Blind Photo Quality Assessment"
                },
                "summary": "We introduce the AIM 2024 UHD-IQA Challenge, a competition to advance the\nNo-Reference Image Quality Assessment (NR-IQA) task for modern, high-resolution\nphotos. The challenge is based on the recently released UHD-IQA Benchmark\nDatabase, which comprises 6,073 UHD-1 (4K) images annotated with perceptual\nquality ratings from expert raters. Unlike previous NR-IQA datasets, UHD-IQA\nfocuses on highly aesthetic photos of superior technical quality, reflecting\nthe ever-increasing standards of digital photography. This challenge aims to\ndevelop efficient and effective NR-IQA models. Participants are tasked with\ncreating novel architectures and training strategies to achieve high predictive\nperformance on UHD-1 images within a computational budget of 50G MACs. This\nenables model deployment on edge devices and scalable processing of extensive\nimage collections. Winners are determined based on a combination of performance\nmetrics, including correlation measures (SRCC, PLCC, KRCC), absolute error\nmetrics (MAE, RMSE), and computational efficiency (G MACs). To excel in this\nchallenge, participants leverage techniques like knowledge distillation,\nlow-precision inference, and multi-scale training. By pushing the boundaries of\nNR-IQA for high-resolution photos, the UHD-IQA Challenge aims to stimulate the\ndevelopment of practical models that can keep pace with the rapidly evolving\nlandscape of digital photography. The innovative solutions emerging from this\ncompetition will have implications for various applications, from photo\ncuration and enhancement to image compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the AIM 2024 UHD-IQA Challenge, a competition to advance the\nNo-Reference Image Quality Assessment (NR-IQA) task for modern, high-resolution\nphotos. The challenge is based on the recently released UHD-IQA Benchmark\nDatabase, which comprises 6,073 UHD-1 (4K) images annotated with perceptual\nquality ratings from expert raters. Unlike previous NR-IQA datasets, UHD-IQA\nfocuses on highly aesthetic photos of superior technical quality, reflecting\nthe ever-increasing standards of digital photography. This challenge aims to\ndevelop efficient and effective NR-IQA models. Participants are tasked with\ncreating novel architectures and training strategies to achieve high predictive\nperformance on UHD-1 images within a computational budget of 50G MACs. This\nenables model deployment on edge devices and scalable processing of extensive\nimage collections. Winners are determined based on a combination of performance\nmetrics, including correlation measures (SRCC, PLCC, KRCC), absolute error\nmetrics (MAE, RMSE), and computational efficiency (G MACs). To excel in this\nchallenge, participants leverage techniques like knowledge distillation,\nlow-precision inference, and multi-scale training. By pushing the boundaries of\nNR-IQA for high-resolution photos, the UHD-IQA Challenge aims to stimulate the\ndevelopment of practical models that can keep pace with the rapidly evolving\nlandscape of digital photography. The innovative solutions emerging from this\ncompetition will have implications for various applications, from photo\ncuration and enhancement to image compression."
                },
                "authors": [
                    {
                        "name": "Vlad Hosu"
                    },
                    {
                        "name": "Marcos V. Conde"
                    },
                    {
                        "name": "Lorenzo Agnolucci"
                    },
                    {
                        "name": "Nabajeet Barman"
                    },
                    {
                        "name": "Saman Zadtootaghaj"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "ECCV 2024 - Advances in Image Manipulation (AIM). arXiv admin note:\n  text overlap with arXiv:2401.10511 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15686v2",
                "updated": "2024-09-24T17:38:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    38,
                    46,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-21T23:28:36Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    23,
                    28,
                    36,
                    4,
                    173,
                    0
                ],
                "title": "Transport-Level Encryption in Datacenter Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transport-Level Encryption in Datacenter Networks"
                },
                "summary": "Cloud applications need network data encryption to isolate from other tenants\nand protect their data from potential eavesdroppers in the network\ninfrastructure. This paper presents SDT, a protocol design for emerging\ndatacenter transport protocols to integrate data encryption while using\nexisting NIC offloading designed for TLS over TCP. Therefore, SDT could enable\na deployment path of new transport protocols in data-centers without giving up\nhardware offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud applications need network data encryption to isolate from other tenants\nand protect their data from potential eavesdroppers in the network\ninfrastructure. This paper presents SDT, a protocol design for emerging\ndatacenter transport protocols to integrate data encryption while using\nexisting NIC offloading designed for TLS over TCP. Therefore, SDT could enable\na deployment path of new transport protocols in data-centers without giving up\nhardware offloading."
                },
                "authors": [
                    {
                        "name": "Tianyi Gao"
                    },
                    {
                        "name": "Xinshu Ma"
                    },
                    {
                        "name": "Suhas Narreddy"
                    },
                    {
                        "name": "Eugenio Luo"
                    },
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16266v1",
                "updated": "2024-09-24T17:37:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    37,
                    54,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:37:54Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    37,
                    54,
                    1,
                    268,
                    0
                ],
                "title": "REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial\n  Task Allocation in Multi-Human Multi-Robot Teams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial\n  Task Allocation in Multi-Human Multi-Robot Teams"
                },
                "summary": "Multi-human multi-robot teams combine the complementary strengths of humans\nand robots to tackle complex tasks across diverse applications. However, the\ninherent heterogeneity of these teams presents significant challenges in\ninitial task allocation (ITA), which involves assigning the most suitable tasks\nto each team member based on their individual capabilities before task\nexecution. While current learning-based methods have shown promising results,\nthey are often computationally expensive to train, and lack the flexibility to\nincorporate user preferences in multi-objective optimization and adapt to\nlast-minute changes in real-world dynamic environments. To address these\nissues, we propose REBEL, an LLM-based ITA framework that integrates rule-based\nand experience-enhanced learning. By leveraging Retrieval-Augmented Generation,\nREBEL dynamically retrieves relevant rules and past experiences, enhancing\nreasoning efficiency. Additionally, REBEL can complement pre-trained RL-based\nITA policies, improving situational awareness and overall team performance.\nExtensive experiments validate the effectiveness of our approach across various\nsettings. More details are available at https://sites.google.com/view/ita-rebel .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-human multi-robot teams combine the complementary strengths of humans\nand robots to tackle complex tasks across diverse applications. However, the\ninherent heterogeneity of these teams presents significant challenges in\ninitial task allocation (ITA), which involves assigning the most suitable tasks\nto each team member based on their individual capabilities before task\nexecution. While current learning-based methods have shown promising results,\nthey are often computationally expensive to train, and lack the flexibility to\nincorporate user preferences in multi-objective optimization and adapt to\nlast-minute changes in real-world dynamic environments. To address these\nissues, we propose REBEL, an LLM-based ITA framework that integrates rule-based\nand experience-enhanced learning. By leveraging Retrieval-Augmented Generation,\nREBEL dynamically retrieves relevant rules and past experiences, enhancing\nreasoning efficiency. Additionally, REBEL can complement pre-trained RL-based\nITA policies, improving situational awareness and overall team performance.\nExtensive experiments validate the effectiveness of our approach across various\nsettings. More details are available at https://sites.google.com/view/ita-rebel ."
                },
                "authors": [
                    {
                        "name": "Arjun Gupte"
                    },
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Vishnunandan L. N. Venkatesh"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Dezhong Zhao"
                    },
                    {
                        "name": "Byung-Cheol Min"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Cheol Min"
                },
                "author": "Byung-Cheol Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07638v2",
                "updated": "2024-09-24T17:34:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    34,
                    7,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-11T21:48:33Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    48,
                    33,
                    2,
                    255,
                    0
                ],
                "title": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities"
                },
                "summary": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance."
                },
                "authors": [
                    {
                        "name": "Thomas Ball"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Cormac Herley"
                    }
                ],
                "author_detail": {
                    "name": "Cormac Herley"
                },
                "author": "Cormac Herley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16264v1",
                "updated": "2024-09-24T17:33:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    33,
                    49,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:33:49Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    33,
                    49,
                    1,
                    268,
                    0
                ],
                "title": "Deployment of a Transportable Yb Optical Lattice Clock",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of a Transportable Yb Optical Lattice Clock"
                },
                "summary": "We report on the first deployment of a ytterbium (Yb) transportable optical\nlattice clock (TOLC), commercially shipping the clock 3,000 km from Boulder,\nColorado to Washington DC. The system, composed of a rigidly mounted optical\nreference cavity, atomic physics package, and an optical frequency comb, fully\nrealizes an independent frequency standard for comparisons in the optical and\nmicrowave domains. The shipped Yb TOLC was fully operational within 2 days of\narrival, enabling frequency comparisons with rubidium (Rb) fountains at the\nUnited States Naval Observatory (USNO). To the best of our knowledge, this\nrepresents the first deployment of a fully independent TOLC, including the\nfrequency comb, coherently uniting the optical stability of the Yb TOLC to the\nmicrowave output of the Rb fountain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the first deployment of a ytterbium (Yb) transportable optical\nlattice clock (TOLC), commercially shipping the clock 3,000 km from Boulder,\nColorado to Washington DC. The system, composed of a rigidly mounted optical\nreference cavity, atomic physics package, and an optical frequency comb, fully\nrealizes an independent frequency standard for comparisons in the optical and\nmicrowave domains. The shipped Yb TOLC was fully operational within 2 days of\narrival, enabling frequency comparisons with rubidium (Rb) fountains at the\nUnited States Naval Observatory (USNO). To the best of our knowledge, this\nrepresents the first deployment of a fully independent TOLC, including the\nfrequency comb, coherently uniting the optical stability of the Yb TOLC to the\nmicrowave output of the Rb fountain."
                },
                "authors": [
                    {
                        "name": "Tobias Bothwell"
                    },
                    {
                        "name": "Wesley Brand"
                    },
                    {
                        "name": "Robert Fasano"
                    },
                    {
                        "name": "Thomas Akin"
                    },
                    {
                        "name": "Joseph Whalen"
                    },
                    {
                        "name": "Tanner Grogan"
                    },
                    {
                        "name": "Yun-Jhih Chen"
                    },
                    {
                        "name": "Marco Pomponio"
                    },
                    {
                        "name": "Takuma Nakamura"
                    },
                    {
                        "name": "Benjamin Rauf"
                    },
                    {
                        "name": "Ignacio Baldoni"
                    },
                    {
                        "name": "Michele Giunta"
                    },
                    {
                        "name": "Ronald Holzwarth"
                    },
                    {
                        "name": "Craig Nelson"
                    },
                    {
                        "name": "Archita Hati"
                    },
                    {
                        "name": "Franklyn Quinlan"
                    },
                    {
                        "name": "Richard Fox"
                    },
                    {
                        "name": "Steven Peil"
                    },
                    {
                        "name": "Andrew Ludlow"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Ludlow"
                },
                "author": "Andrew Ludlow",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15228v2",
                "updated": "2024-09-24T17:13:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    13,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T17:22:09Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    22,
                    9,
                    0,
                    267,
                    0
                ],
                "title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Framework for Evaluating API-oriented Code Generation in\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs."
                },
                "authors": [
                    {
                        "name": "Yixi Wu"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Zehao Wang"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tse-Hsun"
                    },
                    {
                        "name": "Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen"
                },
                "arxiv_affiliation": "Peter",
                "author": "Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13187v2",
                "updated": "2024-09-24T17:13:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    13,
                    7,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-20T03:28:48Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    28,
                    48,
                    4,
                    264,
                    0
                ],
                "title": "Cooperative Resilience in Artificial Intelligence Multiagent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Resilience in Artificial Intelligence Multiagent Systems"
                },
                "summary": "Resilience refers to the ability of systems to withstand, adapt to, and\nrecover from disruptive events. While studies on resilience have attracted\nsignificant attention across various research domains, the precise definition\nof this concept within the field of cooperative artificial intelligence remains\nunclear. This paper addresses this gap by proposing a clear definition of\n`cooperative resilience' and outlining a methodology for its quantitative\nmeasurement. The methodology is validated in an environment with RL-based and\nLLM-augmented autonomous agents, subjected to environmental changes and the\nintroduction of agents with unsustainable behaviors. These events are\nparameterized to create various scenarios for measuring cooperative resilience.\nThe results highlight the crucial role of resilience metrics in analyzing how\nthe collective system prepares for, resists, recovers from, sustains\nwell-being, and transforms in the face of disruptions. These findings provide\nfoundational insights into the definition, measurement, and preliminary\nanalysis of cooperative resilience, offering significant implications for the\nbroader field of AI. Moreover, the methodology and metrics developed here can\nbe adapted to a wide range of AI applications, enhancing the reliability and\neffectiveness of AI in dynamic and unpredictable environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resilience refers to the ability of systems to withstand, adapt to, and\nrecover from disruptive events. While studies on resilience have attracted\nsignificant attention across various research domains, the precise definition\nof this concept within the field of cooperative artificial intelligence remains\nunclear. This paper addresses this gap by proposing a clear definition of\n`cooperative resilience' and outlining a methodology for its quantitative\nmeasurement. The methodology is validated in an environment with RL-based and\nLLM-augmented autonomous agents, subjected to environmental changes and the\nintroduction of agents with unsustainable behaviors. These events are\nparameterized to create various scenarios for measuring cooperative resilience.\nThe results highlight the crucial role of resilience metrics in analyzing how\nthe collective system prepares for, resists, recovers from, sustains\nwell-being, and transforms in the face of disruptions. These findings provide\nfoundational insights into the definition, measurement, and preliminary\nanalysis of cooperative resilience, offering significant implications for the\nbroader field of AI. Moreover, the methodology and metrics developed here can\nbe adapted to a wide range of AI applications, enhancing the reliability and\neffectiveness of AI in dynamic and unpredictable environments."
                },
                "authors": [
                    {
                        "name": "Manuela Chacon-Chamorro"
                    },
                    {
                        "name": "Luis Felipe Giraldo"
                    },
                    {
                        "name": "Nicanor Quijano"
                    },
                    {
                        "name": "Vicente Vargas-Panesso"
                    },
                    {
                        "name": "C√©sar Gonz√°lez"
                    },
                    {
                        "name": "Juan Sebasti√°n Pinz√≥n"
                    },
                    {
                        "name": "Rub√©n Manrique"
                    },
                    {
                        "name": "Manuel R√≠os"
                    },
                    {
                        "name": "Yesid Fonseca"
                    },
                    {
                        "name": "Daniel G√≥mez-Barrera"
                    },
                    {
                        "name": "M√≥nica Perdomo-P√©rez"
                    }
                ],
                "author_detail": {
                    "name": "M√≥nica Perdomo-P√©rez"
                },
                "author": "M√≥nica Perdomo-P√©rez",
                "arxiv_comment": "Supplementary material in\n  https://github.com/mavivi95/resilience/blob/main/Supplementary_File.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16241v1",
                "updated": "2024-09-24T17:04:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    4,
                    12,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:04:12Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    4,
                    12,
                    1,
                    268,
                    0
                ],
                "title": "LLM Echo Chamber: personalized and automated disinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Echo Chamber: personalized and automated disinformation"
                },
                "summary": "Recent advancements have showcased the capabilities of Large Language Models\nlike GPT4 and Llama2 in tasks such as summarization, translation, and content\nreview. However, their widespread use raises concerns, particularly around the\npotential for LLMs to spread persuasive, humanlike misinformation at scale,\nwhich could significantly influence public opinion. This study examines these\nrisks, focusing on LLMs ability to propagate misinformation as factual. To\ninvestigate this, we built the LLM Echo Chamber, a controlled digital\nenvironment simulating social media chatrooms, where misinformation often\nspreads. Echo chambers, where individuals only interact with like minded\npeople, further entrench beliefs. By studying malicious bots spreading\nmisinformation in this environment, we can better understand this phenomenon.\nWe reviewed current LLMs, explored misinformation risks, and applied sota\nfinetuning techniques. Using Microsoft phi2 model, finetuned with our custom\ndataset, we generated harmful content to create the Echo Chamber. This setup,\nevaluated by GPT4 for persuasiveness and harmfulness, sheds light on the\nethical concerns surrounding LLMs and emphasizes the need for stronger\nsafeguards against misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have showcased the capabilities of Large Language Models\nlike GPT4 and Llama2 in tasks such as summarization, translation, and content\nreview. However, their widespread use raises concerns, particularly around the\npotential for LLMs to spread persuasive, humanlike misinformation at scale,\nwhich could significantly influence public opinion. This study examines these\nrisks, focusing on LLMs ability to propagate misinformation as factual. To\ninvestigate this, we built the LLM Echo Chamber, a controlled digital\nenvironment simulating social media chatrooms, where misinformation often\nspreads. Echo chambers, where individuals only interact with like minded\npeople, further entrench beliefs. By studying malicious bots spreading\nmisinformation in this environment, we can better understand this phenomenon.\nWe reviewed current LLMs, explored misinformation risks, and applied sota\nfinetuning techniques. Using Microsoft phi2 model, finetuned with our custom\ndataset, we generated harmful content to create the Echo Chamber. This setup,\nevaluated by GPT4 for persuasiveness and harmfulness, sheds light on the\nethical concerns surrounding LLMs and emphasizes the need for stronger\nsafeguards against misinformation."
                },
                "authors": [
                    {
                        "name": "Tony Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tony Ma"
                },
                "author": "Tony Ma",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16235v1",
                "updated": "2024-09-24T16:51:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    51,
                    36,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:51:36Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    51,
                    36,
                    1,
                    268,
                    0
                ],
                "title": "EuroLLM: Multilingual Language Models for Europe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EuroLLM: Multilingual Language Models for Europe"
                },
                "summary": "The quality of open-weight LLMs has seen significant improvement, yet they\nremain predominantly focused on English. In this paper, we introduce the\nEuroLLM project, aimed at developing a suite of open-weight multilingual LLMs\ncapable of understanding and generating text in all official European Union\nlanguages, as well as several additional relevant languages. We outline the\nprogress made to date, detailing our data collection and filtering process, the\ndevelopment of scaling laws, the creation of our multilingual tokenizer, and\nthe data mix and modeling configurations. Additionally, we release our initial\nmodels: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on\nmultilingual general benchmarks and machine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of open-weight LLMs has seen significant improvement, yet they\nremain predominantly focused on English. In this paper, we introduce the\nEuroLLM project, aimed at developing a suite of open-weight multilingual LLMs\ncapable of understanding and generating text in all official European Union\nlanguages, as well as several additional relevant languages. We outline the\nprogress made to date, detailing our data collection and filtering process, the\ndevelopment of scaling laws, the creation of our multilingual tokenizer, and\nthe data mix and modeling configurations. Additionally, we release our initial\nmodels: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on\nmultilingual general benchmarks and machine translation."
                },
                "authors": [
                    {
                        "name": "Pedro Henrique Martins"
                    },
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Jo√£o Alves"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Duarte M. Alves"
                    },
                    {
                        "name": "Jos√© Pombal"
                    },
                    {
                        "name": "Amin Farajian"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Mateusz Klimaszewski"
                    },
                    {
                        "name": "Pierre Colombo"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Jos√© G. C. de Souza"
                    },
                    {
                        "name": "Alexandra Birch"
                    },
                    {
                        "name": "Andr√© F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr√© F. T. Martins"
                },
                "author": "Andr√© F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01721v2",
                "updated": "2024-09-24T16:40:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    40,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-03T18:27:44Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    27,
                    44,
                    0,
                    155,
                    0
                ],
                "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs"
                },
                "summary": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address\n$\\textit{Normal Outliers}$, which are activations across all tokens with\nrelatively large magnitudes. However, these methods struggle with smoothing\n$\\textit{Massive Outliers}$ that display significantly larger values, which\nleads to significant performance degradation in low-bit quantization. In this\npaper, we introduce DuQuant, a novel approach that utilizes rotation and\npermutation transformations to more effectively mitigate both massive and\nnormal outliers. First, DuQuant starts by constructing rotation matrices, using\nspecific outlier dimensions as prior knowledge, to redistribute outliers to\nadjacent channels by block-wise rotation. Second, We further employ a zigzag\npermutation to balance the distribution of outliers across blocks, thereby\nreducing block-wise variance. A subsequent rotation further smooths the\nactivation landscape, enhancing model performance. DuQuant simplifies the\nquantization process and excels in managing outliers, outperforming the\nstate-of-the-art baselines across various sizes and types of LLMs on multiple\ntasks, even with 4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address\n$\\textit{Normal Outliers}$, which are activations across all tokens with\nrelatively large magnitudes. However, these methods struggle with smoothing\n$\\textit{Massive Outliers}$ that display significantly larger values, which\nleads to significant performance degradation in low-bit quantization. In this\npaper, we introduce DuQuant, a novel approach that utilizes rotation and\npermutation transformations to more effectively mitigate both massive and\nnormal outliers. First, DuQuant starts by constructing rotation matrices, using\nspecific outlier dimensions as prior knowledge, to redistribute outliers to\nadjacent channels by block-wise rotation. Second, We further employ a zigzag\npermutation to balance the distribution of outliers across blocks, thereby\nreducing block-wise variance. A subsequent rotation further smooths the\nactivation landscape, enhancing model performance. DuQuant simplifies the\nquantization process and excels in managing outliers, outperforming the\nstate-of-the-art baselines across various sizes and types of LLMs on multiple\ntasks, even with 4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Jingzhi Cui"
                    },
                    {
                        "name": "Yingtao Zhang"
                    },
                    {
                        "name": "Linzhan Mou"
                    },
                    {
                        "name": "Linqi Song"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Ying Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wei"
                },
                "author": "Ying Wei",
                "arxiv_comment": "26 pages, 13 figures, Website at https://duquant.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16220v1",
                "updated": "2024-09-24T16:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    31,
                    33,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    31,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "Towards Enhancing Linked Data Retrieval in Conversational UIs using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Enhancing Linked Data Retrieval in Conversational UIs using\n  Large Language Models"
                },
                "summary": "Despite the recent broad adoption of Large Language Models (LLMs) across\nvarious domains, their potential for enriching information systems in\nextracting and exploring Linked Data (LD) and Resource Description Framework\n(RDF) triplestores has not been extensively explored. This paper examines the\nintegration of LLMs within existing systems, emphasising the enhancement of\nconversational user interfaces (UIs) and their capabilities for data extraction\nby producing more accurate SPARQL queries without the requirement for model\nretraining. Typically, conversational UI models necessitate retraining with the\nintroduction of new datasets or updates, limiting their functionality as\ngeneral-purpose extraction tools. Our approach addresses this limitation by\nincorporating LLMs into the conversational UI workflow, significantly enhancing\ntheir ability to comprehend and process user queries effectively. By leveraging\nthe advanced natural language understanding capabilities of LLMs, our method\nimproves RDF entity extraction within web systems employing conventional\nchatbots. This integration facilitates a more nuanced and context-aware\ninteraction model, critical for handling the complex query patterns often\nencountered in RDF datasets and Linked Open Data (LOD) endpoints. The\nevaluation of this methodology shows a marked enhancement in system\nexpressivity and the accuracy of responses to user queries, indicating a\npromising direction for future research in this area. This investigation not\nonly underscores the versatility of LLMs in enhancing existing information\nsystems but also sets the stage for further explorations into their potential\napplications within more specialised domains of web information systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent broad adoption of Large Language Models (LLMs) across\nvarious domains, their potential for enriching information systems in\nextracting and exploring Linked Data (LD) and Resource Description Framework\n(RDF) triplestores has not been extensively explored. This paper examines the\nintegration of LLMs within existing systems, emphasising the enhancement of\nconversational user interfaces (UIs) and their capabilities for data extraction\nby producing more accurate SPARQL queries without the requirement for model\nretraining. Typically, conversational UI models necessitate retraining with the\nintroduction of new datasets or updates, limiting their functionality as\ngeneral-purpose extraction tools. Our approach addresses this limitation by\nincorporating LLMs into the conversational UI workflow, significantly enhancing\ntheir ability to comprehend and process user queries effectively. By leveraging\nthe advanced natural language understanding capabilities of LLMs, our method\nimproves RDF entity extraction within web systems employing conventional\nchatbots. This integration facilitates a more nuanced and context-aware\ninteraction model, critical for handling the complex query patterns often\nencountered in RDF datasets and Linked Open Data (LOD) endpoints. The\nevaluation of this methodology shows a marked enhancement in system\nexpressivity and the accuracy of responses to user queries, indicating a\npromising direction for future research in this area. This investigation not\nonly underscores the versatility of LLMs in enhancing existing information\nsystems but also sets the stage for further explorations into their potential\napplications within more specialised domains of web information systems."
                },
                "authors": [
                    {
                        "name": "Omar Mussa"
                    },
                    {
                        "name": "Omer Rana"
                    },
                    {
                        "name": "Beno√Æt Goossens"
                    },
                    {
                        "name": "Pablo Orozco-Terwengel"
                    },
                    {
                        "name": "Charith Perera"
                    }
                ],
                "author_detail": {
                    "name": "Charith Perera"
                },
                "author": "Charith Perera",
                "arxiv_comment": "This paper has been accepted at the 25th International Web\n  Information Systems Engineering Conference (WISE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16215v1",
                "updated": "2024-09-24T16:21:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    21,
                    27,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:21:27Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    21,
                    27,
                    1,
                    268,
                    0
                ],
                "title": "Tiny Robotics Dataset and Benchmark for Continual Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Robotics Dataset and Benchmark for Continual Object Detection"
                },
                "summary": "Detecting objects in mobile robotics is crucial for numerous applications,\nfrom autonomous navigation to inspection. However, robots are often required to\nperform tasks in different domains with respect to the training one and need to\nadapt to these changes. Tiny mobile robots, subject to size, power, and\ncomputational constraints, encounter even more difficulties in running and\nadapting these algorithms. Such adaptability, though, is crucial for real-world\ndeployment, where robots must operate effectively in dynamic and unpredictable\nsettings. In this work, we introduce a novel benchmark to evaluate the\ncontinual learning capabilities of object detection systems in tiny robotic\nplatforms. Our contributions include: (i) Tiny Robotics Object Detection\n(TiROD), a comprehensive dataset collected using a small mobile robot, designed\nto test the adaptability of object detectors across various domains and\nclasses; (ii) an evaluation of state-of-the-art real-time object detectors\ncombined with different continual learning strategies on this dataset,\nproviding detailed insights into their performance and limitations; and (iii)\nwe publish the data and the code to replicate the results to foster continuous\nadvancements in this field. Our benchmark results indicate key challenges that\nmust be addressed to advance the development of robust and efficient object\ndetection systems for tiny robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting objects in mobile robotics is crucial for numerous applications,\nfrom autonomous navigation to inspection. However, robots are often required to\nperform tasks in different domains with respect to the training one and need to\nadapt to these changes. Tiny mobile robots, subject to size, power, and\ncomputational constraints, encounter even more difficulties in running and\nadapting these algorithms. Such adaptability, though, is crucial for real-world\ndeployment, where robots must operate effectively in dynamic and unpredictable\nsettings. In this work, we introduce a novel benchmark to evaluate the\ncontinual learning capabilities of object detection systems in tiny robotic\nplatforms. Our contributions include: (i) Tiny Robotics Object Detection\n(TiROD), a comprehensive dataset collected using a small mobile robot, designed\nto test the adaptability of object detectors across various domains and\nclasses; (ii) an evaluation of state-of-the-art real-time object detectors\ncombined with different continual learning strategies on this dataset,\nproviding detailed insights into their performance and limitations; and (iii)\nwe publish the data and the code to replicate the results to foster continuous\nadvancements in this field. Our benchmark results indicate key challenges that\nmust be addressed to advance the development of robust and efficient object\ndetection systems for tiny robotics."
                },
                "authors": [
                    {
                        "name": "Francesco Pasti"
                    },
                    {
                        "name": "Riccardo De Monte"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    },
                    {
                        "name": "Nicola Bellotto"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Bellotto"
                },
                "author": "Nicola Bellotto",
                "arxiv_comment": "Paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16209v1",
                "updated": "2024-09-24T16:09:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    9,
                    29,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T16:09:29Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    9,
                    29,
                    1,
                    268,
                    0
                ],
                "title": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM"
                },
                "summary": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Shengyi Ding"
                    },
                    {
                        "name": "Deen Ma"
                    },
                    {
                        "name": "Yixuan Wu"
                    },
                    {
                        "name": "Hongjie Liao"
                    },
                    {
                        "name": "Kaiyuan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyuan Hu"
                },
                "author": "Kaiyuan Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v2",
                "updated": "2024-09-24T16:07:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    7,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tom√°≈° Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16202v2",
                "updated": "2024-09-25T03:35:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    35,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T16:00:28Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    0,
                    28,
                    1,
                    268,
                    0
                ],
                "title": "CJEval: A Benchmark for Assessing Large Language Models Using Chinese\n  Junior High School Exam Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CJEval: A Benchmark for Assessing Large Language Models Using Chinese\n  Junior High School Exam Data"
                },
                "summary": "Online education platforms have significantly transformed the dissemination\nof educational resources by providing a dynamic and digital infrastructure.\nWith the further enhancement of this transformation, the advent of Large\nLanguage Models (LLMs) has elevated the intelligence levels of these platforms.\nHowever, current academic benchmarks provide limited guidance for real-world\nindustry scenarios. This limitation arises because educational applications\nrequire more than mere test question responses. To bridge this gap, we\nintroduce CJEval, a benchmark based on Chinese Junior High School Exam\nEvaluations. CJEval consists of 26,136 samples across four application-level\neducational tasks covering ten subjects. These samples include not only\nquestions and answers but also detailed annotations such as question types,\ndifficulty levels, knowledge concepts, and answer explanations. By utilizing\nthis benchmark, we assessed LLMs' potential applications and conducted a\ncomprehensive analysis of their performance by fine-tuning on various\neducational tasks. Extensive experiments and discussions have highlighted the\nopportunities and challenges of applying LLMs in the field of education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online education platforms have significantly transformed the dissemination\nof educational resources by providing a dynamic and digital infrastructure.\nWith the further enhancement of this transformation, the advent of Large\nLanguage Models (LLMs) has elevated the intelligence levels of these platforms.\nHowever, current academic benchmarks provide limited guidance for real-world\nindustry scenarios. This limitation arises because educational applications\nrequire more than mere test question responses. To bridge this gap, we\nintroduce CJEval, a benchmark based on Chinese Junior High School Exam\nEvaluations. CJEval consists of 26,136 samples across four application-level\neducational tasks covering ten subjects. These samples include not only\nquestions and answers but also detailed annotations such as question types,\ndifficulty levels, knowledge concepts, and answer explanations. By utilizing\nthis benchmark, we assessed LLMs' potential applications and conducted a\ncomprehensive analysis of their performance by fine-tuning on various\neducational tasks. Extensive experiments and discussions have highlighted the\nopportunities and challenges of applying LLMs in the field of education."
                },
                "authors": [
                    {
                        "name": "Qian-Wen Zhang"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Lingfeng Qiao"
                    },
                    {
                        "name": "Liangcai Gao"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16191v1",
                "updated": "2024-09-24T15:38:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    38,
                    11,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:38:11Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    38,
                    11,
                    1,
                    268,
                    0
                ],
                "title": "HelloBench: Evaluating Long Text Generation Capabilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HelloBench: Evaluating Long Text Generation Capabilities of Large\n  Language Models"
                },
                "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in various tasks (e.g., long-context understanding), and many\nbenchmarks have been proposed. However, we observe that long text generation\ncapabilities are not well investigated. Therefore, we introduce the\nHierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,\nin-the-wild, and open-ended benchmark to evaluate LLMs' performance in\ngenerating long text. Based on Bloom's Taxonomy, HelloBench categorizes long\ntext generation tasks into five subtasks: open-ended QA, summarization, chat,\ntext completion, and heuristic text generation. Besides, we propose\nHierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation\nmethod that significantly reduces the time and effort required for human\nevaluation while maintaining a high correlation with human evaluation. We have\nconducted extensive experiments across around 30 mainstream LLMs and observed\nthat the current LLMs lack long text generation capabilities. Specifically,\nfirst, regardless of whether the instructions include explicit or implicit\nlength constraints, we observe that most LLMs cannot generate text that is\nlonger than 4000 words. Second, we observe that while some LLMs can generate\nlonger text, many issues exist (e.g., severe repetition and quality\ndegradation). Third, to demonstrate the effectiveness of HelloEval, we compare\nHelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge\nmethods, which show that HelloEval has the highest correlation with human\nevaluation. We release our code in https://github.com/Quehry/HelloBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in various tasks (e.g., long-context understanding), and many\nbenchmarks have been proposed. However, we observe that long text generation\ncapabilities are not well investigated. Therefore, we introduce the\nHierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,\nin-the-wild, and open-ended benchmark to evaluate LLMs' performance in\ngenerating long text. Based on Bloom's Taxonomy, HelloBench categorizes long\ntext generation tasks into five subtasks: open-ended QA, summarization, chat,\ntext completion, and heuristic text generation. Besides, we propose\nHierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation\nmethod that significantly reduces the time and effort required for human\nevaluation while maintaining a high correlation with human evaluation. We have\nconducted extensive experiments across around 30 mainstream LLMs and observed\nthat the current LLMs lack long text generation capabilities. Specifically,\nfirst, regardless of whether the instructions include explicit or implicit\nlength constraints, we observe that most LLMs cannot generate text that is\nlonger than 4000 words. Second, we observe that while some LLMs can generate\nlonger text, many issues exist (e.g., severe repetition and quality\ndegradation). Third, to demonstrate the effectiveness of HelloEval, we compare\nHelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge\nmethods, which show that HelloEval has the highest correlation with human\nevaluation. We release our code in https://github.com/Quehry/HelloBench."
                },
                "authors": [
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Feiyu Duan"
                    },
                    {
                        "name": "Liqun He"
                    },
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Wenge Rong"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07837v2",
                "updated": "2024-09-24T15:25:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    25,
                    59,
                    1,
                    268,
                    0
                ],
                "published": "2024-04-11T15:25:13Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    15,
                    25,
                    13,
                    3,
                    102,
                    0
                ],
                "title": "Data-Driven System Identification of Quadrotors Subject to Motor Delays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven System Identification of Quadrotors Subject to Motor Delays"
                },
                "summary": "Recently non-linear control methods like Model Predictive Control (MPC) and\nReinforcement Learning (RL) have attracted increased interest in the quadrotor\ncontrol community. In contrast to classic control methods like cascaded PID\ncontrollers, MPC and RL heavily rely on an accurate model of the system\ndynamics. The process of quadrotor system identification is notoriously tedious\nand is often pursued with additional equipment like a thrust stand.\nFurthermore, low-level details like motor delays which are crucial for accurate\nend-to-end control are often neglected. In this work, we introduce a\ndata-driven method to identify a quadrotor's inertia parameters, thrust curves,\ntorque coefficients, and first-order motor delay purely based on proprioceptive\ndata. The estimation of the motor delay is particularly challenging as usually,\nthe RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based\nmethod to estimate the latent time constant. Our approach only requires about a\nminute of flying data that can be collected without any additional equipment\nand usually consists of three simple maneuvers. Experimental results\ndemonstrate the ability of our method to accurately recover the parameters of\nmultiple quadrotors. It also facilitates the deployment of RL-based, end-to-end\nquadrotor control of a large quadrotor under harsh, outdoor conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently non-linear control methods like Model Predictive Control (MPC) and\nReinforcement Learning (RL) have attracted increased interest in the quadrotor\ncontrol community. In contrast to classic control methods like cascaded PID\ncontrollers, MPC and RL heavily rely on an accurate model of the system\ndynamics. The process of quadrotor system identification is notoriously tedious\nand is often pursued with additional equipment like a thrust stand.\nFurthermore, low-level details like motor delays which are crucial for accurate\nend-to-end control are often neglected. In this work, we introduce a\ndata-driven method to identify a quadrotor's inertia parameters, thrust curves,\ntorque coefficients, and first-order motor delay purely based on proprioceptive\ndata. The estimation of the motor delay is particularly challenging as usually,\nthe RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based\nmethod to estimate the latent time constant. Our approach only requires about a\nminute of flying data that can be collected without any additional equipment\nand usually consists of three simple maneuvers. Experimental results\ndemonstrate the ability of our method to accurately recover the parameters of\nmultiple quadrotors. It also facilitates the deployment of RL-based, end-to-end\nquadrotor control of a large quadrotor under harsh, outdoor conditions."
                },
                "authors": [
                    {
                        "name": "Jonas Eschmann"
                    },
                    {
                        "name": "Dario Albani"
                    },
                    {
                        "name": "Giuseppe Loianno"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Loianno"
                },
                "author": "Giuseppe Loianno",
                "arxiv_comment": "Accepted at IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16177v1",
                "updated": "2024-09-24T15:21:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    21,
                    53,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:21:53Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    21,
                    53,
                    1,
                    268,
                    0
                ],
                "title": "Microsecond-Latency Feedback at a Particle Accelerator by Online\n  Reinforcement Learning on Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microsecond-Latency Feedback at a Particle Accelerator by Online\n  Reinforcement Learning on Hardware"
                },
                "summary": "The commissioning and operation of future large-scale scientific experiments\nwill challenge current tuning and control methods. Reinforcement learning (RL)\nalgorithms are a promising solution thanks to their capability of autonomously\ntackling a control problem based on a task parameterized by a reward function.\nThe conventionally utilized machine learning (ML) libraries are not intended\nfor microsecond latency applications, as they mostly optimize for throughput\nperformance. On the other hand, most of the programmable logic implementations\nare meant for computation acceleration, not being intended to work in a\nreal-time environment. To overcome these limitations of current\nimplementations, RL needs to be deployed on-the-edge, i.e. on to the device\ngathering the training data. In this paper we present the design and deployment\nof an experience accumulator system in a particle accelerator. In this system\ndeep-RL algorithms run using hardware acceleration and act within a few\nmicroseconds, enabling the use of RL for control of ultra-fast phenomena. The\ntraining is performed offline to reduce the number of operations carried out on\nthe acceleration hardware. The proposed architecture was tested in real\nexperimental conditions at the Karlsruhe research accelerator (KARA), serving\nalso as a synchrotron light source, where the system was used to control\ninduced horizontal betatron oscillations in real-time. The results showed a\nperformance comparable to the commercial feedback system available at the\naccelerator, proving the viability and potential of this approach. Due to the\nself-learning and reconfiguration capability of this implementation, its\nseamless application to other control problems is possible. Applications range\nfrom particle accelerators to large-scale research and industrial facilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The commissioning and operation of future large-scale scientific experiments\nwill challenge current tuning and control methods. Reinforcement learning (RL)\nalgorithms are a promising solution thanks to their capability of autonomously\ntackling a control problem based on a task parameterized by a reward function.\nThe conventionally utilized machine learning (ML) libraries are not intended\nfor microsecond latency applications, as they mostly optimize for throughput\nperformance. On the other hand, most of the programmable logic implementations\nare meant for computation acceleration, not being intended to work in a\nreal-time environment. To overcome these limitations of current\nimplementations, RL needs to be deployed on-the-edge, i.e. on to the device\ngathering the training data. In this paper we present the design and deployment\nof an experience accumulator system in a particle accelerator. In this system\ndeep-RL algorithms run using hardware acceleration and act within a few\nmicroseconds, enabling the use of RL for control of ultra-fast phenomena. The\ntraining is performed offline to reduce the number of operations carried out on\nthe acceleration hardware. The proposed architecture was tested in real\nexperimental conditions at the Karlsruhe research accelerator (KARA), serving\nalso as a synchrotron light source, where the system was used to control\ninduced horizontal betatron oscillations in real-time. The results showed a\nperformance comparable to the commercial feedback system available at the\naccelerator, proving the viability and potential of this approach. Due to the\nself-learning and reconfiguration capability of this implementation, its\nseamless application to other control problems is possible. Applications range\nfrom particle accelerators to large-scale research and industrial facilities."
                },
                "authors": [
                    {
                        "name": "Luca Scomparin"
                    },
                    {
                        "name": "Michele Caselle"
                    },
                    {
                        "name": "Andrea Santamaria Garcia"
                    },
                    {
                        "name": "Chenran Xu"
                    },
                    {
                        "name": "Edmund Blomley"
                    },
                    {
                        "name": "Timo Dritschler"
                    },
                    {
                        "name": "Akira Mochihashi"
                    },
                    {
                        "name": "Marcel Schuh"
                    },
                    {
                        "name": "Johannes L. Steinmann"
                    },
                    {
                        "name": "Erik Br√ºndermann"
                    },
                    {
                        "name": "Andreas Kopmann"
                    },
                    {
                        "name": "J√ºrgen Becker"
                    },
                    {
                        "name": "Anke-Susanne M√ºller"
                    },
                    {
                        "name": "Marc Weber"
                    }
                ],
                "author_detail": {
                    "name": "Marc Weber"
                },
                "arxiv_affiliation": "Karlsruher Institut f√ºr Technologie",
                "author": "Marc Weber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16176v1",
                "updated": "2024-09-24T15:20:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    20,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:20:39Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    20,
                    39,
                    1,
                    268,
                    0
                ],
                "title": "Cyber Knowledge Completion Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber Knowledge Completion Using Large Language Models"
                },
                "summary": "The integration of the Internet of Things (IoT) into Cyber-Physical Systems\n(CPSs) has expanded their cyber-attack surface, introducing new and\nsophisticated threats with potential to exploit emerging vulnerabilities.\nAssessing the risks of CPSs is increasingly difficult due to incomplete and\noutdated cybersecurity knowledge. This highlights the urgent need for\nbetter-informed risk assessments and mitigation strategies. While previous\nefforts have relied on rule-based natural language processing (NLP) tools to\nmap vulnerabilities, weaknesses, and attack patterns, recent advancements in\nLarge Language Models (LLMs) present a unique opportunity to enhance\ncyber-attack knowledge completion through improved reasoning, inference, and\nsummarization capabilities. We apply embedding models to encapsulate\ninformation on attack patterns and adversarial techniques, generating mappings\nbetween them using vector embeddings. Additionally, we propose a\nRetrieval-Augmented Generation (RAG)-based approach that leverages pre-trained\nmodels to create structured mappings between different taxonomies of threat\npatterns. Further, we use a small hand-labeled dataset to compare the proposed\nRAG-based approach to a baseline standard binary classification model. Thus,\nthe proposed approach provides a comprehensive framework to address the\nchallenge of cyber-attack knowledge graph completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of the Internet of Things (IoT) into Cyber-Physical Systems\n(CPSs) has expanded their cyber-attack surface, introducing new and\nsophisticated threats with potential to exploit emerging vulnerabilities.\nAssessing the risks of CPSs is increasingly difficult due to incomplete and\noutdated cybersecurity knowledge. This highlights the urgent need for\nbetter-informed risk assessments and mitigation strategies. While previous\nefforts have relied on rule-based natural language processing (NLP) tools to\nmap vulnerabilities, weaknesses, and attack patterns, recent advancements in\nLarge Language Models (LLMs) present a unique opportunity to enhance\ncyber-attack knowledge completion through improved reasoning, inference, and\nsummarization capabilities. We apply embedding models to encapsulate\ninformation on attack patterns and adversarial techniques, generating mappings\nbetween them using vector embeddings. Additionally, we propose a\nRetrieval-Augmented Generation (RAG)-based approach that leverages pre-trained\nmodels to create structured mappings between different taxonomies of threat\npatterns. Further, we use a small hand-labeled dataset to compare the proposed\nRAG-based approach to a baseline standard binary classification model. Thus,\nthe proposed approach provides a comprehensive framework to address the\nchallenge of cyber-attack knowledge graph completion."
                },
                "authors": [
                    {
                        "name": "Braden K Webb"
                    },
                    {
                        "name": "Sumit Purohit"
                    },
                    {
                        "name": "Rounak Meyur"
                    }
                ],
                "author_detail": {
                    "name": "Rounak Meyur"
                },
                "author": "Rounak Meyur",
                "arxiv_comment": "7 pages, 2 figures. Submitted to 2024 IEEE International Conference\n  on Big Data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16167v1",
                "updated": "2024-09-24T15:08:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    41,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T15:08:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging."
                },
                "authors": [
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Didi Zhu"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Jing Su"
                    },
                    {
                        "name": "Xuwu Wang"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13107v2",
                "updated": "2024-09-24T15:08:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    8,
                    3,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-19T22:24:46Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    22,
                    24,
                    46,
                    3,
                    263,
                    0
                ],
                "title": "Towards Robust Automation of Surgical Systems via Digital Twin-based\n  Scene Representations from Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Automation of Surgical Systems via Digital Twin-based\n  Scene Representations from Foundation Models"
                },
                "summary": "Large language model-based (LLM) agents are emerging as a powerful enabler of\nrobust embodied intelligence due to their capability of planning complex action\nsequences. Sound planning ability is necessary for robust automation in many\ntask domains, but especially in surgical automation. These agents rely on a\nhighly detailed natural language representation of the scene. Thus, to leverage\nthe emergent capabilities of LLM agents for surgical task planning, developing\nsimilarly powerful and robust perception algorithms is necessary to derive a\ndetailed scene representation of the environment from visual input. Previous\nresearch has focused primarily on enabling LLM-based task planning while\nadopting simple yet severely limited perception solutions to meet the needs for\nbench-top experiments but lack the critical flexibility to scale to less\nconstrained settings. In this work, we propose an alternate perception approach\n-- a digital twin-based machine perception approach that capitalizes on the\nconvincing performance and out-of-the-box generalization of recent vision\nfoundation models. Integrating our digital twin-based scene representation and\nLLM agent for planning with the dVRK platform, we develop an embodied\nintelligence system and evaluate its robustness in performing peg transfer and\ngauze retrieval tasks. Our approach shows strong task performance and\ngeneralizability to varied environment settings. Despite convincing\nperformance, this work is merely a first step towards the integration of\ndigital twin-based scene representations. Future studies are necessary for the\nrealization of a comprehensive digital twin framework to improve the\ninterpretability and generalizability of embodied intelligence in surgery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based (LLM) agents are emerging as a powerful enabler of\nrobust embodied intelligence due to their capability of planning complex action\nsequences. Sound planning ability is necessary for robust automation in many\ntask domains, but especially in surgical automation. These agents rely on a\nhighly detailed natural language representation of the scene. Thus, to leverage\nthe emergent capabilities of LLM agents for surgical task planning, developing\nsimilarly powerful and robust perception algorithms is necessary to derive a\ndetailed scene representation of the environment from visual input. Previous\nresearch has focused primarily on enabling LLM-based task planning while\nadopting simple yet severely limited perception solutions to meet the needs for\nbench-top experiments but lack the critical flexibility to scale to less\nconstrained settings. In this work, we propose an alternate perception approach\n-- a digital twin-based machine perception approach that capitalizes on the\nconvincing performance and out-of-the-box generalization of recent vision\nfoundation models. Integrating our digital twin-based scene representation and\nLLM agent for planning with the dVRK platform, we develop an embodied\nintelligence system and evaluate its robustness in performing peg transfer and\ngauze retrieval tasks. Our approach shows strong task performance and\ngeneralizability to varied environment settings. Despite convincing\nperformance, this work is merely a first step towards the integration of\ndigital twin-based scene representations. Future studies are necessary for the\nrealization of a comprehensive digital twin framework to improve the\ninterpretability and generalizability of embodied intelligence in surgery."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Lalithkumar Seenivasan"
                    },
                    {
                        "name": "Hongchao Shu"
                    },
                    {
                        "name": "Grayson Byrd"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Pu Xiao"
                    },
                    {
                        "name": "Juan Antonio Barragan"
                    },
                    {
                        "name": "Russell H. Taylor"
                    },
                    {
                        "name": "Peter Kazanzides"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16354v2",
                "updated": "2024-09-24T15:07:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    7,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-25T01:12:57Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    1,
                    12,
                    57,
                    0,
                    85,
                    0
                ],
                "title": "ChatDBG: An AI-Powered Debugging Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatDBG: An AI-Powered Debugging Assistant"
                },
                "summary": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like `why is x null?'. To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded\nroughly 50,000 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like `why is x null?'. To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded\nroughly 50,000 times."
                },
                "authors": [
                    {
                        "name": "Kyla Levin"
                    },
                    {
                        "name": "Nicolas van Kempen"
                    },
                    {
                        "name": "Emery D. Berger"
                    },
                    {
                        "name": "Stephen N. Freund"
                    }
                ],
                "author_detail": {
                    "name": "Stephen N. Freund"
                },
                "author": "Stephen N. Freund",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12930v2",
                "updated": "2024-09-24T15:00:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    15,
                    0,
                    20,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-19T17:32:17Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    32,
                    17,
                    3,
                    263,
                    0
                ],
                "title": "NL-COMM: Demonstrating Gains of Non-Linear Processing in Open-RAN\n  Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL-COMM: Demonstrating Gains of Non-Linear Processing in Open-RAN\n  Ecosystem"
                },
                "summary": "Multi-user multiple-input, multiple-output (MU-MIMO) designs can\nsubstantially increase wireless systems' achievable throughput and connectivity\ncapabilities. However, existing MU-MIMO deployments typically utilize linear\nprocessing techniques that, despite their practical benefits, such as low\ncomputational complexity and easy integrability, can leave much of the\navailable throughput and connectivity gains unexploited. They typically require\nmany power-intensive antennas and RF chains to support a smaller number of MIMO\nstreams, even when the transmitted information streams are of low rate.\nAlternatively, non-linear (NL) processing methods can maximize the capabilities\nof the MIMO channel. Despite their potential, traditional NL methods are\nchallenged by high computational complexity and processing latency, making them\nimpractical for real-time applications, especially in software-based systems\nenvisioned for emerging Open Radio Access Networks (Open-RAN). Additionally,\nessential functionalities such as rate adaptation (RA) are currently\nunavailable for NL systems, limiting their practicality in real-world\ndeployments. In this demo, we present the latest capabilities of our advanced\nNL processing framework (NL-COMM) in real-time and over-the-air, comparing them\nside-by-side with conventional linear processing. For the first time, NL-COMM\nnot only meets the practical 5G-NR real-time latency requirements in pure\nsoftware but also does so within a standard-compliant ecosystem. To achieve\nthis, we significantly extended the NL-COMM algorithmic framework to support\nthe first practical RA for NL processing. The demonstrated gains include\nenhanced connectivity by supporting four MIMO streams with a single\nbase-station antenna, substantially increased throughput, and the ability to\nhalve the number of base-station antennas without any performance loss to\nlinear approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-user multiple-input, multiple-output (MU-MIMO) designs can\nsubstantially increase wireless systems' achievable throughput and connectivity\ncapabilities. However, existing MU-MIMO deployments typically utilize linear\nprocessing techniques that, despite their practical benefits, such as low\ncomputational complexity and easy integrability, can leave much of the\navailable throughput and connectivity gains unexploited. They typically require\nmany power-intensive antennas and RF chains to support a smaller number of MIMO\nstreams, even when the transmitted information streams are of low rate.\nAlternatively, non-linear (NL) processing methods can maximize the capabilities\nof the MIMO channel. Despite their potential, traditional NL methods are\nchallenged by high computational complexity and processing latency, making them\nimpractical for real-time applications, especially in software-based systems\nenvisioned for emerging Open Radio Access Networks (Open-RAN). Additionally,\nessential functionalities such as rate adaptation (RA) are currently\nunavailable for NL systems, limiting their practicality in real-world\ndeployments. In this demo, we present the latest capabilities of our advanced\nNL processing framework (NL-COMM) in real-time and over-the-air, comparing them\nside-by-side with conventional linear processing. For the first time, NL-COMM\nnot only meets the practical 5G-NR real-time latency requirements in pure\nsoftware but also does so within a standard-compliant ecosystem. To achieve\nthis, we significantly extended the NL-COMM algorithmic framework to support\nthe first practical RA for NL processing. The demonstrated gains include\nenhanced connectivity by supporting four MIMO streams with a single\nbase-station antenna, substantially increased throughput, and the ability to\nhalve the number of base-station antennas without any performance loss to\nlinear approaches."
                },
                "authors": [
                    {
                        "name": "Chathura Jayawardena"
                    },
                    {
                        "name": "Marcin Filo"
                    },
                    {
                        "name": "George N. Katsaros"
                    },
                    {
                        "name": "Konstantinos Nikitopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Nikitopoulos"
                },
                "author": "Konstantinos Nikitopoulos",
                "arxiv_comment": "Accepted for IEEE CAMAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v2",
                "updated": "2024-09-24T14:59:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    59,
                    30,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof tokens that exceed the maximum context length of LLMs, and they experience\nreduced visual clarity due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and introduce Visual Context\nLatent Summarization, which condenses visual contexts into highly compact\nforms. Extensive experiments demonstrate that our model achieves promising\nresults on popular long video understanding benchmarks, despite being trained\non limited image data. Moreover, Video-XL strikes a promising balance between\nefficiency and effectiveness, processing 1024 frames on a single 80GB GPU while\nachieving nearly 100\\% accuracy in the Needle-in-a-Haystack evaluation. We\nenvision Video-XL becoming a valuable tool for long video applications such as\nvideo summarization, surveillance anomaly detection, and Ad placement\nidentification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof tokens that exceed the maximum context length of LLMs, and they experience\nreduced visual clarity due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and introduce Visual Context\nLatent Summarization, which condenses visual contexts into highly compact\nforms. Extensive experiments demonstrate that our model achieves promising\nresults on popular long video understanding benchmarks, despite being trained\non limited image data. Moreover, Video-XL strikes a promising balance between\nefficiency and effectiveness, processing 1024 frames on a single 80GB GPU while\nachieving nearly 100\\% accuracy in the Needle-in-a-Haystack evaluation. We\nenvision Video-XL becoming a valuable tool for long video applications such as\nvideo summarization, surveillance anomaly detection, and Ad placement\nidentification."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16154v2",
                "updated": "2024-09-25T09:00:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    0,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T14:58:27Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    58,
                    27,
                    1,
                    268,
                    0
                ],
                "title": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed"
                },
                "summary": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources."
                },
                "authors": [
                    {
                        "name": "Alexander Prutsch"
                    },
                    {
                        "name": "Horst Bischof"
                    },
                    {
                        "name": "Horst Possegger"
                    }
                ],
                "author_detail": {
                    "name": "Horst Possegger"
                },
                "author": "Horst Possegger",
                "arxiv_comment": "Accepted to IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16136v1",
                "updated": "2024-09-24T14:43:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    43,
                    14,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:43:14Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    43,
                    14,
                    1,
                    268,
                    0
                ],
                "title": "HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear\n  Composition for Open-Vocabulary Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear\n  Composition for Open-Vocabulary Object Detection"
                },
                "summary": "Open-vocabulary object detection (OVD) models are considered to be Large\nMulti-modal Models (LMM), due to their extensive training data and a large\nnumber of parameters. Mainstream OVD models prioritize object coarse-grained\ncategory rather than focus on their fine-grained attributes, e.g., colors or\nmaterials, thus failed to identify objects specified with certain attributes.\nHowever, OVD models are pretrained on large-scale image-text pairs with rich\nattribute words, whose latent feature space can represent the global text\nfeature as a linear composition of fine-grained attribute tokens without\nhighlighting them. Therefore, we propose in this paper a universal and explicit\napproach for frozen mainstream OVD models that boosts their attribute-level\ndetection capabilities by highlighting fine-grained attributes in explicit\nlinear space. Firstly, a LLM is leveraged to highlight attribute words within\nthe input text as a zero-shot prompted task. Secondly, by strategically\nadjusting the token masks, the text encoders of OVD models extract both global\ntext and attribute-specific features, which are then explicitly composited as\ntwo vectors in linear space to form the new attribute-highlighted feature for\ndetection tasks, where corresponding scalars are hand-crafted or learned to\nreweight both two vectors. Notably, these scalars can be seamlessly transferred\namong different OVD models, which proves that such an explicit linear\ncomposition is universal. Empirical evaluation on the FG-OVD dataset\ndemonstrates that our proposed method uniformly improves fine-grained\nattribute-level OVD of various mainstream models and achieves new\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary object detection (OVD) models are considered to be Large\nMulti-modal Models (LMM), due to their extensive training data and a large\nnumber of parameters. Mainstream OVD models prioritize object coarse-grained\ncategory rather than focus on their fine-grained attributes, e.g., colors or\nmaterials, thus failed to identify objects specified with certain attributes.\nHowever, OVD models are pretrained on large-scale image-text pairs with rich\nattribute words, whose latent feature space can represent the global text\nfeature as a linear composition of fine-grained attribute tokens without\nhighlighting them. Therefore, we propose in this paper a universal and explicit\napproach for frozen mainstream OVD models that boosts their attribute-level\ndetection capabilities by highlighting fine-grained attributes in explicit\nlinear space. Firstly, a LLM is leveraged to highlight attribute words within\nthe input text as a zero-shot prompted task. Secondly, by strategically\nadjusting the token masks, the text encoders of OVD models extract both global\ntext and attribute-specific features, which are then explicitly composited as\ntwo vectors in linear space to form the new attribute-highlighted feature for\ndetection tasks, where corresponding scalars are hand-crafted or learned to\nreweight both two vectors. Notably, these scalars can be seamlessly transferred\namong different OVD models, which proves that such an explicit linear\ncomposition is universal. Empirical evaluation on the FG-OVD dataset\ndemonstrates that our proposed method uniformly improves fine-grained\nattribute-level OVD of various mainstream models and achieves new\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Yuqi Ma"
                    },
                    {
                        "name": "Mengyin Liu"
                    },
                    {
                        "name": "Chao Zhu"
                    },
                    {
                        "name": "Xu-Cheng Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xu-Cheng Yin"
                },
                "author": "Xu-Cheng Yin",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16120v1",
                "updated": "2024-09-24T14:30:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    30,
                    21,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:30:21Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    30,
                    21,
                    1,
                    268,
                    0
                ],
                "title": "MOSS: Enabling Code-Driven Evolution and Context Management for AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSS: Enabling Code-Driven Evolution and Context Management for AI\n  Agents"
                },
                "summary": "Developing AI agents powered by large language models (LLMs) faces\nsignificant challenges in achieving true Turing completeness and adaptive,\ncode-driven evolution. Current approaches often generate code independently of\nits runtime context, relying heavily on the LLM's memory, which results in\ninefficiencies and limits adaptability. Manual protocol development in sandbox\nenvironments further constrains the agent's autonomous adaptability. Crucially,\nachieving consistency in code and context across multi-turn interactions and\nensuring isolation of local variables within each interaction remains an\nunsolved problem.\n  We introduce MOSS (llM-oriented Operating System Simulation), a novel\nframework that addresses these challenges by integrating code generation with a\ndynamic context management system. MOSS ensures consistency and adaptability by\nusing a mechanism that maintains the Python context across interactions,\nincluding isolation of local variables and preservation of runtime integrity.\nAt its core, the framework employs an Inversion of Control (IoC) container in\nconjunction with decorators to enforce the least knowledge principle, allowing\nagents to focus on abstract interfaces rather than concrete implementations.\nThis facilitates seamless integration of new tools and libraries, enables\nruntime instance replacement, and reduces prompt complexity, providing a \"what\nyou see is what you get\" environment for the agent.\n  Through a series of case studies, we show how this framework can enhance the\nefficiency and capabilities of agent development and highlight its advantages\nin moving towards Turing-complete agents capable of evolving through code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing AI agents powered by large language models (LLMs) faces\nsignificant challenges in achieving true Turing completeness and adaptive,\ncode-driven evolution. Current approaches often generate code independently of\nits runtime context, relying heavily on the LLM's memory, which results in\ninefficiencies and limits adaptability. Manual protocol development in sandbox\nenvironments further constrains the agent's autonomous adaptability. Crucially,\nachieving consistency in code and context across multi-turn interactions and\nensuring isolation of local variables within each interaction remains an\nunsolved problem.\n  We introduce MOSS (llM-oriented Operating System Simulation), a novel\nframework that addresses these challenges by integrating code generation with a\ndynamic context management system. MOSS ensures consistency and adaptability by\nusing a mechanism that maintains the Python context across interactions,\nincluding isolation of local variables and preservation of runtime integrity.\nAt its core, the framework employs an Inversion of Control (IoC) container in\nconjunction with decorators to enforce the least knowledge principle, allowing\nagents to focus on abstract interfaces rather than concrete implementations.\nThis facilitates seamless integration of new tools and libraries, enables\nruntime instance replacement, and reduces prompt complexity, providing a \"what\nyou see is what you get\" environment for the agent.\n  Through a series of case studies, we show how this framework can enhance the\nefficiency and capabilities of agent development and highlight its advantages\nin moving towards Turing-complete agents capable of evolving through code."
                },
                "authors": [
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16221v2",
                "updated": "2024-09-24T14:25:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    25,
                    58,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-23T06:56:54Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    6,
                    56,
                    54,
                    1,
                    205,
                    0
                ],
                "title": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models"
                },
                "summary": "Abstention Ability (AA) is a critical aspect of Large Language Model (LLM)\nreliability, referring to an LLM's capability to withhold responses when\nuncertain or lacking a definitive answer, without compromising performance.\nAlthough previous studies have attempted to improve AA, they lack a\nstandardised evaluation method and remain unsuitable for black-box models where\ntoken prediction probabilities are inaccessible. This makes comparative\nanalysis challenging, especially for state-of-the-art closed-source commercial\nLLMs. This paper bridges this gap by introducing a black-box evaluation\napproach and a new dataset, Abstain-QA, crafted to rigorously assess AA across\nvaried question types (answerable and unanswerable), domains (well-represented\nand under-represented), and task types (fact centric and reasoning). We also\npropose a new confusion matrix, the ''Answerable-Unanswerable Confusion\nMatrix'' (AUCM) which serves as the basis for evaluating AA, by offering a\nstructured and precise approach for assessment. Finally, we explore the impact\nof three prompting strategies-Strict Prompting, Verbal Confidence Thresholding,\nand Chain-of-Thought (CoT)-on improving AA. Our results indicate that even\npowerful models like GPT-4, Mixtral 8x22b encounter difficulties with\nabstention; however, strategic approaches such as Strict prompting and CoT can\nenhance this capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstention Ability (AA) is a critical aspect of Large Language Model (LLM)\nreliability, referring to an LLM's capability to withhold responses when\nuncertain or lacking a definitive answer, without compromising performance.\nAlthough previous studies have attempted to improve AA, they lack a\nstandardised evaluation method and remain unsuitable for black-box models where\ntoken prediction probabilities are inaccessible. This makes comparative\nanalysis challenging, especially for state-of-the-art closed-source commercial\nLLMs. This paper bridges this gap by introducing a black-box evaluation\napproach and a new dataset, Abstain-QA, crafted to rigorously assess AA across\nvaried question types (answerable and unanswerable), domains (well-represented\nand under-represented), and task types (fact centric and reasoning). We also\npropose a new confusion matrix, the ''Answerable-Unanswerable Confusion\nMatrix'' (AUCM) which serves as the basis for evaluating AA, by offering a\nstructured and precise approach for assessment. Finally, we explore the impact\nof three prompting strategies-Strict Prompting, Verbal Confidence Thresholding,\nand Chain-of-Thought (CoT)-on improving AA. Our results indicate that even\npowerful models like GPT-4, Mixtral 8x22b encounter difficulties with\nabstention; however, strategic approaches such as Strict prompting and CoT can\nenhance this capability."
                },
                "authors": [
                    {
                        "name": "Nishanth Madhusudhan"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Masoud Hashemi"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Hashemi"
                },
                "author": "Masoud Hashemi",
                "arxiv_comment": "8 pages (excluding limitations, references and appendix) and 5\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02731v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02731v4",
                "updated": "2024-09-24T14:14:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    14,
                    40,
                    1,
                    268,
                    0
                ],
                "published": "2024-01-05T09:58:09Z",
                "published_parsed": [
                    2024,
                    1,
                    5,
                    9,
                    58,
                    9,
                    4,
                    5,
                    0
                ],
                "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts\n  for Instruction Tuning on General Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts\n  for Instruction Tuning on General Tasks"
                },
                "summary": "Large language models (LLMs) have demonstrated considerable proficiency in\ngeneral natural language processing (NLP) tasks. Instruction tuning, a\nsuccessful paradigm, enhances the ability of LLMs to follow natural language\ninstructions and exhibit robust generalization across general tasks. However,\nthese models often encounter performance limitations across multiple tasks due\nto constrained model capacity. Expanding this capacity during the instruction\ntuning phase poses significant challenges. To address this issue, we introduce\nparameter-efficient sparsity crafting (PESC), which crafts dense models into\nsparse models using the mixture-of-experts (MoE) architecture. PESC integrates\nadapters into the MoE layers of sparse models, differentiating experts without\naltering the individual weights within these layers. This method significantly\nreduces computational costs and GPU memory requirements, facilitating model\ncapacity expansion through a minimal parameter increase when guaranteeing the\nquality of approximation in function space compared to original sparse\nupcycling. Our empirical evaluation demonstrates the effectiveness of the PESC\nmethod. Using PESC during instruction tuning, our best sparse model outperforms\nother sparse and dense models and exhibits superior general capabilities\ncompared to GPT-3.5. Our code is available at\nhttps://github.com/wuhy68/Parameter-Efficient-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated considerable proficiency in\ngeneral natural language processing (NLP) tasks. Instruction tuning, a\nsuccessful paradigm, enhances the ability of LLMs to follow natural language\ninstructions and exhibit robust generalization across general tasks. However,\nthese models often encounter performance limitations across multiple tasks due\nto constrained model capacity. Expanding this capacity during the instruction\ntuning phase poses significant challenges. To address this issue, we introduce\nparameter-efficient sparsity crafting (PESC), which crafts dense models into\nsparse models using the mixture-of-experts (MoE) architecture. PESC integrates\nadapters into the MoE layers of sparse models, differentiating experts without\naltering the individual weights within these layers. This method significantly\nreduces computational costs and GPU memory requirements, facilitating model\ncapacity expansion through a minimal parameter increase when guaranteeing the\nquality of approximation in function space compared to original sparse\nupcycling. Our empirical evaluation demonstrates the effectiveness of the PESC\nmethod. Using PESC during instruction tuning, our best sparse model outperforms\nother sparse and dense models and exhibits superior general capabilities\ncompared to GPT-3.5. Our code is available at\nhttps://github.com/wuhy68/Parameter-Efficient-MoE."
                },
                "authors": [
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Haisheng Zheng"
                    },
                    {
                        "name": "Zhuolun He"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.02731v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02731v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01238v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01238v3",
                "updated": "2024-09-24T13:53:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    53,
                    59,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-03T11:56:07Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    11,
                    56,
                    7,
                    0,
                    155,
                    0
                ],
                "title": "EffiQA: Efficient Question-Answering with Strategic Multi-Model\n  Collaboration on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EffiQA: Efficient Question-Answering with Strategic Multi-Model\n  Collaboration on Knowledge Graphs"
                },
                "summary": "While large language models (LLMs) have shown remarkable capabilities in\nnatural language processing, they struggle with complex, multi-step reasoning\ntasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs\nand KGs either underutilize the reasoning abilities of LLMs or suffer from\nprohibitive computational costs due to tight coupling. To address these\nlimitations, we propose a novel collaborative framework named EffiQA that can\nstrike a balance between performance and efficiency via an iterative paradigm.\nEffiQA consists of three stages: global planning, efficient KG exploration, and\nself-reflection. Specifically, EffiQA leverages the commonsense capability of\nLLMs to explore potential reasoning pathways through global planning. Then, it\noffloads semantic pruning to a small plug-in model for efficient KG\nexploration. Finally, the exploration results are fed to LLMs for\nself-reflection to further improve the global planning and efficient KG\nexploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's\neffectiveness, achieving an optimal balance between reasoning accuracy and\ncomputational costs. We hope the proposed new framework will pave the way for\nefficient, knowledge-intensive querying by redefining the integration of LLMs\nand KGs, fostering future research on knowledge-based question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown remarkable capabilities in\nnatural language processing, they struggle with complex, multi-step reasoning\ntasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs\nand KGs either underutilize the reasoning abilities of LLMs or suffer from\nprohibitive computational costs due to tight coupling. To address these\nlimitations, we propose a novel collaborative framework named EffiQA that can\nstrike a balance between performance and efficiency via an iterative paradigm.\nEffiQA consists of three stages: global planning, efficient KG exploration, and\nself-reflection. Specifically, EffiQA leverages the commonsense capability of\nLLMs to explore potential reasoning pathways through global planning. Then, it\noffloads semantic pruning to a small plug-in model for efficient KG\nexploration. Finally, the exploration results are fed to LLMs for\nself-reflection to further improve the global planning and efficient KG\nexploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's\neffectiveness, achieving an optimal balance between reasoning accuracy and\ncomputational costs. We hope the proposed new framework will pave the way for\nefficient, knowledge-intensive querying by redefining the integration of LLMs\nand KGs, fostering future research on knowledge-based question answering."
                },
                "authors": [
                    {
                        "name": "Zixuan Dong"
                    },
                    {
                        "name": "Baoyun Peng"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Jia Fu"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Yongxue Shan"
                    },
                    {
                        "name": "Xin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhou"
                },
                "author": "Xin Zhou",
                "arxiv_comment": "10 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01238v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01238v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01833v2",
                "updated": "2024-09-24T13:51:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    51,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-04-02T10:45:49Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    10,
                    45,
                    49,
                    1,
                    93,
                    0
                ],
                "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM\n  Jailbreak Attack"
                },
                "summary": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have risen significantly in popularity and are\nincreasingly being adopted across multiple applications. These LLMs are heavily\naligned to resist engaging in illegal or unethical topics as a means to avoid\ncontributing to responsible AI harms. However, a recent line of attacks, known\nas jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks\naim to narrow the gap between what the model can do and what it is willing to\ndo. In this paper, we introduce a novel jailbreak attack called Crescendo.\nUnlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak\nthat interacts with the model in a seemingly benign manner. It begins with a\ngeneral prompt or question about the task at hand and then gradually escalates\nthe dialogue by referencing the model's replies progressively leading to a\nsuccessful jailbreak. We evaluate Crescendo on various public systems,\nincluding ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat,\nand Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo,\nwith it achieving high attack success rates across all evaluated models and\ntasks. Furthermore, we present Crescendomation, a tool that automates the\nCrescendo attack and demonstrate its efficacy against state-of-the-art models\nthrough our evaluations. Crescendomation surpasses other state-of-the-art\njailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher\nperformance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate\nCrescendo's ability to jailbreak multimodal models."
                },
                "authors": [
                    {
                        "name": "Mark Russinovich"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Ronen Eldan"
                    }
                ],
                "author_detail": {
                    "name": "Ronen Eldan"
                },
                "author": "Ronen Eldan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16096v1",
                "updated": "2024-09-24T13:50:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    50,
                    32,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T13:50:32Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    50,
                    32,
                    1,
                    268,
                    0
                ],
                "title": "Exploring Hint Generation Approaches in Open-Domain Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Hint Generation Approaches in Open-Domain Question Answering"
                },
                "summary": "Automatic Question Answering (QA) systems rely on contextual information to\nprovide accurate answers. Commonly, contexts are prepared through either\nretrieval-based or generation-based methods. The former involves retrieving\nrelevant documents from a corpus like Wikipedia, whereas the latter uses\ngenerative models such as Large Language Models (LLMs) to generate the context.\nIn this paper, we introduce a novel context preparation approach called HINTQA,\nwhich employs Automatic Hint Generation (HG) techniques. Unlike traditional\nmethods, HINTQA prompts LLMs to produce hints about potential answers for the\nquestion rather than generating relevant context. We evaluate our approach\nacross three QA datasets including TriviaQA, NaturalQuestions, and Web\nQuestions, examining how the number and order of hints impact performance. Our\nfindings show that the HINTQA surpasses both retrieval-based and\ngeneration-based approaches. We demonstrate that hints enhance the accuracy of\nanswers more than retrieved and generated contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Question Answering (QA) systems rely on contextual information to\nprovide accurate answers. Commonly, contexts are prepared through either\nretrieval-based or generation-based methods. The former involves retrieving\nrelevant documents from a corpus like Wikipedia, whereas the latter uses\ngenerative models such as Large Language Models (LLMs) to generate the context.\nIn this paper, we introduce a novel context preparation approach called HINTQA,\nwhich employs Automatic Hint Generation (HG) techniques. Unlike traditional\nmethods, HINTQA prompts LLMs to produce hints about potential answers for the\nquestion rather than generating relevant context. We evaluate our approach\nacross three QA datasets including TriviaQA, NaturalQuestions, and Web\nQuestions, examining how the number and order of hints impact performance. Our\nfindings show that the HINTQA surpasses both retrieval-based and\ngeneration-based approaches. We demonstrate that hints enhance the accuracy of\nanswers more than retrieved and generated contexts."
                },
                "authors": [
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03175v2",
                "updated": "2024-09-24T13:30:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    30,
                    25,
                    1,
                    268,
                    0
                ],
                "published": "2024-02-05T16:42:10Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    16,
                    42,
                    10,
                    0,
                    36,
                    0
                ],
                "title": "Beyond the Black Box: A Statistical Model for LLM Reasoning and\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Black Box: A Statistical Model for LLM Reasoning and\n  Inference"
                },
                "summary": "This paper introduces a novel Bayesian learning model to explain the behavior\nof Large Language Models (LLMs), focusing on their core optimization metric of\nnext token prediction. We develop a theoretical framework based on an ideal\ngenerative text model represented by a multinomial transition probability\nmatrix with a prior, and examine how LLMs approximate this matrix. Key\ncontributions include: (i) a continuity theorem relating embeddings to\nmultinomial distributions, (ii) a demonstration that LLM text generation aligns\nwith Bayesian learning principles, (iii) an explanation for the emergence of\nin-context learning in larger models, (iv) empirical validation using\nvisualizations of next token probabilities from an instrumented Llama model Our\nfindings provide new insights into LLM functioning, offering a statistical\nfoundation for understanding their capabilities and limitations. This framework\nhas implications for LLM design, training, and application, potentially guiding\nfuture developments in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel Bayesian learning model to explain the behavior\nof Large Language Models (LLMs), focusing on their core optimization metric of\nnext token prediction. We develop a theoretical framework based on an ideal\ngenerative text model represented by a multinomial transition probability\nmatrix with a prior, and examine how LLMs approximate this matrix. Key\ncontributions include: (i) a continuity theorem relating embeddings to\nmultinomial distributions, (ii) a demonstration that LLM text generation aligns\nwith Bayesian learning principles, (iii) an explanation for the emergence of\nin-context learning in larger models, (iv) empirical validation using\nvisualizations of next token probabilities from an instrumented Llama model Our\nfindings provide new insights into LLM functioning, offering a statistical\nfoundation for understanding their capabilities and limitations. This framework\nhas implications for LLM design, training, and application, potentially guiding\nfuture developments in the field."
                },
                "authors": [
                    {
                        "name": "Siddhartha Dalal"
                    },
                    {
                        "name": "Vishal Misra"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Misra"
                },
                "author": "Vishal Misra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01121v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01121v3",
                "updated": "2024-09-24T13:26:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    26,
                    8,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-02T08:05:03Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    8,
                    5,
                    3,
                    5,
                    62,
                    0
                ],
                "title": "OpenGraph: Towards Open Graph Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenGraph: Towards Open Graph Foundation Models"
                },
                "summary": "Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains."
                },
                "authors": [
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Ben Kao"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "Accepted by EMNLP'2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01121v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01121v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15491v2",
                "updated": "2024-09-24T13:25:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    25,
                    1,
                    1,
                    268,
                    0
                ],
                "published": "2024-03-21T15:41:02Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    15,
                    41,
                    2,
                    3,
                    81,
                    0
                ],
                "title": "Open Conversational LLMs do not know most Spanish words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Conversational LLMs do not know most Spanish words"
                },
                "summary": "The growing interest in Large Language Models (LLMs) and in particular in\nconversational models with which users can interact has led to the development\nof a large number of open-source chat LLMs. These models are evaluated on a\nwide range of benchmarks to assess their capabilities in answering questions or\nsolving problems on almost any possible topic or to test their ability to\nreason or interpret texts. Instead, the evaluation of the knowledge that these\nmodels have of the languages has received much less attention. For example, the\nwords that they can recognize and use in different languages. In this paper, we\nevaluate the knowledge that open-source chat LLMs have of Spanish words by\ntesting a sample of words in a reference dictionary. The results show that\nopen-source chat LLMs produce incorrect meanings for an important fraction of\nthe words and are not able to use most of the words correctly to write\nsentences with context. These results show how Spanish is left behind in the\nopen-source LLM race and highlight the need to push for linguistic fairness in\nconversational LLMs ensuring that they provide similar performance across\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in Large Language Models (LLMs) and in particular in\nconversational models with which users can interact has led to the development\nof a large number of open-source chat LLMs. These models are evaluated on a\nwide range of benchmarks to assess their capabilities in answering questions or\nsolving problems on almost any possible topic or to test their ability to\nreason or interpret texts. Instead, the evaluation of the knowledge that these\nmodels have of the languages has received much less attention. For example, the\nwords that they can recognize and use in different languages. In this paper, we\nevaluate the knowledge that open-source chat LLMs have of Spanish words by\ntesting a sample of words in a reference dictionary. The results show that\nopen-source chat LLMs produce incorrect meanings for an important fraction of\nthe words and are not able to use most of the words correctly to write\nsentences with context. These results show how Spanish is left behind in the\nopen-source LLM race and highlight the need to push for linguistic fairness in\nconversational LLMs ensuring that they provide similar performance across\nlanguages."
                },
                "authors": [
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Miguel Gonz√°lez"
                    },
                    {
                        "name": "Nina Melero"
                    },
                    {
                        "name": "Raquel Ferrando"
                    },
                    {
                        "name": "Gonzalo Mart√≠nez"
                    },
                    {
                        "name": "Elena Merino-G√≥mez"
                    },
                    {
                        "name": "Jos√© Alberto Hern√°ndez"
                    },
                    {
                        "name": "Pedro Reviriego"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Reviriego"
                },
                "author": "Pedro Reviriego",
                "arxiv_doi": "10.26342/2024-73-7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.26342/2024-73-7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.15491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Procesamiento del Lenguaje Natural, 73, 95-108",
                "arxiv_journal_ref": "Procesamiento del Lenguaje Natural, n. 73, 2024.\n  http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6603",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16078v1",
                "updated": "2024-09-24T13:24:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    24,
                    14,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T13:24:14Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    24,
                    14,
                    1,
                    268,
                    0
                ],
                "title": "Assessing strategies to manage distributed photovoltaics in Swiss\n  low-voltage networks: An analysis of curtailment, export tariffs, and\n  resource sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing strategies to manage distributed photovoltaics in Swiss\n  low-voltage networks: An analysis of curtailment, export tariffs, and\n  resource sharing"
                },
                "summary": "The integration of photovoltaic systems poses several challenges for the\ndistribution grid, mainly due to the infrastructure not being designed to\nhandle the upstream flow and being dimensioned for consumption only,\npotentially leading to reliability and stability issues. This study\ninvestigates the use of capacity-based tariffs, export tariffs, and curtailment\npolicies to reduce negative grid impacts without hampering PV deployment. We\nanalyze the effect of such export tariffs on three typical Swiss low-voltage\nnetworks (rural, semi-urban, and urban), using power flow analysis to evaluate\nthe power exchanges at the transformer station, as well as line overloading and\nvoltage violations. Finally, a simple case of mutualization of resources is\nanalyzed to assess its potential contribution to relieving network constraints\nand the economic costs of managing LV networks. We found that the tariff with\ncapacity-based components on the export (CT export daily) severely penalizes PV\npenetration. This applies to other tariffs as well (e.g. IRR monthly,\nCurtailment 30, and DT variable) but to a lesser extent. However, the inclusion\nof curtailment at 50\\% and 70\\%, as well as mixed tariffs with capacity-based\ncomponents at import and curtailment, allow for a high degree of PV\ninstallations in the three zones studied and help to mitigate the impact of PV\non the distributed network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of photovoltaic systems poses several challenges for the\ndistribution grid, mainly due to the infrastructure not being designed to\nhandle the upstream flow and being dimensioned for consumption only,\npotentially leading to reliability and stability issues. This study\ninvestigates the use of capacity-based tariffs, export tariffs, and curtailment\npolicies to reduce negative grid impacts without hampering PV deployment. We\nanalyze the effect of such export tariffs on three typical Swiss low-voltage\nnetworks (rural, semi-urban, and urban), using power flow analysis to evaluate\nthe power exchanges at the transformer station, as well as line overloading and\nvoltage violations. Finally, a simple case of mutualization of resources is\nanalyzed to assess its potential contribution to relieving network constraints\nand the economic costs of managing LV networks. We found that the tariff with\ncapacity-based components on the export (CT export daily) severely penalizes PV\npenetration. This applies to other tariffs as well (e.g. IRR monthly,\nCurtailment 30, and DT variable) but to a lesser extent. However, the inclusion\nof curtailment at 50\\% and 70\\%, as well as mixed tariffs with capacity-based\ncomponents at import and curtailment, allow for a high degree of PV\ninstallations in the three zones studied and help to mitigate the impact of PV\non the distributed network."
                },
                "authors": [
                    {
                        "name": "Alejandro Pena-Bello"
                    },
                    {
                        "name": "Gerard Marias Gonzalez"
                    },
                    {
                        "name": "Nicolas Wyrsch"
                    },
                    {
                        "name": "Christophe Ballif"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Ballif"
                },
                "author": "Christophe Ballif",
                "arxiv_comment": "Preprint version. 25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15188v2",
                "updated": "2024-09-24T13:03:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    3,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T16:39:12Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    16,
                    39,
                    12,
                    0,
                    267,
                    0
                ],
                "title": "PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large\n  Language Models"
                },
                "summary": "Effective patient-provider communication is crucial in clinical care,\ndirectly impacting patient outcomes and quality of life. Traditional evaluation\nmethods, such as human ratings, patient feedback, and provider\nself-assessments, are often limited by high costs and scalability issues.\nAlthough existing natural language processing (NLP) techniques show promise,\nthey struggle with the nuances of clinical communication and require sensitive\nclinical data for training, reducing their effectiveness in real-world\napplications. Emerging large language models (LLMs) offer a new approach to\nassessing complex communication metrics, with the potential to advance the\nfield through integration into passive sensing and just-in-time intervention\nsystems. This study explores LLMs as evaluators of palliative care\ncommunication quality, leveraging their linguistic, in-context learning, and\nreasoning capabilities. Specifically, using simulated scripts crafted and\nlabeled by healthcare professionals, we test proprietary models (e.g., GPT-4)\nand fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset\ngenerated by GPT-4 to evaluate clinical conversations, to identify key metrics\nsuch as `understanding' and `empathy'. Our findings demonstrated LLMs' superior\nperformance in evaluating clinical communication, providing actionable feedback\nwith reasoning, and demonstrating the feasibility and practical viability of\ndeveloping in-house LLMs. This research highlights LLMs' potential to enhance\npatient-provider interactions and lays the groundwork for downstream steps in\ndeveloping LLM-empowered clinical health systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective patient-provider communication is crucial in clinical care,\ndirectly impacting patient outcomes and quality of life. Traditional evaluation\nmethods, such as human ratings, patient feedback, and provider\nself-assessments, are often limited by high costs and scalability issues.\nAlthough existing natural language processing (NLP) techniques show promise,\nthey struggle with the nuances of clinical communication and require sensitive\nclinical data for training, reducing their effectiveness in real-world\napplications. Emerging large language models (LLMs) offer a new approach to\nassessing complex communication metrics, with the potential to advance the\nfield through integration into passive sensing and just-in-time intervention\nsystems. This study explores LLMs as evaluators of palliative care\ncommunication quality, leveraging their linguistic, in-context learning, and\nreasoning capabilities. Specifically, using simulated scripts crafted and\nlabeled by healthcare professionals, we test proprietary models (e.g., GPT-4)\nand fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset\ngenerated by GPT-4 to evaluate clinical conversations, to identify key metrics\nsuch as `understanding' and `empathy'. Our findings demonstrated LLMs' superior\nperformance in evaluating clinical communication, providing actionable feedback\nwith reasoning, and demonstrating the feasibility and practical viability of\ndeveloping in-house LLMs. This research highlights LLMs' potential to enhance\npatient-provider interactions and lays the groundwork for downstream steps in\ndeveloping LLM-empowered clinical health systems."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Fangxu Yuan"
                    },
                    {
                        "name": "Virginia LeBaron"
                    },
                    {
                        "name": "Tabor Flickinger"
                    },
                    {
                        "name": "Laura E. Barnes"
                    }
                ],
                "author_detail": {
                    "name": "Laura E. Barnes"
                },
                "author": "Laura E. Barnes",
                "arxiv_comment": "Accepted by ACM Transactions on Computing for Healthcare, Special\n  Issue on Large Language Models, Conversational Systems, and Generative AI in\n  Health, pending minor revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16048v1",
                "updated": "2024-09-24T12:51:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    51,
                    32,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:51:32Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    51,
                    32,
                    1,
                    268,
                    0
                ],
                "title": "Whole-body end-effector pose tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-body end-effector pose tracking"
                },
                "summary": "Combining manipulation with the mobility of legged robots is essential for a\nwide range of robotic applications. However, integrating an arm with a mobile\nbase significantly increases the system's complexity, making precise\nend-effector control challenging. Existing model-based approaches are often\nconstrained by their modeling assumptions, leading to limited robustness.\nMeanwhile, recent Reinforcement Learning (RL) implementations restrict the\narm's workspace to be in front of the robot or track only the position to\nobtain decent tracking accuracy. In this work, we address these limitations by\nintroducing a whole-body RL formulation for end-effector pose tracking in a\nlarge workspace on rough, unstructured terrains. Our proposed method involves a\nterrain-aware sampling strategy for the robot's initial configuration and\nend-effector pose commands, as well as a game-based curriculum to extend the\nrobot's operating range. We validate our approach on the ANYmal quadrupedal\nrobot with a six DoF robotic arm. Through our experiments, we show that the\nlearned controller achieves precise command tracking over a large workspace and\nadapts across varying terrains such as stairs and slopes. On deployment, it\nachieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming\nexisting competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining manipulation with the mobility of legged robots is essential for a\nwide range of robotic applications. However, integrating an arm with a mobile\nbase significantly increases the system's complexity, making precise\nend-effector control challenging. Existing model-based approaches are often\nconstrained by their modeling assumptions, leading to limited robustness.\nMeanwhile, recent Reinforcement Learning (RL) implementations restrict the\narm's workspace to be in front of the robot or track only the position to\nobtain decent tracking accuracy. In this work, we address these limitations by\nintroducing a whole-body RL formulation for end-effector pose tracking in a\nlarge workspace on rough, unstructured terrains. Our proposed method involves a\nterrain-aware sampling strategy for the robot's initial configuration and\nend-effector pose commands, as well as a game-based curriculum to extend the\nrobot's operating range. We validate our approach on the ANYmal quadrupedal\nrobot with a six DoF robotic arm. Through our experiments, we show that the\nlearned controller achieves precise command tracking over a large workspace and\nadapts across varying terrains such as stairs and slopes. On deployment, it\nachieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming\nexisting competitive baselines."
                },
                "authors": [
                    {
                        "name": "Tifanny Portela"
                    },
                    {
                        "name": "Andrei Cramariuc"
                    },
                    {
                        "name": "Mayank Mittal"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16032v1",
                "updated": "2024-09-24T12:31:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    31,
                    55,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:31:55Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    31,
                    55,
                    1,
                    268,
                    0
                ],
                "title": "Deep chroma compression of tone-mapped images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep chroma compression of tone-mapped images"
                },
                "summary": "Acquisition of high dynamic range (HDR) images is thriving due to the\nincreasing use of smart devices and the demand for high-quality output.\nExtensive research has focused on developing methods for reducing the luminance\nrange in HDR images using conventional and deep learning-based tone mapping\noperators to enable accurate reproduction on conventional 8 and 10-bit digital\ndisplays. However, these methods often fail to account for pixels that may lie\noutside the target display's gamut, resulting in visible chromatic distortions\nor color clipping artifacts. Previous studies suggested that a gamut management\nstep ensures that all pixels remain within the target gamut. However, such\napproaches are computationally expensive and cannot be deployed on devices with\nlimited computational resources. We propose a generative adversarial network\nfor fast and reliable chroma compression of HDR tone-mapped images. We design a\nloss function that considers the hue property of generated images to improve\ncolor accuracy, and train the model on an extensive image dataset. Quantitative\nexperiments demonstrate that the proposed model outperforms state-of-the-art\nimage generation and enhancement networks in color accuracy, while a subjective\nstudy suggests that the generated images are on par or superior to those\nproduced by conventional chroma compression methods in terms of visual quality.\nAdditionally, the model achieves real-time performance, showing promising\nresults for deployment on devices with limited computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquisition of high dynamic range (HDR) images is thriving due to the\nincreasing use of smart devices and the demand for high-quality output.\nExtensive research has focused on developing methods for reducing the luminance\nrange in HDR images using conventional and deep learning-based tone mapping\noperators to enable accurate reproduction on conventional 8 and 10-bit digital\ndisplays. However, these methods often fail to account for pixels that may lie\noutside the target display's gamut, resulting in visible chromatic distortions\nor color clipping artifacts. Previous studies suggested that a gamut management\nstep ensures that all pixels remain within the target gamut. However, such\napproaches are computationally expensive and cannot be deployed on devices with\nlimited computational resources. We propose a generative adversarial network\nfor fast and reliable chroma compression of HDR tone-mapped images. We design a\nloss function that considers the hue property of generated images to improve\ncolor accuracy, and train the model on an extensive image dataset. Quantitative\nexperiments demonstrate that the proposed model outperforms state-of-the-art\nimage generation and enhancement networks in color accuracy, while a subjective\nstudy suggests that the generated images are on par or superior to those\nproduced by conventional chroma compression methods in terms of visual quality.\nAdditionally, the model achieves real-time performance, showing promising\nresults for deployment on devices with limited computational resources."
                },
                "authors": [
                    {
                        "name": "Xenios Milidonis"
                    },
                    {
                        "name": "Francesco Banterle"
                    },
                    {
                        "name": "Alessandro Artusi"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Artusi"
                },
                "author": "Alessandro Artusi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16030v1",
                "updated": "2024-09-24T12:29:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    29,
                    44,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:29:44Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    29,
                    44,
                    1,
                    268,
                    0
                ],
                "title": "MHRC: Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MHRC: Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration\n  with Large Language Models"
                },
                "summary": "The integration of large language models (LLMs) with robotics has\nsignificantly advanced robots' abilities in perception, cognition, and task\nplanning. The use of natural language interfaces offers a unified approach for\nexpressing the capability differences of heterogeneous robots, facilitating\ncommunication between them, and enabling seamless task allocation and\ncollaboration. Currently, the utilization of LLMs to achieve decentralized\nmulti-heterogeneous robot collaborative tasks remains an under-explored area of\nresearch. In this paper, we introduce a novel framework that utilizes LLMs to\nachieve decentralized collaboration among multiple heterogeneous robots. Our\nframework supports three robot categories, mobile robots, manipulation robots,\nand mobile manipulation robots, working together to complete tasks such as\nexploration, transportation, and organization. We developed a rich set of\ntextual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task\nplanning efficiency and overall system performance. The mobile manipulation\nrobot can adjust its base position flexibly, ensuring optimal conditions for\ngrasping tasks. The manipulation robot can comprehend task requirements, seek\nassistance when necessary, and handle objects appropriately. Meanwhile, the\nmobile robot can explore the environment extensively, map object locations, and\ncommunicate this information to the mobile manipulation robot, thus improving\ntask execution efficiency. We evaluated the framework using PyBullet, creating\nscenarios with three different room layouts and three distinct operational\ntasks. We tested various LLM models and conducted ablation studies to assess\nthe contributions of different modules. The experimental results confirm the\neffectiveness and necessity of our proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) with robotics has\nsignificantly advanced robots' abilities in perception, cognition, and task\nplanning. The use of natural language interfaces offers a unified approach for\nexpressing the capability differences of heterogeneous robots, facilitating\ncommunication between them, and enabling seamless task allocation and\ncollaboration. Currently, the utilization of LLMs to achieve decentralized\nmulti-heterogeneous robot collaborative tasks remains an under-explored area of\nresearch. In this paper, we introduce a novel framework that utilizes LLMs to\nachieve decentralized collaboration among multiple heterogeneous robots. Our\nframework supports three robot categories, mobile robots, manipulation robots,\nand mobile manipulation robots, working together to complete tasks such as\nexploration, transportation, and organization. We developed a rich set of\ntextual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task\nplanning efficiency and overall system performance. The mobile manipulation\nrobot can adjust its base position flexibly, ensuring optimal conditions for\ngrasping tasks. The manipulation robot can comprehend task requirements, seek\nassistance when necessary, and handle objects appropriately. Meanwhile, the\nmobile robot can explore the environment extensively, map object locations, and\ncommunicate this information to the mobile manipulation robot, thus improving\ntask execution efficiency. We evaluated the framework using PyBullet, creating\nscenarios with three different room layouts and three distinct operational\ntasks. We tested various LLM models and conducted ablation studies to assess\nthe contributions of different modules. The experimental results confirm the\neffectiveness and necessity of our proposed framework."
                },
                "authors": [
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Yueliang Ying"
                    },
                    {
                        "name": "Sai Li"
                    },
                    {
                        "name": "Jianmin Ji"
                    },
                    {
                        "name": "Yanyong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanyong Zhang"
                },
                "author": "Yanyong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16025v1",
                "updated": "2024-09-24T12:24:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    24,
                    34,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:24:34Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    24,
                    34,
                    1,
                    268,
                    0
                ],
                "title": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question\n  Answering"
                },
                "summary": "Users post numerous product-related questions on e-commerce platforms,\naffecting their purchase decisions. Product-related question answering (PQA)\nentails utilizing product-related resources to provide precise responses to\nusers. We propose a novel task of Multilingual Cross-market Product-based\nQuestion Answering (MCPQA) and define the task as providing answers to\nproduct-related questions in a main marketplace by utilizing information from\nanother resource-rich auxiliary marketplace in a multilingual context. We\nintroduce a large-scale dataset comprising over 7 million questions from 17\nmarketplaces across 11 languages. We then perform automatic translation on the\nElectronics category of our dataset, naming it as McMarket. We focus on two\nsubtasks: review-based answer generation and product-related question ranking.\nFor each subtask, we label a subset of McMarket using an LLM and further\nevaluate the quality of the annotations via human assessment. We then conduct\nexperiments to benchmark our dataset, using models ranging from traditional\nlexical models to LLMs in both single-market and cross-market scenarios across\nMcMarket and the corresponding LLM subset. Results show that incorporating\ncross-market information significantly enhances performance in both tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users post numerous product-related questions on e-commerce platforms,\naffecting their purchase decisions. Product-related question answering (PQA)\nentails utilizing product-related resources to provide precise responses to\nusers. We propose a novel task of Multilingual Cross-market Product-based\nQuestion Answering (MCPQA) and define the task as providing answers to\nproduct-related questions in a main marketplace by utilizing information from\nanother resource-rich auxiliary marketplace in a multilingual context. We\nintroduce a large-scale dataset comprising over 7 million questions from 17\nmarketplaces across 11 languages. We then perform automatic translation on the\nElectronics category of our dataset, naming it as McMarket. We focus on two\nsubtasks: review-based answer generation and product-related question ranking.\nFor each subtask, we label a subset of McMarket using an LLM and further\nevaluate the quality of the annotations via human assessment. We then conduct\nexperiments to benchmark our dataset, using models ranging from traditional\nlexical models to LLMs in both single-market and cross-market scenarios across\nMcMarket and the corresponding LLM subset. Results show that incorporating\ncross-market information significantly enhances performance in both tasks."
                },
                "authors": [
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Anders S√∏gaard"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Aliannejadi"
                },
                "author": "Mohammad Aliannejadi",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16022v1",
                "updated": "2024-09-24T12:23:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    23,
                    15,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:23:15Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    23,
                    15,
                    1,
                    268,
                    0
                ],
                "title": "AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming\n  in LLM-Based Batch Relevance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming\n  in LLM-Based Batch Relevance Assessment"
                },
                "summary": "Cognitive biases are systematic deviations in thinking that lead to\nirrational judgments and problematic decision-making, extensively studied\nacross various fields. Recently, large language models (LLMs) have shown\nadvanced understanding capabilities but may inherit human biases from their\ntraining data. While social biases in LLMs have been well-studied, cognitive\nbiases have received less attention, with existing research focusing on\nspecific scenarios. The broader impact of cognitive biases on LLMs in various\ndecision-making contexts remains underexplored. We investigated whether LLMs\nare influenced by the threshold priming effect in relevance judgments, a core\ntask and widely-discussed research topic in the Information Retrieval (IR)\ncoummunity. The priming effect occurs when exposure to certain stimuli\nunconsciously affects subsequent behavior and decisions. Our experiment\nemployed 10 topics from the TREC 2019 Deep Learning passage track collection,\nand tested AI judgments under different document relevance scores, batch\nlengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.\nResults showed that LLMs tend to give lower scores to later documents if\nearlier ones have high relevance, and vice versa, regardless of the combination\nand model used. Our finding demonstrates that LLM%u2019s judgments, similar to\nhuman judgments, are also influenced by threshold priming biases, and suggests\nthat researchers and system engineers should take into account potential\nhuman-like cognitive biases in designing, evaluating, and auditing LLMs in IR\ntasks and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive biases are systematic deviations in thinking that lead to\nirrational judgments and problematic decision-making, extensively studied\nacross various fields. Recently, large language models (LLMs) have shown\nadvanced understanding capabilities but may inherit human biases from their\ntraining data. While social biases in LLMs have been well-studied, cognitive\nbiases have received less attention, with existing research focusing on\nspecific scenarios. The broader impact of cognitive biases on LLMs in various\ndecision-making contexts remains underexplored. We investigated whether LLMs\nare influenced by the threshold priming effect in relevance judgments, a core\ntask and widely-discussed research topic in the Information Retrieval (IR)\ncoummunity. The priming effect occurs when exposure to certain stimuli\nunconsciously affects subsequent behavior and decisions. Our experiment\nemployed 10 topics from the TREC 2019 Deep Learning passage track collection,\nand tested AI judgments under different document relevance scores, batch\nlengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.\nResults showed that LLMs tend to give lower scores to later documents if\nearlier ones have high relevance, and vice versa, regardless of the combination\nand model used. Our finding demonstrates that LLM%u2019s judgments, similar to\nhuman judgments, are also influenced by threshold priming biases, and suggests\nthat researchers and system engineers should take into account potential\nhuman-like cognitive biases in designing, evaluating, and auditing LLMs in IR\ntasks and beyond."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Jiqun Liu"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16005v1",
                "updated": "2024-09-24T12:06:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    6,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T12:06:31Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    6,
                    31,
                    1,
                    268,
                    0
                ],
                "title": "Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character\n  Pre-training in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character\n  Pre-training in LLMs"
                },
                "summary": "The integration of large language models (LLMs) with pre-trained speech\nmodels has opened up new avenues in automatic speech recognition (ASR). While\nLLMs excel in multimodal understanding tasks, effectively leveraging their\ncapabilities for ASR remains a significant challenge. This paper presents a\nnovel training approach to enhance LLM performance in ASR tasks. We propose\npre-training LLMs on Pinyin embedding sequences, which represent pronunciation\nfeatures, to generate corresponding Chinese characters. This step enables the\nLLM to adapt to generating text from pronunciation features before encountering\nreal speech data. Furthermore, we fine-tune the LoRA parameters to enhance the\nLLM's understanding of speech modality information. In AISHELL-1 corpus, our\napproach yields a 9.5% relative improvement in ASR tasks compared to the\nbaseline without Pinyi-to-Character pre-training. Additionally, incorporating\nauxiliary text data for Pinyi-to-Character pre-training further boosts\nperformance, achieving a 19.0% relative improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) with pre-trained speech\nmodels has opened up new avenues in automatic speech recognition (ASR). While\nLLMs excel in multimodal understanding tasks, effectively leveraging their\ncapabilities for ASR remains a significant challenge. This paper presents a\nnovel training approach to enhance LLM performance in ASR tasks. We propose\npre-training LLMs on Pinyin embedding sequences, which represent pronunciation\nfeatures, to generate corresponding Chinese characters. This step enables the\nLLM to adapt to generating text from pronunciation features before encountering\nreal speech data. Furthermore, we fine-tune the LoRA parameters to enhance the\nLLM's understanding of speech modality information. In AISHELL-1 corpus, our\napproach yields a 9.5% relative improvement in ASR tasks compared to the\nbaseline without Pinyi-to-Character pre-training. Additionally, incorporating\nauxiliary text data for Pinyi-to-Character pre-training further boosts\nperformance, achieving a 19.0% relative improvement."
                },
                "authors": [
                    {
                        "name": "Yang Yuhang"
                    },
                    {
                        "name": "Peng Yizhou"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Xionghu Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Xionghu Zhong"
                },
                "author": "Xionghu Zhong",
                "arxiv_comment": "Accepted by ISCSLP2024-Special session-Speech Processing in LLM Era",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02481v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02481v4",
                "updated": "2024-09-24T12:00:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    0,
                    29,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-04T16:49:06Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    16,
                    49,
                    6,
                    1,
                    156,
                    0
                ],
                "title": "Large Language Models as Carriers of Hidden Messages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Carriers of Hidden Messages"
                },
                "summary": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels)."
                },
                "authors": [
                    {
                        "name": "Jakub Hoscilowicz"
                    },
                    {
                        "name": "Pawel Popiolek"
                    },
                    {
                        "name": "Jan Rudkowski"
                    },
                    {
                        "name": "Jedrzej Bieniasz"
                    },
                    {
                        "name": "Artur Janicki"
                    }
                ],
                "author_detail": {
                    "name": "Artur Janicki"
                },
                "author": "Artur Janicki",
                "arxiv_comment": "Work in progress. Code is available at\n  https://github.com/j-hoscilowic/zurek-stegano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02481v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02481v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15981v1",
                "updated": "2024-09-24T11:22:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    22,
                    55,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T11:22:55Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    22,
                    55,
                    1,
                    268,
                    0
                ],
                "title": "GPT-4 as a Homework Tutor can Improve Student Engagement and Learning\n  Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4 as a Homework Tutor can Improve Student Engagement and Learning\n  Outcomes"
                },
                "summary": "This work contributes to the scarce empirical literature on LLM-based\ninteractive homework in real-world educational settings and offers a practical,\nscalable solution for improving homework in schools. Homework is an important\npart of education in schools across the world, but in order to maximize\nbenefit, it needs to be accompanied with feedback and followup questions. We\ndeveloped a prompting strategy that enables GPT-4 to conduct interactive\nhomework sessions for high-school students learning English as a second\nlanguage. Our strategy requires minimal efforts in content preparation, one of\nthe key challenges of alternatives like home tutors or ITSs. We carried out a\nRandomized Controlled Trial (RCT) in four high-school classes, replacing\ntraditional homework with GPT-4 homework sessions for the treatment group. We\nobserved significant improvements in learning outcomes, specifically a greater\ngain in grammar, and student engagement. In addition, students reported high\nlevels of satisfaction with the system and wanted to continue using it after\nthe end of the RCT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work contributes to the scarce empirical literature on LLM-based\ninteractive homework in real-world educational settings and offers a practical,\nscalable solution for improving homework in schools. Homework is an important\npart of education in schools across the world, but in order to maximize\nbenefit, it needs to be accompanied with feedback and followup questions. We\ndeveloped a prompting strategy that enables GPT-4 to conduct interactive\nhomework sessions for high-school students learning English as a second\nlanguage. Our strategy requires minimal efforts in content preparation, one of\nthe key challenges of alternatives like home tutors or ITSs. We carried out a\nRandomized Controlled Trial (RCT) in four high-school classes, replacing\ntraditional homework with GPT-4 homework sessions for the treatment group. We\nobserved significant improvements in learning outcomes, specifically a greater\ngain in grammar, and student engagement. In addition, students reported high\nlevels of satisfaction with the system and wanted to continue using it after\nthe end of the RCT."
                },
                "authors": [
                    {
                        "name": "Alessandro Vanzo"
                    },
                    {
                        "name": "Sankalan Pal Chowdhury"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "Submitted to LAK25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15979v1",
                "updated": "2024-09-24T11:21:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    21,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T11:21:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    21,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Finetuning LLMs for Comparative Assessment Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning LLMs for Comparative Assessment Tasks"
                },
                "summary": "Automated assessment in natural language generation is a challenging task.\nInstruction-tuned large language models (LLMs) have shown promise in\nreference-free evaluation, particularly through comparative assessment.\nHowever, the quadratic computational complexity of pairwise comparisons limits\nits scalability. To address this, efficient comparative assessment has been\nexplored by applying comparative strategies on zero-shot LLM probabilities. We\npropose a framework for finetuning LLMs for comparative assessment to align the\nmodel's output with the target distribution of comparative probabilities. By\ntraining on soft probabilities, our approach improves state-of-the-art\nperformance while maintaining high performance with an efficient subset of\ncomparisons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated assessment in natural language generation is a challenging task.\nInstruction-tuned large language models (LLMs) have shown promise in\nreference-free evaluation, particularly through comparative assessment.\nHowever, the quadratic computational complexity of pairwise comparisons limits\nits scalability. To address this, efficient comparative assessment has been\nexplored by applying comparative strategies on zero-shot LLM probabilities. We\npropose a framework for finetuning LLMs for comparative assessment to align the\nmodel's output with the target distribution of comparative probabilities. By\ntraining on soft probabilities, our approach improves state-of-the-art\nperformance while maintaining high performance with an efficient subset of\ncomparisons."
                },
                "authors": [
                    {
                        "name": "Vatsal Raina"
                    },
                    {
                        "name": "Adian Liusie"
                    },
                    {
                        "name": "Mark Gales"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gales"
                },
                "author": "Mark Gales",
                "arxiv_comment": "8 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10553v2",
                "updated": "2024-09-24T11:00:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    0,
                    38,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-02T13:27:36Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    27,
                    36,
                    0,
                    246,
                    0
                ],
                "title": "\"Flipped\" University: LLM-Assisted Lifelong Learning Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Flipped\" University: LLM-Assisted Lifelong Learning Environment"
                },
                "summary": "The rapid development of artificial intelligence technologies, particularly\nLarge Language Models (LLMs), has revolutionized the landscape of lifelong\nlearning. This paper introduces a conceptual framework for a self-constructed\nlifelong learning environment supported by LLMs. It highlights the inadequacies\nof traditional education systems in keeping pace with the rapid deactualization\nof knowledge and skills. The proposed framework emphasizes the transformation\nfrom institutionalized education to personalized, self-driven learning. It\nleverages the natural language capabilities of LLMs to provide dynamic and\nadaptive learning experiences, facilitating the creation of personal\nintellectual agents that assist in knowledge acquisition. The framework\nintegrates principles of lifelong learning, including the necessity of building\npersonal world models, the dual modes of learning (training and exploration),\nand the creation of reusable learning artifacts. Additionally, it underscores\nthe importance of curiosity-driven learning and reflective practices in\nmaintaining an effective learning trajectory. The paper envisions the evolution\nof educational institutions into \"flipped\" universities, focusing on supporting\nglobal knowledge consistency rather than merely structuring and transmitting\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of artificial intelligence technologies, particularly\nLarge Language Models (LLMs), has revolutionized the landscape of lifelong\nlearning. This paper introduces a conceptual framework for a self-constructed\nlifelong learning environment supported by LLMs. It highlights the inadequacies\nof traditional education systems in keeping pace with the rapid deactualization\nof knowledge and skills. The proposed framework emphasizes the transformation\nfrom institutionalized education to personalized, self-driven learning. It\nleverages the natural language capabilities of LLMs to provide dynamic and\nadaptive learning experiences, facilitating the creation of personal\nintellectual agents that assist in knowledge acquisition. The framework\nintegrates principles of lifelong learning, including the necessity of building\npersonal world models, the dual modes of learning (training and exploration),\nand the creation of reusable learning artifacts. Additionally, it underscores\nthe importance of curiosity-driven learning and reflective practices in\nmaintaining an effective learning trajectory. The paper envisions the evolution\nof educational institutions into \"flipped\" universities, focusing on supporting\nglobal knowledge consistency rather than merely structuring and transmitting\nknowledge."
                },
                "authors": [
                    {
                        "name": "Kirill Krinkin"
                    },
                    {
                        "name": "Tatiana Berlenko"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Berlenko"
                },
                "author": "Tatiana Berlenko",
                "arxiv_comment": "Pre-print version, accepted for 31st International Conference on\n  Neural Information Processing (ICONIP2024), 13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v2",
                "updated": "2024-09-24T10:57:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    57,
                    18,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11176v2",
                "updated": "2024-09-24T10:01:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    10,
                    1,
                    31,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-17T03:29:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    3,
                    29,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process\n  Refinement"
                },
                "summary": "Large language model agents have exhibited exceptional performance across a\nrange of complex interactive tasks. Recent approaches have utilized tuning with\nexpert trajectories to enhance agent performance, yet they primarily\nconcentrate on outcome rewards, which may lead to errors or suboptimal actions\ndue to the absence of process supervision signals. In this paper, we introduce\nthe Iterative step-level Process Refinement (IPR) framework, which provides\ndetailed step-by-step guidance to enhance agent training. Specifically, we\nadopt the Monte Carlo method to estimate step-level rewards. During each\niteration, the agent explores along the expert trajectory and generates new\nactions. These actions are then evaluated against the corresponding step of\nexpert trajectory using step-level rewards. Such comparison helps identify\ndiscrepancies, yielding contrastive action pairs that serve as training data\nfor the agent. Our experiments on three complex agent tasks demonstrate that\nour framework outperforms a variety of strong baselines. Moreover, our\nanalytical findings highlight the effectiveness of IPR in augmenting action\nefficiency and its applicability to diverse models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model agents have exhibited exceptional performance across a\nrange of complex interactive tasks. Recent approaches have utilized tuning with\nexpert trajectories to enhance agent performance, yet they primarily\nconcentrate on outcome rewards, which may lead to errors or suboptimal actions\ndue to the absence of process supervision signals. In this paper, we introduce\nthe Iterative step-level Process Refinement (IPR) framework, which provides\ndetailed step-by-step guidance to enhance agent training. Specifically, we\nadopt the Monte Carlo method to estimate step-level rewards. During each\niteration, the agent explores along the expert trajectory and generates new\nactions. These actions are then evaluated against the corresponding step of\nexpert trajectory using step-level rewards. Such comparison helps identify\ndiscrepancies, yielding contrastive action pairs that serve as training data\nfor the agent. Our experiments on three complex agent tasks demonstrate that\nour framework outperforms a variety of strong baselines. Moreover, our\nanalytical findings highlight the effectiveness of IPR in augmenting action\nefficiency and its applicability to diverse models."
                },
                "authors": [
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Xiutian Zhao"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "Accepted to EMNLP 2024 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15934v1",
                "updated": "2024-09-24T09:57:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:57:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents"
                },
                "summary": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains."
                },
                "authors": [
                    {
                        "name": "Samuel Arcadinho"
                    },
                    {
                        "name": "David Aparicio"
                    },
                    {
                        "name": "Mariana Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Almeida"
                },
                "author": "Mariana Almeida",
                "arxiv_comment": "14 pages, 5 figures, Submitted to GenBench@EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15933v1",
                "updated": "2024-09-24T09:57:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    25,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:57:25Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    25,
                    1,
                    268,
                    0
                ],
                "title": "SLIMER-IT: Zero-Shot NER on Italian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIMER-IT: Zero-Shot NER on Italian Language"
                },
                "summary": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags."
                },
                "authors": [
                    {
                        "name": "Andrew Zamai"
                    },
                    {
                        "name": "Leonardo Rigutini"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Andrea Zugarini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zugarini"
                },
                "author": "Andrea Zugarini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02089v2",
                "updated": "2024-09-24T09:50:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    50,
                    58,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-02T09:25:58Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    25,
                    58,
                    1,
                    184,
                    0
                ],
                "title": "GPTCast: a weather language model for precipitation nowcasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTCast: a weather language model for precipitation nowcasting"
                },
                "summary": "This work introduces GPTCast, a generative deep-learning method for ensemble\nnowcast of radar-based precipitation, inspired by advancements in large\nlanguage models (LLMs). We employ a GPT model as a forecaster to learn\nspatiotemporal precipitation dynamics using tokenized radar images. The\ntokenizer is based on a Quantized Variational Autoencoder featuring a novel\nreconstruction loss tailored for the skewed distribution of precipitation that\npromotes faithful reconstruction of high rainfall rates. The approach produces\nrealistic ensemble forecasts and provides probabilistic outputs with accurate\nuncertainty estimation. The model is trained without resorting to randomness,\nall variability is learned solely from the data and exposed by model at\ninference for ensemble generation. We train and test GPTCast using a 6-year\nradar dataset over the Emilia-Romagna region in Northern Italy, showing\nsuperior results compared to state-of-the-art ensemble extrapolation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces GPTCast, a generative deep-learning method for ensemble\nnowcast of radar-based precipitation, inspired by advancements in large\nlanguage models (LLMs). We employ a GPT model as a forecaster to learn\nspatiotemporal precipitation dynamics using tokenized radar images. The\ntokenizer is based on a Quantized Variational Autoencoder featuring a novel\nreconstruction loss tailored for the skewed distribution of precipitation that\npromotes faithful reconstruction of high rainfall rates. The approach produces\nrealistic ensemble forecasts and provides probabilistic outputs with accurate\nuncertainty estimation. The model is trained without resorting to randomness,\nall variability is learned solely from the data and exposed by model at\ninference for ensemble generation. We train and test GPTCast using a 6-year\nradar dataset over the Emilia-Romagna region in Northern Italy, showing\nsuperior results compared to state-of-the-art ensemble extrapolation methods."
                },
                "authors": [
                    {
                        "name": "Gabriele Franch"
                    },
                    {
                        "name": "Elena Tomasi"
                    },
                    {
                        "name": "Rishabh Wanjari"
                    },
                    {
                        "name": "Virginia Poli"
                    },
                    {
                        "name": "Chiara Cardinali"
                    },
                    {
                        "name": "Pier Paolo Alberoni"
                    },
                    {
                        "name": "Marco Cristoforetti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Cristoforetti"
                },
                "author": "Marco Cristoforetti",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09920v2",
                "updated": "2024-09-24T09:48:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    48,
                    36,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-14T11:02:21Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    11,
                    2,
                    21,
                    4,
                    166,
                    0
                ],
                "title": "Knowledge Editing in Language Models via Adapted Direct Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing in Language Models via Adapted Direct Preference\n  Optimization"
                },
                "summary": "Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods."
                },
                "authors": [
                    {
                        "name": "Amit Rozner"
                    },
                    {
                        "name": "Barak Battash"
                    },
                    {
                        "name": "Lior Wolf"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15915v1",
                "updated": "2024-09-24T09:33:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    33,
                    12,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:33:12Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    33,
                    12,
                    1,
                    268,
                    0
                ],
                "title": "Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts"
                },
                "summary": "Large Language Models (LLMs) have shown promise in solving natural\nlanguage-described planning tasks, but their direct use often leads to\ninconsistent reasoning and hallucination. While hybrid LLM-symbolic planning\npipelines have emerged as a more robust alternative, they typically require\nextensive expert intervention to refine and validate generated action schemas.\nIt not only limits scalability but also introduces a potential for biased\ninterpretation, as a single expert's interpretation of ambiguous natural\nlanguage descriptions might not align with the user's actual intent. To address\nthis, we propose a novel approach that constructs an action schema library to\ngenerate multiple candidates, accounting for the diverse possible\ninterpretations of natural language descriptions. We further introduce a\nsemantic validation and ranking module that automatically filter and rank the\ngenerated schemas and plans without expert-in-the-loop. The experiments showed\nour pipeline maintains superiority in planning over the direct LLM planning\napproach. These findings demonstrate the feasibility of a fully automated\nend-to-end LLM-symbolic planner that requires no expert intervention, opening\nup the possibility for a broader audience to engage with AI planning with less\nprerequisite of domain expertise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise in solving natural\nlanguage-described planning tasks, but their direct use often leads to\ninconsistent reasoning and hallucination. While hybrid LLM-symbolic planning\npipelines have emerged as a more robust alternative, they typically require\nextensive expert intervention to refine and validate generated action schemas.\nIt not only limits scalability but also introduces a potential for biased\ninterpretation, as a single expert's interpretation of ambiguous natural\nlanguage descriptions might not align with the user's actual intent. To address\nthis, we propose a novel approach that constructs an action schema library to\ngenerate multiple candidates, accounting for the diverse possible\ninterpretations of natural language descriptions. We further introduce a\nsemantic validation and ranking module that automatically filter and rank the\ngenerated schemas and plans without expert-in-the-loop. The experiments showed\nour pipeline maintains superiority in planning over the direct LLM planning\napproach. These findings demonstrate the feasibility of a fully automated\nend-to-end LLM-symbolic planner that requires no expert intervention, opening\nup the possibility for a broader audience to engage with AI planning with less\nprerequisite of domain expertise."
                },
                "authors": [
                    {
                        "name": "Sukai Huang"
                    },
                    {
                        "name": "Nir Lipovetzky"
                    },
                    {
                        "name": "Trevor Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohn"
                },
                "author": "Trevor Cohn",
                "arxiv_comment": "8 main body pages, 10 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15907v1",
                "updated": "2024-09-24T09:24:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    24,
                    3,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:24:03Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    24,
                    3,
                    1,
                    268,
                    0
                ],
                "title": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection"
                },
                "summary": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper."
                },
                "authors": [
                    {
                        "name": "Xingyu Ma"
                    },
                    {
                        "name": "Xin Tian"
                    },
                    {
                        "name": "Lingxiang Wu"
                    },
                    {
                        "name": "Xuepeng Wang"
                    },
                    {
                        "name": "Xueming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "arxiv_comment": "This paper has been accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15905v1",
                "updated": "2024-09-24T09:20:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    20,
                    22,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:20:22Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    20,
                    22,
                    1,
                    268,
                    0
                ],
                "title": "Boosting Code-Switching ASR with Mixture of Experts Enhanced\n  Speech-Conditioned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Code-Switching ASR with Mixture of Experts Enhanced\n  Speech-Conditioned LLM"
                },
                "summary": "In this paper, we introduce a speech-conditioned Large Language Model (LLM)\nintegrated with a Mixture of Experts (MoE) based connector to address the\nchallenge of Code-Switching (CS) in Automatic Speech Recognition (ASR).\nSpecifically, we propose an Insertion and Deletion of Interruption Token (IDIT)\nmechanism for better transfer text generation ability of LLM to speech\nrecognition task. We also present a connecter with MoE architecture that\nmanages multiple languages efficiently. To further enhance the collaboration of\nmultiple experts and leverage the understanding capabilities of LLM, we propose\na two-stage progressive training strategy: 1) The connector is unfrozen and\ntrained with language-specialized experts to map speech representations to the\ntext space. 2) The connector and LLM LoRA adaptor are trained with the proposed\nIDIT mechanism and all experts are activated to learn general representations.\nExperimental results demonstrate that our method significantly outperforms\nstate-of-the-art models, including end-to-end and large-scale audio-language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a speech-conditioned Large Language Model (LLM)\nintegrated with a Mixture of Experts (MoE) based connector to address the\nchallenge of Code-Switching (CS) in Automatic Speech Recognition (ASR).\nSpecifically, we propose an Insertion and Deletion of Interruption Token (IDIT)\nmechanism for better transfer text generation ability of LLM to speech\nrecognition task. We also present a connecter with MoE architecture that\nmanages multiple languages efficiently. To further enhance the collaboration of\nmultiple experts and leverage the understanding capabilities of LLM, we propose\na two-stage progressive training strategy: 1) The connector is unfrozen and\ntrained with language-specialized experts to map speech representations to the\ntext space. 2) The connector and LLM LoRA adaptor are trained with the proposed\nIDIT mechanism and all experts are activated to learn general representations.\nExperimental results demonstrate that our method significantly outperforms\nstate-of-the-art models, including end-to-end and large-scale audio-language\nmodels."
                },
                "authors": [
                    {
                        "name": "Fengrun Zhang"
                    },
                    {
                        "name": "Wang Geng"
                    },
                    {
                        "name": "Hukai Huang"
                    },
                    {
                        "name": "Cheng Yi"
                    },
                    {
                        "name": "He Qu"
                    }
                ],
                "author_detail": {
                    "name": "He Qu"
                },
                "author": "He Qu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15890v1",
                "updated": "2024-09-24T09:02:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    2,
                    28,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T09:02:28Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    2,
                    28,
                    1,
                    268,
                    0
                ],
                "title": "HLB: Benchmarking LLMs' Humanlikeness in Language Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HLB: Benchmarking LLMs' Humanlikeness in Language Use"
                },
                "summary": "As synthetic data becomes increasingly prevalent in training language models,\nparticularly through generated dialogue, concerns have emerged that these\nmodels may deviate from authentic human language patterns, potentially losing\nthe richness and creativity inherent in human communication. This highlights\nthe critical need to assess the humanlikeness of language models in real-world\nlanguage use. In this paper, we present a comprehensive humanlikeness benchmark\n(HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic\nexperiments designed to probe core linguistic aspects, including sound, word,\nsyntax, semantics, and discourse (see\nhttps://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these\ncomparisons, we collected responses from over 2,000 human participants and\ncompared them to outputs from the LLMs in these experiments.\n  For rigorous evaluation, we developed a coding algorithm that accurately\nidentified language use patterns, enabling the extraction of response\ndistributions for each task. By comparing the response distributions between\nhuman participants and LLMs, we quantified humanlikeness through distributional\nsimilarity. Our results reveal fine-grained differences in how well LLMs\nreplicate human responses across various linguistic levels. Importantly, we\nfound that improvements in other performance metrics did not necessarily lead\nto greater humanlikeness, and in some cases, even resulted in a decline. By\nintroducing psycholinguistic methods to model evaluation, this benchmark offers\nthe first framework for systematically assessing the humanlikeness of LLMs in\nlanguage use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As synthetic data becomes increasingly prevalent in training language models,\nparticularly through generated dialogue, concerns have emerged that these\nmodels may deviate from authentic human language patterns, potentially losing\nthe richness and creativity inherent in human communication. This highlights\nthe critical need to assess the humanlikeness of language models in real-world\nlanguage use. In this paper, we present a comprehensive humanlikeness benchmark\n(HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic\nexperiments designed to probe core linguistic aspects, including sound, word,\nsyntax, semantics, and discourse (see\nhttps://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these\ncomparisons, we collected responses from over 2,000 human participants and\ncompared them to outputs from the LLMs in these experiments.\n  For rigorous evaluation, we developed a coding algorithm that accurately\nidentified language use patterns, enabling the extraction of response\ndistributions for each task. By comparing the response distributions between\nhuman participants and LLMs, we quantified humanlikeness through distributional\nsimilarity. Our results reveal fine-grained differences in how well LLMs\nreplicate human responses across various linguistic levels. Importantly, we\nfound that improvements in other performance metrics did not necessarily lead\nto greater humanlikeness, and in some cases, even resulted in a decline. By\nintroducing psycholinguistic methods to model evaluation, this benchmark offers\nthe first framework for systematically assessing the humanlikeness of LLMs in\nlanguage use."
                },
                "authors": [
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Bei Xiao"
                    },
                    {
                        "name": "Xuemei Tang"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15881v1",
                "updated": "2024-09-24T08:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    55,
                    7,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    55,
                    7,
                    1,
                    268,
                    0
                ],
                "title": "Automatic Bottom-Up Taxonomy Construction: A Software Application Domain\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Bottom-Up Taxonomy Construction: A Software Application Domain\n  Study"
                },
                "summary": "Previous research in software application domain classification has faced\nchallenges due to the lack of a proper taxonomy that explicitly models\nrelations between classes. As a result, current solutions are less effective\nfor real-world usage. This study aims to develop a comprehensive software\napplication domain taxonomy by integrating multiple datasources and leveraging\nensemble methods. The goal is to overcome the limitations of individual sources\nand configurations by creating a more robust, accurate, and reproducible\ntaxonomy. This study employs a quantitative research design involving three\ndifferent datasources: an existing Computer Science Ontology (CSO), Wikidata,\nand LLMs. The study utilises a combination of automated and human evaluations\nto assess the quality of a taxonomy. The outcome measures include the number of\nunlinked terms, self-loops, and overall connectivity of the taxonomy. The\nresults indicate that individual datasources have advantages and drawbacks: the\nCSO datasource showed minimal variance across different configurations, but a\nnotable issue of missing technical terms and a high number of self-loops. The\nWikipedia datasource required significant filtering during construction to\nimprove metric performance. LLM-generated taxonomies demonstrated better\nperformance when using context-rich prompts. An ensemble approach showed the\nmost promise, successfully reducing the number of unlinked terms and\nself-loops, thus creating a more connected and comprehensive taxonomy. The\nstudy addresses the construction of a software application domain taxonomy\nrelying on pre-existing resources. Our results indicate that an ensemble\napproach to taxonomy construction can effectively address the limitations of\nindividual datasources. Future work should focus on refining the ensemble\ntechniques and exploring additional datasources to enhance the taxonomy's\naccuracy and completeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research in software application domain classification has faced\nchallenges due to the lack of a proper taxonomy that explicitly models\nrelations between classes. As a result, current solutions are less effective\nfor real-world usage. This study aims to develop a comprehensive software\napplication domain taxonomy by integrating multiple datasources and leveraging\nensemble methods. The goal is to overcome the limitations of individual sources\nand configurations by creating a more robust, accurate, and reproducible\ntaxonomy. This study employs a quantitative research design involving three\ndifferent datasources: an existing Computer Science Ontology (CSO), Wikidata,\nand LLMs. The study utilises a combination of automated and human evaluations\nto assess the quality of a taxonomy. The outcome measures include the number of\nunlinked terms, self-loops, and overall connectivity of the taxonomy. The\nresults indicate that individual datasources have advantages and drawbacks: the\nCSO datasource showed minimal variance across different configurations, but a\nnotable issue of missing technical terms and a high number of self-loops. The\nWikipedia datasource required significant filtering during construction to\nimprove metric performance. LLM-generated taxonomies demonstrated better\nperformance when using context-rich prompts. An ensemble approach showed the\nmost promise, successfully reducing the number of unlinked terms and\nself-loops, thus creating a more connected and comprehensive taxonomy. The\nstudy addresses the construction of a software application domain taxonomy\nrelying on pre-existing resources. Our results indicate that an ensemble\napproach to taxonomy construction can effectively address the limitations of\nindividual datasources. Future work should focus on refining the ensemble\ntechniques and exploring additional datasources to enhance the taxonomy's\naccuracy and completeness."
                },
                "authors": [
                    {
                        "name": "Cezar Sas"
                    },
                    {
                        "name": "Andrea Capiluppi"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Capiluppi"
                },
                "author": "Andrea Capiluppi",
                "arxiv_comment": "17 pages, 8 tables, 6 figures, and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12623v2",
                "updated": "2024-09-24T08:49:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    49,
                    21,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-19T09:52:35Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    9,
                    52,
                    35,
                    3,
                    263,
                    0
                ],
                "title": "CamelEval: Advancing Culturally Aligned Arabic Language Models and\n  Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CamelEval: Advancing Culturally Aligned Arabic Language Models and\n  Benchmarks"
                },
                "summary": "Large Language Models (LLMs) are the cornerstones of modern artificial\nintelligence systems. This paper introduces Juhaina, a Arabic-English bilingual\nLLM specifically designed to align with the values and preferences of Arabic\nspeakers. Juhaina inherently supports advanced functionalities such as\ninstruction following, open-ended question answering, information provisioning,\nand text processing. Our model contains 9.24 billion parameters and is trained\non a context window of up to 8,192 tokens. This paper details the creation\nprocess of Juhaina and provides an extensive empirical evaluation. Furthermore,\nwe identify the limitations of widely-adopted Open Arabic LLM Leaderboard\n(OALL) and propose a new evaluation benchmark, CamelEval. Our findings\ndemonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as\nthe Llama and Gemma families, in generating helpful responses in Arabic,\nproviding factually accurate information about the region, and understanding\nnuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI\ntechnologies, serving over 400 million Arabic speakers by offering LLMs that\nnot only communicate in their language but also comprehend their culture. We\npublicly release all models on Huggingface \\url{https://huggingface.co/elmrc}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are the cornerstones of modern artificial\nintelligence systems. This paper introduces Juhaina, a Arabic-English bilingual\nLLM specifically designed to align with the values and preferences of Arabic\nspeakers. Juhaina inherently supports advanced functionalities such as\ninstruction following, open-ended question answering, information provisioning,\nand text processing. Our model contains 9.24 billion parameters and is trained\non a context window of up to 8,192 tokens. This paper details the creation\nprocess of Juhaina and provides an extensive empirical evaluation. Furthermore,\nwe identify the limitations of widely-adopted Open Arabic LLM Leaderboard\n(OALL) and propose a new evaluation benchmark, CamelEval. Our findings\ndemonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as\nthe Llama and Gemma families, in generating helpful responses in Arabic,\nproviding factually accurate information about the region, and understanding\nnuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI\ntechnologies, serving over 400 million Arabic speakers by offering LLMs that\nnot only communicate in their language but also comprehend their culture. We\npublicly release all models on Huggingface \\url{https://huggingface.co/elmrc}."
                },
                "authors": [
                    {
                        "name": "Zhaozhi Qian"
                    },
                    {
                        "name": "Faroq Altam"
                    },
                    {
                        "name": "Muhammad Alqurishi"
                    },
                    {
                        "name": "Riad Souissi"
                    }
                ],
                "author_detail": {
                    "name": "Riad Souissi"
                },
                "author": "Riad Souissi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15868v2",
                "updated": "2024-09-25T07:03:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T08:41:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    41,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Privacy Evaluation Benchmarks for NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Evaluation Benchmarks for NLP Models"
                },
                "summary": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "arxiv_comment": "Needs further optimization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15865v1",
                "updated": "2024-09-24T08:37:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    37,
                    4,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:37:04Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    37,
                    4,
                    1,
                    268,
                    0
                ],
                "title": "BeSimulator: A Large Language Model Powered Text-based Behavior\n  Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeSimulator: A Large Language Model Powered Text-based Behavior\n  Simulator"
                },
                "summary": "Traditional robot simulators focus on physical process modeling and realistic\nrendering, often suffering from high computational costs, inefficiencies, and\nlimited adaptability. To handle this issue, we propose Behavior Simulation in\nrobotics to emphasize checking the behavior logic of robots and achieving\nsufficient alignment between the outcome of robot actions and real scenarios.\nIn this paper, we introduce BeSimulator, a modular and novel LLM-powered\nframework, as an attempt towards behavior simulation in the context of\ntext-based environments. By constructing text-based virtual environments and\nperforming semantic-level simulation, BeSimulator can generalize across\nscenarios and achieve long-horizon complex simulation. Inspired by human\ncognition processes, it employs a \"consider-decide-capture-transfer\"\nmethodology, termed Chain of Behavior Simulation, which excels at analyzing\naction feasibility and state transitions. Additionally, BeSimulator\nincorporates code-driven reasoning to enable arithmetic operations and enhance\nreliability, as well as integrates reflective feedback to refine simulation.\nBased on our manually constructed behavior-tree-based simulation benchmark\nBTSIMBENCH, our experiments show a significant performance improvement in\nbehavior simulation compared to baselines, ranging from 14.7% to 26.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional robot simulators focus on physical process modeling and realistic\nrendering, often suffering from high computational costs, inefficiencies, and\nlimited adaptability. To handle this issue, we propose Behavior Simulation in\nrobotics to emphasize checking the behavior logic of robots and achieving\nsufficient alignment between the outcome of robot actions and real scenarios.\nIn this paper, we introduce BeSimulator, a modular and novel LLM-powered\nframework, as an attempt towards behavior simulation in the context of\ntext-based environments. By constructing text-based virtual environments and\nperforming semantic-level simulation, BeSimulator can generalize across\nscenarios and achieve long-horizon complex simulation. Inspired by human\ncognition processes, it employs a \"consider-decide-capture-transfer\"\nmethodology, termed Chain of Behavior Simulation, which excels at analyzing\naction feasibility and state transitions. Additionally, BeSimulator\nincorporates code-driven reasoning to enable arithmetic operations and enhance\nreliability, as well as integrates reflective feedback to refine simulation.\nBased on our manually constructed behavior-tree-based simulation benchmark\nBTSIMBENCH, our experiments show a significant performance improvement in\nbehavior simulation compared to baselines, ranging from 14.7% to 26.6%."
                },
                "authors": [
                    {
                        "name": "Jianan Wang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Xueying Wang"
                    },
                    {
                        "name": "Fu Li"
                    },
                    {
                        "name": "Yunlong Wu"
                    },
                    {
                        "name": "Juan Chen"
                    },
                    {
                        "name": "Xiaodong Yi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Yi"
                },
                "author": "Xiaodong Yi",
                "arxiv_comment": "7 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15861v1",
                "updated": "2024-09-24T08:33:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    33,
                    41,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T08:33:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    33,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding"
                },
                "summary": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API."
                },
                "authors": [
                    {
                        "name": "Abdulfattah Safa"
                    },
                    {
                        "name": "G√∂zde G√ºl ≈ûahin"
                    }
                ],
                "author_detail": {
                    "name": "G√∂zde G√ºl ≈ûahin"
                },
                "author": "G√∂zde G√ºl ≈ûahin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13731v2",
                "updated": "2024-09-24T08:24:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    24,
                    39,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-10T02:00:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    0,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented\n  Generation"
                },
                "summary": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently developed retrieval-augmented generation (RAG) technology has\nenabled the efficient construction of domain-specific applications. However, it\nalso has limitations, including the gap between vector similarity and the\nrelevance of knowledge reasoning, as well as insensitivity to knowledge logic,\nsuch as numerical values, temporal relations, expert rules, and others, which\nhinder the effectiveness of professional knowledge services. In this work, we\nintroduce a professional domain knowledge service framework called Knowledge\nAugmented Generation (KAG). KAG is designed to address the aforementioned\nchallenges with the motivation of making full use of the advantages of\nknowledge graph(KG) and vector retrieval, and to improve generation and\nreasoning performance by bidirectionally enhancing large language models (LLMs)\nand KGs through five key aspects: (1) LLM-friendly knowledge representation,\n(2) mutual-indexing between knowledge graphs and original chunks, (3)\nlogical-form-guided hybrid reasoning engine, (4) knowledge alignment with\nsemantic reasoning, and (5) model capability enhancement for KAG. We compared\nKAG with existing RAG methods in multihop question answering and found that it\nsignificantly outperforms state-of-theart methods, achieving a relative\nimprovement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We\nhave successfully applied KAG to two professional knowledge Q&A tasks of Ant\nGroup, including E-Government Q&A and E-Health Q&A, achieving significant\nimprovement in professionalism compared to RAG methods."
                },
                "authors": [
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "Zhongshu Zhu"
                    },
                    {
                        "name": "Zhouyu Jiang"
                    },
                    {
                        "name": "Ling Zhong"
                    },
                    {
                        "name": "Yuan Qu"
                    },
                    {
                        "name": "Peilong Zhao"
                    },
                    {
                        "name": "Zhongpu Bo"
                    },
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Huaidong Xiong"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Zaoyang Wang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wenguang Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15113v2",
                "updated": "2024-09-24T08:18:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    18,
                    34,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T15:20:27Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    20,
                    27,
                    0,
                    267,
                    0
                ],
                "title": "Inferring Scientific Cross-Document Coreference and Hierarchy with\n  Definition-Augmented Relational Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Scientific Cross-Document Coreference and Hierarchy with\n  Definition-Augmented Relational Reasoning"
                },
                "summary": "We address the fundamental task of inferring cross-document coreference and\nhierarchy in scientific texts, which has important applications in knowledge\ngraph construction, search, recommendation and discovery. LLMs can struggle\nwhen faced with many long-tail technical concepts with nuanced variations. We\npresent a novel method which generates context-dependent definitions of concept\nmentions by retrieving full-text literature, and uses the definitions to\nenhance detection of cross-document relations. We further generate relational\ndefinitions, which describe how two concept mentions are related or different,\nand design an efficient re-ranking approach to address the combinatorial\nexplosion involved in inferring links across papers. In both fine-tuning and\nin-context learning settings we achieve large gains in performance. We provide\nanalysis of generated definitions, shedding light on the relational reasoning\nability of LLMs over fine-grained scientific concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the fundamental task of inferring cross-document coreference and\nhierarchy in scientific texts, which has important applications in knowledge\ngraph construction, search, recommendation and discovery. LLMs can struggle\nwhen faced with many long-tail technical concepts with nuanced variations. We\npresent a novel method which generates context-dependent definitions of concept\nmentions by retrieving full-text literature, and uses the definitions to\nenhance detection of cross-document relations. We further generate relational\ndefinitions, which describe how two concept mentions are related or different,\nand design an efficient re-ranking approach to address the combinatorial\nexplosion involved in inferring links across papers. In both fine-tuning and\nin-context learning settings we achieve large gains in performance. We provide\nanalysis of generated definitions, shedding light on the relational reasoning\nability of LLMs over fine-grained scientific concepts."
                },
                "authors": [
                    {
                        "name": "Lior Forer"
                    },
                    {
                        "name": "Tom Hope"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hope"
                },
                "author": "Tom Hope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14605v2",
                "updated": "2024-09-24T08:02:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    2,
                    21,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T21:52:58Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    21,
                    52,
                    58,
                    6,
                    266,
                    0
                ],
                "title": "First Field Trial of LLM-Powered AI Agent for Lifecycle Management of\n  Autonomous Driving Optical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Field Trial of LLM-Powered AI Agent for Lifecycle Management of\n  Autonomous Driving Optical Networks"
                },
                "summary": "We design and demonstrate the first field trial of LLM-powered AI Agent for\nADON. Three operation modes of the Agent are proposed for network lifecycle\nmanagement. The Agent efficiently processes wavelength add/drop and soft/hard\nfailures, and achieves comparable performance to human-designed algorithms for\npower optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design and demonstrate the first field trial of LLM-powered AI Agent for\nADON. Three operation modes of the Agent are proposed for network lifecycle\nmanagement. The Agent efficiently processes wavelength add/drop and soft/hard\nfailures, and achieves comparable performance to human-designed algorithms for\npower optimization."
                },
                "authors": [
                    {
                        "name": "Xiaomin Liu"
                    },
                    {
                        "name": "Qizhi Qiu"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Yuming Cheng"
                    },
                    {
                        "name": "Lilin Yi"
                    },
                    {
                        "name": "Weisheng Hu"
                    },
                    {
                        "name": "Qunbi Zhuge"
                    }
                ],
                "author_detail": {
                    "name": "Qunbi Zhuge"
                },
                "author": "Qunbi Zhuge",
                "arxiv_comment": "Version submitted to ECOC PDP 2024 on September 6th",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10918v2",
                "updated": "2024-09-24T07:44:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    44,
                    27,
                    1,
                    268,
                    0
                ],
                "published": "2024-08-20T15:03:35Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    3,
                    35,
                    1,
                    233,
                    0
                ],
                "title": "CHECKWHY: Causal Fact Verification via Argument Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHECKWHY: Causal Fact Verification via Argument Structure"
                },
                "summary": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing complexity of fact verification tasks, the concern with\n\"thoughtful\" reasoning capabilities is increasing. However, recent fact\nverification benchmarks mainly focus on checking a narrow scope of semantic\nfactoids within claims and lack an explicit logical reasoning process. In this\npaper, we introduce CheckWhy, a challenging dataset tailored to a novel causal\nfact verification task: checking the truthfulness of the causal relation within\nclaims through rigorous reasoning steps. CheckWhy consists of over 19K \"why\"\nclaim-evidence-argument structure triplets with supports, refutes, and not\nenough info labels. Each argument structure is composed of connected evidence,\nrepresenting the reasoning process that begins with foundational evidence and\nprogresses toward claim establishment. Through extensive experiments on\nstate-of-the-art models, we validate the importance of incorporating the\nargument structure for causal fact verification. Moreover, the automated and\nhuman evaluation of argument structure generation reveals the difficulty in\nproducing satisfying argument structure by fine-tuned models or\nChain-of-Thought prompted LLMs, leaving considerable room for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Jiasheng Si"
                    },
                    {
                        "name": "Yibo Zhao"
                    },
                    {
                        "name": "Yingjie Zhu"
                    },
                    {
                        "name": "Haiyang Zhu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Accepted by ACL2024; Awarded as Outstanding Paper Award and Area\n  Chair Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08438v2",
                "updated": "2024-09-24T07:41:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    41,
                    19,
                    1,
                    268,
                    0
                ],
                "published": "2024-01-06T03:59:59Z",
                "published_parsed": [
                    2024,
                    1,
                    6,
                    3,
                    59,
                    59,
                    5,
                    6,
                    0
                ],
                "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language\n  Models"
                },
                "summary": "Cognitive dynamics are pivotal to advance human understanding of the world.\nRecent advancements in large language models (LLMs) reveal their potential for\ncognitive simulation. However, these LLM-based cognitive studies primarily\nfocus on static modeling, overlooking the dynamic nature of cognition. To\nbridge this gap, we propose the concept of the cognitive dynamics of LLMs and\npresent a corresponding task with the inspiration of longitudinal studies.\nTowards the task, we develop CogBench, a novel benchmark to assess the\ncognitive dynamics of LLMs and validate it through participant surveys. We also\ndesign two evaluation metrics for CogBench, including Authenticity and\nRationality. Recognizing the inherent static nature of LLMs, we introduce\nCogGPT for the task, which features an innovative iterative cognitive mechanism\naimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate\nthe superiority of CogGPT over existing methods, particularly in its ability to\nfacilitate role-specific cognitive dynamics under continuous information flows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive dynamics are pivotal to advance human understanding of the world.\nRecent advancements in large language models (LLMs) reveal their potential for\ncognitive simulation. However, these LLM-based cognitive studies primarily\nfocus on static modeling, overlooking the dynamic nature of cognition. To\nbridge this gap, we propose the concept of the cognitive dynamics of LLMs and\npresent a corresponding task with the inspiration of longitudinal studies.\nTowards the task, we develop CogBench, a novel benchmark to assess the\ncognitive dynamics of LLMs and validate it through participant surveys. We also\ndesign two evaluation metrics for CogBench, including Authenticity and\nRationality. Recognizing the inherent static nature of LLMs, we introduce\nCogGPT for the task, which features an innovative iterative cognitive mechanism\naimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate\nthe superiority of CogGPT over existing methods, particularly in its ability to\nfacilitate role-specific cognitive dynamics under continuous information flows."
                },
                "authors": [
                    {
                        "name": "Yaojia Lv"
                    },
                    {
                        "name": "Haojie Pan"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jiafeng Liang"
                    },
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Ruiji Fu"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted to EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15827v1",
                "updated": "2024-09-24T07:40:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    40,
                    33,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T07:40:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    40,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to\n  Model Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to\n  Model Interpretability"
                },
                "summary": "As large language models (LLMs) become advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms,\nwhich are well-suited for probing deeper cognitive aspects of language\nprocessing, to explore neuron-level representations in language model across\nthree tasks: sound-shape association, sound-gender association, and implicit\ncausality. Our findings indicate that while GPT-2-XL struggles with the\nsound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: when GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in transformer based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms,\nwhich are well-suited for probing deeper cognitive aspects of language\nprocessing, to explore neuron-level representations in language model across\nthree tasks: sound-shape association, sound-gender association, and implicit\ncausality. Our findings indicate that while GPT-2-XL struggles with the\nsound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: when GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in transformer based LLMs."
                },
                "authors": [
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Bei Xiao"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15825v1",
                "updated": "2024-09-24T07:38:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    38,
                    38,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T07:38:38Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    38,
                    38,
                    1,
                    268,
                    0
                ],
                "title": "Empirical Insights on Fine-Tuning Large Language Models for\n  Question-Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Insights on Fine-Tuning Large Language Models for\n  Question-Answering"
                },
                "summary": "Large language models (LLMs) encode extensive world knowledge through\npre-training on massive datasets, which can then be fine-tuned for the\nquestion-answering (QA) task. However, effective strategies for fine-tuning\nLLMs for the QA task remain largely unexplored. To address this gap, we\ncategorize supervised fine-tuning (SFT) data based on the extent of knowledge\nmemorized by the pretrained LLMs and conduct a series of empirical analyses.\nOur experiments, involving four LLMs from three different model families, focus\non three key factors: the amount of data required for SFT, the impact of\ndifferent SFT datasets on model performance, and how data requirements vary\nacross LLMs. The results show that as few as 60 data points during the SFT\nstage can activate the knowledge encoded during pre-training, enabling LLMs to\nperform the QA task. Additionally, SFT with data of varying memory levels has a\nsignificant impact on LLM performance, with the optimal dataset differing based\non the specific model being fine-tuned. Future research will delve deeper into\nthe mechanisms underlying these phenomena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) encode extensive world knowledge through\npre-training on massive datasets, which can then be fine-tuned for the\nquestion-answering (QA) task. However, effective strategies for fine-tuning\nLLMs for the QA task remain largely unexplored. To address this gap, we\ncategorize supervised fine-tuning (SFT) data based on the extent of knowledge\nmemorized by the pretrained LLMs and conduct a series of empirical analyses.\nOur experiments, involving four LLMs from three different model families, focus\non three key factors: the amount of data required for SFT, the impact of\ndifferent SFT datasets on model performance, and how data requirements vary\nacross LLMs. The results show that as few as 60 data points during the SFT\nstage can activate the knowledge encoded during pre-training, enabling LLMs to\nperform the QA task. Additionally, SFT with data of varying memory levels has a\nsignificant impact on LLM performance, with the optimal dataset differing based\non the specific model being fine-tuned. Future research will delve deeper into\nthe mechanisms underlying these phenomena."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Zhongchao Shi"
                    },
                    {
                        "name": "Jianping Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Fan"
                },
                "author": "Jianping Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15820v1",
                "updated": "2024-09-24T07:34:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    34,
                    50,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T07:34:50Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    34,
                    50,
                    1,
                    268,
                    0
                ],
                "title": "Supervised Fine-Tuning: An Activation Pattern Optimization Process for\n  Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning: An Activation Pattern Optimization Process for\n  Attention Heads"
                },
                "summary": "Though demonstrating promising potential, LLMs' performance on complex tasks,\nsuch as advanced mathematics and complex disease diagnosis is still\nunsatisfactory. A key issue is the present LLMs learn in a data-driven schema,\nwhile the instruction dataset about these complex tasks is both scarce and hard\nto collect or construct. On the contrary, a prominent phenomenon is that LLMs\ncan learn rather fast on those simpler tasks with adequate prior knowledge\ncaptured during pretraining stage. Thus, if the prerequisite and mechanism of\nsuch rapid generalization could be elucidated, it could be highly beneficial in\nenhancing the efficiency and effectiveness of the LLM's ability to learn\ncomplex tasks. Thus, in this paper, we employ a gradient-based method, to\ndissect the process that the SFT process adapts LLMs to downstream tasks via\nthe perspective of attention patterns. We find that: (1) LLMs selectively\nactivate task-specific attention heads during SFT; (2) activation patterns for\ncomplex tasks are combinations of basic task patterns; and (3) changes in a few\nparameters can significantly impact activation patterns after SFT on a small\nnumber of samples. Based on these insights, we conduct experiments to examine\nwhether these conclusions could effectively enhance the efficiency and\neffectiveness of SFT, particularly in handling complex tasks and when\ninstructional resources are scarce. Our research not only uncovers the\nunderlying reasons behind LLMs' rapid learning and generalization mechanisms\nbut also provides practical solutions for addressing data challenges in complex\nand specialized tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though demonstrating promising potential, LLMs' performance on complex tasks,\nsuch as advanced mathematics and complex disease diagnosis is still\nunsatisfactory. A key issue is the present LLMs learn in a data-driven schema,\nwhile the instruction dataset about these complex tasks is both scarce and hard\nto collect or construct. On the contrary, a prominent phenomenon is that LLMs\ncan learn rather fast on those simpler tasks with adequate prior knowledge\ncaptured during pretraining stage. Thus, if the prerequisite and mechanism of\nsuch rapid generalization could be elucidated, it could be highly beneficial in\nenhancing the efficiency and effectiveness of the LLM's ability to learn\ncomplex tasks. Thus, in this paper, we employ a gradient-based method, to\ndissect the process that the SFT process adapts LLMs to downstream tasks via\nthe perspective of attention patterns. We find that: (1) LLMs selectively\nactivate task-specific attention heads during SFT; (2) activation patterns for\ncomplex tasks are combinations of basic task patterns; and (3) changes in a few\nparameters can significantly impact activation patterns after SFT on a small\nnumber of samples. Based on these insights, we conduct experiments to examine\nwhether these conclusions could effectively enhance the efficiency and\neffectiveness of SFT, particularly in handling complex tasks and when\ninstructional resources are scarce. Our research not only uncovers the\nunderlying reasons behind LLMs' rapid learning and generalization mechanisms\nbut also provides practical solutions for addressing data challenges in complex\nand specialized tasks."
                },
                "authors": [
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "in review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15817v1",
                "updated": "2024-09-24T07:29:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    29,
                    5,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T07:29:05Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    29,
                    5,
                    1,
                    268,
                    0
                ],
                "title": "SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs\n  and Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs\n  and Agents"
                },
                "summary": "The advancement of artificial intelligence algorithms has expanded their\napplication to several fields such as the biomedical domain. Artificial\nintelligence systems, including Large Language Models (LLMs), can be\nparticularly advantageous in drug discovery, which is a very long and expensive\nprocess. However, LLMs by themselves lack in-depth knowledge about specific\ndomains and can generate factually incorrect information. Moreover, they are\nnot able to perform more complex actions that imply the usage of external\ntools. Our work is focused on these two issues. Firstly, we show how the\nimplementation of an advanced RAG system can help the LLM to generate more\naccurate answers to drug-discovery-related questions. The results show that the\nanswers generated by the LLM with the RAG system surpass in quality the answers\nproduced by the model without RAG. Secondly, we show how to create an automatic\ntarget dossier using LLMs and incorporating them with external tools that they\ncan use to execute more intricate tasks to gather data such as accessing\ndatabases and executing code. The result is a production-ready target dossier\ncontaining the acquired information summarized into a PDF and a PowerPoint\npresentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of artificial intelligence algorithms has expanded their\napplication to several fields such as the biomedical domain. Artificial\nintelligence systems, including Large Language Models (LLMs), can be\nparticularly advantageous in drug discovery, which is a very long and expensive\nprocess. However, LLMs by themselves lack in-depth knowledge about specific\ndomains and can generate factually incorrect information. Moreover, they are\nnot able to perform more complex actions that imply the usage of external\ntools. Our work is focused on these two issues. Firstly, we show how the\nimplementation of an advanced RAG system can help the LLM to generate more\naccurate answers to drug-discovery-related questions. The results show that the\nanswers generated by the LLM with the RAG system surpass in quality the answers\nproduced by the model without RAG. Secondly, we show how to create an automatic\ntarget dossier using LLMs and incorporating them with external tools that they\ncan use to execute more intricate tasks to gather data such as accessing\ndatabases and executing code. The result is a production-ready target dossier\ncontaining the acquired information summarized into a PDF and a PowerPoint\npresentation."
                },
                "authors": [
                    {
                        "name": "Gabriele Fossi"
                    },
                    {
                        "name": "Youssef Boulaimen"
                    },
                    {
                        "name": "Leila Outemzabet"
                    },
                    {
                        "name": "Nathalie Jeanray"
                    },
                    {
                        "name": "Stephane Gerart"
                    },
                    {
                        "name": "Sebastien Vachenc"
                    },
                    {
                        "name": "Joanna Giemza"
                    },
                    {
                        "name": "Salvatore Raieli"
                    }
                ],
                "author_detail": {
                    "name": "Salvatore Raieli"
                },
                "author": "Salvatore Raieli",
                "arxiv_comment": "10 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 92C50, 68T09",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01054v2",
                "updated": "2024-09-24T07:06:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    6,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-01T08:07:02Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    7,
                    2,
                    0,
                    183,
                    0
                ],
                "title": "Joint Pruning and Channel-wise Mixed-Precision Quantization for\n  Efficient Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Pruning and Channel-wise Mixed-Precision Quantization for\n  Efficient Deep Neural Networks"
                },
                "summary": "The resource requirements of deep neural networks (DNNs) pose significant\nchallenges to their deployment on edge devices. Common approaches to address\nthis issue are pruning and mixed-precision quantization, which lead to latency\nand memory occupation improvements. These optimization techniques are usually\napplied independently. We propose a novel methodology to apply them jointly via\na lightweight gradient-based search, and in a hardware-aware manner, greatly\nreducing the time required to generate Pareto-optimal DNNs in terms of accuracy\nversus cost (i.e., latency or memory). We test our approach on three\nedge-relevant benchmarks, namely CIFAR-10, Google Speech Commands, and Tiny\nImageNet. When targeting the optimization of the memory footprint, we are able\nto achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the\nbaseline networks with all weights quantized at 8 and 2-bit, respectively. Our\nmethod surpasses a previous state-of-the-art approach with up to 56.17% size\nreduction at iso-accuracy. With respect to the sequential application of\nstate-of-the-art pruning and mixed-precision optimizations, we obtain\ncomparable or superior results, but with a significantly lowered training time.\nIn addition, we show how well-tailored cost models can improve the cost versus\naccuracy trade-offs when targeting specific hardware for deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The resource requirements of deep neural networks (DNNs) pose significant\nchallenges to their deployment on edge devices. Common approaches to address\nthis issue are pruning and mixed-precision quantization, which lead to latency\nand memory occupation improvements. These optimization techniques are usually\napplied independently. We propose a novel methodology to apply them jointly via\na lightweight gradient-based search, and in a hardware-aware manner, greatly\nreducing the time required to generate Pareto-optimal DNNs in terms of accuracy\nversus cost (i.e., latency or memory). We test our approach on three\nedge-relevant benchmarks, namely CIFAR-10, Google Speech Commands, and Tiny\nImageNet. When targeting the optimization of the memory footprint, we are able\nto achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the\nbaseline networks with all weights quantized at 8 and 2-bit, respectively. Our\nmethod surpasses a previous state-of-the-art approach with up to 56.17% size\nreduction at iso-accuracy. With respect to the sequential application of\nstate-of-the-art pruning and mixed-precision optimizations, we obtain\ncomparable or superior results, but with a significantly lowered training time.\nIn addition, we show how well-tailored cost models can improve the cost versus\naccuracy trade-offs when targeting specific hardware for deployment."
                },
                "authors": [
                    {
                        "name": "Beatrice Alessandra Motetti"
                    },
                    {
                        "name": "Matteo Risso"
                    },
                    {
                        "name": "Alessio Burrello"
                    },
                    {
                        "name": "Enrico Macii"
                    },
                    {
                        "name": "Massimo Poncino"
                    },
                    {
                        "name": "Daniele Jahier Pagliari"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Jahier Pagliari"
                },
                "author": "Daniele Jahier Pagliari",
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Computers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14495v2",
                "updated": "2024-09-24T07:00:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    0,
                    24,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-22T15:44:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    44,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation\n  for Logical Reading Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation\n  for Logical Reading Comprehension"
                },
                "summary": "Logical reading comprehension is a challenging task that entails grasping the\nunderlying semantics of text and applying reasoning to deduce the correct\nanswer. Prior researches have primarily focused on enhancing logical reasoning\ncapabilities through Chain-of-Thought (CoT) or data augmentation. However,\nprevious work constructing chain-of-thought rationales concentrates solely on\nanalyzing correct options, neglecting the incorrect alternatives. Addtionally,\nearlier efforts on data augmentation by altering contexts rely on rule-based\nmethods, which result in generated contexts that lack diversity and coherence.\nTo address these issues, we propose a Premise-Oriented Data Augmentation (PODA)\nframework. This framework can generate CoT rationales including analyses for\nboth correct and incorrect options, while constructing diverse and high-quality\ncounterfactual contexts from incorrect candidate options. We integrate\nsummarizing premises and identifying premises for each option into rationales.\nSubsequently, we employ multi-step prompts with identified premises to\nconstruct counterfactual context. To facilitate the model's capabilities to\nbetter differentiate the reasoning process associated with each option, we\nintroduce a novel thought-path contrastive learning method that compares\nreasoning paths between the original and counterfactual samples. Experimental\nresults on three representative LLMs demonstrate that our method can improve\nthe baselines substantially across two challenging logical reasoning benchmarks\n(ReClor and LogiQA 2.0). The data and code are released at\nhttps://github.com/lalalamdbf/TPReasoner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reading comprehension is a challenging task that entails grasping the\nunderlying semantics of text and applying reasoning to deduce the correct\nanswer. Prior researches have primarily focused on enhancing logical reasoning\ncapabilities through Chain-of-Thought (CoT) or data augmentation. However,\nprevious work constructing chain-of-thought rationales concentrates solely on\nanalyzing correct options, neglecting the incorrect alternatives. Addtionally,\nearlier efforts on data augmentation by altering contexts rely on rule-based\nmethods, which result in generated contexts that lack diversity and coherence.\nTo address these issues, we propose a Premise-Oriented Data Augmentation (PODA)\nframework. This framework can generate CoT rationales including analyses for\nboth correct and incorrect options, while constructing diverse and high-quality\ncounterfactual contexts from incorrect candidate options. We integrate\nsummarizing premises and identifying premises for each option into rationales.\nSubsequently, we employ multi-step prompts with identified premises to\nconstruct counterfactual context. To facilitate the model's capabilities to\nbetter differentiate the reasoning process associated with each option, we\nintroduce a novel thought-path contrastive learning method that compares\nreasoning paths between the original and counterfactual samples. Experimental\nresults on three representative LLMs demonstrate that our method can improve\nthe baselines substantially across two challenging logical reasoning benchmarks\n(ReClor and LogiQA 2.0). The data and code are released at\nhttps://github.com/lalalamdbf/TPReasoner."
                },
                "authors": [
                    {
                        "name": "Chenxu Wang"
                    },
                    {
                        "name": "Ping Jian"
                    },
                    {
                        "name": "Zhen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Yang"
                },
                "author": "Zhen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15799v1",
                "updated": "2024-09-24T06:47:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    6,
                    47,
                    12,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T06:47:12Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    6,
                    47,
                    12,
                    1,
                    268,
                    0
                ],
                "title": "WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target\n  Speaker Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target\n  Speaker Extraction"
                },
                "summary": "Target speaker extraction (TSE) focuses on isolating the speech of a specific\ntarget speaker from overlapped multi-talker speech, which is a typical setup in\nthe cocktail party problem. In recent years, TSE draws increasing attention due\nto its potential for various applications such as user-customized interfaces\nand hearing aids, or as a crutial front-end processing technologies for\nsubsequential tasks such as speech recognition and speaker recongtion. However,\nthere are currently few open-source toolkits or available pre-trained models\nfor off-the-shelf usage. In this work, we introduce WeSep, a toolkit designed\nfor research and practical applications in TSE. WeSep is featured with flexible\ntarget speaker modeling, scalable data management, effective on-the-fly data\nsimulation, structured recipes and deployment support. The toolkit is publicly\navaliable at \\url{https://github.com/wenet-e2e/WeSep.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target speaker extraction (TSE) focuses on isolating the speech of a specific\ntarget speaker from overlapped multi-talker speech, which is a typical setup in\nthe cocktail party problem. In recent years, TSE draws increasing attention due\nto its potential for various applications such as user-customized interfaces\nand hearing aids, or as a crutial front-end processing technologies for\nsubsequential tasks such as speech recognition and speaker recongtion. However,\nthere are currently few open-source toolkits or available pre-trained models\nfor off-the-shelf usage. In this work, we introduce WeSep, a toolkit designed\nfor research and practical applications in TSE. WeSep is featured with flexible\ntarget speaker modeling, scalable data management, effective on-the-fly data\nsimulation, structured recipes and deployment support. The toolkit is publicly\navaliable at \\url{https://github.com/wenet-e2e/WeSep.}"
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Shaoxiong Lin"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Xuefei Wang"
                    },
                    {
                        "name": "Meng Ge"
                    },
                    {
                        "name": "Jianwei Yu"
                    },
                    {
                        "name": "Yanmin Qian"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Interspeech 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15790v1",
                "updated": "2024-09-24T06:36:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    6,
                    36,
                    56,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T06:36:56Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    6,
                    36,
                    56,
                    1,
                    268,
                    0
                ],
                "title": "Small Language Models: Survey, Measurements, and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models: Survey, Measurements, and Insights"
                },
                "summary": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 59 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 59 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field."
                },
                "authors": [
                    {
                        "name": "Zhenyan Lu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Dongqi Cai"
                    },
                    {
                        "name": "Rongjie Yi"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xiwen Zhang"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    },
                    {
                        "name": "Mengwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mengwei Xu"
                },
                "author": "Mengwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05902v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05902v3",
                "updated": "2024-09-24T06:11:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    6,
                    11,
                    12,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-06T02:33:20Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    2,
                    33,
                    20,
                    4,
                    250,
                    0
                ],
                "title": "OPAL: Outlier-Preserved Microscaling Quantization Accelerator for\n  Generative Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPAL: Outlier-Preserved Microscaling Quantization Accelerator for\n  Generative Large Language Models"
                },
                "summary": "To overcome the burden on the memory size and bandwidth due to\never-increasing size of large language models (LLMs), aggressive weight\nquantization has been recently studied, while lacking research on quantizing\nactivations. In this paper, we present a hardware-software co-design method\nthat results in an energy-efficient LLM accelerator, named OPAL, for generation\ntasks. First of all, a novel activation quantization method that leverages the\nmicroscaling data format while preserving several outliers per sub-tensor block\n(e.g., four out of 128 elements) is proposed. Second, on top of preserving\noutliers, mixed precision is utilized that sets 5-bit for inputs to sensitive\nlayers in the decoder block of an LLM, while keeping inputs to less sensitive\nlayers to 3-bit. Finally, we present the OPAL hardware architecture that\nconsists of FP units for handling outliers and vectorized INT multipliers for\ndominant non-outlier related operations. In addition, OPAL uses log2-based\napproximation on softmax operations that only requires shift and subtraction to\nmaximize power efficiency. As a result, we are able to improve the energy\nefficiency by 1.6~2.2x, and reduce the area by 2.4~3.1x with negligible\naccuracy loss, i.e., <1 perplexity increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To overcome the burden on the memory size and bandwidth due to\never-increasing size of large language models (LLMs), aggressive weight\nquantization has been recently studied, while lacking research on quantizing\nactivations. In this paper, we present a hardware-software co-design method\nthat results in an energy-efficient LLM accelerator, named OPAL, for generation\ntasks. First of all, a novel activation quantization method that leverages the\nmicroscaling data format while preserving several outliers per sub-tensor block\n(e.g., four out of 128 elements) is proposed. Second, on top of preserving\noutliers, mixed precision is utilized that sets 5-bit for inputs to sensitive\nlayers in the decoder block of an LLM, while keeping inputs to less sensitive\nlayers to 3-bit. Finally, we present the OPAL hardware architecture that\nconsists of FP units for handling outliers and vectorized INT multipliers for\ndominant non-outlier related operations. In addition, OPAL uses log2-based\napproximation on softmax operations that only requires shift and subtraction to\nmaximize power efficiency. As a result, we are able to improve the energy\nefficiency by 1.6~2.2x, and reduce the area by 2.4~3.1x with negligible\naccuracy loss, i.e., <1 perplexity increase."
                },
                "authors": [
                    {
                        "name": "Jahyun Koo"
                    },
                    {
                        "name": "Dahoon Park"
                    },
                    {
                        "name": "Sangwoo Jung"
                    },
                    {
                        "name": "Jaeha Kung"
                    }
                ],
                "author_detail": {
                    "name": "Jaeha Kung"
                },
                "author": "Jaeha Kung",
                "arxiv_comment": "7 pages, 8 figures, DAC2024 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05902v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05902v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15766v1",
                "updated": "2024-09-24T05:44:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    44,
                    46,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T05:44:46Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    44,
                    46,
                    1,
                    268,
                    0
                ],
                "title": "CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHBench: A Chinese Dataset for Evaluating Health in Large Language\n  Models"
                },
                "summary": "With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. It\nis critical that these models provide accurate and trustworthy health\ninformation, as their application in real-world contexts--where misinformation\ncan have serious consequences for individuals seeking medical advice and\nsupport--depends on their reliability. In this work, we present CHBench, the\nfirst comprehensive Chinese Health-related Benchmark designed to evaluate LLMs'\ncapabilities in understanding physical and mental health across diverse\nscenarios. CHBench includes 6,493 entries related to mental health and 2,999\nentries focused on physical health, covering a broad spectrum of topics. This\ndataset serves as a foundation for evaluating Chinese LLMs' capacity to\ncomprehend and generate accurate health-related information. Our extensive\nevaluations of four popular Chinese LLMs demonstrate that there remains\nconsiderable room for improvement in their understanding of health-related\ninformation. The code is available at https://github.com/TracyGuo2001/CHBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), assessing their\nperformance on health-related inquiries has become increasingly essential. It\nis critical that these models provide accurate and trustworthy health\ninformation, as their application in real-world contexts--where misinformation\ncan have serious consequences for individuals seeking medical advice and\nsupport--depends on their reliability. In this work, we present CHBench, the\nfirst comprehensive Chinese Health-related Benchmark designed to evaluate LLMs'\ncapabilities in understanding physical and mental health across diverse\nscenarios. CHBench includes 6,493 entries related to mental health and 2,999\nentries focused on physical health, covering a broad spectrum of topics. This\ndataset serves as a foundation for evaluating Chinese LLMs' capacity to\ncomprehend and generate accurate health-related information. Our extensive\nevaluations of four popular Chinese LLMs demonstrate that there remains\nconsiderable room for improvement in their understanding of health-related\ninformation. The code is available at https://github.com/TracyGuo2001/CHBench."
                },
                "authors": [
                    {
                        "name": "Chenlu Guo"
                    },
                    {
                        "name": "Nuo Xu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15763v1",
                "updated": "2024-09-24T05:39:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    39,
                    53,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T05:39:53Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    39,
                    53,
                    1,
                    268,
                    0
                ],
                "title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through\n  Semantic Comprehension in Retrieval-Augmented Generation Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through\n  Semantic Comprehension in Retrieval-Augmented Generation Scenarios"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) tasks using Large Language Models\n(LLMs), the quality of retrieved information is critical to the final output.\nThis paper introduces the IRSC benchmark for evaluating the performance of\nembedding models in multilingual RAG tasks. The benchmark encompasses five\nretrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,\nkeyword retrieval, and summary retrieval. Our research addresses the current\nlack of comprehensive testing and effective comparison methods for embedding\nmodels in RAG scenarios. We introduced new metrics: the Similarity of Semantic\nComprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),\nand evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our\ncontributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and\n3) insights into the cross-lingual limitations of embedding models. The IRSC\nbenchmark aims to enhance the understanding and development of accurate\nretrieval systems in RAG tasks. All code and datasets are available at:\nhttps://github.com/Jasaxion/IRSC\\_Benchmark",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) tasks using Large Language Models\n(LLMs), the quality of retrieved information is critical to the final output.\nThis paper introduces the IRSC benchmark for evaluating the performance of\nembedding models in multilingual RAG tasks. The benchmark encompasses five\nretrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,\nkeyword retrieval, and summary retrieval. Our research addresses the current\nlack of comprehensive testing and effective comparison methods for embedding\nmodels in RAG scenarios. We introduced new metrics: the Similarity of Semantic\nComprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),\nand evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our\ncontributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and\n3) insights into the cross-lingual limitations of embedding models. The IRSC\nbenchmark aims to enhance the understanding and development of accurate\nretrieval systems in RAG tasks. All code and datasets are available at:\nhttps://github.com/Jasaxion/IRSC\\_Benchmark"
                },
                "authors": [
                    {
                        "name": "Hai Lin"
                    },
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Junyou Su"
                    },
                    {
                        "name": "Haitao Zheng"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15762v1",
                "updated": "2024-09-24T05:38:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    38,
                    33,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T05:38:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    38,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "XTRUST: On the Multilingual Trustworthiness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XTRUST: On the Multilingual Trustworthiness of Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing (NLP) tasks, capturing the attention of\nboth practitioners and the broader public. A key question that now preoccupies\nthe AI community concerns the capabilities and limitations of these models,\nwith trustworthiness emerging as a central issue, particularly as LLMs are\nincreasingly applied in sensitive fields like healthcare and finance, where\nerrors can have serious consequences. However, most previous studies on the\ntrustworthiness of LLMs have been limited to a single language, typically the\npredominant one in the dataset, such as English. In response to the growing\nglobal deployment of LLMs, we introduce XTRUST, the first comprehensive\nmultilingual trustworthiness benchmark. XTRUST encompasses a diverse range of\ntopics, including illegal activities, hallucination, out-of-distribution (OOD)\nrobustness, physical and mental health, toxicity, fairness, misinformation,\nprivacy, and machine ethics, across 10 different languages. Using XTRUST, we\nconduct an empirical evaluation of the multilingual trustworthiness of five\nwidely used LLMs, offering an in-depth analysis of their performance across\nlanguages and tasks. Our results indicate that many LLMs struggle with certain\nlow-resource languages, such as Arabic and Russian, highlighting the\nconsiderable room for improvement in the multilingual trustworthiness of\ncurrent language models. The code is available at\nhttps://github.com/LluckyYH/XTRUST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing (NLP) tasks, capturing the attention of\nboth practitioners and the broader public. A key question that now preoccupies\nthe AI community concerns the capabilities and limitations of these models,\nwith trustworthiness emerging as a central issue, particularly as LLMs are\nincreasingly applied in sensitive fields like healthcare and finance, where\nerrors can have serious consequences. However, most previous studies on the\ntrustworthiness of LLMs have been limited to a single language, typically the\npredominant one in the dataset, such as English. In response to the growing\nglobal deployment of LLMs, we introduce XTRUST, the first comprehensive\nmultilingual trustworthiness benchmark. XTRUST encompasses a diverse range of\ntopics, including illegal activities, hallucination, out-of-distribution (OOD)\nrobustness, physical and mental health, toxicity, fairness, misinformation,\nprivacy, and machine ethics, across 10 different languages. Using XTRUST, we\nconduct an empirical evaluation of the multilingual trustworthiness of five\nwidely used LLMs, offering an in-depth analysis of their performance across\nlanguages and tasks. Our results indicate that many LLMs struggle with certain\nlow-resource languages, such as Arabic and Russian, highlighting the\nconsiderable room for improvement in the multilingual trustworthiness of\ncurrent language models. The code is available at\nhttps://github.com/LluckyYH/XTRUST."
                },
                "authors": [
                    {
                        "name": "Yahan Li"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15757v1",
                "updated": "2024-09-24T05:26:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    26,
                    20,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T05:26:20Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    26,
                    20,
                    1,
                    268,
                    0
                ],
                "title": "Smart Grid Security: A Verified Deep Reinforcement Learning Framework to\n  Counter Cyber-Physical Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Grid Security: A Verified Deep Reinforcement Learning Framework to\n  Counter Cyber-Physical Attacks"
                },
                "summary": "The distributed nature of smart grids, combined with sophisticated sensors,\ncontrol algorithms, and data collection facilities at Supervisory Control and\nData Acquisition (SCADA) centers, makes them vulnerable to strategically\ncrafted cyber-physical attacks. These malicious attacks can manipulate power\ndemands using high-wattage Internet of Things (IoT) botnet devices, such as\nrefrigerators and air conditioners, or introduce false values into transmission\nline power flow sensor readings. Consequently, grids experience blackouts and\nhigh power flow oscillations. Existing grid protection mechanisms, originally\ndesigned to tackle natural faults in transmission lines and generator outages,\nare ineffective against such intelligently crafted attacks. This is because\ngrid operators overlook potential scenarios of cyber-physical attacks during\ntheir design phase. In this work, we propose a safe Deep Reinforcement Learning\n(DRL)-based framework for mitigating attacks on smart grids. The DRL agent\neffectively neutralizes cyber-physical attacks on grid surfaces by triggering\nappropriate sequences of existing protection schemes. The safety of the DRL\nagent is formally verified through a reachability analysis method.\nAdditionally, our framework is designed for deployment on CUDA-enabled GPU\nsystems, which enables faster execution of these protection sequences and their\nreal-time validation. Our framework establishes a new set of protection rules\nfor grid models, successfully thwarting existing cyber-physical attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The distributed nature of smart grids, combined with sophisticated sensors,\ncontrol algorithms, and data collection facilities at Supervisory Control and\nData Acquisition (SCADA) centers, makes them vulnerable to strategically\ncrafted cyber-physical attacks. These malicious attacks can manipulate power\ndemands using high-wattage Internet of Things (IoT) botnet devices, such as\nrefrigerators and air conditioners, or introduce false values into transmission\nline power flow sensor readings. Consequently, grids experience blackouts and\nhigh power flow oscillations. Existing grid protection mechanisms, originally\ndesigned to tackle natural faults in transmission lines and generator outages,\nare ineffective against such intelligently crafted attacks. This is because\ngrid operators overlook potential scenarios of cyber-physical attacks during\ntheir design phase. In this work, we propose a safe Deep Reinforcement Learning\n(DRL)-based framework for mitigating attacks on smart grids. The DRL agent\neffectively neutralizes cyber-physical attacks on grid surfaces by triggering\nappropriate sequences of existing protection schemes. The safety of the DRL\nagent is formally verified through a reachability analysis method.\nAdditionally, our framework is designed for deployment on CUDA-enabled GPU\nsystems, which enables faster execution of these protection sequences and their\nreal-time validation. Our framework establishes a new set of protection rules\nfor grid models, successfully thwarting existing cyber-physical attacks."
                },
                "authors": [
                    {
                        "name": "Suman Maiti"
                    },
                    {
                        "name": "Soumyajit Dey"
                    }
                ],
                "author_detail": {
                    "name": "Soumyajit Dey"
                },
                "author": "Soumyajit Dey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07234v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07234v3",
                "updated": "2024-09-24T05:16:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    16,
                    59,
                    1,
                    268,
                    0
                ],
                "published": "2024-04-06T06:17:10Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    6,
                    17,
                    10,
                    5,
                    97,
                    0
                ],
                "title": "Goal-guided Generative Prompt Injection Attack on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-guided Generative Prompt Injection Attack on Large Language Models"
                },
                "summary": "Current large language models (LLMs) provide a strong foundation for\nlarge-scale user-oriented natural language tasks. A large number of users can\neasily inject adversarial text or instructions through the user interface, thus\ncausing LLMs model security challenges. Although there is currently a large\namount of research on prompt injection attacks, most of these black-box attacks\nuse heuristic strategies. It is unclear how these heuristic strategies relate\nto the success rate of attacks and thus effectively improve model robustness.\nTo solve this problem, we redefine the goal of the attack: to maximize the KL\ndivergence between the conditional probabilities of the clean text and the\nadversarial text. Furthermore, we prove that maximizing the KL divergence is\nequivalent to maximizing the Mahalanobis distance between the embedded\nrepresentation $x$ and $x'$ of the clean text and the adversarial text when the\nconditional probability is a Gaussian distribution and gives a quantitative\nrelationship on $x$ and $x'$. Then we designed a simple and effective\ngoal-guided generative prompt injection strategy (G2PIA) to find an injection\ntext that satisfies specific constraints to achieve the optimal attack effect\napproximately. It is particularly noteworthy that our attack method is a\nquery-free black-box attack method with low computational cost. Experimental\nresults on seven LLM models and four datasets show the effectiveness of our\nattack method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) provide a strong foundation for\nlarge-scale user-oriented natural language tasks. A large number of users can\neasily inject adversarial text or instructions through the user interface, thus\ncausing LLMs model security challenges. Although there is currently a large\namount of research on prompt injection attacks, most of these black-box attacks\nuse heuristic strategies. It is unclear how these heuristic strategies relate\nto the success rate of attacks and thus effectively improve model robustness.\nTo solve this problem, we redefine the goal of the attack: to maximize the KL\ndivergence between the conditional probabilities of the clean text and the\nadversarial text. Furthermore, we prove that maximizing the KL divergence is\nequivalent to maximizing the Mahalanobis distance between the embedded\nrepresentation $x$ and $x'$ of the clean text and the adversarial text when the\nconditional probability is a Gaussian distribution and gives a quantitative\nrelationship on $x$ and $x'$. Then we designed a simple and effective\ngoal-guided generative prompt injection strategy (G2PIA) to find an injection\ntext that satisfies specific constraints to achieve the optimal attack effect\napproximately. It is particularly noteworthy that our attack method is a\nquery-free black-box attack method with low computational cost. Experimental\nresults on seven LLM models and four datasets show the effectiveness of our\nattack method."
                },
                "authors": [
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Chengzhi Liu"
                    },
                    {
                        "name": "Haochen Xue"
                    },
                    {
                        "name": "Xiaobo Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobo Jin"
                },
                "author": "Xiaobo Jin",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07234v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07234v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15749v1",
                "updated": "2024-09-24T05:10:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    10,
                    13,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T05:10:13Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    10,
                    13,
                    1,
                    268,
                    0
                ],
                "title": "Automated Assessment of Multimodal Answer Sheets in the STEM domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Assessment of Multimodal Answer Sheets in the STEM domain"
                },
                "summary": "In the domain of education, the integration of,technology has led to a\ntransformative era, reshaping traditional,learning paradigms. Central to this\nevolution is the automation,of grading processes, particularly within the STEM\ndomain encompassing Science, Technology, Engineering, and Mathematics.,While\nefforts to automate grading have been made in subjects,like Literature, the\nmultifaceted nature of STEM assessments,presents unique challenges, ranging\nfrom quantitative analysis,to the interpretation of handwritten diagrams. To\naddress these,challenges, this research endeavors to develop efficient and\nreliable grading methods through the implementation of automated,assessment\ntechniques using Artificial Intelligence (AI). Our,contributions lie in two key\nareas: firstly, the development of a,robust system for evaluating textual\nanswers in STEM, leveraging,sample answers for precise comparison and grading,\nenabled by,advanced algorithms and natural language processing\ntechniques.,Secondly, a focus on enhancing diagram evaluation,\nparticularly,flowcharts, within the STEM context, by transforming diagrams,into\ntextual representations for nuanced assessment using a,Large Language Model\n(LLM). By bridging the gap between,visual representation and semantic meaning,\nour approach ensures accurate evaluation while minimizing manual\nintervention.,Through the integration of models such as CRAFT for\ntext,extraction and YoloV5 for object detection, coupled with LLMs,like\nMistral-7B for textual evaluation, our methodology facilitates,comprehensive\nassessment of multimodal answer sheets. This,paper provides a detailed account\nof our methodology, challenges,encountered, results, and implications,\nemphasizing the potential,of AI-driven approaches in revolutionizing grading\npractices in,STEM education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of education, the integration of,technology has led to a\ntransformative era, reshaping traditional,learning paradigms. Central to this\nevolution is the automation,of grading processes, particularly within the STEM\ndomain encompassing Science, Technology, Engineering, and Mathematics.,While\nefforts to automate grading have been made in subjects,like Literature, the\nmultifaceted nature of STEM assessments,presents unique challenges, ranging\nfrom quantitative analysis,to the interpretation of handwritten diagrams. To\naddress these,challenges, this research endeavors to develop efficient and\nreliable grading methods through the implementation of automated,assessment\ntechniques using Artificial Intelligence (AI). Our,contributions lie in two key\nareas: firstly, the development of a,robust system for evaluating textual\nanswers in STEM, leveraging,sample answers for precise comparison and grading,\nenabled by,advanced algorithms and natural language processing\ntechniques.,Secondly, a focus on enhancing diagram evaluation,\nparticularly,flowcharts, within the STEM context, by transforming diagrams,into\ntextual representations for nuanced assessment using a,Large Language Model\n(LLM). By bridging the gap between,visual representation and semantic meaning,\nour approach ensures accurate evaluation while minimizing manual\nintervention.,Through the integration of models such as CRAFT for\ntext,extraction and YoloV5 for object detection, coupled with LLMs,like\nMistral-7B for textual evaluation, our methodology facilitates,comprehensive\nassessment of multimodal answer sheets. This,paper provides a detailed account\nof our methodology, challenges,encountered, results, and implications,\nemphasizing the potential,of AI-driven approaches in revolutionizing grading\npractices in,STEM education."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Patil"
                    },
                    {
                        "name": "Aditya Ashutosh Kulkarni"
                    },
                    {
                        "name": "Ruturaj Ghatage"
                    },
                    {
                        "name": "Sharvi Endait"
                    },
                    {
                        "name": "Geetanjali Kale"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15254v2",
                "updated": "2024-09-24T05:08:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    8,
                    18,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T17:53:42Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    53,
                    42,
                    0,
                    267,
                    0
                ],
                "title": "Archon: An Architecture Search Framework for Inference-Time Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Archon: An Architecture Search Framework for Inference-Time Techniques"
                },
                "summary": "Inference-time techniques are emerging as highly effective tools to increase\nlarge language model (LLM) capabilities. However, there is still limited\nunderstanding of the best practices for developing systems that combine\ninference-time techniques with one or more LLMs, with challenges including: (1)\neffectively allocating inference compute budget, (2) understanding the\ninteractions between different combinations of inference-time techniques and\ntheir impact on downstream performance, and 3) efficiently searching over the\nlarge space of model choices, inference-time techniques, and their\ncompositions. To address these challenges, we introduce Archon, an automated\nframework for designing inference-time architectures. Archon defines an\nextensible design space, encompassing methods such as generation ensembling,\nmulti-sampling, ranking, fusion, critiquing, verification, and unit testing. It\nthen transforms the problem of selecting and combining LLMs and inference-time\ntechniques into a hyperparameter optimization objective. To optimize this\nobjective, we introduce automated Inference-Time Architecture Search (ITAS)\nalgorithms. Given target benchmark(s), an inference compute budget, and\navailable LLMs, ITAS outputs optimized architectures. We evaluate Archon\narchitectures across a wide range of instruction-following and reasoning\nbenchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,\nMixEval Hard, MATH, and CodeContests. We show that automatically designed\ninference-time architectures by Archon outperform strong models such as GPT-4o\nand Claude 3.5 Sonnet on these benchmarks, achieving an average increase of\n14.1 and 10.3 percentage points with all-source models and open-source models,\nrespectively. We make our code and datasets available publicly on Github:\nhttps://github.com/ScalingIntelligence/Archon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time techniques are emerging as highly effective tools to increase\nlarge language model (LLM) capabilities. However, there is still limited\nunderstanding of the best practices for developing systems that combine\ninference-time techniques with one or more LLMs, with challenges including: (1)\neffectively allocating inference compute budget, (2) understanding the\ninteractions between different combinations of inference-time techniques and\ntheir impact on downstream performance, and 3) efficiently searching over the\nlarge space of model choices, inference-time techniques, and their\ncompositions. To address these challenges, we introduce Archon, an automated\nframework for designing inference-time architectures. Archon defines an\nextensible design space, encompassing methods such as generation ensembling,\nmulti-sampling, ranking, fusion, critiquing, verification, and unit testing. It\nthen transforms the problem of selecting and combining LLMs and inference-time\ntechniques into a hyperparameter optimization objective. To optimize this\nobjective, we introduce automated Inference-Time Architecture Search (ITAS)\nalgorithms. Given target benchmark(s), an inference compute budget, and\navailable LLMs, ITAS outputs optimized architectures. We evaluate Archon\narchitectures across a wide range of instruction-following and reasoning\nbenchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,\nMixEval Hard, MATH, and CodeContests. We show that automatically designed\ninference-time architectures by Archon outperform strong models such as GPT-4o\nand Claude 3.5 Sonnet on these benchmarks, achieving an average increase of\n14.1 and 10.3 percentage points with all-source models and open-source models,\nrespectively. We make our code and datasets available publicly on Github:\nhttps://github.com/ScalingIntelligence/Archon."
                },
                "authors": [
                    {
                        "name": "Jon Saad-Falcon"
                    },
                    {
                        "name": "Adrian Gamarra Lafuente"
                    },
                    {
                        "name": "Shlok Natarajan"
                    },
                    {
                        "name": "Nahum Maru"
                    },
                    {
                        "name": "Hristo Todorov"
                    },
                    {
                        "name": "Etash Guha"
                    },
                    {
                        "name": "E. Kelly Buchanan"
                    },
                    {
                        "name": "Mayee Chen"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Christopher R√©"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10702v2",
                "updated": "2024-09-24T05:00:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    5,
                    0,
                    7,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-16T20:05:57Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    20,
                    5,
                    57,
                    0,
                    260,
                    0
                ],
                "title": "Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation\n  with LLMs"
                },
                "summary": "The growing demand for AI training data has transformed data annotation into\na global industry, but traditional approaches relying on human annotators are\noften time-consuming, labor-intensive, and prone to inconsistent quality. We\npropose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models\ninto the annotation process. Our research introduces a collaborative paradigm\nthat leverages the strengths of both professional human annotators and large\nlanguage models (LLMs). By employing LLMs as pre-annotation and real-time\nassistants, and judges on annotator responses, MILO enables effective\ninteraction patterns between human annotators and LLMs. Three empirical studies\non multimodal data annotation demonstrate MILO's efficacy in reducing handling\ntime, improving data quality, and enhancing annotator experiences. We also\nintroduce quality rubrics for flexible evaluation and fine-grained feedback on\nopen-ended annotations. The MILO framework has implications for accelerating\nAI/ML development, reducing reliance on human annotation alone, and promoting\nbetter alignment between human and machine values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for AI training data has transformed data annotation into\na global industry, but traditional approaches relying on human annotators are\noften time-consuming, labor-intensive, and prone to inconsistent quality. We\npropose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models\ninto the annotation process. Our research introduces a collaborative paradigm\nthat leverages the strengths of both professional human annotators and large\nlanguage models (LLMs). By employing LLMs as pre-annotation and real-time\nassistants, and judges on annotator responses, MILO enables effective\ninteraction patterns between human annotators and LLMs. Three empirical studies\non multimodal data annotation demonstrate MILO's efficacy in reducing handling\ntime, improving data quality, and enhancing annotator experiences. We also\nintroduce quality rubrics for flexible evaluation and fine-grained feedback on\nopen-ended annotations. The MILO framework has implications for accelerating\nAI/ML development, reducing reliance on human annotation alone, and promoting\nbetter alignment between human and machine values."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "David Stevens"
                    },
                    {
                        "name": "Pranay Shah"
                    },
                    {
                        "name": "Wenwen Jiang"
                    },
                    {
                        "name": "Miao Liu"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Robert Kuo"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Boying Gong"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Jiabo Hu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Bob Kamma"
                    }
                ],
                "author_detail": {
                    "name": "Bob Kamma"
                },
                "author": "Bob Kamma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15735v1",
                "updated": "2024-09-24T04:42:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    42,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T04:42:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    42,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "LSAST -- Enhancing Cybersecurity through LLM-supported Static\n  Application Security Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSAST -- Enhancing Cybersecurity through LLM-supported Static\n  Application Security Testing"
                },
                "summary": "In the fast-evolving landscape of cybersecurity, Large Language Models (LLMs)\nplay a pivotal role, continually improving their ability to analyze software\ncode. This paper introduces a novel approach to vulnerability scanning by\nintegrating conservative SAST (Static Application Security Testing) scanners\nwith LLM capabilities, resulting in the creation of LSAST (LLM-supported Static\nApplication Security Testing). Our approach significantly enhances the\nperformance of LLMs in vulnerability scanning, establishing a new standard in\nthis field. We benchmark LSAST's efficiency and compare its results with a\nstate-of-the-art LLM. Additionally, we address the inherent drawbacks of LLMs\nin vulnerability scanning: their reliance on static training datasets, which\nleads to the exclusion of the latest vulnerabilities, and the privacy concerns\nassociated with sending code to third-party LLM providers. To mitigate these\nissues, we utilize an open-source LLM to ensure privacy and employ a novel\napproach to gather relevant vulnerability information, thereby equipping the\nLLM with up-to-date knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fast-evolving landscape of cybersecurity, Large Language Models (LLMs)\nplay a pivotal role, continually improving their ability to analyze software\ncode. This paper introduces a novel approach to vulnerability scanning by\nintegrating conservative SAST (Static Application Security Testing) scanners\nwith LLM capabilities, resulting in the creation of LSAST (LLM-supported Static\nApplication Security Testing). Our approach significantly enhances the\nperformance of LLMs in vulnerability scanning, establishing a new standard in\nthis field. We benchmark LSAST's efficiency and compare its results with a\nstate-of-the-art LLM. Additionally, we address the inherent drawbacks of LLMs\nin vulnerability scanning: their reliance on static training datasets, which\nleads to the exclusion of the latest vulnerabilities, and the privacy concerns\nassociated with sending code to third-party LLM providers. To mitigate these\nissues, we utilize an open-source LLM to ensure privacy and employ a novel\napproach to gather relevant vulnerability information, thereby equipping the\nLLM with up-to-date knowledge."
                },
                "authors": [
                    {
                        "name": "Mete Keltek"
                    },
                    {
                        "name": "Ziyue Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Li"
                },
                "author": "Ziyue Li",
                "arxiv_comment": "Under Review of IEEE SaTML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14638v2",
                "updated": "2024-09-24T04:41:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    41,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T00:35:23Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    0,
                    35,
                    23,
                    0,
                    267,
                    0
                ],
                "title": "Harmonising the Clinical Melody: Tuning Large Language Models for\n  Hospital Course Summarisation in Clinical Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonising the Clinical Melody: Tuning Large Language Models for\n  Hospital Course Summarisation in Clinical Coding"
                },
                "summary": "The increasing volume and complexity of clinical documentation in Electronic\nMedical Records systems pose significant challenges for clinical coders, who\nmust mentally process and summarise vast amounts of clinical text to extract\nessential information needed for coding tasks. While large language models have\nbeen successfully applied to shorter summarisation tasks in recent years, the\nchallenge of summarising a hospital course remains an open area for further\nresearch and development. In this study, we adapted three pre trained LLMs,\nLlama 3, BioMistral, Mistral Instruct v0.1 for the hospital course\nsummarisation task, using Quantized Low Rank Adaptation fine tuning. We created\na free text clinical dataset from MIMIC III data by concatenating various\nclinical notes as the input clinical text, paired with ground truth Brief\nHospital Course sections extracted from the discharge summaries for model\ntraining. The fine tuned models were evaluated using BERTScore and ROUGE\nmetrics to assess the effectiveness of clinical domain fine tuning.\nAdditionally, we validated their practical utility using a novel hospital\ncourse summary assessment metric specifically tailored for clinical coding. Our\nfindings indicate that fine tuning pre trained LLMs for the clinical domain can\nsignificantly enhance their performance in hospital course summarisation and\nsuggest their potential as assistive tools for clinical coding. Future work\nshould focus on refining data curation methods to create higher quality\nclinical datasets tailored for hospital course summary tasks and adapting more\nadvanced open source LLMs comparable to proprietary models to further advance\nthis research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume and complexity of clinical documentation in Electronic\nMedical Records systems pose significant challenges for clinical coders, who\nmust mentally process and summarise vast amounts of clinical text to extract\nessential information needed for coding tasks. While large language models have\nbeen successfully applied to shorter summarisation tasks in recent years, the\nchallenge of summarising a hospital course remains an open area for further\nresearch and development. In this study, we adapted three pre trained LLMs,\nLlama 3, BioMistral, Mistral Instruct v0.1 for the hospital course\nsummarisation task, using Quantized Low Rank Adaptation fine tuning. We created\na free text clinical dataset from MIMIC III data by concatenating various\nclinical notes as the input clinical text, paired with ground truth Brief\nHospital Course sections extracted from the discharge summaries for model\ntraining. The fine tuned models were evaluated using BERTScore and ROUGE\nmetrics to assess the effectiveness of clinical domain fine tuning.\nAdditionally, we validated their practical utility using a novel hospital\ncourse summary assessment metric specifically tailored for clinical coding. Our\nfindings indicate that fine tuning pre trained LLMs for the clinical domain can\nsignificantly enhance their performance in hospital course summarisation and\nsuggest their potential as assistive tools for clinical coding. Future work\nshould focus on refining data curation methods to create higher quality\nclinical datasets tailored for hospital course summary tasks and adapting more\nadvanced open source LLMs comparable to proprietary models to further advance\nthis research."
                },
                "authors": [
                    {
                        "name": "Bokang Bi"
                    },
                    {
                        "name": "Leibo Liu"
                    },
                    {
                        "name": "Sanja Lujic"
                    },
                    {
                        "name": "Louisa Jorm"
                    },
                    {
                        "name": "Oscar Perez-Concha"
                    }
                ],
                "author_detail": {
                    "name": "Oscar Perez-Concha"
                },
                "author": "Oscar Perez-Concha",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00634v2",
                "updated": "2024-09-24T04:41:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    41,
                    8,
                    1,
                    268,
                    0
                ],
                "published": "2024-06-30T09:21:01Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    9,
                    21,
                    1,
                    6,
                    182,
                    0
                ],
                "title": "Tarsier: Recipes for Training and Evaluating Large Video Description\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tarsier: Recipes for Training and Evaluating Large Video Description\n  Models"
                },
                "summary": "Generating fine-grained video descriptions is a fundamental challenge in\nvideo understanding. In this work, we introduce Tarsier, a family of\nlarge-scale video-language models designed to generate high-quality video\ndescriptions. Tarsier employs CLIP-ViT to encode frames separately and then\nuses an LLM to model temporal relationships. Despite its simple architecture,\nwe demonstrate that with a meticulously designed two-stage training procedure,\nthe Tarsier models exhibit substantially stronger video description\ncapabilities than any existing open-source model, showing a $+51.4\\%$ advantage\nin human side-by-side evaluation over the strongest model. Additionally, they\nare comparable to state-of-the-art proprietary models, with a $+12.3\\%$\nadvantage against GPT-4V and a $-6.7\\%$ disadvantage against Gemini 1.5 Pro.\nWhen upgraded to Tarsier2 by building upon SigLIP and Qwen2-7B, it further\nimproves significantly with a $+4.8\\%$ advantage against GPT-4o. Besides video\ndescription, Tarsier proves to be a versatile generalist model, achieving new\nstate-of-the-art results across nine public benchmarks, including multi-choice\nVQA, open-ended VQA, and zero-shot video captioning. Our second contribution is\nthe introduction of a new benchmark -- DREAM-1K\n(https://tarsier-vlm.github.io/) for evaluating video description models,\nconsisting of a new challenging dataset featuring videos from diverse sources\nand varying complexity, along with an automatic method specifically designed to\nassess the quality of fine-grained video descriptions. We make our models and\nevaluation benchmark publicly available at\nhttps://github.com/bytedance/tarsier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating fine-grained video descriptions is a fundamental challenge in\nvideo understanding. In this work, we introduce Tarsier, a family of\nlarge-scale video-language models designed to generate high-quality video\ndescriptions. Tarsier employs CLIP-ViT to encode frames separately and then\nuses an LLM to model temporal relationships. Despite its simple architecture,\nwe demonstrate that with a meticulously designed two-stage training procedure,\nthe Tarsier models exhibit substantially stronger video description\ncapabilities than any existing open-source model, showing a $+51.4\\%$ advantage\nin human side-by-side evaluation over the strongest model. Additionally, they\nare comparable to state-of-the-art proprietary models, with a $+12.3\\%$\nadvantage against GPT-4V and a $-6.7\\%$ disadvantage against Gemini 1.5 Pro.\nWhen upgraded to Tarsier2 by building upon SigLIP and Qwen2-7B, it further\nimproves significantly with a $+4.8\\%$ advantage against GPT-4o. Besides video\ndescription, Tarsier proves to be a versatile generalist model, achieving new\nstate-of-the-art results across nine public benchmarks, including multi-choice\nVQA, open-ended VQA, and zero-shot video captioning. Our second contribution is\nthe introduction of a new benchmark -- DREAM-1K\n(https://tarsier-vlm.github.io/) for evaluating video description models,\nconsisting of a new challenging dataset featuring videos from diverse sources\nand varying complexity, along with an automatic method specifically designed to\nassess the quality of fine-grained video descriptions. We make our models and\nevaluation benchmark publicly available at\nhttps://github.com/bytedance/tarsier."
                },
                "authors": [
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Liping Yuan"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Haomiao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Haomiao Sun"
                },
                "author": "Haomiao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15724v1",
                "updated": "2024-09-24T04:17:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    17,
                    21,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T04:17:21Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    17,
                    21,
                    1,
                    268,
                    0
                ],
                "title": "LLM-Cure: LLM-based Competitor User Review Analysis for Feature\n  Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Cure: LLM-based Competitor User Review Analysis for Feature\n  Enhancement"
                },
                "summary": "The exponential growth of the mobile app market underscores the importance of\nconstant innovation and rapid response to user demands. As user satisfaction is\nparamount to the success of a mobile application (app), developers typically\nrely on user reviews, which represent user feedback that includes ratings and\ncomments to identify areas for improvement. However, the sheer volume of user\nreviews poses challenges in manual analysis, necessitating automated\napproaches. Existing automated approaches either analyze only the target apps\nreviews, neglecting the comparison of similar features to competitors or fail\nto provide suggestions for feature enhancement. To address these gaps, we\npropose a Large Language Model (LLM)-based Competitive User Review Analysis for\nFeature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically\ngenerate suggestion s for mobile app feature improvements. More specifically,\nLLM-Cure identifies and categorizes features within reviews by applying LLMs.\nWhen provided with a complaint in a user review, LLM-Cure curates highly rated\n(4 and 5 stars) reviews in competing apps related to the complaint and proposes\npotential improvements tailored to the target application. We evaluate LLM-Cure\non 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates\nthat LLM-Cure significantly outperforms the state-of-the-art approaches in\nassigning features to reviews by up to 13% in F1-score, up to 16% in recall and\nup to 11% in precision. Additionally, LLM-Cure demonstrates its capability to\nprovide suggestions for resolving user complaints. We verify the suggestions\nusing the release notes that reflect the changes of features in the target\nmobile app. LLM-Cure achieves a promising average of 73% of the implementation\nof the provided suggestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of the mobile app market underscores the importance of\nconstant innovation and rapid response to user demands. As user satisfaction is\nparamount to the success of a mobile application (app), developers typically\nrely on user reviews, which represent user feedback that includes ratings and\ncomments to identify areas for improvement. However, the sheer volume of user\nreviews poses challenges in manual analysis, necessitating automated\napproaches. Existing automated approaches either analyze only the target apps\nreviews, neglecting the comparison of similar features to competitors or fail\nto provide suggestions for feature enhancement. To address these gaps, we\npropose a Large Language Model (LLM)-based Competitive User Review Analysis for\nFeature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically\ngenerate suggestion s for mobile app feature improvements. More specifically,\nLLM-Cure identifies and categorizes features within reviews by applying LLMs.\nWhen provided with a complaint in a user review, LLM-Cure curates highly rated\n(4 and 5 stars) reviews in competing apps related to the complaint and proposes\npotential improvements tailored to the target application. We evaluate LLM-Cure\non 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates\nthat LLM-Cure significantly outperforms the state-of-the-art approaches in\nassigning features to reviews by up to 13% in F1-score, up to 16% in recall and\nup to 11% in precision. Additionally, LLM-Cure demonstrates its capability to\nprovide suggestions for resolving user complaints. We verify the suggestions\nusing the release notes that reflect the changes of features in the target\nmobile app. LLM-Cure achieves a promising average of 73% of the implementation\nof the provided suggestions."
                },
                "authors": [
                    {
                        "name": "Maram Assi"
                    },
                    {
                        "name": "Safwat Hassan"
                    },
                    {
                        "name": "Ying Zou"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zou"
                },
                "author": "Ying Zou",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15723v1",
                "updated": "2024-09-24T04:14:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    14,
                    33,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T04:14:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    14,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "Federated Large Language Models: Current Progress and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Large Language Models: Current Progress and Future Directions"
                },
                "summary": "Large language models are rapidly gaining popularity and have been widely\nadopted in real-world applications. While the quality of training data is\nessential, privacy concerns arise during data collection. Federated learning\noffers a solution by allowing multiple clients to collaboratively train LLMs\nwithout sharing local data. However, FL introduces new challenges, such as\nmodel convergence issues due to heterogeneous data and high communication\ncosts. A comprehensive study is required to address these challenges and guide\nfuture research. This paper surveys Federated learning for LLMs (FedLLM),\nhighlighting recent advances and future directions. We focus on two key\naspects: fine-tuning and prompt learning in a federated setting, discussing\nexisting work and associated research challenges. We finally propose potential\nresearch directions for federated LLMs, including pre-training and how LLMs can\nfurther enhance federated learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are rapidly gaining popularity and have been widely\nadopted in real-world applications. While the quality of training data is\nessential, privacy concerns arise during data collection. Federated learning\noffers a solution by allowing multiple clients to collaboratively train LLMs\nwithout sharing local data. However, FL introduces new challenges, such as\nmodel convergence issues due to heterogeneous data and high communication\ncosts. A comprehensive study is required to address these challenges and guide\nfuture research. This paper surveys Federated learning for LLMs (FedLLM),\nhighlighting recent advances and future directions. We focus on two key\naspects: fine-tuning and prompt learning in a federated setting, discussing\nexisting work and associated research challenges. We finally propose potential\nresearch directions for federated LLMs, including pre-training and how LLMs can\nfurther enhance federated learning."
                },
                "authors": [
                    {
                        "name": "Yuhang Yao"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Chengkai Huang"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Sungchul Kim"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15706v1",
                "updated": "2024-09-24T03:47:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    3,
                    47,
                    2,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T03:47:02Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    3,
                    47,
                    2,
                    1,
                    268,
                    0
                ],
                "title": "Improving Emotional Support Delivery in Text-Based Community Safety\n  Reporting Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Emotional Support Delivery in Text-Based Community Safety\n  Reporting Using Large Language Models"
                },
                "summary": "Emotional support is a crucial aspect of communication between community\nmembers and police dispatchers during incident reporting. However, there is a\nlack of understanding about how emotional support is delivered through\ntext-based systems, especially in various non-emergency contexts. In this\nstudy, we analyzed two years of chat logs comprising 57,114 messages across\n8,239 incidents from 130 higher education institutions. Our empirical findings\nrevealed significant variations in emotional support provided by dispatchers,\ninfluenced by the type of incident, service time, and a noticeable decline in\nsupport over time across multiple organizations. To improve the consistency and\nquality of emotional support, we developed and implemented a fine-tuned Large\nLanguage Model (LLM), named dispatcherLLM. We evaluated dispatcherLLM by\ncomparing its generated responses to those of human dispatchers and other\noff-the-shelf models using real chat messages. Additionally, we conducted a\nhuman evaluation to assess the perceived effectiveness of the support provided\nby dispatcherLLM. This study not only contributes new empirical understandings\nof emotional support in text-based dispatch systems but also demonstrates the\nsignificant potential of generative AI in improving service delivery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional support is a crucial aspect of communication between community\nmembers and police dispatchers during incident reporting. However, there is a\nlack of understanding about how emotional support is delivered through\ntext-based systems, especially in various non-emergency contexts. In this\nstudy, we analyzed two years of chat logs comprising 57,114 messages across\n8,239 incidents from 130 higher education institutions. Our empirical findings\nrevealed significant variations in emotional support provided by dispatchers,\ninfluenced by the type of incident, service time, and a noticeable decline in\nsupport over time across multiple organizations. To improve the consistency and\nquality of emotional support, we developed and implemented a fine-tuned Large\nLanguage Model (LLM), named dispatcherLLM. We evaluated dispatcherLLM by\ncomparing its generated responses to those of human dispatchers and other\noff-the-shelf models using real chat messages. Additionally, we conducted a\nhuman evaluation to assess the perceived effectiveness of the support provided\nby dispatcherLLM. This study not only contributes new empirical understandings\nof emotional support in text-based dispatch systems but also demonstrates the\nsignificant potential of generative AI in improving service delivery."
                },
                "authors": [
                    {
                        "name": "Yiren Liu"
                    },
                    {
                        "name": "Yerong Li"
                    },
                    {
                        "name": "Ryan Mayfield"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15700v1",
                "updated": "2024-09-24T03:30:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    3,
                    30,
                    19,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T03:30:19Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    3,
                    30,
                    19,
                    1,
                    268,
                    0
                ],
                "title": "Making Text Embedders Few-Shot Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Text Embedders Few-Shot Learners"
                },
                "summary": "Large language models (LLMs) with decoder-only architectures demonstrate\nremarkable in-context learning (ICL) capabilities. This feature enables them to\neffectively handle both familiar and novel tasks by utilizing examples provided\nwithin their input context. Recognizing the potential of this capability, we\npropose leveraging the ICL feature in LLMs to enhance the process of text\nembedding generation. To this end, we introduce a novel model bge-en-icl, which\nemploys few-shot examples to produce high-quality text embeddings. Our approach\nintegrates task-related examples directly into the query side, resulting in\nsignificant improvements across various tasks. Additionally, we have\ninvestigated how to effectively utilize LLMs as embedding models, including\nvarious attention mechanisms, pooling methods, etc. Our findings suggest that\nretaining the original framework often yields the best results, underscoring\nthat simplicity is best. Experimental results on the MTEB and AIR-Bench\nbenchmarks demonstrate that our approach sets new state-of-the-art (SOTA)\nperformance. Our model, code and dataset are freely available at\nhttps://github.com/FlagOpen/FlagEmbedding .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with decoder-only architectures demonstrate\nremarkable in-context learning (ICL) capabilities. This feature enables them to\neffectively handle both familiar and novel tasks by utilizing examples provided\nwithin their input context. Recognizing the potential of this capability, we\npropose leveraging the ICL feature in LLMs to enhance the process of text\nembedding generation. To this end, we introduce a novel model bge-en-icl, which\nemploys few-shot examples to produce high-quality text embeddings. Our approach\nintegrates task-related examples directly into the query side, resulting in\nsignificant improvements across various tasks. Additionally, we have\ninvestigated how to effectively utilize LLMs as embedding models, including\nvarious attention mechanisms, pooling methods, etc. Our findings suggest that\nretaining the original framework often yields the best results, underscoring\nthat simplicity is best. Experimental results on the MTEB and AIR-Bench\nbenchmarks demonstrate that our approach sets new state-of-the-art (SOTA)\nperformance. Our model, code and dataset are freely available at\nhttps://github.com/FlagOpen/FlagEmbedding ."
                },
                "authors": [
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "MingHao Qin"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Jianlyu Chen"
                    },
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Yingxia Shao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15699v1",
                "updated": "2024-09-24T03:25:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    3,
                    25,
                    36,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T03:25:36Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    3,
                    25,
                    36,
                    1,
                    268,
                    0
                ],
                "title": "Lighter And Better: Towards Flexible Context Adaptation For Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lighter And Better: Towards Flexible Context Adaptation For Retrieval\n  Augmented Generation"
                },
                "summary": "The existing Retrieval-Augmented Generation (RAG) systems face significant\nchallenges in terms of cost and effectiveness. On one hand, they need to encode\nthe lengthy retrieved contexts before responding to the input tasks, which\nimposes substantial computational overhead. On the other hand, directly using\ngeneric Large Language Models (LLMs) often leads to sub-optimal answers, while\ntask-specific fine-tuning may compromise the LLMs' general capabilities. To\naddress these challenges, we introduce a novel approach called FlexRAG\n(Flexible Context Adaptation for RAG). In this approach, the retrieved contexts\nare compressed into compact embeddings before being encoded by the LLMs.\nSimultaneously, these compressed embeddings are optimized to enhance downstream\nRAG performance. A key feature of FlexRAG is its flexibility, which enables\neffective support for diverse compression ratios and selective preservation of\nimportant contexts. Thanks to these technical designs, FlexRAG achieves\nsuperior generation quality while significantly reducing running costs.\nComprehensive experiments on various question-answering datasets validate our\napproach as a cost-effective and flexible solution for RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existing Retrieval-Augmented Generation (RAG) systems face significant\nchallenges in terms of cost and effectiveness. On one hand, they need to encode\nthe lengthy retrieved contexts before responding to the input tasks, which\nimposes substantial computational overhead. On the other hand, directly using\ngeneric Large Language Models (LLMs) often leads to sub-optimal answers, while\ntask-specific fine-tuning may compromise the LLMs' general capabilities. To\naddress these challenges, we introduce a novel approach called FlexRAG\n(Flexible Context Adaptation for RAG). In this approach, the retrieved contexts\nare compressed into compact embeddings before being encoded by the LLMs.\nSimultaneously, these compressed embeddings are optimized to enhance downstream\nRAG performance. A key feature of FlexRAG is its flexibility, which enables\neffective support for diverse compression ratios and selective preservation of\nimportant contexts. Thanks to these technical designs, FlexRAG achieves\nsuperior generation quality while significantly reducing running costs.\nComprehensive experiments on various question-answering datasets validate our\napproach as a cost-effective and flexible solution for RAG systems."
                },
                "authors": [
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chenyuan Wu"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14165v2",
                "updated": "2024-09-24T03:12:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    3,
                    12,
                    12,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-21T15:07:37Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    15,
                    7,
                    37,
                    5,
                    265,
                    0
                ],
                "title": "Will Large Language Models be a Panacea to Autonomous Driving?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will Large Language Models be a Panacea to Autonomous Driving?"
                },
                "summary": "Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)\nresearch, propelling its development towards intelligence and efficiency.\nCurrently, the development of AD technology follows two main technical paths:\nmodularization and end-to-end. Modularization decompose the driving task into\nmodules such as perception, prediction, planning, and control, and train them\nseparately. Due to the inconsistency of training objectives between modules,\nthe integrated effect suffers from bias. End-to-end attempts to address this\nissue by utilizing a single model that directly maps from sensor data to\ncontrol signals. This path has limited learning capabilities in a comprehensive\nset of features and struggles to handle unpredictable long-tail events and\ncomplex urban traffic scenarios. In the face of challenges encountered in both\npaths, many researchers believe that large language models (LLMs) with powerful\nreasoning capabilities and extensive knowledge understanding may be the\nsolution, expecting LLMs to provide AD systems with deeper levels of\nunderstanding and decision-making capabilities. In light of the challenges\nfaced by both paths, many researchers believe that LLMs, with their powerful\nreasoning abilities and extensive knowledge, could offer a solution. To\nunderstand if LLMs could enhance AD, this paper conducts a thorough analysis of\nthe potential applications of LLMs in AD systems, including exploring their\noptimization strategies in both modular and end-to-end approaches, with a\nparticular focus on how LLMs can tackle the problems and challenges present in\ncurrent solutions. Furthermore, we discuss an important question: Can LLM-based\nartificial general intelligence (AGI) be a key to achieve high-level AD? We\nfurther analyze the potential limitations and challenges that LLMs may\nencounter in promoting the development of AD technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)\nresearch, propelling its development towards intelligence and efficiency.\nCurrently, the development of AD technology follows two main technical paths:\nmodularization and end-to-end. Modularization decompose the driving task into\nmodules such as perception, prediction, planning, and control, and train them\nseparately. Due to the inconsistency of training objectives between modules,\nthe integrated effect suffers from bias. End-to-end attempts to address this\nissue by utilizing a single model that directly maps from sensor data to\ncontrol signals. This path has limited learning capabilities in a comprehensive\nset of features and struggles to handle unpredictable long-tail events and\ncomplex urban traffic scenarios. In the face of challenges encountered in both\npaths, many researchers believe that large language models (LLMs) with powerful\nreasoning capabilities and extensive knowledge understanding may be the\nsolution, expecting LLMs to provide AD systems with deeper levels of\nunderstanding and decision-making capabilities. In light of the challenges\nfaced by both paths, many researchers believe that LLMs, with their powerful\nreasoning abilities and extensive knowledge, could offer a solution. To\nunderstand if LLMs could enhance AD, this paper conducts a thorough analysis of\nthe potential applications of LLMs in AD systems, including exploring their\noptimization strategies in both modular and end-to-end approaches, with a\nparticular focus on how LLMs can tackle the problems and challenges present in\ncurrent solutions. Furthermore, we discuss an important question: Can LLM-based\nartificial general intelligence (AGI) be a key to achieve high-level AD? We\nfurther analyze the potential limitations and challenges that LLMs may\nencounter in promoting the development of AD technology."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Shiyi Wang"
                    },
                    {
                        "name": "Wenqing Zhong"
                    },
                    {
                        "name": "Nianchen Shen"
                    },
                    {
                        "name": "Yunqi Li"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Zhiheng Li"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15687v1",
                "updated": "2024-09-24T02:58:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    58,
                    52,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T02:58:52Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    58,
                    52,
                    1,
                    268,
                    0
                ],
                "title": "A Comprehensive Evaluation of Large Language Models on Mental Illnesses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Large Language Models on Mental Illnesses"
                },
                "summary": "Large language models have shown promise in various domains, including\nhealthcare. In this study, we conduct a comprehensive evaluation of LLMs in the\ncontext of mental health tasks using social media data. We explore the\nzero-shot (ZS) and few-shot (FS) capabilities of various LLMs, including GPT-4,\nLlama 3, Gemini, and others, on tasks such as binary disorder detection,\ndisorder severity evaluation, and psychiatric knowledge assessment. Our\nevaluation involved 33 models testing 9 main prompt templates across the tasks.\nKey findings revealed that models like GPT-4 and Llama 3 exhibited superior\nperformance in binary disorder detection, with accuracies reaching up to 85% on\ncertain datasets. Moreover, prompt engineering played a crucial role in\nenhancing model performance. Notably, the Mixtral 8x22b model showed an\nimprovement of over 20%, while Gemma 7b experienced a similar boost in\nperformance. In the task of disorder severity evaluation, we observed that FS\nlearning significantly improved the model's accuracy, highlighting the\nimportance of contextual examples in complex assessments. Notably, the\nPhi-3-mini model exhibited a substantial increase in performance, with balanced\naccuracy improving by over 6.80% and mean average error dropping by nearly 1.3\nwhen moving from ZS to FS learning. In the psychiatric knowledge task, recent\nmodels generally outperformed older, larger counterparts, with the Llama 3.1\n405b achieving an accuracy of 91.2%. Despite promising results, our analysis\nidentified several challenges, including variability in performance across\ndatasets and the need for careful prompt engineering. Furthermore, the ethical\nguards imposed by many LLM providers hamper the ability to accurately evaluate\ntheir performance, due to tendency to not respond to potentially sensitive\nqueries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown promise in various domains, including\nhealthcare. In this study, we conduct a comprehensive evaluation of LLMs in the\ncontext of mental health tasks using social media data. We explore the\nzero-shot (ZS) and few-shot (FS) capabilities of various LLMs, including GPT-4,\nLlama 3, Gemini, and others, on tasks such as binary disorder detection,\ndisorder severity evaluation, and psychiatric knowledge assessment. Our\nevaluation involved 33 models testing 9 main prompt templates across the tasks.\nKey findings revealed that models like GPT-4 and Llama 3 exhibited superior\nperformance in binary disorder detection, with accuracies reaching up to 85% on\ncertain datasets. Moreover, prompt engineering played a crucial role in\nenhancing model performance. Notably, the Mixtral 8x22b model showed an\nimprovement of over 20%, while Gemma 7b experienced a similar boost in\nperformance. In the task of disorder severity evaluation, we observed that FS\nlearning significantly improved the model's accuracy, highlighting the\nimportance of contextual examples in complex assessments. Notably, the\nPhi-3-mini model exhibited a substantial increase in performance, with balanced\naccuracy improving by over 6.80% and mean average error dropping by nearly 1.3\nwhen moving from ZS to FS learning. In the psychiatric knowledge task, recent\nmodels generally outperformed older, larger counterparts, with the Llama 3.1\n405b achieving an accuracy of 91.2%. Despite promising results, our analysis\nidentified several challenges, including variability in performance across\ndatasets and the need for careful prompt engineering. Furthermore, the ethical\nguards imposed by many LLM providers hamper the ability to accurately evaluate\ntheir performance, due to tendency to not respond to potentially sensitive\nqueries."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Hanafi"
                    },
                    {
                        "name": "Mohammed Saad"
                    },
                    {
                        "name": "Noureldin Zahran"
                    },
                    {
                        "name": "Radwa J. Hanafy"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14961v2",
                "updated": "2024-09-24T02:56:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    56,
                    0,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-23T12:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    26,
                    42,
                    0,
                    267,
                    0
                ],
                "title": "UELLM: A Unified and Efficient Approach for LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UELLM: A Unified and Efficient Approach for LLM Inference Serving"
                },
                "summary": "In the context of Machine Learning as a Service (MLaaS) clouds, the extensive\nuse of Large Language Models (LLMs) often requires efficient management of\nsignificant query loads. When providing real-time inference services, several\nchallenges arise. Firstly, increasing the number of GPUs may lead to a decrease\nin inference speed due to heightened communication overhead, while an\ninadequate number of GPUs can lead to out-of-memory errors. Secondly, different\ndeployment strategies need to be evaluated to guarantee optimal utilization and\nminimal inference latency. Lastly, inefficient orchestration of inference\nqueries can easily lead to significant Service Level Objective (SLO)\nviolations. Lastly, inefficient orchestration of inference queries can easily\nlead to significant Service Level Objective (SLO) violations. To address these\nchallenges, we propose a Unified and Efficient approach for Large Language\nModel inference serving (UELLM), which consists of three main components: 1)\nresource profiler, 2) batch scheduler, and 3) LLM deployer. UELLM minimizes\nresource overhead, reduces inference latency, and lowers SLO violation rates.\nCompared with state-of-the-art (SOTA) techniques, UELLM reduces the inference\nlatency by 72.3% to 90.3%, enhances GPU utilization by 1.2X to 4.1X, and\nincreases throughput by 1.92X to 4.98X, it can also serve without violating the\ninference latency SLO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of Machine Learning as a Service (MLaaS) clouds, the extensive\nuse of Large Language Models (LLMs) often requires efficient management of\nsignificant query loads. When providing real-time inference services, several\nchallenges arise. Firstly, increasing the number of GPUs may lead to a decrease\nin inference speed due to heightened communication overhead, while an\ninadequate number of GPUs can lead to out-of-memory errors. Secondly, different\ndeployment strategies need to be evaluated to guarantee optimal utilization and\nminimal inference latency. Lastly, inefficient orchestration of inference\nqueries can easily lead to significant Service Level Objective (SLO)\nviolations. Lastly, inefficient orchestration of inference queries can easily\nlead to significant Service Level Objective (SLO) violations. To address these\nchallenges, we propose a Unified and Efficient approach for Large Language\nModel inference serving (UELLM), which consists of three main components: 1)\nresource profiler, 2) batch scheduler, and 3) LLM deployer. UELLM minimizes\nresource overhead, reduces inference latency, and lowers SLO violation rates.\nCompared with state-of-the-art (SOTA) techniques, UELLM reduces the inference\nlatency by 72.3% to 90.3%, enhances GPU utilization by 1.2X to 4.1X, and\nincreases throughput by 1.92X to 4.98X, it can also serve without violating the\ninference latency SLO."
                },
                "authors": [
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Wanyi Zheng"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 5 figures, ICSOC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15684v1",
                "updated": "2024-09-24T02:55:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    55,
                    54,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T02:55:54Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    55,
                    54,
                    1,
                    268,
                    0
                ],
                "title": "SYNERGAI: Perception Alignment for Human-Robot Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYNERGAI: Perception Alignment for Human-Robot Collaboration"
                },
                "summary": "Recently, large language models (LLMs) have shown strong potential in\nfacilitating human-robotic interaction and collaboration. However, existing\nLLM-based systems often overlook the misalignment between human and robot\nperceptions, which hinders their effective communication and real-world robot\ndeployment. To address this issue, we introduce SYNERGAI, a unified system\ndesigned to achieve both perceptual alignment and human-robot collaboration. At\nits core, SYNERGAI employs 3D Scene Graph (3DSG) as its explicit and innate\nrepresentation. This enables the system to leverage LLM to break down complex\ntasks and allocate appropriate tools in intermediate steps to extract relevant\ninformation from the 3DSG, modify its structure, or generate responses.\nImportantly, SYNERGAI incorporates an automatic mechanism that enables\nperceptual misalignment correction with users by updating its 3DSG with online\ninteraction. SYNERGAI achieves comparable performance with the data-driven\nmodels in ScanQA in a zero-shot manner. Through comprehensive experiments\nacross 10 real-world scenes, SYNERGAI demonstrates its effectiveness in\nestablishing common ground with humans, realizing a success rate of 61.9% in\nalignment tasks. It also significantly improves the success rate from 3.7% to\n45.68% on novel tasks by transferring the knowledge acquired during alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have shown strong potential in\nfacilitating human-robotic interaction and collaboration. However, existing\nLLM-based systems often overlook the misalignment between human and robot\nperceptions, which hinders their effective communication and real-world robot\ndeployment. To address this issue, we introduce SYNERGAI, a unified system\ndesigned to achieve both perceptual alignment and human-robot collaboration. At\nits core, SYNERGAI employs 3D Scene Graph (3DSG) as its explicit and innate\nrepresentation. This enables the system to leverage LLM to break down complex\ntasks and allocate appropriate tools in intermediate steps to extract relevant\ninformation from the 3DSG, modify its structure, or generate responses.\nImportantly, SYNERGAI incorporates an automatic mechanism that enables\nperceptual misalignment correction with users by updating its 3DSG with online\ninteraction. SYNERGAI achieves comparable performance with the data-driven\nmodels in ScanQA in a zero-shot manner. Through comprehensive experiments\nacross 10 real-world scenes, SYNERGAI demonstrates its effectiveness in\nestablishing common ground with humans, realizing a success rate of 61.9% in\nalignment tasks. It also significantly improves the success rate from 3.7% to\n45.68% on novel tasks by transferring the knowledge acquired during alignment."
                },
                "authors": [
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Guoxi Zhang"
                    },
                    {
                        "name": "Yaowei Zhang"
                    },
                    {
                        "name": "Hongming Xu"
                    },
                    {
                        "name": "Peiyuan Zhi"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "arxiv_comment": "Project page: https://synerg-ai.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09682v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09682v3",
                "updated": "2024-09-24T02:35:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    35,
                    41,
                    1,
                    268,
                    0
                ],
                "published": "2024-04-15T11:36:10Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    11,
                    36,
                    10,
                    0,
                    106,
                    0
                ],
                "title": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data\n  Annotation"
                },
                "summary": "The quality of the dataset is crucial for ensuring optimal performance and\nreliability of downstream task models. However, datasets often contain noisy\ndata inadvertently included during the construction process. Numerous attempts\nhave been made to correct this issue through human annotators. However, hiring\nand managing human annotators is expensive and time-consuming. As an\nalternative, recent studies are exploring the use of large language models\n(LLMs) for data annotation.\n  In this study, we present a case study that extends the application of\nLLM-based data annotation to enhance the quality of existing datasets through a\ncleansing strategy. Specifically, we leverage approaches such as\nchain-of-thought and majority voting to imitate human annotation and classify\nunrelated documents from the Multi-News dataset, which is widely used for the\nmulti-document summarization task. Through our proposed cleansing method, we\nintroduce an enhanced Multi-News+. By employing LLMs for data cleansing, we\ndemonstrate an efficient and effective approach to improving dataset quality\nwithout relying on expensive human annotation efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of the dataset is crucial for ensuring optimal performance and\nreliability of downstream task models. However, datasets often contain noisy\ndata inadvertently included during the construction process. Numerous attempts\nhave been made to correct this issue through human annotators. However, hiring\nand managing human annotators is expensive and time-consuming. As an\nalternative, recent studies are exploring the use of large language models\n(LLMs) for data annotation.\n  In this study, we present a case study that extends the application of\nLLM-based data annotation to enhance the quality of existing datasets through a\ncleansing strategy. Specifically, we leverage approaches such as\nchain-of-thought and majority voting to imitate human annotation and classify\nunrelated documents from the Multi-News dataset, which is widely used for the\nmulti-document summarization task. Through our proposed cleansing method, we\nintroduce an enhanced Multi-News+. By employing LLMs for data cleansing, we\ndemonstrate an efficient and effective approach to improving dataset quality\nwithout relying on expensive human annotation efforts."
                },
                "authors": [
                    {
                        "name": "Juhwan Choi"
                    },
                    {
                        "name": "Jungmin Yun"
                    },
                    {
                        "name": "Kyohoon Jin"
                    },
                    {
                        "name": "YoungBin Kim"
                    }
                ],
                "author_detail": {
                    "name": "YoungBin Kim"
                },
                "author": "YoungBin Kim",
                "arxiv_comment": "EMNLP 2024: Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09682v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09682v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15678v1",
                "updated": "2024-09-24T02:34:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    34,
                    2,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T02:34:02Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    34,
                    2,
                    1,
                    268,
                    0
                ],
                "title": "Objectively Evaluating the Reliability of Cell Type Annotation Using\n  LLM-Based Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectively Evaluating the Reliability of Cell Type Annotation Using\n  LLM-Based Strategies"
                },
                "summary": "Reliability in cell type annotation is challenging in single-cell\nRNA-sequencing data analysis because both expert-driven and automated methods\ncan be biased or constrained by their training data, especially for novel or\nrare cell types. Although large language models (LLMs) are useful, our\nevaluation found that only a few matched expert annotations due to biased data\nsources and inflexible training inputs. To overcome these limitations, we\ndeveloped the LICT (Large language model-based Identifier for Cell Types)\nsoftware package using a multi-model fusion and \"talk-to-machine\" strategy.\nTested across various single-cell RNA sequencing datasets, our approach\nsignificantly improved annotation reliability, especially in datasets with low\ncellular heterogeneity. Notably, we established objective criteria to assess\nannotation reliability using the \"talk-to-machine\" approach, which addresses\ndiscrepancies between our annotations and expert ones, enabling reliable\nevaluation even without reference data. This strategy enhances annotation\ncredibility and sets the stage for advancing future LLM-based cell type\nannotation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability in cell type annotation is challenging in single-cell\nRNA-sequencing data analysis because both expert-driven and automated methods\ncan be biased or constrained by their training data, especially for novel or\nrare cell types. Although large language models (LLMs) are useful, our\nevaluation found that only a few matched expert annotations due to biased data\nsources and inflexible training inputs. To overcome these limitations, we\ndeveloped the LICT (Large language model-based Identifier for Cell Types)\nsoftware package using a multi-model fusion and \"talk-to-machine\" strategy.\nTested across various single-cell RNA sequencing datasets, our approach\nsignificantly improved annotation reliability, especially in datasets with low\ncellular heterogeneity. Notably, we established objective criteria to assess\nannotation reliability using the \"talk-to-machine\" approach, which addresses\ndiscrepancies between our annotations and expert ones, enabling reliable\nevaluation even without reference data. This strategy enhances annotation\ncredibility and sets the stage for advancing future LLM-based cell type\nannotation methods."
                },
                "authors": [
                    {
                        "name": "Wenjin Ye"
                    },
                    {
                        "name": "Yuanchen Ma"
                    },
                    {
                        "name": "Junkai Xiang"
                    },
                    {
                        "name": "Hongjie Liang"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Qiuling Xiang"
                    },
                    {
                        "name": "Andy Peng Xiang"
                    },
                    {
                        "name": "Wu Song"
                    },
                    {
                        "name": "Weiqiang Li"
                    },
                    {
                        "name": "Weijun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weijun Huang"
                },
                "arxiv_affiliation": "National-Local Joint Engineering Research Center for Stem Cells and Regenerative Medicine, Zhongshan School of Medicine, Sun Yat-Sen University, Guangzhou, Guangdong, China",
                "author": "Weijun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15675v1",
                "updated": "2024-09-24T02:27:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    27,
                    10,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T02:27:10Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    2,
                    27,
                    10,
                    1,
                    268,
                    0
                ],
                "title": "Northeast Materials Database (NEMAD): Enabling Discovery of High\n  Transition Temperature Magnetic Compounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Northeast Materials Database (NEMAD): Enabling Discovery of High\n  Transition Temperature Magnetic Compounds"
                },
                "summary": "The discovery of novel magnetic materials with greater operating temperature\nranges and optimized performance is essential for advanced applications.\nCurrent data-driven approaches are challenging and limited due to the lack of\naccurate, comprehensive, and feature-rich databases. This study aims to address\nthis challenge by introducing a new approach that uses Large Language Models\n(LLMs) to create a comprehensive, experiment-based, magnetic materials database\nnamed the Northeast Materials Database (NEMAD), which consists of 26,706\nmagnetic materials (www.nemad.org). The database incorporates chemical\ncomposition, magnetic phase transition temperatures, structural details, and\nmagnetic properties. Enabled by NEMAD, machine learning models were developed\nto classify materials and predict transition temperatures. Our classification\nmodel achieved an accuracy of 90% in categorizing materials as ferromagnetic\n(FM), antiferromagnetic (AFM), and non-magnetic (NM). The regression models\npredict Curie (N\\'eel) temperature with a coefficient of determination (R2) of\n0.86 (0.85) and a mean absolute error (MAE) of 62K (32K). These models\nidentified 62 (19) FM (AFM) candidates with a predicted Curie (N\\'eel)\ntemperature above 500K (100K) from the Materials Project. This work shows the\nfeasibility of combining LLMs for automated data extraction and machine\nlearning models in accelerating the discovery of magnetic materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The discovery of novel magnetic materials with greater operating temperature\nranges and optimized performance is essential for advanced applications.\nCurrent data-driven approaches are challenging and limited due to the lack of\naccurate, comprehensive, and feature-rich databases. This study aims to address\nthis challenge by introducing a new approach that uses Large Language Models\n(LLMs) to create a comprehensive, experiment-based, magnetic materials database\nnamed the Northeast Materials Database (NEMAD), which consists of 26,706\nmagnetic materials (www.nemad.org). The database incorporates chemical\ncomposition, magnetic phase transition temperatures, structural details, and\nmagnetic properties. Enabled by NEMAD, machine learning models were developed\nto classify materials and predict transition temperatures. Our classification\nmodel achieved an accuracy of 90% in categorizing materials as ferromagnetic\n(FM), antiferromagnetic (AFM), and non-magnetic (NM). The regression models\npredict Curie (N\\'eel) temperature with a coefficient of determination (R2) of\n0.86 (0.85) and a mean absolute error (MAE) of 62K (32K). These models\nidentified 62 (19) FM (AFM) candidates with a predicted Curie (N\\'eel)\ntemperature above 500K (100K) from the Materials Project. This work shows the\nfeasibility of combining LLMs for automated data extraction and machine\nlearning models in accelerating the discovery of magnetic materials."
                },
                "authors": [
                    {
                        "name": "Suman Itani"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiadong Zang"
                    }
                ],
                "author_detail": {
                    "name": "Jiadong Zang"
                },
                "author": "Jiadong Zang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]