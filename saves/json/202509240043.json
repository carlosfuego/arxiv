[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v1",
                "updated": "2025-09-22T06:56:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v4",
                "updated": "2025-09-22T06:35:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    35,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v1",
                "updated": "2025-09-19T20:31:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v2",
                "updated": "2025-09-19T14:14:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    14,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15763v1",
                "updated": "2025-09-19T08:47:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T08:47:37Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression"
                },
                "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling."
                },
                "authors": [
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v2",
                "updated": "2025-09-19T06:20:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    20,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference"
                },
                "summary": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15529v1",
                "updated": "2025-09-19T02:27:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T02:27:01Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "title": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB"
                },
                "summary": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads."
                },
                "authors": [
                    {
                        "name": "Mashkhal A. Sidiq"
                    },
                    {
                        "name": "Aras A. Salih"
                    },
                    {
                        "name": "Samrand M. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Samrand M. Hassan"
                },
                "author": "Samrand M. Hassan",
                "arxiv_doi": "10.5121/ijdms.2025.17501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijdms.2025.17501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 4 figures, 1 Table",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15515v1",
                "updated": "2025-09-19T01:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T01:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference"
                },
                "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%."
                },
                "authors": [
                    {
                        "name": "Hantao Yang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v3",
                "updated": "2025-09-18T23:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    34,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazn"
                    },
                    {
                        "name": "V. lvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodrguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Crcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cosso"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Daz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. Garca-Barrena"
                    },
                    {
                        "name": "J. J. Gmez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gmez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervs Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. Lpez-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martn-Albo"
                    },
                    {
                        "name": "G. Martnez-Lema"
                    },
                    {
                        "name": "M. Martnez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Prez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simn"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnel"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usn"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16278v1",
                "updated": "2025-09-18T17:38:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:38:48Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "title": "Language Modeling with Learned Meta-Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling with Learned Meta-Tokens"
                },
                "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization."
                },
                "authors": [
                    {
                        "name": "Alok N. Shah"
                    },
                    {
                        "name": "Khush Gupta"
                    },
                    {
                        "name": "Keshav Ramji"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Chaudhari"
                },
                "author": "Pratik Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14403v1",
                "updated": "2025-09-17T20:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T20:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "title": "Kilovolt-Class $-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt-Class $-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers"
                },
                "summary": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers."
                },
                "authors": [
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Rachel Kahler"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Akhila Mattapalli"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14347v1",
                "updated": "2025-09-17T18:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T18:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI"
                },
                "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."
                },
                "authors": [
                    {
                        "name": "Henri Adasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Blint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preu"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Mller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernstrm"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernstrm"
                },
                "author": "J. Tjernstrm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strmpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.13804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13804v2",
                "updated": "2025-09-22T17:59:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-08-19T13:05:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    5,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding"
                },
                "summary": "How do Large Language Models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluated the best language models (Claude\nSonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from nearly\n700 annotators in 100K+ texts spanning social networks, news and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators,\nperforming much better than average balanced accuracy. Importantly, we find\nthat AI produces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Large Language Models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluated the best language models (Claude\nSonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from nearly\n700 annotators in 100K+ texts spanning social networks, news and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators,\nperforming much better than average balanced accuracy. Importantly, we find\nthat AI produces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities."
                },
                "authors": [
                    {
                        "name": "Maciej Skorski"
                    },
                    {
                        "name": "Alina Landowska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Landowska"
                },
                "author": "Alina Landowska",
                "arxiv_comment": "Appears in UncertaiNLP@EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 62F15, 62P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18094v1",
                "updated": "2025-09-22T17:59:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:59:40Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning"
                },
                "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Zongyang Ma"
                    },
                    {
                        "name": "Junfu Pu"
                    },
                    {
                        "name": "Zhongang Qi"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Ying Shan"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "arxiv_comment": "NeurIPS 2025 Camera Ready. Project Page:\n  https://polyu-chenlab.github.io/unipixel/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18091v1",
                "updated": "2025-09-22T17:59:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    7,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:59:07Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    7,
                    0,
                    265,
                    0
                ],
                "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System"
                },
                "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue."
                },
                "authors": [
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Jiahua Wu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Bingjun Chen"
                    },
                    {
                        "name": "Bangyang Hong"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Cong Fu"
                    },
                    {
                        "name": "Kangle Wu"
                    },
                    {
                        "name": "Yabo Ni"
                    },
                    {
                        "name": "Anxiang Zeng"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    }
                ],
                "author_detail": {
                    "name": "See-Kiong Ng"
                },
                "author": "See-Kiong Ng",
                "arxiv_comment": "OnePiece Technical Report; Applied in Shopee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18083v1",
                "updated": "2025-09-22T17:56:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    56,
                    38,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:56:38Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    56,
                    38,
                    0,
                    265,
                    0
                ],
                "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning"
                },
                "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models."
                },
                "authors": [
                    {
                        "name": "Valentin Lacombe"
                    },
                    {
                        "name": "Valentin Quesnel"
                    },
                    {
                        "name": "Damien Sileo"
                    }
                ],
                "author_detail": {
                    "name": "Damien Sileo"
                },
                "author": "Damien Sileo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18076v1",
                "updated": "2025-09-22T17:55:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    55,
                    14,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:55:14Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    55,
                    14,
                    0,
                    265,
                    0
                ],
                "title": "Improving Large Language Models Function Calling and Interpretability\n  via Guided-Structured Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Models Function Calling and Interpretability\n  via Guided-Structured Templates"
                },
                "summary": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Hy Dang"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Zhuofeng Wu"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Haoming Jiang"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Helen Wang"
                    },
                    {
                        "name": "Huasheng Li"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11940v2",
                "updated": "2025-09-22T17:48:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    48,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-15T13:59:42Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    13,
                    59,
                    42,
                    0,
                    258,
                    0
                ],
                "title": "Neuromorphic Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Intelligence"
                },
                "summary": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which suffer from the Von Neumann bottleneck\nand depend on massive computational and energy resources, neuromorphic systems\nexploit brain-inspired principles of computation to achieve orders of magnitude\ngreater energy efficiency. By drawing on insights from a wide range of\ndisciplines, including artificial intelligence, physics, chemistry, biology,\nneuroscience, cognitive science and materials science, neuromorphic computing\npromises to deliver intelligent systems that are sustainable, transparent, and\nwidely accessible. A central challenge, however, is to identify a unifying\ntheoretical framework capable of bridging these diverse disciplines. We argue\nthat dynamical systems theory provides such a foundation. Rooted in\ndifferential calculus, it offers a principled language for modeling inference,\nlearning, and control in both natural and artificial substrates. Within this\nframework, noise can be harnessed as a resource for learning, while\ndifferential genetic programming enables the discovery of dynamical systems\nthat implement adaptive behaviors. Embracing this perspective paves the way\ntoward emergent neuromorphic intelligence, where intelligent behavior arises\nfrom the dynamics of physical substrates, advancing both the science and\nsustainability of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic computing seeks to replicate the remarkable efficiency,\nflexibility, and adaptability of the human brain in artificial systems. Unlike\nconventional digital approaches, which suffer from the Von Neumann bottleneck\nand depend on massive computational and energy resources, neuromorphic systems\nexploit brain-inspired principles of computation to achieve orders of magnitude\ngreater energy efficiency. By drawing on insights from a wide range of\ndisciplines, including artificial intelligence, physics, chemistry, biology,\nneuroscience, cognitive science and materials science, neuromorphic computing\npromises to deliver intelligent systems that are sustainable, transparent, and\nwidely accessible. A central challenge, however, is to identify a unifying\ntheoretical framework capable of bridging these diverse disciplines. We argue\nthat dynamical systems theory provides such a foundation. Rooted in\ndifferential calculus, it offers a principled language for modeling inference,\nlearning, and control in both natural and artificial substrates. Within this\nframework, noise can be harnessed as a resource for learning, while\ndifferential genetic programming enables the discovery of dynamical systems\nthat implement adaptive behaviors. Embracing this perspective paves the way\ntoward emergent neuromorphic intelligence, where intelligent behavior arises\nfrom the dynamics of physical substrates, advancing both the science and\nsustainability of AI."
                },
                "authors": [
                    {
                        "name": "Marcel van Gerven"
                    }
                ],
                "author_detail": {
                    "name": "Marcel van Gerven"
                },
                "author": "Marcel van Gerven",
                "arxiv_comment": "18 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08283v2",
                "updated": "2025-09-22T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    48,
                    24,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-09T23:13:22Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    23,
                    13,
                    22,
                    0,
                    160,
                    0
                ],
                "title": "Serendipitous Recommendation with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serendipitous Recommendation with Multimodal LLM"
                },
                "summary": "Conventional recommendation systems succeed in identifying relevant content\nbut often fail to provide users with surprising or novel items. Multimodal\nLarge Language Models (MLLMs) possess the world knowledge and multimodal\nunderstanding needed for serendipity, but their integration into\nbillion-item-scale platforms presents significant challenges. In this paper, we\npropose a novel hierarchical framework where fine-tuned MLLMs provide\nhigh-level guidance to conventional recommendation models, steering them\ntowards more serendipitous suggestions. This approach leverages MLLM strengths\nin understanding multimodal content and user interests while retaining the\nefficiency of traditional models for item-level recommendation. This mitigates\nthe complexity of applying MLLMs directly to vast action spaces. We also\ndemonstrate a chain-of-thought strategy enabling MLLMs to discover novel user\ninterests by first understanding video content and then identifying relevant\nyet unexplored interest clusters. Through live experiments within a commercial\nshort-form video platform serving billions of users, we show that our\nMLLM-powered approach significantly improves both recommendation serendipity\nand user satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional recommendation systems succeed in identifying relevant content\nbut often fail to provide users with surprising or novel items. Multimodal\nLarge Language Models (MLLMs) possess the world knowledge and multimodal\nunderstanding needed for serendipity, but their integration into\nbillion-item-scale platforms presents significant challenges. In this paper, we\npropose a novel hierarchical framework where fine-tuned MLLMs provide\nhigh-level guidance to conventional recommendation models, steering them\ntowards more serendipitous suggestions. This approach leverages MLLM strengths\nin understanding multimodal content and user interests while retaining the\nefficiency of traditional models for item-level recommendation. This mitigates\nthe complexity of applying MLLMs directly to vast action spaces. We also\ndemonstrate a chain-of-thought strategy enabling MLLMs to discover novel user\ninterests by first understanding video content and then identifying relevant\nyet unexplored interest clusters. Through live experiments within a commercial\nshort-form video platform serving billions of users, we show that our\nMLLM-powered approach significantly improves both recommendation serendipity\nand user satisfaction."
                },
                "authors": [
                    {
                        "name": "Haoting Wang"
                    },
                    {
                        "name": "Jianling Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Fangjun Yi"
                    },
                    {
                        "name": "Mengyu Fu"
                    },
                    {
                        "name": "Youwei Zhang"
                    },
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Minmin Chen"
                    },
                    {
                        "name": "Ed H. Chi"
                    },
                    {
                        "name": "Lichan Hong"
                    },
                    {
                        "name": "Haokai Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haokai Lu"
                },
                "author": "Haokai Lu",
                "arxiv_comment": "Accepted by 2025 Recsys EARL Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18063v1",
                "updated": "2025-09-22T17:40:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    40,
                    5,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:40:05Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    40,
                    5,
                    0,
                    265,
                    0
                ],
                "title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring\n  Commonsense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring\n  Commonsense Reasoning"
                },
                "summary": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability."
                },
                "authors": [
                    {
                        "name": "Jan-Felix Klein"
                    },
                    {
                        "name": "Lars Ohnemus"
                    }
                ],
                "author_detail": {
                    "name": "Lars Ohnemus"
                },
                "author": "Lars Ohnemus",
                "arxiv_comment": "Work in Progess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18058v1",
                "updated": "2025-09-22T17:30:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:30:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM"
                },
                "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict."
                },
                "authors": [
                    {
                        "name": "Alexander Panfilov"
                    },
                    {
                        "name": "Evgenii Kortukov"
                    },
                    {
                        "name": "Kristina Nikoli"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v1",
                "updated": "2025-09-22T17:30:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve provable limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve provable limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18056v1",
                "updated": "2025-09-22T17:30:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    15,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:30:15Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    15,
                    0,
                    265,
                    0
                ],
                "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs"
                },
                "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1"
                },
                "authors": [
                    {
                        "name": "Yunheng Li"
                    },
                    {
                        "name": "Jing Cheng"
                    },
                    {
                        "name": "Shaoyong Jia"
                    },
                    {
                        "name": "Hangyi Kuang"
                    },
                    {
                        "name": "Shaohui Jiao"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Ming Cheng"
                },
                "author": "Ming-Ming Cheng",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18054v1",
                "updated": "2025-09-22T17:29:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    29,
                    10,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:29:10Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    29,
                    10,
                    0,
                    265,
                    0
                ],
                "title": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for\n  Algorithm Selection in the Facility Layout Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for\n  Algorithm Selection in the Facility Layout Problem"
                },
                "summary": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot."
                },
                "authors": [
                    {
                        "name": "Nikhil N S"
                    },
                    {
                        "name": "Amol Dilip Joshi"
                    },
                    {
                        "name": "Bilal Muhammed"
                    },
                    {
                        "name": "Soban Babu"
                    }
                ],
                "author_detail": {
                    "name": "Soban Babu"
                },
                "arxiv_affiliation": "TCS Research, Tata Consultancy Services Ltd",
                "author": "Soban Babu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18052v1",
                "updated": "2025-09-22T17:27:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    27,
                    29,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:27:29Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    27,
                    29,
                    0,
                    265,
                    0
                ],
                "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM\n  Societies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM\n  Societies"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\""
                },
                "authors": [
                    {
                        "name": "Jiaxu Zhou"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08274v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08274v4",
                "updated": "2025-09-22T17:24:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    24,
                    56,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-09T22:32:51Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    22,
                    32,
                    51,
                    0,
                    160,
                    0
                ],
                "title": "The Impact of Feature Scaling In Machine Learning: Effects on Regression\n  and Classification Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Feature Scaling In Machine Learning: Effects on Regression\n  and Classification Tasks"
                },
                "summary": "This research addresses the critical lack of comprehensive studies on feature\nscaling by systematically evaluating 12 scaling techniques - including several\nless common transformations - across 14 different Machine Learning algorithms\nand 16 datasets for classification and regression tasks. We meticulously\nanalyzed impacts on predictive performance (using metrics such as accuracy,\nMAE, MSE, and $R^2$) and computational costs (training time, inference time,\nand memory usage). Key findings reveal that while ensemble methods (such as\nRandom Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)\ndemonstrate robust performance largely independent of scaling, other widely\nused models such as Logistic Regression, SVMs, TabNet, and MLPs show\nsignificant performance variations highly dependent on the chosen scaler. This\nextensive empirical analysis, with all source code, experimental results, and\nmodel parameters made publicly available to ensure complete transparency and\nreproducibility, offers model-specific crucial guidance to practitioners on the\nneed for an optimal selection of feature scaling techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the critical lack of comprehensive studies on feature\nscaling by systematically evaluating 12 scaling techniques - including several\nless common transformations - across 14 different Machine Learning algorithms\nand 16 datasets for classification and regression tasks. We meticulously\nanalyzed impacts on predictive performance (using metrics such as accuracy,\nMAE, MSE, and $R^2$) and computational costs (training time, inference time,\nand memory usage). Key findings reveal that while ensemble methods (such as\nRandom Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)\ndemonstrate robust performance largely independent of scaling, other widely\nused models such as Logistic Regression, SVMs, TabNet, and MLPs show\nsignificant performance variations highly dependent on the chosen scaler. This\nextensive empirical analysis, with all source code, experimental results, and\nmodel parameters made publicly available to ensure complete transparency and\nreproducibility, offers model-specific crucial guidance to practitioners on the\nneed for an optimal selection of feature scaling techniques."
                },
                "authors": [
                    {
                        "name": "Joo Manoel Herrera Pinheiro"
                    },
                    {
                        "name": "Suzana Vilas Boas de Oliveira"
                    },
                    {
                        "name": "Thiago Henrique Segreto Silva"
                    },
                    {
                        "name": "Pedro Antonio Rabelo Saraiva"
                    },
                    {
                        "name": "Enzo Ferreira de Souza"
                    },
                    {
                        "name": "Ricardo V. Godoy"
                    },
                    {
                        "name": "Leonardo Andr Ambrosio"
                    },
                    {
                        "name": "Marcelo Becker"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Becker"
                },
                "author": "Marcelo Becker",
                "arxiv_comment": "36 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08274v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08274v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09372v2",
                "updated": "2025-09-22T17:22:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    22,
                    18,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-11T11:42:21Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    42,
                    21,
                    3,
                    254,
                    0
                ],
                "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model"
                },
                "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Lingxiao Li"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Zirui Ge"
                    },
                    {
                        "name": "Xinyang Tong"
                    },
                    {
                        "name": "Wenxuan Song"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Pengxu Hou"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Yifan Tang"
                    },
                    {
                        "name": "Wenhui Wang"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Jianyi Liu"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_comment": "28 pages; Project page: https://vla-adapter.github.io/; Github:\n  https://github.com/OpenHelix-Team/VLA-Adapter; HuggingFace:\n  https://huggingface.co/VLA-Adapter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00300v2",
                "updated": "2025-09-22T17:11:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    11,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2024-11-01T01:40:23Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    40,
                    23,
                    4,
                    306,
                    0
                ],
                "title": "Rationale-Guided Retrieval Augmented Generation for Medical Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Guided Retrieval Augmented Generation for Medical Question\n  Answering"
                },
                "summary": "Large language models (LLM) hold significant potential for applications in\nbiomedicine, but they struggle with hallucinations and outdated knowledge.\nWhile retrieval-augmented generation (RAG) is generally employed to address\nthese issues, it also has its own set of challenges: (1) LLMs are vulnerable to\nirrelevant or incorrect context, (2) medical queries are often not\nwell-targeted for helpful information, and (3) retrievers are prone to bias\ntoward the specific source corpus they were trained on. In this study, we\npresent RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the\nreliability of RAG in biomedical contexts. RAG$^2$ incorporates three key\ninnovations: a small filtering model trained on perplexity-based labels of\nrationales, which selectively augments informative snippets of documents while\nfiltering out distractors; LLM-generated rationales as queries to improve the\nutility of retrieved snippets; a structure designed to retrieve snippets evenly\nfrom a comprehensive set of four biomedical corpora, effectively mitigating\nretriever bias. Our experiments demonstrate that RAG$^2$ improves the\nstate-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and\nit outperforms the previous best medical RAG model by up to 5.6\\% across three\nmedical question-answering benchmarks. Our code is available at\nhttps://github.com/dmis-lab/RAG2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) hold significant potential for applications in\nbiomedicine, but they struggle with hallucinations and outdated knowledge.\nWhile retrieval-augmented generation (RAG) is generally employed to address\nthese issues, it also has its own set of challenges: (1) LLMs are vulnerable to\nirrelevant or incorrect context, (2) medical queries are often not\nwell-targeted for helpful information, and (3) retrievers are prone to bias\ntoward the specific source corpus they were trained on. In this study, we\npresent RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the\nreliability of RAG in biomedical contexts. RAG$^2$ incorporates three key\ninnovations: a small filtering model trained on perplexity-based labels of\nrationales, which selectively augments informative snippets of documents while\nfiltering out distractors; LLM-generated rationales as queries to improve the\nutility of retrieved snippets; a structure designed to retrieve snippets evenly\nfrom a comprehensive set of four biomedical corpora, effectively mitigating\nretriever bias. Our experiments demonstrate that RAG$^2$ improves the\nstate-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and\nit outperforms the previous best medical RAG model by up to 5.6\\% across three\nmedical question-answering benchmarks. Our code is available at\nhttps://github.com/dmis-lab/RAG2."
                },
                "authors": [
                    {
                        "name": "Jiwoong Sohn"
                    },
                    {
                        "name": "Yein Park"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Sihyeon Park"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Mujeen Sung"
                    },
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "arxiv_comment": "Accepted to NAACL 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02954v2",
                "updated": "2025-09-22T17:05:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    5,
                    3,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-29T19:54:57Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    19,
                    54,
                    57,
                    6,
                    180,
                    0
                ],
                "title": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of\n  Large Language Models on CFA Level III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of\n  Large Language Models on CFA Level III"
                },
                "summary": "As financial institutions increasingly adopt Large Language Models (LLMs),\nrigorous domain-specific evaluation becomes critical for responsible\ndeployment. This paper presents a comprehensive benchmark evaluating 23\nstate-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -\nthe gold standard for advanced financial reasoning. We assess both\nmultiple-choice questions (MCQs) and essay-style responses using multiple\nprompting strategies including Chain-of-Thought and Self-Discover. Our\nevaluation reveals that leading models demonstrate strong capabilities, with\ncomposite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA\nLevel III. These results, achieved under a revised, stricter essay grading\nmethodology, indicate significant progress in LLM capabilities for high-stakes\nfinancial applications. Our findings provide crucial guidance for practitioners\non model selection and highlight remaining challenges in cost-effective\ndeployment and the need for nuanced interpretation of performance against\nprofessional benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial institutions increasingly adopt Large Language Models (LLMs),\nrigorous domain-specific evaluation becomes critical for responsible\ndeployment. This paper presents a comprehensive benchmark evaluating 23\nstate-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -\nthe gold standard for advanced financial reasoning. We assess both\nmultiple-choice questions (MCQs) and essay-style responses using multiple\nprompting strategies including Chain-of-Thought and Self-Discover. Our\nevaluation reveals that leading models demonstrate strong capabilities, with\ncomposite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA\nLevel III. These results, achieved under a revised, stricter essay grading\nmethodology, indicate significant progress in LLM capabilities for high-stakes\nfinancial applications. Our findings provide crucial guidance for practitioners\non model selection and highlight remaining challenges in cost-effective\ndeployment and the need for nuanced interpretation of performance against\nprofessional benchmarks."
                },
                "authors": [
                    {
                        "name": "Pranam Shetty"
                    },
                    {
                        "name": "Abhisek Upadhayaya"
                    },
                    {
                        "name": "Parth Mitesh Shah"
                    },
                    {
                        "name": "Srikanth Jagabathula"
                    },
                    {
                        "name": "Shilpi Nayak"
                    },
                    {
                        "name": "Anna Joo Fee"
                    }
                ],
                "author_detail": {
                    "name": "Anna Joo Fee"
                },
                "author": "Anna Joo Fee",
                "arxiv_comment": "Accepted at FinLLM @ IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18030v1",
                "updated": "2025-09-22T17:03:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    3,
                    48,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:03:48Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    3,
                    48,
                    0,
                    265,
                    0
                ],
                "title": "RadEval: A framework for radiology text evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadEval: A framework for radiology text evaluation"
                },
                "summary": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration."
                },
                "authors": [
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Javid Abderezaei"
                    },
                    {
                        "name": "Julie Bauml"
                    },
                    {
                        "name": "Roger Boodoo"
                    },
                    {
                        "name": "Fatemeh Haghighi"
                    },
                    {
                        "name": "Ali Ganjizadeh"
                    },
                    {
                        "name": "Eric Brattain"
                    },
                    {
                        "name": "Dave Van Veen"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    },
                    {
                        "name": "David Eyre"
                    },
                    {
                        "name": "Jean-Benoit Delbrouck"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Benoit Delbrouck"
                },
                "author": "Jean-Benoit Delbrouck",
                "arxiv_comment": "Accepted to EMNLP 2025 Demo track - Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15712v2",
                "updated": "2025-09-22T17:02:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    2,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-21T16:22:32Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    22,
                    32,
                    2,
                    141,
                    0
                ],
                "title": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games"
                },
                "summary": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Muyu He"
                    },
                    {
                        "name": "Muhammad Adil Shahid"
                    },
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "In EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18015v1",
                "updated": "2025-09-22T16:54:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    54,
                    23,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:54:23Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    54,
                    23,
                    0,
                    265,
                    0
                ],
                "title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization\n  in Chest Radiographs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization\n  in Chest Radiographs"
                },
                "summary": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use."
                },
                "authors": [
                    {
                        "name": "Advait Gosai"
                    },
                    {
                        "name": "Arun Kavishwar"
                    },
                    {
                        "name": "Stephanie L. McNamara"
                    },
                    {
                        "name": "Soujanya Samineni"
                    },
                    {
                        "name": "Renato Umeton"
                    },
                    {
                        "name": "Alexander Chowdhury"
                    },
                    {
                        "name": "William Lotter"
                    }
                ],
                "author_detail": {
                    "name": "William Lotter"
                },
                "author": "William Lotter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18014v1",
                "updated": "2025-09-22T16:53:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    53,
                    38,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:53:38Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    53,
                    38,
                    0,
                    265,
                    0
                ],
                "title": "Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synth-MIA: A Testbed for Auditing Privacy Leakage in Tabular Data\n  Synthesis"
                },
                "summary": "Tabular Generative Models are often argued to preserve privacy by creating\nsynthetic datasets that resemble training data. However, auditing their\nempirical privacy remains challenging, as commonly used similarity metrics fail\nto effectively characterize privacy risk. Membership Inference Attacks (MIAs)\nhave recently emerged as a method for evaluating privacy leakage in synthetic\ndata, but their practical effectiveness is limited. Numerous attacks exist\nacross different threat models, each with distinct implementations targeting\nvarious sources of privacy leakage, making them difficult to apply\nconsistently. Moreover, no single attack consistently outperforms the others,\nleading to a routine underestimation of privacy risk.\n  To address these issues, we propose a unified, model-agnostic threat\nframework that deploys a collection of attacks to estimate the maximum\nempirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an\nopen-source Python library that streamlines this auditing process through a\nnovel testbed that integrates seamlessly into existing synthetic data\nevaluation pipelines through a Scikit-Learn-like API. Our software implements\n13 attack methods through a Scikit-Learn-like API, designed to enable fast\nsystematic estimation of privacy leakage for practitioners as well as\nfacilitate the development of new attacks and experiments for researchers.\n  We demonstrate our framework's utility in the largest tabular synthesis\nprivacy benchmark to date, revealing that higher synthetic data quality\ncorresponds to greater privacy leakage, that similarity-based privacy metrics\nshow weak correlation with MIA results, and that the differentially private\ngenerator PATEGAN can fail to preserve privacy under such attacks. This\nunderscores the necessity of MIA-based auditing when designing and deploying\nTabular Generative Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular Generative Models are often argued to preserve privacy by creating\nsynthetic datasets that resemble training data. However, auditing their\nempirical privacy remains challenging, as commonly used similarity metrics fail\nto effectively characterize privacy risk. Membership Inference Attacks (MIAs)\nhave recently emerged as a method for evaluating privacy leakage in synthetic\ndata, but their practical effectiveness is limited. Numerous attacks exist\nacross different threat models, each with distinct implementations targeting\nvarious sources of privacy leakage, making them difficult to apply\nconsistently. Moreover, no single attack consistently outperforms the others,\nleading to a routine underestimation of privacy risk.\n  To address these issues, we propose a unified, model-agnostic threat\nframework that deploys a collection of attacks to estimate the maximum\nempirical privacy leakage in synthetic datasets. We introduce Synth-MIA, an\nopen-source Python library that streamlines this auditing process through a\nnovel testbed that integrates seamlessly into existing synthetic data\nevaluation pipelines through a Scikit-Learn-like API. Our software implements\n13 attack methods through a Scikit-Learn-like API, designed to enable fast\nsystematic estimation of privacy leakage for practitioners as well as\nfacilitate the development of new attacks and experiments for researchers.\n  We demonstrate our framework's utility in the largest tabular synthesis\nprivacy benchmark to date, revealing that higher synthetic data quality\ncorresponds to greater privacy leakage, that similarity-based privacy metrics\nshow weak correlation with MIA results, and that the differentially private\ngenerator PATEGAN can fail to preserve privacy under such attacks. This\nunderscores the necessity of MIA-based auditing when designing and deploying\nTabular Generative Models."
                },
                "authors": [
                    {
                        "name": "Joshua Ward"
                    },
                    {
                        "name": "Xiaofeng Lin"
                    },
                    {
                        "name": "Chi-Hua Wang"
                    },
                    {
                        "name": "Guang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guang Cheng"
                },
                "author": "Guang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07287v2",
                "updated": "2025-09-22T16:50:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    50,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-04-09T21:27:54Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    27,
                    54,
                    2,
                    99,
                    0
                ],
                "title": "Hybrid Privilege Escalation and Remote Code Execution Exploit Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Privilege Escalation and Remote Code Execution Exploit Chains"
                },
                "summary": "Research on exploit chains predominantly focuses on sequences with one type\nof exploit, e.g., either escalating privileges on a machine or executing remote\ncode. In networks, hybrid exploit chains are critical because of their linkable\nvulnerabilities. Moreover, developing hybrid exploit chains is challenging\nbecause it requires understanding the diverse and independent dependencies and\noutcomes. We present hybrid chains encompassing privilege escalation (PE) and\nremote code execution (RCE) exploits. These chains are executable and can span\nlarge networks, where numerous potential exploit combinations arise from the\nlarge array of network assets, their hardware, software, configurations, and\nvulnerabilities. The chains are generated by ALFA-Chains, an AI-supported\nframework for the automated discovery of multi-step PE and RCE exploit chains\nin networks across arbitrary environments and segmented networks. Through an\nLLM-based classification, ALFA-Chains describes exploits in Planning Domain\nDescription Language (PDDL). PDDL exploit and network descriptions then use\noff-the-shelf AI planners to find multiple exploit chains. ALFA-Chains finds 12\nunknown chains on an example with a known three-step chain. A red-team exercise\nvalidates the executability with Metasploit. ALFA-Chains is efficient, finding\nan exploit chain in 0.01 seconds in an enterprise network with 83\nvulnerabilities, 20 hosts, and 6 subnets. In addition, it is scalable, it finds\nan exploit chain in an industrial network with 114 vulnerabilities, 200 hosts,\nand 6 subnets in 3.16 seconds. It is comprehensive, finding 13 exploit chains\nin 26.26 seconds in the network. Finally, ALFA-Chains demonstrates flexibility\nacross different exploit sources, ability to generalize across diverse network\ntypes, and robustness in discovering chains under constrained privilege\nassumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on exploit chains predominantly focuses on sequences with one type\nof exploit, e.g., either escalating privileges on a machine or executing remote\ncode. In networks, hybrid exploit chains are critical because of their linkable\nvulnerabilities. Moreover, developing hybrid exploit chains is challenging\nbecause it requires understanding the diverse and independent dependencies and\noutcomes. We present hybrid chains encompassing privilege escalation (PE) and\nremote code execution (RCE) exploits. These chains are executable and can span\nlarge networks, where numerous potential exploit combinations arise from the\nlarge array of network assets, their hardware, software, configurations, and\nvulnerabilities. The chains are generated by ALFA-Chains, an AI-supported\nframework for the automated discovery of multi-step PE and RCE exploit chains\nin networks across arbitrary environments and segmented networks. Through an\nLLM-based classification, ALFA-Chains describes exploits in Planning Domain\nDescription Language (PDDL). PDDL exploit and network descriptions then use\noff-the-shelf AI planners to find multiple exploit chains. ALFA-Chains finds 12\nunknown chains on an example with a known three-step chain. A red-team exercise\nvalidates the executability with Metasploit. ALFA-Chains is efficient, finding\nan exploit chain in 0.01 seconds in an enterprise network with 83\nvulnerabilities, 20 hosts, and 6 subnets. In addition, it is scalable, it finds\nan exploit chain in an industrial network with 114 vulnerabilities, 200 hosts,\nand 6 subnets in 3.16 seconds. It is comprehensive, finding 13 exploit chains\nin 26.26 seconds in the network. Finally, ALFA-Chains demonstrates flexibility\nacross different exploit sources, ability to generalize across diverse network\ntypes, and robustness in discovering chains under constrained privilege\nassumptions."
                },
                "authors": [
                    {
                        "name": "Miguel Tulla"
                    },
                    {
                        "name": "Andrea Vignali"
                    },
                    {
                        "name": "Christian Colon"
                    },
                    {
                        "name": "Giancarlo Sperli"
                    },
                    {
                        "name": "Simon Pietro Romano"
                    },
                    {
                        "name": "Masataro Asai"
                    },
                    {
                        "name": "Una-May O'Reilly"
                    },
                    {
                        "name": "Erik Hemberg"
                    }
                ],
                "author_detail": {
                    "name": "Erik Hemberg"
                },
                "author": "Erik Hemberg",
                "arxiv_comment": "16 pages, 12 Tables, 6 Figures, 7 Listing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18011v1",
                "updated": "2025-09-22T16:49:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    49,
                    49,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:49:49Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    49,
                    49,
                    0,
                    265,
                    0
                ],
                "title": "Robust, Online, and Adaptive Decentralized Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Online, and Adaptive Decentralized Gaussian Processes"
                },
                "summary": "Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for\nmodeling complex signals, but scale cubically with data, assume static targets,\nand are brittle to outliers, limiting their applicability in large-scale\nproblems with dynamic and noisy environments. Recent work introduced\ndecentralized random Fourier feature Gaussian processes (DRFGP), an online and\ndistributed algorithm that casts GPs in an information-filter form, enabling\nexact sequential inference and fully distributed computation without reliance\non a fusion center. In this paper, we extend DRFGP along two key directions:\nfirst, by introducing a robust-filtering update that downweights the impact of\natypical observations; and second, by incorporating a dynamic adaptation\nmechanism that adapts to time-varying functions. The resulting algorithm\nretains the recursive information-filter structure while enhancing stability\nand accuracy. We demonstrate its effectiveness on a large-scale Earth system\napplication, underscoring its potential for in-situ modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for\nmodeling complex signals, but scale cubically with data, assume static targets,\nand are brittle to outliers, limiting their applicability in large-scale\nproblems with dynamic and noisy environments. Recent work introduced\ndecentralized random Fourier feature Gaussian processes (DRFGP), an online and\ndistributed algorithm that casts GPs in an information-filter form, enabling\nexact sequential inference and fully distributed computation without reliance\non a fusion center. In this paper, we extend DRFGP along two key directions:\nfirst, by introducing a robust-filtering update that downweights the impact of\natypical observations; and second, by incorporating a dynamic adaptation\nmechanism that adapts to time-varying functions. The resulting algorithm\nretains the recursive information-filter structure while enhancing stability\nand accuracy. We demonstrate its effectiveness on a large-scale Earth system\napplication, underscoring its potential for in-situ modeling."
                },
                "authors": [
                    {
                        "name": "Fernando Llorente"
                    },
                    {
                        "name": "Daniel Waxman"
                    },
                    {
                        "name": "Sanket Jantre"
                    },
                    {
                        "name": "Nathan M. Urban"
                    },
                    {
                        "name": "Susan E. Minkoff"
                    }
                ],
                "author_detail": {
                    "name": "Susan E. Minkoff"
                },
                "author": "Susan E. Minkoff",
                "arxiv_comment": "Submitted to Icassp 2026 Special Session on \"Bridging Signal\n  Processing and Machine Learning with Gaussian Processes.\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03116v2",
                "updated": "2025-09-22T16:47:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    47,
                    45,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-03T08:19:13Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    19,
                    13,
                    2,
                    246,
                    0
                ],
                "title": "Measuring Scalar Constructs in Social Science with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Scalar Constructs in Social Science with LLMs"
                },
                "summary": "Many constructs that characterize language, like its complexity or\nemotionality, have a naturally continuous semantic structure; a public speech\nis not just \"simple\" or \"complex,\" but exists on a continuum between extremes.\nAlthough large language models (LLMs) are an attractive tool for measuring\nscalar constructs, their idiosyncratic treatment of numerical outputs raises\nquestions of how to best apply them. We address these questions with a\ncomprehensive evaluation of LLM-based approaches to scalar construct\nmeasurement in social science. Using multiple datasets sourced from the\npolitical science literature, we evaluate four approaches: unweighted direct\npointwise scoring, aggregation of pairwise comparisons,\ntoken-probability-weighted pointwise scoring, and finetuning. Our study finds\nthat pairwise comparisons made by LLMs produce better measurements than simply\nprompting the LLM to directly output the scores, which suffers from bunching\naround arbitrary numbers. However, taking the weighted mean over the token\nprobability of scores further improves the measurements over the two previous\napproaches. Finally, finetuning smaller models with as few as 1,000 training\npairs can match or exceed the performance of prompted LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many constructs that characterize language, like its complexity or\nemotionality, have a naturally continuous semantic structure; a public speech\nis not just \"simple\" or \"complex,\" but exists on a continuum between extremes.\nAlthough large language models (LLMs) are an attractive tool for measuring\nscalar constructs, their idiosyncratic treatment of numerical outputs raises\nquestions of how to best apply them. We address these questions with a\ncomprehensive evaluation of LLM-based approaches to scalar construct\nmeasurement in social science. Using multiple datasets sourced from the\npolitical science literature, we evaluate four approaches: unweighted direct\npointwise scoring, aggregation of pairwise comparisons,\ntoken-probability-weighted pointwise scoring, and finetuning. Our study finds\nthat pairwise comparisons made by LLMs produce better measurements than simply\nprompting the LLM to directly output the scores, which suffers from bunching\naround arbitrary numbers. However, taking the weighted mean over the token\nprobability of scores further improves the measurements over the two previous\napproaches. Finally, finetuning smaller models with as few as 1,000 training\npairs can match or exceed the performance of prompted LLMs."
                },
                "authors": [
                    {
                        "name": "Hauke Licht"
                    },
                    {
                        "name": "Rupak Sarkar"
                    },
                    {
                        "name": "Patrick Y. Wu"
                    },
                    {
                        "name": "Pranav Goel"
                    },
                    {
                        "name": "Niklas Stoehr"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Alexander Miserlis Hoyle"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Miserlis Hoyle"
                },
                "author": "Alexander Miserlis Hoyle",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18008v1",
                "updated": "2025-09-22T16:47:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    47,
                    8,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:47:08Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    47,
                    8,
                    0,
                    265,
                    0
                ],
                "title": "Through the Lens of Human-Human Collaboration: A Configurable Research\n  Platform for Exploring Human-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Lens of Human-Human Collaboration: A Configurable Research\n  Platform for Exploring Human-Agent Collaboration"
                },
                "summary": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis."
                },
                "authors": [
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "April Wang"
                    },
                    {
                        "name": "Toby Jia-jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v4",
                "updated": "2025-09-22T16:46:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    46,
                    17,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "MALTA: An Automated CGRA Design Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALTA: An Automated CGRA Design Framework"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MALTA-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MALTA iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MALTA efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MALTA-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MALTA iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MALTA efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "arxiv_comment": "Due to certain confidentiality requirements, this article needs to be\n  withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18005v1",
                "updated": "2025-09-22T16:44:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    44,
                    34,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:44:34Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    44,
                    34,
                    0,
                    265,
                    0
                ],
                "title": "M3ET: Efficient Vision-Language Learning for Robotics based on\n  Multimodal Mamba-Enhanced Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3ET: Efficient Vision-Language Learning for Robotics based on\n  Multimodal Mamba-Enhanced Transformer"
                },
                "summary": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms."
                },
                "authors": [
                    {
                        "name": "Yanxin Zhang"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Zeyi Kang"
                    },
                    {
                        "name": "Zuheng Ming"
                    },
                    {
                        "name": "Kaixing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Kaixing Zhao"
                },
                "arxiv_affiliation": "School of Software Yangtze River Delta Research Institute",
                "author": "Kaixing Zhao",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18761v2",
                "updated": "2025-09-22T16:41:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    41,
                    27,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-24T15:56:22Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    15,
                    56,
                    22,
                    5,
                    144,
                    0
                ],
                "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using\n  a Controlled Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using\n  a Controlled Benchmark"
                },
                "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions."
                },
                "authors": [
                    {
                        "name": "Minglai Yang"
                    },
                    {
                        "name": "Ethan Huang"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "William Wang"
                    },
                    {
                        "name": "Liangming Pan"
                    }
                ],
                "author_detail": {
                    "name": "Liangming Pan"
                },
                "author": "Liangming Pan",
                "arxiv_comment": "19 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17999v1",
                "updated": "2025-09-22T16:39:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:39:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "The Narcissus Hypothesis:Descending to the Rung of Illusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Narcissus Hypothesis:Descending to the Rung of Illusion"
                },
                "summary": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion."
                },
                "authors": [
                    {
                        "name": "Riccardo Cadei"
                    },
                    {
                        "name": "Christian Intern"
                    }
                ],
                "author_detail": {
                    "name": "Christian Intern"
                },
                "author": "Christian Intern",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17998v1",
                "updated": "2025-09-22T16:39:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    12,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:39:12Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    12,
                    0,
                    265,
                    0
                ],
                "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs"
                },
                "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake."
                },
                "authors": [
                    {
                        "name": "Richard Cornelius Suwandi"
                    },
                    {
                        "name": "Feng Yin"
                    },
                    {
                        "name": "Juntao Wang"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Tsung-Hui Chang"
                    },
                    {
                        "name": "Sergios Theodoridis"
                    }
                ],
                "author_detail": {
                    "name": "Sergios Theodoridis"
                },
                "author": "Sergios Theodoridis",
                "arxiv_comment": "Accepted as Poster at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17995v1",
                "updated": "2025-09-22T16:36:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    36,
                    56,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:36:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    36,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Variation in Verification: Understanding Verification Dynamics in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variation in Verification: Understanding Verification Dynamics in Large\n  Language Models"
                },
                "summary": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges."
                },
                "authors": [
                    {
                        "name": "Yefan Zhou"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Janvijay Singh"
                    },
                    {
                        "name": "Jiang Gui"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04183v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04183v4",
                "updated": "2025-09-23T03:53:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    53,
                    5,
                    1,
                    266,
                    0
                ],
                "published": "2024-09-06T10:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding"
                },
                "summary": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04183v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04183v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16531v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16531v4",
                "updated": "2025-09-22T16:30:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    30,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2024-10-21T21:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    45,
                    22,
                    0,
                    295,
                    0
                ],
                "title": "Bayesian scaling laws for in-context learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian scaling laws for in-context learning"
                },
                "summary": "In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a novel Bayesian scaling law for ICL. In experiments\nwith \\mbox{GPT-2} models of different sizes, our scaling law matches existing\nscaling laws in accuracy while also offering interpretable terms for task\npriors, learning efficiency, and per-example probabilities. To illustrate the\nanalytic power that such interpretable scaling laws provide, we report on\ncontrolled synthetic dataset experiments designed to inform real-world studies\nof safety alignment. In our experimental protocol, we use SFT or DPO to\nsuppress an unwanted existing model capability and then use ICL to try to bring\nthat capability back (many-shot jailbreaking). We then study ICL on real-world\ninstruction-tuned LLMs using capabilities benchmarks as well as a new many-shot\njailbreaking dataset. In all cases, Bayesian scaling laws accurately predict\nthe conditions under which ICL will cause suppressed behaviors to reemerge,\nwhich sheds light on the ineffectiveness of post-training at increasing LLM\nsafety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a novel Bayesian scaling law for ICL. In experiments\nwith \\mbox{GPT-2} models of different sizes, our scaling law matches existing\nscaling laws in accuracy while also offering interpretable terms for task\npriors, learning efficiency, and per-example probabilities. To illustrate the\nanalytic power that such interpretable scaling laws provide, we report on\ncontrolled synthetic dataset experiments designed to inform real-world studies\nof safety alignment. In our experimental protocol, we use SFT or DPO to\nsuppress an unwanted existing model capability and then use ICL to try to bring\nthat capability back (many-shot jailbreaking). We then study ICL on real-world\ninstruction-tuned LLMs using capabilities benchmarks as well as a new many-shot\njailbreaking dataset. In all cases, Bayesian scaling laws accurately predict\nthe conditions under which ICL will cause suppressed behaviors to reemerge,\nwhich sheds light on the ineffectiveness of post-training at increasing LLM\nsafety."
                },
                "authors": [
                    {
                        "name": "Aryaman Arora"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "COLM 2025 camera-ready version; 9 pages main text, 39 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16531v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16531v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17970v2",
                "updated": "2025-09-23T02:59:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    2,
                    59,
                    9,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T16:20:29Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    20,
                    29,
                    0,
                    265,
                    0
                ],
                "title": "Joint Memory Frequency and Computing Frequency Scaling for\n  Energy-efficient DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Memory Frequency and Computing Frequency Scaling for\n  Energy-efficient DNN Inference"
                },
                "summary": "Deep neural networks (DNNs) have been widely applied in diverse applications,\nbut the problems of high latency and energy overhead are inevitable on\nresource-constrained devices. To address this challenge, most researchers focus\non the dynamic voltage and frequency scaling (DVFS) technique to balance the\nlatency and energy consumption by changing the computing frequency of\nprocessors. However, the adjustment of memory frequency is usually ignored and\nnot fully utilized to achieve efficient DNN inference, which also plays a\nsignificant role in the inference time and energy consumption. In this paper,\nwe first investigate the impact of joint memory frequency and computing\nfrequency scaling on the inference time and energy consumption with a\nmodel-based and data-driven method. Then by combining with the fitting\nparameters of different DNN models, we give a preliminary analysis for the\nproposed model to see the effects of adjusting memory frequency and computing\nfrequency simultaneously. Finally, simulation results in local inference and\ncooperative inference cases further validate the effectiveness of jointly\nscaling the memory frequency and computing frequency to reduce the energy\nconsumption of devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) have been widely applied in diverse applications,\nbut the problems of high latency and energy overhead are inevitable on\nresource-constrained devices. To address this challenge, most researchers focus\non the dynamic voltage and frequency scaling (DVFS) technique to balance the\nlatency and energy consumption by changing the computing frequency of\nprocessors. However, the adjustment of memory frequency is usually ignored and\nnot fully utilized to achieve efficient DNN inference, which also plays a\nsignificant role in the inference time and energy consumption. In this paper,\nwe first investigate the impact of joint memory frequency and computing\nfrequency scaling on the inference time and energy consumption with a\nmodel-based and data-driven method. Then by combining with the fitting\nparameters of different DNN models, we give a preliminary analysis for the\nproposed model to see the effects of adjusting memory frequency and computing\nfrequency simultaneously. Finally, simulation results in local inference and\ncooperative inference cases further validate the effectiveness of jointly\nscaling the memory frequency and computing frequency to reduce the energy\nconsumption of devices."
                },
                "authors": [
                    {
                        "name": "Yunchu Han"
                    },
                    {
                        "name": "Zhaojun Nan"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Zhisheng Niu"
                    }
                ],
                "author_detail": {
                    "name": "Zhisheng Niu"
                },
                "author": "Zhisheng Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17969v1",
                "updated": "2025-09-22T16:19:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    19,
                    8,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:19:08Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    19,
                    8,
                    0,
                    265,
                    0
                ],
                "title": "The Reverse File System: Towards open cost-effective secure WORM storage\n  devices for logging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reverse File System: Towards open cost-effective secure WORM storage\n  devices for logging"
                },
                "summary": "Write Once Read Many (WORM) properties for storage devices are desirable to\nensure data immutability for applications such as secure logging, regulatory\ncompliance, archival storage, and other types of backup systems. WORM devices\nguarantee that data, once written, cannot be altered or deleted. However,\nimplementing secure and compatible WORM storage remains a challenge.\nTraditional solutions often rely on specialized hardware, which is either\ncostly, closed, or inaccessible to the general public. Distributed approaches,\nwhile promising, introduce additional risks such as denial-of-service\nvulnerabilities and operational complexity. We introduce Socarrat, a novel,\ncost-effective, and local WORM storage solution that leverages a simple\nexternal USB device (specifically, a single-board computer running Linux with\nUSB On-The-Go support). The resulting device can be connected via USB,\nappearing as an ordinary external disk formatted with an ext4 or exFAT file\nsystem, without requiring any specialized software or drivers. By isolating the\nWORM enforcement mechanism in a dedicated USB hardware module, Socarrat\nsignificantly reduces the attack surface and ensures that even privileged\nattackers cannot modify or erase stored data. In addition to the WORM capacity,\nthe system is designed to be tamper-evident, becoming resilient against\nadvanced attacks. This work describes a novel approach, the Reverse File\nSystem, based on inferring the file system operations occurring at higher\nlayers in the host computer where Socarrat is mounted. The paper also describes\nthe current Socarrat prototype, implemented in Go and available as free/libre\nsoftware. Finally, it provides a complete evaluation of the logging performance\non different single-board computers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write Once Read Many (WORM) properties for storage devices are desirable to\nensure data immutability for applications such as secure logging, regulatory\ncompliance, archival storage, and other types of backup systems. WORM devices\nguarantee that data, once written, cannot be altered or deleted. However,\nimplementing secure and compatible WORM storage remains a challenge.\nTraditional solutions often rely on specialized hardware, which is either\ncostly, closed, or inaccessible to the general public. Distributed approaches,\nwhile promising, introduce additional risks such as denial-of-service\nvulnerabilities and operational complexity. We introduce Socarrat, a novel,\ncost-effective, and local WORM storage solution that leverages a simple\nexternal USB device (specifically, a single-board computer running Linux with\nUSB On-The-Go support). The resulting device can be connected via USB,\nappearing as an ordinary external disk formatted with an ext4 or exFAT file\nsystem, without requiring any specialized software or drivers. By isolating the\nWORM enforcement mechanism in a dedicated USB hardware module, Socarrat\nsignificantly reduces the attack surface and ensures that even privileged\nattackers cannot modify or erase stored data. In addition to the WORM capacity,\nthe system is designed to be tamper-evident, becoming resilient against\nadvanced attacks. This work describes a novel approach, the Reverse File\nSystem, based on inferring the file system operations occurring at higher\nlayers in the host computer where Socarrat is mounted. The paper also describes\nthe current Socarrat prototype, implemented in Go and available as free/libre\nsoftware. Finally, it provides a complete evaluation of the logging performance\non different single-board computers."
                },
                "authors": [
                    {
                        "name": "Gorka Guardiola Mzquiz"
                    },
                    {
                        "name": "Juan Gonzlez-Gmez"
                    },
                    {
                        "name": "Enrique Soriano-Salvador"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Soriano-Salvador"
                },
                "author": "Enrique Soriano-Salvador",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17965v1",
                "updated": "2025-09-22T16:18:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    18,
                    5,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:18:05Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    18,
                    5,
                    0,
                    265,
                    0
                ],
                "title": "Benchmarking Humans and Machines on Complex Multilingual Speech\n  Understanding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Humans and Machines on Complex Multilingual Speech\n  Understanding Tasks"
                },
                "summary": "Auditory attention and selective phase-locking are central to human speech\nunderstanding in complex acoustic scenes and cocktail party settings, yet these\ncapabilities in multilingual subjects remain poorly understood. While machine\nunderstanding of natural speech has advanced in recent years, questions persist\nabout comprehension of overlapped and mixed-channel speech. We propose a\nsystematic paradigm for studying humans and machines in speech\nquestion-answering tasks in multilingual settings with clean and mixed-channel\nspeech. For human listeners, selective attention to a target speaker was\nsignificantly better in their native language (L1) than in their second\nlanguage (L2). For machine listening, speech-based large language models (LLMs)\nmatch or exceed human performance in clean, single-speaker conditions but often\nstruggle to selectively attend in two-speaker settings. These results reveal a\nkey divergence: humans rely on attentional cues that are more streamlined in\ntheir native language, whereas LLMs default to parallel information extraction\nwhich exceed human skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditory attention and selective phase-locking are central to human speech\nunderstanding in complex acoustic scenes and cocktail party settings, yet these\ncapabilities in multilingual subjects remain poorly understood. While machine\nunderstanding of natural speech has advanced in recent years, questions persist\nabout comprehension of overlapped and mixed-channel speech. We propose a\nsystematic paradigm for studying humans and machines in speech\nquestion-answering tasks in multilingual settings with clean and mixed-channel\nspeech. For human listeners, selective attention to a target speaker was\nsignificantly better in their native language (L1) than in their second\nlanguage (L2). For machine listening, speech-based large language models (LLMs)\nmatch or exceed human performance in clean, single-speaker conditions but often\nstruggle to selectively attend in two-speaker settings. These results reveal a\nkey divergence: humans rely on attentional cues that are more streamlined in\ntheir native language, whereas LLMs default to parallel information extraction\nwhich exceed human skills."
                },
                "authors": [
                    {
                        "name": "Sai Samrat Kankanala"
                    },
                    {
                        "name": "Ram Chandra"
                    },
                    {
                        "name": "Sriram Ganapathy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Ganapathy"
                },
                "author": "Sriram Ganapathy",
                "arxiv_comment": "5 Pages, 1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17960v1",
                "updated": "2025-09-22T16:15:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    15,
                    53,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:15:53Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    15,
                    53,
                    0,
                    265,
                    0
                ],
                "title": "Everything all at once: On choosing an estimand for multi-component\n  environmental exposures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Everything all at once: On choosing an estimand for multi-component\n  environmental exposures"
                },
                "summary": "Many research questions -- particularly those in environmental health -- do\nnot involve binary exposures. In environmental epidemiology, this includes\nmultivariate exposure mixtures with nondiscrete components. Causal inference\nestimands and estimators to quantify the relationship between an exposure\nmixture and an outcome are relatively few. We propose an approach to quantify a\nrelationship between a shift in the exposure mixture and the outcome -- either\nin the single timepoint or longitudinal setting. The shift in the exposure\nmixture can be defined flexibly in terms of shifting one or more components,\nincluding examining interaction between mixture components, and in terms of\nshifting the same or different amounts across components. The estimand we\ndiscuss has a similar interpretation as a main effect regression coefficient.\nFirst, we focus on choosing a shift in the exposure mixture supported by\nobserved data. We demonstrate how to assess extrapolation and modify the shift\nto minimize reliance on extrapolation. Second, we propose estimating the\nrelationship between the exposure mixture shift and outcome completely\nnonparametrically, using machine learning in model-fitting. This is in contrast\nto other current approaches, which employ parametric modeling for at least some\nrelationships, which we would like to avoid because parametric modeling\nassumptions in complex, nonrandomized settings are tenuous at best. We are\nmotivated by longitudinal data on pesticide exposures among participants in the\nCHAMACOS Maternal Cognition cohort. We examine the relationship between\nlongitudinal exposure to agricultural pesticides and risk of hypertension. We\nprovide step-by-step code to facilitate the easy replication and adaptation of\nthe approaches we use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many research questions -- particularly those in environmental health -- do\nnot involve binary exposures. In environmental epidemiology, this includes\nmultivariate exposure mixtures with nondiscrete components. Causal inference\nestimands and estimators to quantify the relationship between an exposure\nmixture and an outcome are relatively few. We propose an approach to quantify a\nrelationship between a shift in the exposure mixture and the outcome -- either\nin the single timepoint or longitudinal setting. The shift in the exposure\nmixture can be defined flexibly in terms of shifting one or more components,\nincluding examining interaction between mixture components, and in terms of\nshifting the same or different amounts across components. The estimand we\ndiscuss has a similar interpretation as a main effect regression coefficient.\nFirst, we focus on choosing a shift in the exposure mixture supported by\nobserved data. We demonstrate how to assess extrapolation and modify the shift\nto minimize reliance on extrapolation. Second, we propose estimating the\nrelationship between the exposure mixture shift and outcome completely\nnonparametrically, using machine learning in model-fitting. This is in contrast\nto other current approaches, which employ parametric modeling for at least some\nrelationships, which we would like to avoid because parametric modeling\nassumptions in complex, nonrandomized settings are tenuous at best. We are\nmotivated by longitudinal data on pesticide exposures among participants in the\nCHAMACOS Maternal Cognition cohort. We examine the relationship between\nlongitudinal exposure to agricultural pesticides and risk of hypertension. We\nprovide step-by-step code to facilitate the easy replication and adaptation of\nthe approaches we use."
                },
                "authors": [
                    {
                        "name": "Kara E. Rudolph"
                    },
                    {
                        "name": "Shodai Inose"
                    },
                    {
                        "name": "Nicholas Williams"
                    },
                    {
                        "name": "Ivan Diaz"
                    },
                    {
                        "name": "Lucia Calderon"
                    },
                    {
                        "name": "Jacqueline M. Torres"
                    },
                    {
                        "name": "Marianthi-Anna Kioumourtzoglou"
                    }
                ],
                "author_detail": {
                    "name": "Marianthi-Anna Kioumourtzoglou"
                },
                "author": "Marianthi-Anna Kioumourtzoglou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07004v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07004v4",
                "updated": "2025-09-22T16:14:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    14,
                    50,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-11T14:55:09Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    14,
                    55,
                    9,
                    6,
                    131,
                    0
                ],
                "title": "GuidedQuant: Large Language Model Quantization via Exploiting End Loss\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuidedQuant: Large Language Model Quantization via Exploiting End Loss\n  Guidance"
                },
                "summary": "Post-training quantization is a key technique for reducing the memory and\ninference latency of large language models by quantizing weights and\nactivations without requiring retraining. However, existing methods either (1)\nfail to account for the varying importance of hidden features to the end loss\nor, when incorporating end loss, (2) neglect the critical interactions between\nmodel weights. To address these limitations, we propose GuidedQuant, a novel\nquantization approach that integrates gradient information from the end loss\ninto the quantization objective while preserving cross-weight dependencies\nwithin output channels. GuidedQuant consistently boosts the performance of\nstate-of-the-art quantization methods across weight-only scalar, weight-only\nvector, and weight-and-activation quantization. Additionally, we introduce a\nnovel non-uniform scalar quantization algorithm, which is guaranteed to\nmonotonically decrease the quantization objective value, and outperforms\nexisting methods in this category. We release the code at\nhttps://github.com/snu-mllab/GuidedQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization is a key technique for reducing the memory and\ninference latency of large language models by quantizing weights and\nactivations without requiring retraining. However, existing methods either (1)\nfail to account for the varying importance of hidden features to the end loss\nor, when incorporating end loss, (2) neglect the critical interactions between\nmodel weights. To address these limitations, we propose GuidedQuant, a novel\nquantization approach that integrates gradient information from the end loss\ninto the quantization objective while preserving cross-weight dependencies\nwithin output channels. GuidedQuant consistently boosts the performance of\nstate-of-the-art quantization methods across weight-only scalar, weight-only\nvector, and weight-and-activation quantization. Additionally, we introduce a\nnovel non-uniform scalar quantization algorithm, which is guaranteed to\nmonotonically decrease the quantization objective value, and outperforms\nexisting methods in this category. We release the code at\nhttps://github.com/snu-mllab/GuidedQuant."
                },
                "authors": [
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Marwa El Halabi"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Clemens JS Schaefer"
                    },
                    {
                        "name": "Deokjae Lee"
                    },
                    {
                        "name": "Yeonhong Park"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07004v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07004v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17957v1",
                "updated": "2025-09-22T16:13:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    13,
                    6,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:13:06Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    13,
                    6,
                    0,
                    265,
                    0
                ],
                "title": "On the Variational Costs of Changing Our Minds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Variational Costs of Changing Our Minds"
                },
                "summary": "The human mind is capable of extraordinary achievements, yet it often appears\nto work against itself. It actively defends its cherished beliefs even in the\nface of contradictory evidence, conveniently interprets information to conform\nto desired narratives, and selectively searches for or avoids information to\nsuit its various purposes. Despite these behaviours deviating from common\nnormative standards for belief updating, we argue that such 'biases' are not\ninherently cognitive flaws, but rather an adaptive response to the significant\npragmatic and cognitive costs associated with revising one's beliefs. This\npaper introduces a formal framework that aims to model the influence of these\ncosts on our belief updating mechanisms.\n  We treat belief updating as a motivated variational decision, where agents\nweigh the perceived 'utility' of a belief against the informational cost\nrequired to adopt a new belief state, quantified by the Kullback-Leibler\ndivergence from the prior to the variational posterior. We perform\ncomputational experiments to demonstrate that simple instantiations of this\nresource-rational model can be used to qualitatively emulate commonplace human\nbehaviours, including confirmation bias and attitude polarisation. In doing so,\nwe suggest that this framework makes steps toward a more holistic account of\nthe motivated Bayesian mechanics of belief change and provides practical\ninsights for predicting, compensating for, and correcting deviations from\ndesired belief updating processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The human mind is capable of extraordinary achievements, yet it often appears\nto work against itself. It actively defends its cherished beliefs even in the\nface of contradictory evidence, conveniently interprets information to conform\nto desired narratives, and selectively searches for or avoids information to\nsuit its various purposes. Despite these behaviours deviating from common\nnormative standards for belief updating, we argue that such 'biases' are not\ninherently cognitive flaws, but rather an adaptive response to the significant\npragmatic and cognitive costs associated with revising one's beliefs. This\npaper introduces a formal framework that aims to model the influence of these\ncosts on our belief updating mechanisms.\n  We treat belief updating as a motivated variational decision, where agents\nweigh the perceived 'utility' of a belief against the informational cost\nrequired to adopt a new belief state, quantified by the Kullback-Leibler\ndivergence from the prior to the variational posterior. We perform\ncomputational experiments to demonstrate that simple instantiations of this\nresource-rational model can be used to qualitatively emulate commonplace human\nbehaviours, including confirmation bias and attitude polarisation. In doing so,\nwe suggest that this framework makes steps toward a more holistic account of\nthe motivated Bayesian mechanics of belief change and provides practical\ninsights for predicting, compensating for, and correcting deviations from\ndesired belief updating processes."
                },
                "authors": [
                    {
                        "name": "David Hyland"
                    },
                    {
                        "name": "Mahault Albarracin"
                    }
                ],
                "author_detail": {
                    "name": "Mahault Albarracin"
                },
                "author": "Mahault Albarracin",
                "arxiv_comment": "Accepted as a full paper at the 6th International Workshop on Active\n  Inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17951v1",
                "updated": "2025-09-22T16:10:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    10,
                    13,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:10:13Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    10,
                    13,
                    0,
                    265,
                    0
                ],
                "title": "DragOSM: Extract Building Roofs and Footprints from Aerial Images by\n  Aligning Historical Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DragOSM: Extract Building Roofs and Footprints from Aerial Images by\n  Aligning Historical Labels"
                },
                "summary": "Extracting polygonal roofs and footprints from remote sensing images is\ncritical for large-scale urban analysis. Most existing methods rely on\nsegmentation-based models that assume clear semantic boundaries of roofs, but\nthese approaches struggle in off- nadir images, where the roof and footprint\nare significantly displaced, and facade pixels are fused with the roof\nboundary. With the increasing availability of open vector map annotations,\ne.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation\nhas become viable because remote sensing images are georeferenced once\ncaptured. However, these historical labels commonly suffer from significant\npositional discrepancies with new images and only have one annotation (roof or\nfootprint), which fails to describe the correct structures of a building. To\naddress these discrepancies, we first introduce a concept of an alignment\ntoken, which encodes the correction vector to guide the label correction. Based\non this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel\nmodel designed to align dislocated historical labels with roofs and footprints.\nSpecifically, DragOSM formulates the label alignment as an interactive\ndenoising process, modeling the positional discrepancy as a Gaussian\ndistribution. During training, it learns to correct these errors by simulating\nmisalignment with random Gaussian perturbations; during inference, it\niteratively refines the positions of input labels. To validate our method, we\nfurther present a new dataset, Repairing Buildings in OSM (ReBO), comprising\n179,265 buildings with both OpenStreetMap and manually corrected annotations\nacross 5,473 images from 41 cities. Experimental results on ReBO demonstrate\nthe effectiveness of DragOSM. Code, dataset, and trained models are publicly\navailable at https://github.com/likaiucas/DragOSM.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting polygonal roofs and footprints from remote sensing images is\ncritical for large-scale urban analysis. Most existing methods rely on\nsegmentation-based models that assume clear semantic boundaries of roofs, but\nthese approaches struggle in off- nadir images, where the roof and footprint\nare significantly displaced, and facade pixels are fused with the roof\nboundary. With the increasing availability of open vector map annotations,\ne.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation\nhas become viable because remote sensing images are georeferenced once\ncaptured. However, these historical labels commonly suffer from significant\npositional discrepancies with new images and only have one annotation (roof or\nfootprint), which fails to describe the correct structures of a building. To\naddress these discrepancies, we first introduce a concept of an alignment\ntoken, which encodes the correction vector to guide the label correction. Based\non this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel\nmodel designed to align dislocated historical labels with roofs and footprints.\nSpecifically, DragOSM formulates the label alignment as an interactive\ndenoising process, modeling the positional discrepancy as a Gaussian\ndistribution. During training, it learns to correct these errors by simulating\nmisalignment with random Gaussian perturbations; during inference, it\niteratively refines the positions of input labels. To validate our method, we\nfurther present a new dataset, Repairing Buildings in OSM (ReBO), comprising\n179,265 buildings with both OpenStreetMap and manually corrected annotations\nacross 5,473 images from 41 cities. Experimental results on ReBO demonstrate\nthe effectiveness of DragOSM. Code, dataset, and trained models are publicly\navailable at https://github.com/likaiucas/DragOSM.git."
                },
                "authors": [
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Xingxing Weng"
                    },
                    {
                        "name": "Yupeng Deng"
                    },
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Chao Pang"
                    },
                    {
                        "name": "Gui-Song Xia"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "17 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17949v1",
                "updated": "2025-09-22T16:08:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    8,
                    46,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:08:46Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    8,
                    46,
                    0,
                    265,
                    0
                ],
                "title": "Local Projections Bootstrap Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Projections Bootstrap Inference"
                },
                "summary": "Bootstrap procedures for local projections typically rely on assuming that\nthe data generating process (DGP) is a finite order vector autoregression\n(VAR), often taken to be that implied by the local projection at horizon 1.\nAlthough convenient, it is well documented that a VAR can be a poor\napproximation to impulse dynamics at horizons beyond its lag length. In this\npaper we assume instead that the precise form of the parametric model\ngenerating the data is not known. If one is willing to assume that the DGP is\nperhaps an infinite order process, a larger class of models can be accommodated\nand more tailored bootstrap procedures can be constructed. Using the moving\naverage representation of the data, we construct appropriate bootstrap\nprocedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrap procedures for local projections typically rely on assuming that\nthe data generating process (DGP) is a finite order vector autoregression\n(VAR), often taken to be that implied by the local projection at horizon 1.\nAlthough convenient, it is well documented that a VAR can be a poor\napproximation to impulse dynamics at horizons beyond its lag length. In this\npaper we assume instead that the precise form of the parametric model\ngenerating the data is not known. If one is willing to assume that the DGP is\nperhaps an infinite order process, a larger class of models can be accommodated\nand more tailored bootstrap procedures can be constructed. Using the moving\naverage representation of the data, we construct appropriate bootstrap\nprocedures."
                },
                "authors": [
                    {
                        "name": "Mara Dolores Gadea"
                    },
                    {
                        "name": "scar Jord"
                    }
                ],
                "author_detail": {
                    "name": "scar Jord"
                },
                "author": "scar Jord",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17946v1",
                "updated": "2025-09-22T16:07:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    7,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:07:11Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    7,
                    11,
                    0,
                    265,
                    0
                ],
                "title": "HICode: Hierarchical Inductive Coding with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HICode: Hierarchical Inductive Coding with LLMs"
                },
                "summary": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data."
                },
                "authors": [
                    {
                        "name": "Mian Zhong"
                    },
                    {
                        "name": "Pristina Wang"
                    },
                    {
                        "name": "Anjalie Field"
                    }
                ],
                "author_detail": {
                    "name": "Anjalie Field"
                },
                "author": "Anjalie Field",
                "arxiv_comment": "Long paper accepted at EMNLP 2025 main conference, 19 pages, 8\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17938v1",
                "updated": "2025-09-22T15:59:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:59:40Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language\n  Models"
                },
                "summary": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs."
                },
                "authors": [
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Rahul Gupta"
                    },
                    {
                        "name": "Eliot Krzysztof Jones"
                    },
                    {
                        "name": "Nick Winter"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Spyros Matsoukas"
                    }
                ],
                "author_detail": {
                    "name": "Spyros Matsoukas"
                },
                "author": "Spyros Matsoukas",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01259v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01259v3",
                "updated": "2025-09-22T15:57:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    57,
                    13,
                    0,
                    265,
                    0
                ],
                "published": "2023-10-02T14:51:10Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    14,
                    51,
                    10,
                    0,
                    275,
                    0
                ],
                "title": "SINF: Semantic Neural Network Inference with Semantic Subgraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SINF: Semantic Neural Network Inference with Semantic Subgraphs"
                },
                "summary": "This paper proposes Semantic Inference (SINF) that creates semantic subgraphs\nin a Deep Neural Network(DNN) based on a new Discriminative Capability Score\n(DCS) to drastically reduce the DNN computational load with limited performance\nloss.~We evaluate the performance SINF on VGG16, VGG19, and ResNet50 DNNs\ntrained on CIFAR100 and a subset of the ImageNet dataset. Moreover, we compare\nits performance against 6 state-of-the-art pruning approaches. Our results show\nthat (i) on average, SINF reduces the inference time of VGG16, VGG19, and\nResNet50 respectively by up to 29%, 35%, and 15% with only 3.75%, 0.17%, and\n6.75% accuracy loss for CIFAR100 while for ImageNet benchmark, the reduction in\ninference time is 18%, 22%, and 9% for accuracy drop of 3%, 2.5%, and 6%; (ii)\nDCS achieves respectively up to 3.65%, 4.25%, and 2.36% better accuracy with\nVGG16, VGG19, and ResNet50 with respect to existing discriminative scores for\nCIFAR100 and the same for ImageNet is 8.9%, 5.8%, and 5.2% respectively.\nThrough experimental evaluation on Raspberry Pi and NVIDIA Jetson Nano, we show\nSINF is about 51% and 38% more energy efficient and takes about 25% and 17%\nless inference time than the base model for CIFAR100 and ImageNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Semantic Inference (SINF) that creates semantic subgraphs\nin a Deep Neural Network(DNN) based on a new Discriminative Capability Score\n(DCS) to drastically reduce the DNN computational load with limited performance\nloss.~We evaluate the performance SINF on VGG16, VGG19, and ResNet50 DNNs\ntrained on CIFAR100 and a subset of the ImageNet dataset. Moreover, we compare\nits performance against 6 state-of-the-art pruning approaches. Our results show\nthat (i) on average, SINF reduces the inference time of VGG16, VGG19, and\nResNet50 respectively by up to 29%, 35%, and 15% with only 3.75%, 0.17%, and\n6.75% accuracy loss for CIFAR100 while for ImageNet benchmark, the reduction in\ninference time is 18%, 22%, and 9% for accuracy drop of 3%, 2.5%, and 6%; (ii)\nDCS achieves respectively up to 3.65%, 4.25%, and 2.36% better accuracy with\nVGG16, VGG19, and ResNet50 with respect to existing discriminative scores for\nCIFAR100 and the same for ImageNet is 8.9%, 5.8%, and 5.2% respectively.\nThrough experimental evaluation on Raspberry Pi and NVIDIA Jetson Nano, we show\nSINF is about 51% and 38% more energy efficient and takes about 25% and 17%\nless inference time than the base model for CIFAR100 and ImageNet."
                },
                "authors": [
                    {
                        "name": "A. Q. M. Sazzad Sayyed"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "arxiv_comment": "12 pages, 13 figures, conference format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01259v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01259v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15017v2",
                "updated": "2025-09-22T15:55:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    55,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-21T01:55:04Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    55,
                    4,
                    2,
                    141,
                    0
                ],
                "title": "PsyScam: A Benchmark for Psychological Techniques in Real-World Scams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyScam: A Benchmark for Psychological Techniques in Real-World Scams"
                },
                "summary": "Over the years, online scams have grown dramatically, with nearly 50% of\nglobal consumers encountering scam attempts each week. These scams cause not\nonly significant financial losses to individuals and businesses, but also\nlasting psychological trauma, largely due to scammers' strategic employment of\npsychological techniques (PTs) to manipulate victims. Meanwhile, scammers\ncontinually evolve their tactics by leveraging advances in Large Language\nModels (LLMs) to generate diverse scam variants that easily bypass existing\ndefenses.\n  To address this pressing problem, we introduce PsyScam, a benchmark designed\nto systematically capture the PTs employed in real-world scam reports, and\ninvestigate how LLMs can be utilized to generate variants of scams based on the\nPTs and the contexts provided by these scams. Specifically, we collect a wide\nrange of scam reports and ground its annotations of employed PTs in\nwell-established cognitive and psychological theories. We further demonstrate\nLLMs' capabilities in generating through two downstream tasks: scam completion,\nand scam augmentation. Experimental results show that PsyScam presents\nsignificant challenges to existing models in both detecting and generating scam\ncontent based on the PTs used by real-world scammers. Our code and dataset are\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the years, online scams have grown dramatically, with nearly 50% of\nglobal consumers encountering scam attempts each week. These scams cause not\nonly significant financial losses to individuals and businesses, but also\nlasting psychological trauma, largely due to scammers' strategic employment of\npsychological techniques (PTs) to manipulate victims. Meanwhile, scammers\ncontinually evolve their tactics by leveraging advances in Large Language\nModels (LLMs) to generate diverse scam variants that easily bypass existing\ndefenses.\n  To address this pressing problem, we introduce PsyScam, a benchmark designed\nto systematically capture the PTs employed in real-world scam reports, and\ninvestigate how LLMs can be utilized to generate variants of scams based on the\nPTs and the contexts provided by these scams. Specifically, we collect a wide\nrange of scam reports and ground its annotations of employed PTs in\nwell-established cognitive and psychological theories. We further demonstrate\nLLMs' capabilities in generating through two downstream tasks: scam completion,\nand scam augmentation. Experimental results show that PsyScam presents\nsignificant challenges to existing models in both detecting and generating scam\ncontent based on the PTs used by real-world scammers. Our code and dataset are\navailable."
                },
                "authors": [
                    {
                        "name": "Shang Ma"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Zhenkai Liang"
                    },
                    {
                        "name": "Xusheng Xiao"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17932v1",
                "updated": "2025-09-22T15:54:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    54,
                    29,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:54:29Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    54,
                    29,
                    0,
                    265,
                    0
                ],
                "title": "Training-free Truthfulness Detection via Value Vectors in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Truthfulness Detection via Value Vectors in LLMs"
                },
                "summary": "Large language models often generate factually incorrect outputs, motivating\nefforts to detect the truthfulness of their content. Most existing approaches\nrely on training probes over internal activations, but these methods suffer\nfrom scalability and generalization issues. A recent training-free method,\nNoVo, addresses this challenge by exploiting statistical patterns from the\nmodel itself. However, it focuses exclusively on attention mechanisms,\npotentially overlooking the MLP module-a core component of Transformer models\nknown to support factual recall. In this paper, we show that certain value\nvectors within MLP modules exhibit truthfulness-related statistical patterns.\nBuilding on this insight, we propose TruthV, a simple and interpretable\ntraining-free method that detects content truthfulness by leveraging these\nvalue vectors. On the NoVo benchmark, TruthV significantly outperforms both\nNoVo and log-likelihood baselines, demonstrating that MLP modules-despite being\nneglected in prior training-free efforts-encode rich and useful signals for\ntruthfulness detection. These findings offer new insights into how truthfulness\nis internally represented in LLMs and motivate further research on scalable and\ninterpretable truthfulness detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models often generate factually incorrect outputs, motivating\nefforts to detect the truthfulness of their content. Most existing approaches\nrely on training probes over internal activations, but these methods suffer\nfrom scalability and generalization issues. A recent training-free method,\nNoVo, addresses this challenge by exploiting statistical patterns from the\nmodel itself. However, it focuses exclusively on attention mechanisms,\npotentially overlooking the MLP module-a core component of Transformer models\nknown to support factual recall. In this paper, we show that certain value\nvectors within MLP modules exhibit truthfulness-related statistical patterns.\nBuilding on this insight, we propose TruthV, a simple and interpretable\ntraining-free method that detects content truthfulness by leveraging these\nvalue vectors. On the NoVo benchmark, TruthV significantly outperforms both\nNoVo and log-likelihood baselines, demonstrating that MLP modules-despite being\nneglected in prior training-free efforts-encode rich and useful signals for\ntruthfulness detection. These findings offer new insights into how truthfulness\nis internally represented in LLMs and motivate further research on scalable and\ninterpretable truthfulness detection."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01659v2",
                "updated": "2025-09-22T15:50:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    50,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2025-08-03T08:32:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    8,
                    32,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "From Contrast to Commonality: Audio Commonality Captioning for Enhanced\n  Audio-Text Cross-modal Understanding in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Contrast to Commonality: Audio Commonality Captioning for Enhanced\n  Audio-Text Cross-modal Understanding in Multimodal LLMs"
                },
                "summary": "Audio Captioning (AC) plays a pivotal role in enhancing audio-text\ncross-modal understanding during the pretraining and finetuning of Multimodal\nLLMs (MLLMs). To strengthen this alignment, recent works propose Audio\nDifference Captioning (ADC), which takes multiple audio inputs and encourages\nthe model to describe their differences, thereby promoting fine-grained\ndiscrimination. However, despite its effectiveness, ADC introduces a semantic\ngap between input audios-often rich in diverse events-and the brief,\ndifference-focused short caption. This deviation from AC-style task causes a\nmismatch with the pretraining objective, leading to catastrophic forgetting. To\naddress this, we propose Audio Commonality Captioning (ACC), a comparably\nchallenging but gentler alternative that guides the model to capture shared\nsemantics across audio clips rather than detailed differences. Experiments show\nthat ACC not only improves audio-text understanding on captioning benchmarks\nbut also better preserves general capabilities across diverse speech and music\ntasks, confirming its ability to enable more robust cross-modal understanding\nand achieve a better balance between generalization and task-specific\nperformance in MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Captioning (AC) plays a pivotal role in enhancing audio-text\ncross-modal understanding during the pretraining and finetuning of Multimodal\nLLMs (MLLMs). To strengthen this alignment, recent works propose Audio\nDifference Captioning (ADC), which takes multiple audio inputs and encourages\nthe model to describe their differences, thereby promoting fine-grained\ndiscrimination. However, despite its effectiveness, ADC introduces a semantic\ngap between input audios-often rich in diverse events-and the brief,\ndifference-focused short caption. This deviation from AC-style task causes a\nmismatch with the pretraining objective, leading to catastrophic forgetting. To\naddress this, we propose Audio Commonality Captioning (ACC), a comparably\nchallenging but gentler alternative that guides the model to capture shared\nsemantics across audio clips rather than detailed differences. Experiments show\nthat ACC not only improves audio-text understanding on captioning benchmarks\nbut also better preserves general capabilities across diverse speech and music\ntasks, confirming its ability to enable more robust cross-modal understanding\nand achieve a better balance between generalization and task-specific\nperformance in MLLMs."
                },
                "authors": [
                    {
                        "name": "Yuhang Jia"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Yujie Guo"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Shiwan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwan Zhao"
                },
                "author": "Shiwan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09767v2",
                "updated": "2025-09-22T15:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    46,
                    38,
                    0,
                    265,
                    0
                ],
                "published": "2025-03-12T19:10:20Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    19,
                    10,
                    20,
                    2,
                    71,
                    0
                ],
                "title": "Cover Learning for Large-Scale Topology Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cover Learning for Large-Scale Topology Representation"
                },
                "summary": "Classical unsupervised learning methods like clustering and linear\ndimensionality reduction parametrize large-scale geometry when it is discrete\nor linear, while more modern methods from manifold learning find low\ndimensional representation or infer local geometry by constructing a graph on\nthe input data. More recently, topological data analysis popularized the use of\nsimplicial complexes to represent data topology with two main methodologies:\ntopological inference with geometric complexes and large-scale topology\nvisualization with Mapper graphs -- central to these is the nerve construction\nfrom topology, which builds a simplicial complex given a cover of a space by\nsubsets. While successful, these have limitations: geometric complexes scale\npoorly with data size, and Mapper graphs can be hard to tune and only contain\nlow dimensional information. In this paper, we propose to study the problem of\nlearning covers in its own right, and from the perspective of optimization. We\ndescribe a method for learning topologically-faithful covers of geometric\ndatasets, and show that the simplicial complexes thus obtained can outperform\nstandard topological inference approaches in terms of size, and Mapper-type\nalgorithms in terms of representation of large-scale topology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical unsupervised learning methods like clustering and linear\ndimensionality reduction parametrize large-scale geometry when it is discrete\nor linear, while more modern methods from manifold learning find low\ndimensional representation or infer local geometry by constructing a graph on\nthe input data. More recently, topological data analysis popularized the use of\nsimplicial complexes to represent data topology with two main methodologies:\ntopological inference with geometric complexes and large-scale topology\nvisualization with Mapper graphs -- central to these is the nerve construction\nfrom topology, which builds a simplicial complex given a cover of a space by\nsubsets. While successful, these have limitations: geometric complexes scale\npoorly with data size, and Mapper graphs can be hard to tune and only contain\nlow dimensional information. In this paper, we propose to study the problem of\nlearning covers in its own right, and from the perspective of optimization. We\ndescribe a method for learning topologically-faithful covers of geometric\ndatasets, and show that the simplicial complexes thus obtained can outperform\nstandard topological inference approaches in terms of size, and Mapper-type\nalgorithms in terms of representation of large-scale topology."
                },
                "authors": [
                    {
                        "name": "Luis Scoccola"
                    },
                    {
                        "name": "Uzu Lim"
                    },
                    {
                        "name": "Heather A. Harrington"
                    }
                ],
                "author_detail": {
                    "name": "Heather A. Harrington"
                },
                "author": "Heather A. Harrington",
                "arxiv_comment": "29 pages, 19 figures, 5 tables; final version at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17775v3",
                "updated": "2025-09-22T15:43:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    43,
                    4,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-25T02:10:30Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    10,
                    30,
                    1,
                    56,
                    0
                ],
                "title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks"
                },
                "summary": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference, which identifies the\nperspective of spatial expressions. Despite its significance, FoR has received\nlimited attention in AI models that need spatial intelligence. There is a lack\nof dedicated benchmarks and in-depth evaluation of large language models (LLMs)\nin this area. To address this issue, we introduce the Frame of Reference\nEvaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess\nFoR comprehension in LLMs. We evaluate LLMs on answering questions that require\nFoR comprehension and layout generation in text-to-image models using FoREST.\nOur results reveal a notable performance gap across different FoR classes in\nvarious LLMs, affecting their ability to generate accurate layouts for\ntext-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference, which identifies the\nperspective of spatial expressions. Despite its significance, FoR has received\nlimited attention in AI models that need spatial intelligence. There is a lack\nof dedicated benchmarks and in-depth evaluation of large language models (LLMs)\nin this area. To address this issue, we introduce the Frame of Reference\nEvaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess\nFoR comprehension in LLMs. We evaluate LLMs on answering questions that require\nFoR comprehension and layout generation in text-to-image models using FoREST.\nOur results reveal a notable performance gap across different FoR classes in\nvarious LLMs, affecting their ability to generate accurate layouts for\ntext-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Tanawan Premsri"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "arxiv_comment": "10 pages, 3 Figures, 4 Tables, EMNLP-2025 Main (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17917v1",
                "updated": "2025-09-22T15:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    40,
                    31,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:40:31Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    40,
                    31,
                    0,
                    265,
                    0
                ],
                "title": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent"
                },
                "summary": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities."
                },
                "authors": [
                    {
                        "name": "Junyu Lu"
                    },
                    {
                        "name": "Songxin Zhang"
                    },
                    {
                        "name": "Zejian Xie"
                    },
                    {
                        "name": "Zhuoyang Song"
                    },
                    {
                        "name": "Jiaxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Zhang"
                },
                "author": "Jiaxing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17914v1",
                "updated": "2025-09-22T15:39:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    39,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:39:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    39,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "XaaS Containers: Performance-Portable Representation With Source and IR\n  Containers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XaaS Containers: Performance-Portable Representation With Source and IR\n  Containers"
                },
                "summary": "High-performance computing (HPC) systems and cloud data centers are\nconverging, and containers are becoming the default method of portable software\ndeployment. Yet, while containers simplify software management, they face\nsignificant performance challenges in HPC environments as they must sacrifice\nhardware-specific optimizations to achieve portability. Although HPC containers\ncan use runtime hooks to access optimized MPI libraries and GPU devices, they\nare limited by application binary interface (ABI) compatibility and cannot\novercome the effects of early-stage compilation decisions. Acceleration as a\nService (XaaS) proposes a vision of performance-portable containers, where a\ncontainerized application should achieve peak performance across all HPC\nsystems. We present a practical realization of this vision through Source and\nIntermediate Representation (IR) containers, where we delay\nperformance-critical decisions until the target system specification is known.\nWe analyze specialization mechanisms in HPC software and propose a new\nLLM-assisted method for automatic discovery of specializations. By examining\nthe compilation pipeline, we develop a methodology to build containers\noptimized for target architectures at deployment time. Our prototype\ndemonstrates that new XaaS containers combine the convenience of\ncontainerization with the performance benefits of system-specialized builds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-performance computing (HPC) systems and cloud data centers are\nconverging, and containers are becoming the default method of portable software\ndeployment. Yet, while containers simplify software management, they face\nsignificant performance challenges in HPC environments as they must sacrifice\nhardware-specific optimizations to achieve portability. Although HPC containers\ncan use runtime hooks to access optimized MPI libraries and GPU devices, they\nare limited by application binary interface (ABI) compatibility and cannot\novercome the effects of early-stage compilation decisions. Acceleration as a\nService (XaaS) proposes a vision of performance-portable containers, where a\ncontainerized application should achieve peak performance across all HPC\nsystems. We present a practical realization of this vision through Source and\nIntermediate Representation (IR) containers, where we delay\nperformance-critical decisions until the target system specification is known.\nWe analyze specialization mechanisms in HPC software and propose a new\nLLM-assisted method for automatic discovery of specializations. By examining\nthe compilation pipeline, we develop a methodology to build containers\noptimized for target architectures at deployment time. Our prototype\ndemonstrates that new XaaS containers combine the convenience of\ncontainerization with the performance benefits of system-specialized builds."
                },
                "authors": [
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Eiman Alnuaimi"
                    },
                    {
                        "name": "Alok Kamatar"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Alberto Madonna"
                    },
                    {
                        "name": "Todd Gamblin"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1145/3712285.3759868",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759868",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.17914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC'25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10369v2",
                "updated": "2025-09-22T15:39:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    39,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-04-14T16:15:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    15,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning"
                },
                "summary": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yiting Wang"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Bowei Tian"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Sihan Chen"
                    },
                    {
                        "name": "Ankur Srivastava"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17905v2",
                "updated": "2025-09-23T05:27:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    27,
                    9,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T15:30:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Mitigating Strategy-Selection Bias in Reasoning for More Effective\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Strategy-Selection Bias in Reasoning for More Effective\n  Test-Time Scaling"
                },
                "summary": "Test-time scaling (TTS) has been shown to improve the performance of large\nlanguage models (LLMs) by sampling and aggregating diverse reasoning paths.\nHowever, existing research has overlooked a critical issue: selection bias of\nreasoning strategies during scaling. Specifically, when generating reasoning\nprocesses, LLMs tend to follow certain strategies (e.g., algebraic solutions\nfor math problems) while neglecting other valid alternatives (e.g., geometric\nsolutions), resulting in insufficient exploration of the solution space. To\nfurther understand the impact of this bias, we present a theoretical analysis\nthat reveals when it undermines the effectiveness of test-time scaling.\nMotivated by this theoretical insight, we introduce TTS-Uniform, a framework\ndesigned to mitigate the selection bias of reasoning strategies. It (i)\nidentifies potential strategies, (ii) uniformly allocates the sampling budget\nacross them, and (iii) filters out unstable strategies prior to aggregation.\nExperimental results show that TTS-Uniform significantly enhances scaling\neffectiveness across multiple mainstream LLMs and benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) has been shown to improve the performance of large\nlanguage models (LLMs) by sampling and aggregating diverse reasoning paths.\nHowever, existing research has overlooked a critical issue: selection bias of\nreasoning strategies during scaling. Specifically, when generating reasoning\nprocesses, LLMs tend to follow certain strategies (e.g., algebraic solutions\nfor math problems) while neglecting other valid alternatives (e.g., geometric\nsolutions), resulting in insufficient exploration of the solution space. To\nfurther understand the impact of this bias, we present a theoretical analysis\nthat reveals when it undermines the effectiveness of test-time scaling.\nMotivated by this theoretical insight, we introduce TTS-Uniform, a framework\ndesigned to mitigate the selection bias of reasoning strategies. It (i)\nidentifies potential strategies, (ii) uniformly allocates the sampling budget\nacross them, and (iii) filters out unstable strategies prior to aggregation.\nExperimental results show that TTS-Uniform significantly enhances scaling\neffectiveness across multiple mainstream LLMs and benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Zongqian Wu"
                    },
                    {
                        "name": "Baoduo Xu"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Lei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Lei Feng"
                },
                "author": "Lei Feng",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17901v1",
                "updated": "2025-09-22T15:28:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    28,
                    54,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:28:54Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    28,
                    54,
                    0,
                    265,
                    0
                ],
                "title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?"
                },
                "summary": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM."
                },
                "authors": [
                    {
                        "name": "Geewook Kim"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "5 pages, 2 figures, under review. Project page:\n  https://github.com/naver-ai/LLaVA-AV-SSM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17894v2",
                "updated": "2025-09-23T05:20:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    20,
                    5,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T15:25:28Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    25,
                    28,
                    0,
                    265,
                    0
                ],
                "title": "Optimizing Inference in Transformer-Based Models: A Multi-Method\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Inference in Transformer-Based Models: A Multi-Method\n  Benchmark"
                },
                "summary": "Efficient inference is a critical challenge in deep generative modeling,\nparticularly as diffusion models grow in capacity and complexity. While\nincreased complexity often improves accuracy, it raises compute costs, latency,\nand memory requirements. This work investigates techniques such as pruning,\nquantization, knowledge distillation, and simplified attention to reduce\ncomputational overhead without impacting performance. The study also explores\nthe Mixture of Experts (MoE) approach to further enhance efficiency. These\nexperiments provide insights into optimizing inference for the state-of-the-art\nFast Diffusion Transformer (fast-DiT) model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference is a critical challenge in deep generative modeling,\nparticularly as diffusion models grow in capacity and complexity. While\nincreased complexity often improves accuracy, it raises compute costs, latency,\nand memory requirements. This work investigates techniques such as pruning,\nquantization, knowledge distillation, and simplified attention to reduce\ncomputational overhead without impacting performance. The study also explores\nthe Mixture of Experts (MoE) approach to further enhance efficiency. These\nexperiments provide insights into optimizing inference for the state-of-the-art\nFast Diffusion Transformer (fast-DiT) model."
                },
                "authors": [
                    {
                        "name": "Siu Hang Ho"
                    },
                    {
                        "name": "Prasad Ganesan"
                    },
                    {
                        "name": "Nguyen Duong"
                    },
                    {
                        "name": "Daniel Schlabig"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Schlabig"
                },
                "author": "Daniel Schlabig",
                "arxiv_comment": "6 pages, 4 figures. Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07635v3",
                "updated": "2025-09-22T15:22:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    22,
                    36,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-12T15:05:46Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    5,
                    46,
                    0,
                    132,
                    0
                ],
                "title": "Interpreting Graph Inference with Skyline Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Graph Inference with Skyline Explanations"
                },
                "summary": "Inference queries have been routinely issued to graph machine learning models\nsuch as graph neural networks (GNNs) for various network analytical tasks.\nNevertheless, GNN outputs are often hard to interpret comprehensively. Existing\nmethods typically conform to individual pre-defined explainability measures\n(such as fidelity), which often leads to biased, ``one-side'' interpretations.\nThis paper introduces skyline explanation, a new paradigm that interprets GNN\noutputs by simultaneously optimizing multiple explainability measures of users'\ninterests. (1) We propose skyline explanations as a Pareto set of explanatory\nsubgraphs that dominate others over multiple explanatory measures. We formulate\nskyline explanation as a multi-criteria optimization problem, and establish its\nhardness results. (2) We design efficient algorithms with an onion-peeling\napproach, which strategically prioritizes nodes and removes unpromising edges\nto incrementally assemble skyline explanations. (3) We also develop an\nalgorithm to diversify the skyline explanations to enrich the comprehensive\ninterpretation. (4) We introduce efficient parallel algorithms with\nload-balancing strategies to scale skyline explanation for large-scale\nGNN-based inference. Using real-world and synthetic graphs, we experimentally\nverify our algorithms' effectiveness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference queries have been routinely issued to graph machine learning models\nsuch as graph neural networks (GNNs) for various network analytical tasks.\nNevertheless, GNN outputs are often hard to interpret comprehensively. Existing\nmethods typically conform to individual pre-defined explainability measures\n(such as fidelity), which often leads to biased, ``one-side'' interpretations.\nThis paper introduces skyline explanation, a new paradigm that interprets GNN\noutputs by simultaneously optimizing multiple explainability measures of users'\ninterests. (1) We propose skyline explanations as a Pareto set of explanatory\nsubgraphs that dominate others over multiple explanatory measures. We formulate\nskyline explanation as a multi-criteria optimization problem, and establish its\nhardness results. (2) We design efficient algorithms with an onion-peeling\napproach, which strategically prioritizes nodes and removes unpromising edges\nto incrementally assemble skyline explanations. (3) We also develop an\nalgorithm to diversify the skyline explanations to enrich the comprehensive\ninterpretation. (4) We introduce efficient parallel algorithms with\nload-balancing strategies to scale skyline explanation for large-scale\nGNN-based inference. Using real-world and synthetic graphs, we experimentally\nverify our algorithms' effectiveness and scalability."
                },
                "authors": [
                    {
                        "name": "Dazhuo Qiu"
                    },
                    {
                        "name": "Haolai Che"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Yinghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wu"
                },
                "author": "Yinghui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17885v1",
                "updated": "2025-09-22T15:18:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    18,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:18:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    18,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Confidence-gated training for efficient early-exit neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-gated training for efficient early-exit neural networks"
                },
                "summary": "Early-exit neural networks reduce inference cost by enabling confident\npredictions at intermediate layers. However, joint training often leads to\ngradient interference, with deeper classifiers dominating optimization. We\npropose Confidence-Gated Training (CGT), a paradigm that conditionally\npropagates gradients from deeper exits only when preceding exits fail. This\nencourages shallow classifiers to act as primary decision points while\nreserving deeper layers for harder inputs. By aligning training with the\ninference-time policy, CGT mitigates overthinking, improves early-exit\naccuracy, and preserves efficiency. Experiments on the Indian Pines and\nFashion-MNIST benchmarks show that CGT lowers average inference cost while\nimproving overall accuracy, offering a practical solution for deploying deep\nmodels in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-exit neural networks reduce inference cost by enabling confident\npredictions at intermediate layers. However, joint training often leads to\ngradient interference, with deeper classifiers dominating optimization. We\npropose Confidence-Gated Training (CGT), a paradigm that conditionally\npropagates gradients from deeper exits only when preceding exits fail. This\nencourages shallow classifiers to act as primary decision points while\nreserving deeper layers for harder inputs. By aligning training with the\ninference-time policy, CGT mitigates overthinking, improves early-exit\naccuracy, and preserves efficiency. Experiments on the Indian Pines and\nFashion-MNIST benchmarks show that CGT lowers average inference cost while\nimproving overall accuracy, offering a practical solution for deploying deep\nmodels in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Saad Mokssit"
                    },
                    {
                        "name": "Ouassim Karrakchou"
                    },
                    {
                        "name": "Alejandro Mousist"
                    },
                    {
                        "name": "Mounir Ghogho"
                    }
                ],
                "author_detail": {
                    "name": "Mounir Ghogho"
                },
                "author": "Mounir Ghogho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13535v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13535v5",
                "updated": "2025-09-22T15:14:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    14,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2024-05-22T11:11:42Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    11,
                    11,
                    42,
                    2,
                    143,
                    0
                ],
                "title": "Addressing the Inconsistency in Bayesian Deep Learning via Generalized\n  Laplace Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the Inconsistency in Bayesian Deep Learning via Generalized\n  Laplace Approximation"
                },
                "summary": "In recent years, inconsistency in Bayesian deep learning has attracted\nsignificant attention. Tempered or generalized posterior distributions are\nfrequently employed as direct and effective solutions. Nonetheless, the\nunderlying mechanisms and the effectiveness of generalized posteriors remain\nactive research topics. In this work, we interpret posterior tempering as a\ncorrection for model misspecification via adjustments to the joint probability,\nand as a recalibration of priors by reducing aleatoric uncertainty. We also\nintroduce the generalized Laplace approximation, which requires only a simple\nmodification to the Hessian calculation of the regularized loss and provides a\nflexible and scalable framework for high-quality posterior inference. We\nevaluate the proposed method on state-of-the-art neural networks and real-world\ndatasets, demonstrating that the generalized Laplace approximation enhances\npredictive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, inconsistency in Bayesian deep learning has attracted\nsignificant attention. Tempered or generalized posterior distributions are\nfrequently employed as direct and effective solutions. Nonetheless, the\nunderlying mechanisms and the effectiveness of generalized posteriors remain\nactive research topics. In this work, we interpret posterior tempering as a\ncorrection for model misspecification via adjustments to the joint probability,\nand as a recalibration of priors by reducing aleatoric uncertainty. We also\nintroduce the generalized Laplace approximation, which requires only a simple\nmodification to the Hessian calculation of the regularized loss and provides a\nflexible and scalable framework for high-quality posterior inference. We\nevaluate the proposed method on state-of-the-art neural networks and real-world\ndatasets, demonstrating that the generalized Laplace approximation enhances\npredictive performance."
                },
                "authors": [
                    {
                        "name": "Yinsong Chen"
                    },
                    {
                        "name": "Samson S. Yu"
                    },
                    {
                        "name": "Zhong Li"
                    },
                    {
                        "name": "Chee Peng Lim"
                    }
                ],
                "author_detail": {
                    "name": "Chee Peng Lim"
                },
                "author": "Chee Peng Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13535v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13535v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17874v1",
                "updated": "2025-09-22T15:13:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    13,
                    14,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:13:14Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    13,
                    14,
                    0,
                    265,
                    0
                ],
                "title": "Deep Hierarchical Learning with Nested Subspace Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Hierarchical Learning with Nested Subspace Networks"
                },
                "summary": "Large neural networks are typically trained for a fixed computational budget,\ncreating a rigid trade-off between performance and efficiency that is\nill-suited for deployment in resource-constrained or dynamic environments.\nExisting approaches to this problem present a difficult choice: training a\ndiscrete collection of specialist models is computationally prohibitive, while\ndynamic methods like slimmable networks often lack the flexibility to be\napplied to large, pre-trained foundation models. In this work, we propose\nNested Subspace Networks (NSNs), a novel architectural paradigm that enables a\nsingle model to be dynamically and granularly adjusted across a continuous\nspectrum of compute budgets at inference time. The core of our approach is to\nre-parameterize linear layers to satisfy a nested subspace property, such that\nthe function computed at a given rank is a strict subspace of the function at\nany higher rank. We show that this entire hierarchy of models can be optimized\njointly via an uncertainty-aware objective that learns to balance the\ncontributions of different ranks based on their intrinsic difficulty. We\ndemonstrate empirically that NSNs can be surgically applied to pre-trained LLMs\nand unlock a smooth and predictable compute-performance frontier. For example,\na single NSN-adapted model can achieve a 50% reduction in inference FLOPs with\nonly a 5 percentage point loss in accuracy. Our findings establish NSNs as a\npowerful framework for creating the next generation of adaptive foundation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large neural networks are typically trained for a fixed computational budget,\ncreating a rigid trade-off between performance and efficiency that is\nill-suited for deployment in resource-constrained or dynamic environments.\nExisting approaches to this problem present a difficult choice: training a\ndiscrete collection of specialist models is computationally prohibitive, while\ndynamic methods like slimmable networks often lack the flexibility to be\napplied to large, pre-trained foundation models. In this work, we propose\nNested Subspace Networks (NSNs), a novel architectural paradigm that enables a\nsingle model to be dynamically and granularly adjusted across a continuous\nspectrum of compute budgets at inference time. The core of our approach is to\nre-parameterize linear layers to satisfy a nested subspace property, such that\nthe function computed at a given rank is a strict subspace of the function at\nany higher rank. We show that this entire hierarchy of models can be optimized\njointly via an uncertainty-aware objective that learns to balance the\ncontributions of different ranks based on their intrinsic difficulty. We\ndemonstrate empirically that NSNs can be surgically applied to pre-trained LLMs\nand unlock a smooth and predictable compute-performance frontier. For example,\na single NSN-adapted model can achieve a 50% reduction in inference FLOPs with\nonly a 5 percentage point loss in accuracy. Our findings establish NSNs as a\npowerful framework for creating the next generation of adaptive foundation\nmodels."
                },
                "authors": [
                    {
                        "name": "Paulius Rauba"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17866v1",
                "updated": "2025-09-22T15:03:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    3,
                    36,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:03:36Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    3,
                    36,
                    0,
                    265,
                    0
                ],
                "title": "Understanding Post-Training Structural Changes in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Post-Training Structural Changes in Large Language Models"
                },
                "summary": "Post-training fundamentally alters the behavior of large language models\n(LLMs), yet its impact on the internal parameter space remains poorly\nunderstood. In this work, we conduct a systematic singular value decomposition\n(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two\nwidely adopted post-training methods: instruction tuning and\nlong-chain-of-thought (Long-CoT) distillation. Our analysis reveals two\nconsistent and unexpected structural changes:(1) a near-uniform geometric\nscaling of singular values across layers, which theoretically modulates\nattention scores; and (2) highly consistent orthogonal transformations are\napplied to the left and right singular vectors of each matrix. Disrupting this\northogonal consistency leads to catastrophic performance degradation. Based on\nthese findings, we propose a simple yet effective framework that interprets\npost-training as a reparameterization of fixed subspaces in the pretrained\nparameter space. Further experiments reveal that singular value scaling behaves\nas a secondary effect, analogous to a temperature adjustment, whereas the core\nfunctional transformation lies in the coordinated rotation of singular vectors.\nThese results challenge the prevailing view of the parameter space in large\nmodels as a black box, uncovering the first clear regularities in how\nparameters evolve during training, and providing a new perspective for deeper\ninvestigation into model parameter changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training fundamentally alters the behavior of large language models\n(LLMs), yet its impact on the internal parameter space remains poorly\nunderstood. In this work, we conduct a systematic singular value decomposition\n(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two\nwidely adopted post-training methods: instruction tuning and\nlong-chain-of-thought (Long-CoT) distillation. Our analysis reveals two\nconsistent and unexpected structural changes:(1) a near-uniform geometric\nscaling of singular values across layers, which theoretically modulates\nattention scores; and (2) highly consistent orthogonal transformations are\napplied to the left and right singular vectors of each matrix. Disrupting this\northogonal consistency leads to catastrophic performance degradation. Based on\nthese findings, we propose a simple yet effective framework that interprets\npost-training as a reparameterization of fixed subspaces in the pretrained\nparameter space. Further experiments reveal that singular value scaling behaves\nas a secondary effect, analogous to a temperature adjustment, whereas the core\nfunctional transformation lies in the coordinated rotation of singular vectors.\nThese results challenge the prevailing view of the parameter space in large\nmodels as a black box, uncovering the first clear regularities in how\nparameters evolve during training, and providing a new perspective for deeper\ninvestigation into model parameter changes."
                },
                "authors": [
                    {
                        "name": "Xinyu He"
                    },
                    {
                        "name": "Xianghui Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xianghui Cao"
                },
                "author": "Xianghui Cao",
                "arxiv_comment": "38 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21076v2",
                "updated": "2025-09-22T14:56:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    56,
                    46,
                    0,
                    265,
                    0
                ],
                "published": "2025-04-29T18:00:00Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    0,
                    0,
                    1,
                    119,
                    0
                ],
                "title": "Detecting genuine multipartite entanglement in multi-qubit devices with\n  restricted measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting genuine multipartite entanglement in multi-qubit devices with\n  restricted measurements"
                },
                "summary": "Detecting genuine multipartite entanglement (GME) is a state-characterization\ntask that benchmarks coherence and experimental control in quantum systems.\nExisting GME tests often require joint measurements on many qubits, posing\nexperimental challenges for systems like time-bin encoded qubits and microwave\nphotons from superconducting circuits, where qubit connectivity is limited or\nmeasurement noise grows with the number of jointly measured qubits. Here we\nintroduce versatile GME and $k$-inseparability criteria applicable to any\nstate, which only require measuring $O(n^2)$ out of $2^n$ (at most) $m$-body\nstabilizers of $n$-qubit target graph states, with $m$ upper-bounded by twice\nthe underlying graph's maximum degree. For cluster or ring-graph states, only\nconstant-weight stabilizers are needed. Using semidefinite programming, we\nfurther reduce both the number and weight of required stabilizers. Analytical\nand numerical results show that our criteria are noise-robust and can infer\nstate infidelity from certified $k$-inseparability in microwave photonic graph\nstates generated under realistic conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting genuine multipartite entanglement (GME) is a state-characterization\ntask that benchmarks coherence and experimental control in quantum systems.\nExisting GME tests often require joint measurements on many qubits, posing\nexperimental challenges for systems like time-bin encoded qubits and microwave\nphotons from superconducting circuits, where qubit connectivity is limited or\nmeasurement noise grows with the number of jointly measured qubits. Here we\nintroduce versatile GME and $k$-inseparability criteria applicable to any\nstate, which only require measuring $O(n^2)$ out of $2^n$ (at most) $m$-body\nstabilizers of $n$-qubit target graph states, with $m$ upper-bounded by twice\nthe underlying graph's maximum degree. For cluster or ring-graph states, only\nconstant-weight stabilizers are needed. Using semidefinite programming, we\nfurther reduce both the number and weight of required stabilizers. Analytical\nand numerical results show that our criteria are noise-robust and can infer\nstate infidelity from certified $k$-inseparability in microwave photonic graph\nstates generated under realistic conditions."
                },
                "authors": [
                    {
                        "name": "Nicky Kai Hong Li"
                    },
                    {
                        "name": "Xi Dai"
                    },
                    {
                        "name": "Manuel H. Muoz-Arias"
                    },
                    {
                        "name": "Kevin Reuer"
                    },
                    {
                        "name": "Marcus Huber"
                    },
                    {
                        "name": "Nicolai Friis"
                    }
                ],
                "author_detail": {
                    "name": "Nicolai Friis"
                },
                "author": "Nicolai Friis",
                "arxiv_comment": "8+15 pages, 9 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17861v1",
                "updated": "2025-09-22T14:56:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    56,
                    1,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:56:01Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    56,
                    1,
                    0,
                    265,
                    0
                ],
                "title": "Ion-scale Turbulence and Energy Cascade Rate in the Solar Corona and\n  Inner Heliosphere",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ion-scale Turbulence and Energy Cascade Rate in the Solar Corona and\n  Inner Heliosphere"
                },
                "summary": "Plasma turbulence cascading from MHD to kinetic scales in the heliospheric\nplasma is believed to play a key role in coronal heating and fast solar wind\nacceleration, but the properties of the turbulence remain poorly constrained by\nobservations. Here we compare the ion-scale density fluctuation levels inferred\nfrom the properties of solar radio bursts with the magnetic field fluctuation\nlevels obtained through in-situ measurements in the inner heliosphere. We find\nthat the observed magnetic and density fluctuation amplitudes are consistent\nwith excitation by kinetic Alfv\\'en waves and/or KAW structures over broad\nrange of distances from the Sun. We then use the radio diagnostics and the KAW\nscenario to deduce the radial variation of magnetic fluctuation amplitudes in\nregions close to the Sun where in-situ measurements cannot be obtained.\nFurther, we calculate the energy cascade rate (plasma heating rate) profile\nover a region that extends from the low corona ($\\sim 0.1$~R$_\\odot$) into the\nheliosphere (out to $\\sim 1$~au), and compare it to the energy deposition rate\nrequired to drive the solar wind. The cascade rate agrees with the available\nin-situ measurements and also provides predictions closer than $\\sim\n10$~R$_\\odot$ where in-situ approaches are not available. The results provide\nunique diagnostics of the ion-scale plasma turbulence amplitude and energy\ncascade rate spanning over three orders of magnitude in solar distance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma turbulence cascading from MHD to kinetic scales in the heliospheric\nplasma is believed to play a key role in coronal heating and fast solar wind\nacceleration, but the properties of the turbulence remain poorly constrained by\nobservations. Here we compare the ion-scale density fluctuation levels inferred\nfrom the properties of solar radio bursts with the magnetic field fluctuation\nlevels obtained through in-situ measurements in the inner heliosphere. We find\nthat the observed magnetic and density fluctuation amplitudes are consistent\nwith excitation by kinetic Alfv\\'en waves and/or KAW structures over broad\nrange of distances from the Sun. We then use the radio diagnostics and the KAW\nscenario to deduce the radial variation of magnetic fluctuation amplitudes in\nregions close to the Sun where in-situ measurements cannot be obtained.\nFurther, we calculate the energy cascade rate (plasma heating rate) profile\nover a region that extends from the low corona ($\\sim 0.1$~R$_\\odot$) into the\nheliosphere (out to $\\sim 1$~au), and compare it to the energy deposition rate\nrequired to drive the solar wind. The cascade rate agrees with the available\nin-situ measurements and also provides predictions closer than $\\sim\n10$~R$_\\odot$ where in-situ approaches are not available. The results provide\nunique diagnostics of the ion-scale plasma turbulence amplitude and energy\ncascade rate spanning over three orders of magnitude in solar distance."
                },
                "authors": [
                    {
                        "name": "Eduard P. Kontar"
                    },
                    {
                        "name": "A. Gordon Emslie"
                    },
                    {
                        "name": "Daniel L. Clarkson"
                    },
                    {
                        "name": "Alexander Pitna"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Pitna"
                },
                "author": "Alexander Pitna",
                "arxiv_comment": "11 pages, 4 figures, to be published in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20608v2",
                "updated": "2025-09-22T14:54:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    54,
                    39,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-25T17:00:05Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    5,
                    2,
                    176,
                    0
                ],
                "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base"
                },
                "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery."
                },
                "authors": [
                    {
                        "name": "Barry Smith"
                    },
                    {
                        "name": "Junchao Zhang"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Lois Curfman McInnes"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Archit Vasan"
                    },
                    {
                        "name": "Satish Balay"
                    },
                    {
                        "name": "Toby Isaac"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "arxiv_journal_ref": "54th International Conference on Parallel Processing Companion\n  (ICPP Companion '25), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07809v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07809v4",
                "updated": "2025-09-23T05:59:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    59,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-11T09:49:01Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
                },
                "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on teacher models for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens CoT steps to expand the space in a controlled way. The\nframework enables LLMs to stably learn from initially unsolved hard problems\nunder sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen,\nDeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve\npreviously unsolved problems, improves reasoning capability without external\nCoT supervision, and is compatible with various RL fine-tuning methods. We\nrelease the source code to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on teacher models for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens CoT steps to expand the space in a controlled way. The\nframework enables LLMs to stably learn from initially unsolved hard problems\nunder sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen,\nDeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve\npreviously unsolved problems, improves reasoning capability without external\nCoT supervision, and is compatible with various RL fine-tuning methods. We\nrelease the source code to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Taozhi Chen"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "XiaoLong Hu"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07809v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07809v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17858v1",
                "updated": "2025-09-22T14:51:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    51,
                    37,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:51:37Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    51,
                    37,
                    0,
                    265,
                    0
                ],
                "title": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual\n  Coreference Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual\n  Coreference Resolution"
                },
                "summary": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on\nMultilingual Coreference Resolution. This fourth iteration of the shared task\nintroduces a new LLM track alongside the original unconstrained track, features\nreduced development and test sets to lower computational requirements, and\nincludes additional datasets. CorPipe 25 represents a complete reimplementation\nof our previous systems, migrating from TensorFlow to PyTorch. Our system\nsignificantly outperforms all other submissions in both the LLM and\nunconstrained tracks by a substantial margin of 8 percentage points. The source\ncode and trained models are publicly available at\nhttps://github.com/ufal/crac2025-corpipe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on\nMultilingual Coreference Resolution. This fourth iteration of the shared task\nintroduces a new LLM track alongside the original unconstrained track, features\nreduced development and test sets to lower computational requirements, and\nincludes additional datasets. CorPipe 25 represents a complete reimplementation\nof our previous systems, migrating from TensorFlow to PyTorch. Our system\nsignificantly outperforms all other submissions in both the LLM and\nunconstrained tracks by a substantial margin of 8 percentage points. The source\ncode and trained models are publicly available at\nhttps://github.com/ufal/crac2025-corpipe."
                },
                "authors": [
                    {
                        "name": "Milan Straka"
                    }
                ],
                "author_detail": {
                    "name": "Milan Straka"
                },
                "author": "Milan Straka",
                "arxiv_comment": "Accepted to CODI-CRAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17855v1",
                "updated": "2025-09-22T14:49:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    49,
                    8,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:49:08Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    49,
                    8,
                    0,
                    265,
                    0
                ],
                "title": "Make Every Letter Count: Building Dialect Variation Dictionaries from\n  Monolingual Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Every Letter Count: Building Dialect Variation Dictionaries from\n  Monolingual Corpora"
                },
                "summary": "Dialects exhibit a substantial degree of variation due to the lack of a\nstandard orthography. At the same time, the ability of Large Language Models\n(LLMs) to process dialects remains largely understudied. To address this gap,\nwe use Bavarian as a case study and investigate the lexical dialect\nunderstanding capability of LLMs by examining how well they recognize and\ntranslate dialectal terms across different parts-of-speech. To this end, we\nintroduce DiaLemma, a novel annotation framework for creating dialect variation\ndictionaries from monolingual data only, and use it to compile a ground truth\ndataset consisting of 100K human-annotated German-Bavarian word pairs. We\nevaluate how well nine state-of-the-art LLMs can judge Bavarian terms as\ndialect translations, inflected variants, or unrelated forms of a given German\nlemma. Our results show that LLMs perform best on nouns and lexically similar\nword pairs, and struggle most in distinguishing between direct translations and\ninflected variants. Interestingly, providing additional context in the form of\nexample usages improves the translation performance, but reduces their ability\nto recognize dialect variants. This study highlights the limitations of LLMs in\ndealing with orthographic dialect variation and emphasizes the need for future\nwork on adapting LLMs to dialects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialects exhibit a substantial degree of variation due to the lack of a\nstandard orthography. At the same time, the ability of Large Language Models\n(LLMs) to process dialects remains largely understudied. To address this gap,\nwe use Bavarian as a case study and investigate the lexical dialect\nunderstanding capability of LLMs by examining how well they recognize and\ntranslate dialectal terms across different parts-of-speech. To this end, we\nintroduce DiaLemma, a novel annotation framework for creating dialect variation\ndictionaries from monolingual data only, and use it to compile a ground truth\ndataset consisting of 100K human-annotated German-Bavarian word pairs. We\nevaluate how well nine state-of-the-art LLMs can judge Bavarian terms as\ndialect translations, inflected variants, or unrelated forms of a given German\nlemma. Our results show that LLMs perform best on nouns and lexically similar\nword pairs, and struggle most in distinguishing between direct translations and\ninflected variants. Interestingly, providing additional context in the form of\nexample usages improves the translation performance, but reduces their ability\nto recognize dialect variants. This study highlights the limitations of LLMs in\ndealing with orthographic dialect variation and emphasizes the need for future\nwork on adapting LLMs to dialects."
                },
                "authors": [
                    {
                        "name": "Robert Litschko"
                    },
                    {
                        "name": "Verena Blaschke"
                    },
                    {
                        "name": "Diana Burkhardt"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Diego Frassinelli"
                    }
                ],
                "author_detail": {
                    "name": "Diego Frassinelli"
                },
                "author": "Diego Frassinelli",
                "arxiv_comment": "Accepted at EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17850v1",
                "updated": "2025-09-22T14:42:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    42,
                    51,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:42:51Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    42,
                    51,
                    0,
                    265,
                    0
                ],
                "title": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for\n  Autonomous Driving via Conditional Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for\n  Autonomous Driving via Conditional Diffusion Model"
                },
                "summary": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for\nautonomous driving systems to avoid misguided decisions and potential\naccidents. However, achieving reliable predictions in highly dynamic and\ncomplex traffic scenarios remains a significant challenge. One of the key\nimpediments lies in the limited effectiveness of current approaches to capture\nthe multi-modal behaviors of drivers, which leads to predicted trajectories\nthat deviate from actual future motions. To address this issue, we propose\nSocialTraj, a novel trajectory prediction framework integrating social\npsychology principles through social value orientation (SVO). By utilizing\nBayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we\nobtain the critical social context to infer the future interaction trend. To\nensure modal consistency in predicted behaviors, the estimated SVOs of SVs are\nembedded into a conditional denoising diffusion model that aligns generated\ntrajectories with historical driving styles. Additionally, the planned future\ntrajectory of the ego vehicle (EV) is explicitly incorporated to enhance\ninteraction modeling. Extensive experiments on NGSIM and HighD datasets\ndemonstrate that SocialTraj is capable of adapting to highly dynamic and\ninteractive scenarios while generating socially compliant and behaviorally\nconsistent trajectory predictions, outperforming existing baselines. Ablation\nstudies demonstrate that dynamic SVO estimation and explicit ego-planning\ncomponents notably improve prediction accuracy and substantially reduce\ninference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for\nautonomous driving systems to avoid misguided decisions and potential\naccidents. However, achieving reliable predictions in highly dynamic and\ncomplex traffic scenarios remains a significant challenge. One of the key\nimpediments lies in the limited effectiveness of current approaches to capture\nthe multi-modal behaviors of drivers, which leads to predicted trajectories\nthat deviate from actual future motions. To address this issue, we propose\nSocialTraj, a novel trajectory prediction framework integrating social\npsychology principles through social value orientation (SVO). By utilizing\nBayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we\nobtain the critical social context to infer the future interaction trend. To\nensure modal consistency in predicted behaviors, the estimated SVOs of SVs are\nembedded into a conditional denoising diffusion model that aligns generated\ntrajectories with historical driving styles. Additionally, the planned future\ntrajectory of the ego vehicle (EV) is explicitly incorporated to enhance\ninteraction modeling. Extensive experiments on NGSIM and HighD datasets\ndemonstrate that SocialTraj is capable of adapting to highly dynamic and\ninteractive scenarios while generating socially compliant and behaviorally\nconsistent trajectory predictions, outperforming existing baselines. Ablation\nstudies demonstrate that dynamic SVO estimation and explicit ego-planning\ncomponents notably improve prediction accuracy and substantially reduce\ninference time."
                },
                "authors": [
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Zengqi Peng"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14568v2",
                "updated": "2025-09-22T14:37:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    37,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-18T03:06:14Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    3,
                    6,
                    14,
                    3,
                    261,
                    0
                ],
                "title": "Evidential Physics-Informed Neural Networks for Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidential Physics-Informed Neural Networks for Scientific Discovery"
                },
                "summary": "We present the fundamental theory and implementation guidelines underlying\nEvidential Physics-Informed Neural Network (E-PINN) -- a novel class of\nuncertainty-aware PINN. It leverages the marginal distribution loss function of\nevidential deep learning for estimating uncertainty of outputs, and infers\nunknown parameters of the PDE via a learned posterior distribution. Validating\nour model on two illustrative case studies -- the 1D Poisson equation with a\nGaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated\nempirical coverage probabilities that were calibrated significantly better than\nBayesian PINN and Deep Ensemble methods. To demonstrate real-world\napplicability, we also present a brief case study on applying E-PINN to analyze\nclinical glucose-insulin datasets that have featured in medical research on\ndiabetes pathophysiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the fundamental theory and implementation guidelines underlying\nEvidential Physics-Informed Neural Network (E-PINN) -- a novel class of\nuncertainty-aware PINN. It leverages the marginal distribution loss function of\nevidential deep learning for estimating uncertainty of outputs, and infers\nunknown parameters of the PDE via a learned posterior distribution. Validating\nour model on two illustrative case studies -- the 1D Poisson equation with a\nGaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated\nempirical coverage probabilities that were calibrated significantly better than\nBayesian PINN and Deep Ensemble methods. To demonstrate real-world\napplicability, we also present a brief case study on applying E-PINN to analyze\nclinical glucose-insulin datasets that have featured in medical research on\ndiabetes pathophysiology."
                },
                "authors": [
                    {
                        "name": "Hai Siong Tan"
                    },
                    {
                        "name": "Kuancheng Wang"
                    },
                    {
                        "name": "Rafe McBeth"
                    }
                ],
                "author_detail": {
                    "name": "Rafe McBeth"
                },
                "author": "Rafe McBeth",
                "arxiv_comment": "15 pages, 4 figures; minor typos fixed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17832v1",
                "updated": "2025-09-22T14:23:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    23,
                    4,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:23:04Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    23,
                    4,
                    0,
                    265,
                    0
                ],
                "title": "AEAS: Actionable Exploit Assessment System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEAS: Actionable Exploit Assessment System"
                },
                "summary": "Security practitioners face growing challenges in exploit assessment, as\npublic vulnerability repositories are increasingly populated with inconsistent\nand low-quality exploit artifacts. Existing scoring systems, such as CVSS and\nEPSS, offer limited support for this task. They either rely on theoretical\nmetrics or produce opaque probability estimates without assessing whether\nusable exploit code exists. In practice, security teams often resort to manual\ntriage of exploit repositories, which is time-consuming, error-prone, and\ndifficult to scale. We present AEAS, an automated system designed to assess and\nprioritize actionable exploits through static analysis. AEAS analyzes both\nexploit code and associated documentation to extract a structured set of\nfeatures reflecting exploit availability, functionality, and setup complexity.\nIt then computes an actionability score for each exploit and produces ranked\nexploit recommendations. We evaluate AEAS on a dataset of over 5,000\nvulnerabilities derived from 600+ real-world applications frequently\nencountered by red teams. Manual validation and expert review on representative\nsubsets show that AEAS achieves a 100% top-3 success rate in recommending\nfunctional exploits and shows strong alignment with expert-validated rankings.\nThese results demonstrate the effectiveness of AEAS in supporting\nexploit-driven vulnerability prioritization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security practitioners face growing challenges in exploit assessment, as\npublic vulnerability repositories are increasingly populated with inconsistent\nand low-quality exploit artifacts. Existing scoring systems, such as CVSS and\nEPSS, offer limited support for this task. They either rely on theoretical\nmetrics or produce opaque probability estimates without assessing whether\nusable exploit code exists. In practice, security teams often resort to manual\ntriage of exploit repositories, which is time-consuming, error-prone, and\ndifficult to scale. We present AEAS, an automated system designed to assess and\nprioritize actionable exploits through static analysis. AEAS analyzes both\nexploit code and associated documentation to extract a structured set of\nfeatures reflecting exploit availability, functionality, and setup complexity.\nIt then computes an actionability score for each exploit and produces ranked\nexploit recommendations. We evaluate AEAS on a dataset of over 5,000\nvulnerabilities derived from 600+ real-world applications frequently\nencountered by red teams. Manual validation and expert review on representative\nsubsets show that AEAS achieves a 100% top-3 success rate in recommending\nfunctional exploits and shows strong alignment with expert-validated rankings.\nThese results demonstrate the effectiveness of AEAS in supporting\nexploit-driven vulnerability prioritization."
                },
                "authors": [
                    {
                        "name": "Xiangmin Shen"
                    },
                    {
                        "name": "Wenyuan Cheng"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Zhenyuan Li"
                    },
                    {
                        "name": "Yuqiao Gu"
                    },
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Wencheng Zhao"
                    },
                    {
                        "name": "Dawei Sun"
                    },
                    {
                        "name": "Jiashui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiashui Wang"
                },
                "author": "Jiashui Wang",
                "arxiv_comment": "AEAS has been implemented in the planning agent of PentestAgent, our\n  LLM-driven automated penetration testing framework. Check out our repository:\n  https://github.com/nbshenxm/pentest-agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14498v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14498v4",
                "updated": "2025-09-22T14:02:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    2,
                    3,
                    0,
                    265,
                    0
                ],
                "published": "2024-06-20T17:00:34Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    0,
                    34,
                    3,
                    172,
                    0
                ],
                "title": "LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human\n  Activity from IMU Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human\n  Activity from IMU Data"
                },
                "summary": "Wearable systems can recognize activities from IMU data but often fail to\nexplain their underlying causes or contextual significance. To address this\nlimitation, we introduce two large-scale resources: SensorCap, comprising\n35,960 IMU--caption pairs, and OpenSQA, with 199,701 question--answer pairs\ndesigned for causal and explanatory reasoning. OpenSQA includes a curated\ntuning split (Tune-OpenSQA) optimized for scientific accuracy, narrative\nclarity, and diagnostic insight. Leveraging these datasets, we develop LLaSA\n(Large Language and Sensor Assistant), a family of compact sensor-aware\nlanguage models (7B and 13B) that generate interpretable, context-rich\nresponses to open-ended questions grounded in raw IMU data. LLaSA outperforms\ncommercial LLMs, including GPT-3.5 and GPT-4o-mini, on benchmark and real-world\ntasks, demonstrating the effectiveness of domain supervision and model\nalignment for sensor reasoning. Our code repository and datasets can be found\nat https://github.com/BASHLab/LLaSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable systems can recognize activities from IMU data but often fail to\nexplain their underlying causes or contextual significance. To address this\nlimitation, we introduce two large-scale resources: SensorCap, comprising\n35,960 IMU--caption pairs, and OpenSQA, with 199,701 question--answer pairs\ndesigned for causal and explanatory reasoning. OpenSQA includes a curated\ntuning split (Tune-OpenSQA) optimized for scientific accuracy, narrative\nclarity, and diagnostic insight. Leveraging these datasets, we develop LLaSA\n(Large Language and Sensor Assistant), a family of compact sensor-aware\nlanguage models (7B and 13B) that generate interpretable, context-rich\nresponses to open-ended questions grounded in raw IMU data. LLaSA outperforms\ncommercial LLMs, including GPT-3.5 and GPT-4o-mini, on benchmark and real-world\ntasks, demonstrating the effectiveness of domain supervision and model\nalignment for sensor reasoning. Our code repository and datasets can be found\nat https://github.com/BASHLab/LLaSA."
                },
                "authors": [
                    {
                        "name": "Sheikh Asif Imran"
                    },
                    {
                        "name": "Mohammad Nur Hossain Khan"
                    },
                    {
                        "name": "Subrata Biswas"
                    },
                    {
                        "name": "Bashima Islam"
                    }
                ],
                "author_detail": {
                    "name": "Bashima Islam"
                },
                "author": "Bashima Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14498v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14498v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17806v1",
                "updated": "2025-09-22T14:01:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    1,
                    1,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:01:01Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    1,
                    1,
                    0,
                    265,
                    0
                ],
                "title": "Bayesian Nonhomogeneous hidden Markov models to leverage routine in\n  physical activity monitoring with informative wear time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Nonhomogeneous hidden Markov models to leverage routine in\n  physical activity monitoring with informative wear time"
                },
                "summary": "Missing data is among the most prominent challenges in the analysis of\nphysical activity (PA) data collected from wearable devices, with the threat of\nnonignorabile missingness arising when patterns of device wear relate to\nunderlying activity patterns. We offer a rigorous consideration of assumptions\nabout missing data mechanisms in the context of the common modeling paradigm of\nstate space models with a finite, meaningful, set of underlying PA states.\nFocusing in particular on hidden Markov models, we identify inherent\nlimitations in the presence of missing data when covariates are required to\nsatisfy common missing data assumptions. In response to this limitation, we\npropose a Bayesian non-homogeneous state space model that can accommodate\ncovariate dependence in the transitions between latent activity states, which\nin this case relates to whether patients' routine behavior can inform how they\ntransition between PA states and thus support imputation of missing PA data. We\nshow the benefits of the proposed model for missing data imputation and\ninference for relevant PA summaries. Our development advances analytic capacity\nto confront the ubiquitous challenge of missing data when analyzing PA studies\nusing wearables. We illustrate with the analysis of a cohort of adolescent and\nyoung adult (AYA) cancer patients who wore commercial Fitbit devices for\nvarying durations during the course of treatment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing data is among the most prominent challenges in the analysis of\nphysical activity (PA) data collected from wearable devices, with the threat of\nnonignorabile missingness arising when patterns of device wear relate to\nunderlying activity patterns. We offer a rigorous consideration of assumptions\nabout missing data mechanisms in the context of the common modeling paradigm of\nstate space models with a finite, meaningful, set of underlying PA states.\nFocusing in particular on hidden Markov models, we identify inherent\nlimitations in the presence of missing data when covariates are required to\nsatisfy common missing data assumptions. In response to this limitation, we\npropose a Bayesian non-homogeneous state space model that can accommodate\ncovariate dependence in the transitions between latent activity states, which\nin this case relates to whether patients' routine behavior can inform how they\ntransition between PA states and thus support imputation of missing PA data. We\nshow the benefits of the proposed model for missing data imputation and\ninference for relevant PA summaries. Our development advances analytic capacity\nto confront the ubiquitous challenge of missing data when analyzing PA studies\nusing wearables. We illustrate with the analysis of a cohort of adolescent and\nyoung adult (AYA) cancer patients who wore commercial Fitbit devices for\nvarying durations during the course of treatment."
                },
                "authors": [
                    {
                        "name": "Beatrice Cantoni"
                    },
                    {
                        "name": "Savannah V. Rauschendorfer"
                    },
                    {
                        "name": "Michael E. Roth"
                    },
                    {
                        "name": "J. Andrew Livingston"
                    },
                    {
                        "name": "Eugenie S. Kleinerman"
                    },
                    {
                        "name": "Corwin M. Zigler"
                    }
                ],
                "author_detail": {
                    "name": "Corwin M. Zigler"
                },
                "author": "Corwin M. Zigler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17800v1",
                "updated": "2025-09-22T13:55:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    55,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:55:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    55,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Convolutional Neural Network Optimization for Beehive Classification\n  Using Bioacoustic Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Network Optimization for Beehive Classification\n  Using Bioacoustic Signals"
                },
                "summary": "The behavior of honeybees is an important ecological phenomenon not only in\nterms of honey and beeswax production but also due to the proliferation of\nflora and fauna around it. The best way to study this significant phenomenon is\nby non-invasive monitoring of beehives using the sounds produced by various\nbody movements that give out audio signals which can be exploited for various\npredictions related to the objectives mentioned above. This study investigates\nthe application of Convolutional Neural Networks to classify and monitor\ndifferent hive states with the help of joint time and frequency image\nrepresentations such as Spectrogram, Mel-Spectrogram, Smoothed-Spectrogram, and\nCochleagram. Our findings indicate that the Cochleagram outperformed all the\nother representations, achieving an accuracy of 98.31% on unseen data.\nFurthermore, we employed various strategies including pruning, quantization,\nand knowledge distillation to optimize the network and prevent any potential\nissues with model size. With these optimizations, the network size was lowered\nby 91.8% and the inference time was accelerated by 66%, increasing its\nsuitability for real-time applications. Thus our study emphasizes the\nsignificance of using optimization approaches to minimize model size, avoid\ndeployment problems, and expedite inference for real-time application as well\nas the selection of an appropriate time-frequency representation for optimal\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The behavior of honeybees is an important ecological phenomenon not only in\nterms of honey and beeswax production but also due to the proliferation of\nflora and fauna around it. The best way to study this significant phenomenon is\nby non-invasive monitoring of beehives using the sounds produced by various\nbody movements that give out audio signals which can be exploited for various\npredictions related to the objectives mentioned above. This study investigates\nthe application of Convolutional Neural Networks to classify and monitor\ndifferent hive states with the help of joint time and frequency image\nrepresentations such as Spectrogram, Mel-Spectrogram, Smoothed-Spectrogram, and\nCochleagram. Our findings indicate that the Cochleagram outperformed all the\nother representations, achieving an accuracy of 98.31% on unseen data.\nFurthermore, we employed various strategies including pruning, quantization,\nand knowledge distillation to optimize the network and prevent any potential\nissues with model size. With these optimizations, the network size was lowered\nby 91.8% and the inference time was accelerated by 66%, increasing its\nsuitability for real-time applications. Thus our study emphasizes the\nsignificance of using optimization approaches to minimize model size, avoid\ndeployment problems, and expedite inference for real-time application as well\nas the selection of an appropriate time-frequency representation for optimal\nperformance."
                },
                "authors": [
                    {
                        "name": "Harshit"
                    },
                    {
                        "name": "Rahul Jana"
                    },
                    {
                        "name": "Ritesh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ritesh Kumar"
                },
                "author": "Ritesh Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17796v1",
                "updated": "2025-09-22T13:52:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    52,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:52:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    52,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Findings of the Fourth Shared Task on Multilingual Coreference\n  Resolution: Can LLMs Dethrone Traditional Approaches?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the Fourth Shared Task on Multilingual Coreference\n  Resolution: Can LLMs Dethrone Traditional Approaches?"
                },
                "summary": "The paper presents an overview of the fourth edition of the Shared Task on\nMultilingual Coreference Resolution, organized as part of the CODI-CRAC 2025\nworkshop. As in the previous editions, participants were challenged to develop\nsystems that identify mentions and cluster them according to identity\ncoreference.\n  A key innovation of this year's task was the introduction of a dedicated\nLarge Language Model (LLM) track, featuring a simplified plaintext format\ndesigned to be more suitable for LLMs than the original CoNLL-U representation.\n  The task also expanded its coverage with three new datasets in two additional\nlanguages, using version 1.3 of CorefUD - a harmonized multilingual collection\nof 22 datasets in 17 languages.\n  In total, nine systems participated, including four LLM-based approaches (two\nfine-tuned and two using few-shot adaptation). While traditional systems still\nkept the lead, LLMs showed clear potential, suggesting they may soon challenge\nestablished approaches in future editions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper presents an overview of the fourth edition of the Shared Task on\nMultilingual Coreference Resolution, organized as part of the CODI-CRAC 2025\nworkshop. As in the previous editions, participants were challenged to develop\nsystems that identify mentions and cluster them according to identity\ncoreference.\n  A key innovation of this year's task was the introduction of a dedicated\nLarge Language Model (LLM) track, featuring a simplified plaintext format\ndesigned to be more suitable for LLMs than the original CoNLL-U representation.\n  The task also expanded its coverage with three new datasets in two additional\nlanguages, using version 1.3 of CorefUD - a harmonized multilingual collection\nof 22 datasets in 17 languages.\n  In total, nine systems participated, including four LLM-based approaches (two\nfine-tuned and two using few-shot adaptation). While traditional systems still\nkept the lead, LLMs showed clear potential, suggesting they may soon challenge\nestablished approaches in future editions."
                },
                "authors": [
                    {
                        "name": "Michal Novk"
                    },
                    {
                        "name": "Miloslav Konopk"
                    },
                    {
                        "name": "Anna Nedoluzhko"
                    },
                    {
                        "name": "Martin Popel"
                    },
                    {
                        "name": "Ondej Prak"
                    },
                    {
                        "name": "Jakub Sido"
                    },
                    {
                        "name": "Milan Straka"
                    },
                    {
                        "name": "Zdenk abokrtsk"
                    },
                    {
                        "name": "Daniel Zeman"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Zeman"
                },
                "author": "Daniel Zeman",
                "arxiv_comment": "Accepted to CODI-CRAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11405v2",
                "updated": "2025-09-22T13:51:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    51,
                    23,
                    0,
                    265,
                    0
                ],
                "published": "2025-07-15T15:23:53Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    15,
                    23,
                    53,
                    1,
                    196,
                    0
                ],
                "title": "DCR: Quantifying Data Contamination in LLMs Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCR: Quantifying Data Contamination in LLMs Evaluation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data during the training process, inflating performance metrics, and\nundermining genuine generalization assessment. This paper introduces the Data\nContamination Risk (DCR) framework, a lightweight, interpretable pipeline\ndesigned to detect and quantify BDC risk across four granular levels: semantic,\ninformational, data, and label. By synthesizing contamination scores via a\nfuzzy inference system, DCR produces a unified DCR Factor that adjusts raw\naccuracy to reflect contamination-aware performance. Validated on 9 LLMs\n(0.5B-72B) across sentiment analysis, fake news detection, and arithmetic\nreasoning tasks, the DCR framework reliably diagnoses contamination severity\nand with accuracy adjusted using the DCR Factor to within 4% average error\nacross the three benchmarks compared to the uncontaminated baseline.\nEmphasizing computational efficiency and transparency, DCR provides a practical\ntool for integrating contamination assessment into routine evaluations,\nfostering fairer comparisons and enhancing the credibility of LLM benchmarking\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data during the training process, inflating performance metrics, and\nundermining genuine generalization assessment. This paper introduces the Data\nContamination Risk (DCR) framework, a lightweight, interpretable pipeline\ndesigned to detect and quantify BDC risk across four granular levels: semantic,\ninformational, data, and label. By synthesizing contamination scores via a\nfuzzy inference system, DCR produces a unified DCR Factor that adjusts raw\naccuracy to reflect contamination-aware performance. Validated on 9 LLMs\n(0.5B-72B) across sentiment analysis, fake news detection, and arithmetic\nreasoning tasks, the DCR framework reliably diagnoses contamination severity\nand with accuracy adjusted using the DCR Factor to within 4% average error\nacross the three benchmarks compared to the uncontaminated baseline.\nEmphasizing computational efficiency and transparency, DCR provides a practical\ntool for integrating contamination assessment into routine evaluations,\nfostering fairer comparisons and enhancing the credibility of LLM benchmarking\npractices."
                },
                "authors": [
                    {
                        "name": "Cheng Xu"
                    },
                    {
                        "name": "Nan Yan"
                    },
                    {
                        "name": "Shuhao Guan"
                    },
                    {
                        "name": "Changhong Jin"
                    },
                    {
                        "name": "Yuke Mei"
                    },
                    {
                        "name": "Yibing Guo"
                    },
                    {
                        "name": "M-Tahar Kechadi"
                    }
                ],
                "author_detail": {
                    "name": "M-Tahar Kechadi"
                },
                "author": "M-Tahar Kechadi",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17792v1",
                "updated": "2025-09-22T13:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    51,
                    9,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:51:09Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    51,
                    9,
                    0,
                    265,
                    0
                ],
                "title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding"
                },
                "summary": "Real-world images often suffer from spatially diverse degradations such as\nhaze, rain, snow, and low-light, significantly impacting visual quality and\ndownstream vision tasks. Existing all-in-one restoration (AIR) approaches\neither depend on external text prompts or embed hand-crafted architectural\npriors (e.g., frequency heuristics); both impose discrete, brittle assumptions\nthat weaken generalization to unseen or mixed degradations. To address this\nlimitation, we propose to reframe AIR as learned latent prior inference, where\ndegradation-aware representations are automatically inferred from the input\nwithout explicit task cues. Based on latent priors, we formulate AIR as a\nstructured reasoning paradigm: (1) which features to route (adaptive feature\nselection), (2) where to restore (spatial localization), and (3) what to\nrestore (degradation semantics). We design a lightweight decoding module that\nefficiently leverages these latent encoded cues for spatially-adaptive\nrestoration. Extensive experiments across six common degradation tasks, five\ncompound settings, and previously unseen degradations demonstrate that our\nmethod outperforms state-of-the-art (SOTA) approaches, achieving an average\nPSNR improvement of 1.68 dB while being three times more efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world images often suffer from spatially diverse degradations such as\nhaze, rain, snow, and low-light, significantly impacting visual quality and\ndownstream vision tasks. Existing all-in-one restoration (AIR) approaches\neither depend on external text prompts or embed hand-crafted architectural\npriors (e.g., frequency heuristics); both impose discrete, brittle assumptions\nthat weaken generalization to unseen or mixed degradations. To address this\nlimitation, we propose to reframe AIR as learned latent prior inference, where\ndegradation-aware representations are automatically inferred from the input\nwithout explicit task cues. Based on latent priors, we formulate AIR as a\nstructured reasoning paradigm: (1) which features to route (adaptive feature\nselection), (2) where to restore (spatial localization), and (3) what to\nrestore (degradation semantics). We design a lightweight decoding module that\nefficiently leverages these latent encoded cues for spatially-adaptive\nrestoration. Extensive experiments across six common degradation tasks, five\ncompound settings, and previously unseen degradations demonstrate that our\nmethod outperforms state-of-the-art (SOTA) approaches, achieving an average\nPSNR improvement of 1.68 dB while being three times more efficient."
                },
                "authors": [
                    {
                        "name": "S M A Sharif"
                    },
                    {
                        "name": "Abdur Rehman"
                    },
                    {
                        "name": "Fayaz Ali Dharejo"
                    },
                    {
                        "name": "Radu Timofte"
                    },
                    {
                        "name": "Rizwan Ali Naqvi"
                    }
                ],
                "author_detail": {
                    "name": "Rizwan Ali Naqvi"
                },
                "author": "Rizwan Ali Naqvi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17790v1",
                "updated": "2025-09-22T13:50:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    50,
                    28,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:50:28Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    50,
                    28,
                    0,
                    265,
                    0
                ],
                "title": "Conditional Diffusion Models for CT Image Synthesis from CBCT: A\n  Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Diffusion Models for CT Image Synthesis from CBCT: A\n  Systematic Review"
                },
                "summary": "Objective: Cone-beam computed tomography (CBCT) provides a low-dose imaging\nalternative to conventional CT, but suffers from noise, scatter, and artifacts\nthat degrade image quality. Synthetic CT (sCT) aims to translate CBCT to\nhigh-quality CT-like images for improved anatomical accuracy and dosimetric\nprecision. Although deep learning approaches have shown promise, they often\nface limitations in generalizability and detail preservation. Conditional\ndiffusion models (CDMs), with their iterative refinement process, offers a\nnovel solution. This review systematically examines the use of CDMs for\nCBCT-to-sCT synthesis.\n  Methods: A systematic search was conducted in Web of Science, Scopus, and\nGoogle Scholar for studies published between 2013 and 2024. Inclusion criteria\ntargeted works employing conditional diffusion models specifically for sCT\ngeneration. Eleven relevant studies were identified and analyzed to address\nthree questions: (1) What conditional diffusion methods are used? (2) How do\nthey compare to conventional deep learning in accuracy? (3) What are their\nclinical implications?\n  Results: CDMs incorporating anatomical priors and spatial-frequency features\ndemonstrated improved structural preservation and noise robustness.\nEnergy-guided and hybrid latent models enabled enhanced dosimetric accuracy and\npersonalized image synthesis. Across studies, CDMs consistently outperformed\ntraditional deep learning models in noise suppression and artefact reduction,\nespecially in challenging cases like lung imaging and dual-energy CT.\n  Conclusion: Conditional diffusion models show strong potential for\ngeneralized, accurate sCT generation from CBCT. However, clinical adoption\nremains limited. Future work should focus on scalability, real-time inference,\nand integration with multi-modal imaging to enhance clinical relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Cone-beam computed tomography (CBCT) provides a low-dose imaging\nalternative to conventional CT, but suffers from noise, scatter, and artifacts\nthat degrade image quality. Synthetic CT (sCT) aims to translate CBCT to\nhigh-quality CT-like images for improved anatomical accuracy and dosimetric\nprecision. Although deep learning approaches have shown promise, they often\nface limitations in generalizability and detail preservation. Conditional\ndiffusion models (CDMs), with their iterative refinement process, offers a\nnovel solution. This review systematically examines the use of CDMs for\nCBCT-to-sCT synthesis.\n  Methods: A systematic search was conducted in Web of Science, Scopus, and\nGoogle Scholar for studies published between 2013 and 2024. Inclusion criteria\ntargeted works employing conditional diffusion models specifically for sCT\ngeneration. Eleven relevant studies were identified and analyzed to address\nthree questions: (1) What conditional diffusion methods are used? (2) How do\nthey compare to conventional deep learning in accuracy? (3) What are their\nclinical implications?\n  Results: CDMs incorporating anatomical priors and spatial-frequency features\ndemonstrated improved structural preservation and noise robustness.\nEnergy-guided and hybrid latent models enabled enhanced dosimetric accuracy and\npersonalized image synthesis. Across studies, CDMs consistently outperformed\ntraditional deep learning models in noise suppression and artefact reduction,\nespecially in challenging cases like lung imaging and dual-energy CT.\n  Conclusion: Conditional diffusion models show strong potential for\ngeneralized, accurate sCT generation from CBCT. However, clinical adoption\nremains limited. Future work should focus on scalability, real-time inference,\nand integration with multi-modal imaging to enhance clinical relevance."
                },
                "authors": [
                    {
                        "name": "Alzahra Altalib"
                    },
                    {
                        "name": "Chunhui Li"
                    },
                    {
                        "name": "Alessandro Perelli"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Perelli"
                },
                "author": "Alessandro Perelli",
                "arxiv_comment": "36 pages, 8 figures, 3 tables, submitted to Elsevier Computerized\n  Medical Imaging and Graphics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17789v1",
                "updated": "2025-09-22T13:50:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    50,
                    20,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:50:20Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    50,
                    20,
                    0,
                    265,
                    0
                ],
                "title": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for\n  Underwater Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for\n  Underwater Scenes"
                },
                "summary": "Underwater image degradation poses significant challenges for 3D\nreconstruction, where simplified physical models often fail in complex scenes.\nWe propose \\textbf{R-Splatting}, a unified framework that bridges underwater\nimage restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both\nrendering quality and geometric fidelity. Our method integrates multiple\nenhanced views produced by diverse UIR models into a single reconstruction\npipeline. During inference, a lightweight illumination generator samples latent\ncodes to support diverse yet coherent renderings, while a contrastive loss\nensures disentangled and stable illumination representations. Furthermore, we\npropose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models\nopacity as a stochastic function to regularize training. This suppresses abrupt\ngradient responses triggered by illumination variation and mitigates\noverfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF\nand our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong\nbaselines in both rendering quality and geometric accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater image degradation poses significant challenges for 3D\nreconstruction, where simplified physical models often fail in complex scenes.\nWe propose \\textbf{R-Splatting}, a unified framework that bridges underwater\nimage restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both\nrendering quality and geometric fidelity. Our method integrates multiple\nenhanced views produced by diverse UIR models into a single reconstruction\npipeline. During inference, a lightweight illumination generator samples latent\ncodes to support diverse yet coherent renderings, while a contrastive loss\nensures disentangled and stable illumination representations. Furthermore, we\npropose \\textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models\nopacity as a stochastic function to regularize training. This suppresses abrupt\ngradient responses triggered by illumination variation and mitigates\noverfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF\nand our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong\nbaselines in both rendering quality and geometric accuracy."
                },
                "authors": [
                    {
                        "name": "Guoxi Huang"
                    },
                    {
                        "name": "Haoran Wang"
                    },
                    {
                        "name": "Zipeng Qi"
                    },
                    {
                        "name": "Wenjun Lu"
                    },
                    {
                        "name": "David Bull"
                    },
                    {
                        "name": "Nantheera Anantrasirichai"
                    }
                ],
                "author_detail": {
                    "name": "Nantheera Anantrasirichai"
                },
                "author": "Nantheera Anantrasirichai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14429v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14429v2",
                "updated": "2025-09-22T13:47:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    47,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-20T14:38:53Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    53,
                    1,
                    140,
                    0
                ],
                "title": "Compositional amortized inference for large-scale hierarchical Bayesian\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional amortized inference for large-scale hierarchical Bayesian\n  models"
                },
                "summary": "Amortized Bayesian inference (ABI) has emerged as a powerful simulation-based\napproach for estimating complex mechanistic models, offering fast posterior\nsampling via generative neural networks. However, extending ABI to hierarchical\nmodels, a cornerstone of modern Bayesian analysis, remains a major challenge\ndue to the need to simulate massive data sets and estimate thousands of\nparameters. In this work, we build on compositional score matching (CSM), a\ndivide-and-conquer strategy for Bayesian updating using diffusion models. To\naddress existing stability issues of CSM in dealing with large data sets, we\ncouple adaptive solvers with a novel, error-damping compositional estimator.\nOur estimator remains stable even with hundreds of thousands of data points and\nparameters. We validate our approach on a controlled toy example, a\nhigh-dimensional autoregressive model, and a real-world advanced microscopy\napplication involving over 750,000 parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian inference (ABI) has emerged as a powerful simulation-based\napproach for estimating complex mechanistic models, offering fast posterior\nsampling via generative neural networks. However, extending ABI to hierarchical\nmodels, a cornerstone of modern Bayesian analysis, remains a major challenge\ndue to the need to simulate massive data sets and estimate thousands of\nparameters. In this work, we build on compositional score matching (CSM), a\ndivide-and-conquer strategy for Bayesian updating using diffusion models. To\naddress existing stability issues of CSM in dealing with large data sets, we\ncouple adaptive solvers with a novel, error-damping compositional estimator.\nOur estimator remains stable even with hundreds of thousands of data points and\nparameters. We validate our approach on a controlled toy example, a\nhigh-dimensional autoregressive model, and a real-world advanced microscopy\napplication involving over 750,000 parameters."
                },
                "authors": [
                    {
                        "name": "Jonas Arruda"
                    },
                    {
                        "name": "Vikas Pandey"
                    },
                    {
                        "name": "Catherine Sherry"
                    },
                    {
                        "name": "Margarida Barroso"
                    },
                    {
                        "name": "Xavier Intes"
                    },
                    {
                        "name": "Jan Hasenauer"
                    },
                    {
                        "name": "Stefan T. Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T. Radev"
                },
                "author": "Stefan T. Radev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14429v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14429v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17784v1",
                "updated": "2025-09-22T13:45:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    45,
                    17,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:45:17Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    45,
                    17,
                    0,
                    265,
                    0
                ],
                "title": "Revealing Multimodal Causality with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Multimodal Causality with Large Language Models"
                },
                "summary": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data."
                },
                "authors": [
                    {
                        "name": "Jin Li"
                    },
                    {
                        "name": "Shoujin Wang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Longbing Cao"
                    },
                    {
                        "name": "Shui Yu"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04931v2",
                "updated": "2025-09-22T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    39,
                    16,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-07T13:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    52,
                    37,
                    4,
                    38,
                    0
                ],
                "title": "Breaking the News: Taking the Roles of Influencer vs. Journalist in a\n  LLM-Based Game for Raising Misinformation Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the News: Taking the Roles of Influencer vs. Journalist in a\n  LLM-Based Game for Raising Misinformation Awareness"
                },
                "summary": "Effectively mitigating online misinformation requires understanding of their\nmechanisms and learning of practical skills for identification and\ncounteraction. Serious games may serve as tools for combating misinformation,\nteaching players to recognize common misinformation tactics, and improving\ntheir skills of discernment. However, current interventions are designed as\nsingle-player, choice-based games, which present players with limited\npredefined choices. Such restrictions reduce replayability and may lead to an\noverly simplistic understanding of misinformation and how to debunk them. This\nstudy seeks to empower people to understand opinion-influencing and\nmisinformation-debunking processes. We created a Player vs. Player (PvP) game\nin which participants attempt to generate or debunk misinformation to convince\nthe public opinion represented by LLM. Using a within-subjects mixed-methods\nstudy design (N=47), we found that this game significantly raised participants'\nmedia literacy and improved their ability to identify misinformation.\nQualitative analyses revealed how participants' use of debunking and content\ncreation strategies deepened their understanding of misinformation. This work\nshows the potential for illuminating contrasting viewpoints of social issues by\nLLM-based mechanics in PvP games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively mitigating online misinformation requires understanding of their\nmechanisms and learning of practical skills for identification and\ncounteraction. Serious games may serve as tools for combating misinformation,\nteaching players to recognize common misinformation tactics, and improving\ntheir skills of discernment. However, current interventions are designed as\nsingle-player, choice-based games, which present players with limited\npredefined choices. Such restrictions reduce replayability and may lead to an\noverly simplistic understanding of misinformation and how to debunk them. This\nstudy seeks to empower people to understand opinion-influencing and\nmisinformation-debunking processes. We created a Player vs. Player (PvP) game\nin which participants attempt to generate or debunk misinformation to convince\nthe public opinion represented by LLM. Using a within-subjects mixed-methods\nstudy design (N=47), we found that this game significantly raised participants'\nmedia literacy and improved their ability to identify misinformation.\nQualitative analyses revealed how participants' use of debunking and content\ncreation strategies deepened their understanding of misinformation. This work\nshows the potential for illuminating contrasting viewpoints of social issues by\nLLM-based mechanics in PvP games."
                },
                "authors": [
                    {
                        "name": "Huiyun Tang"
                    },
                    {
                        "name": "Songqi Sun"
                    },
                    {
                        "name": "Kexin Nie"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Anastasia Sergeeva"
                    },
                    {
                        "name": "Ray LC"
                    }
                ],
                "author_detail": {
                    "name": "Ray LC"
                },
                "author": "Ray LC",
                "arxiv_comment": "Accepted to ACM CHI PLAY 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21693v3",
                "updated": "2025-09-22T13:38:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    38,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-27T19:29:40Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    19,
                    29,
                    40,
                    1,
                    147,
                    0
                ],
                "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs"
                },
                "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Hedderich"
                },
                "author": "Michael A. Hedderich",
                "arxiv_comment": "Accepted by EMNLP 2025 Findings, 33 pages, 30 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14746v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14746v4",
                "updated": "2025-09-22T13:37:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    37,
                    52,
                    0,
                    265,
                    0
                ],
                "published": "2024-02-22T18:06:19Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    6,
                    19,
                    3,
                    53,
                    0
                ],
                "title": "Scaling Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Efficient LLMs"
                },
                "summary": "Trained LLMs in the transformer architecture are typically sparse in that\nmost of the parameters are negligible, raising questions on efficiency.\nFurthermore, the so called \"AI scaling law\" for transformers suggests that the\nnumber of parameters must scale linearly with the size of the data. In\nresponse, we inquire into efficient LLMs, i.e. those with the fewest parameters\nthat achieve the desired accuracy on a training corpus. Specifically, by\ncomparing theoretical and empirical estimates of the Kullback-Liebler\ndivergence, we derive a natural AI scaling law that the number of parameters in\nan efficient LLM scales as $D^{\\gamma}$ where $D$ is the size of the training\ndata and $ \\gamma \\in [0.44, 0.72]$, suggesting the existence of more efficient\narchitectures. Against this backdrop, we propose recurrent transformers,\ncombining the efficacy of transformers with the efficiency of recurrent\nnetworks, progressively applying a single transformer layer to a fixed-width\nsliding window across the input sequence. Recurrent transformers (a) run in\nlinear time in the sequence length, (b) are memory-efficient and amenable to\nparallel processing in large batches, (c) learn to forget history for language\ntasks, or accumulate history for long range tasks like copy and selective copy,\nand (d) are amenable to curriculum training to overcome vanishing gradients. In\nour experiments, we find that recurrent transformers perform favorably on\nbenchmark tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained LLMs in the transformer architecture are typically sparse in that\nmost of the parameters are negligible, raising questions on efficiency.\nFurthermore, the so called \"AI scaling law\" for transformers suggests that the\nnumber of parameters must scale linearly with the size of the data. In\nresponse, we inquire into efficient LLMs, i.e. those with the fewest parameters\nthat achieve the desired accuracy on a training corpus. Specifically, by\ncomparing theoretical and empirical estimates of the Kullback-Liebler\ndivergence, we derive a natural AI scaling law that the number of parameters in\nan efficient LLM scales as $D^{\\gamma}$ where $D$ is the size of the training\ndata and $ \\gamma \\in [0.44, 0.72]$, suggesting the existence of more efficient\narchitectures. Against this backdrop, we propose recurrent transformers,\ncombining the efficacy of transformers with the efficiency of recurrent\nnetworks, progressively applying a single transformer layer to a fixed-width\nsliding window across the input sequence. Recurrent transformers (a) run in\nlinear time in the sequence length, (b) are memory-efficient and amenable to\nparallel processing in large batches, (c) learn to forget history for language\ntasks, or accumulate history for long range tasks like copy and selective copy,\nand (d) are amenable to curriculum training to overcome vanishing gradients. In\nour experiments, we find that recurrent transformers perform favorably on\nbenchmark tests."
                },
                "authors": [
                    {
                        "name": "B. N. Kausik"
                    }
                ],
                "author_detail": {
                    "name": "B. N. Kausik"
                },
                "author": "B. N. Kausik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14746v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14746v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17773v1",
                "updated": "2025-09-22T13:37:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    37,
                    37,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:37:37Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    37,
                    37,
                    0,
                    265,
                    0
                ],
                "title": "I2VWM: Robust Watermarking for Image to Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I2VWM: Robust Watermarking for Image to Video Generation"
                },
                "summary": "The rapid progress of image-guided video generation (I2V) has raised concerns\nabout its potential misuse in misinformation and fraud, underscoring the urgent\nneed for effective digital watermarking. While existing watermarking methods\ndemonstrate robustness within a single modality, they fail to trace source\nimages in I2V settings. To address this gap, we introduce the concept of Robust\nDiffusion Distance, which measures the temporal persistence of watermark\nsignals in generated videos. Building on this, we propose I2VWM, a cross-modal\nwatermarking framework designed to enhance watermark robustness across time.\nI2VWM leverages a video-simulation noise layer during training and employs an\noptical-flow-based alignment module during inference. Experiments on both\nopen-source and commercial I2V models demonstrate that I2VWM significantly\nimproves robustness while maintaining imperceptibility, establishing a new\nparadigm for cross-modal watermarking in the era of generative video.\n\\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code\nReleased.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress of image-guided video generation (I2V) has raised concerns\nabout its potential misuse in misinformation and fraud, underscoring the urgent\nneed for effective digital watermarking. While existing watermarking methods\ndemonstrate robustness within a single modality, they fail to trace source\nimages in I2V settings. To address this gap, we introduce the concept of Robust\nDiffusion Distance, which measures the temporal persistence of watermark\nsignals in generated videos. Building on this, we propose I2VWM, a cross-modal\nwatermarking framework designed to enhance watermark robustness across time.\nI2VWM leverages a video-simulation noise layer during training and employs an\noptical-flow-based alignment module during inference. Experiments on both\nopen-source and commercial I2V models demonstrate that I2VWM significantly\nimproves robustness while maintaining imperceptibility, establishing a new\nparadigm for cross-modal watermarking in the era of generative video.\n\\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code\nReleased.}"
                },
                "authors": [
                    {
                        "name": "Guanjie Wang"
                    },
                    {
                        "name": "Zehua Ma"
                    },
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Weiming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Zhang"
                },
                "author": "Weiming Zhang",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17455v2",
                "updated": "2025-09-22T13:35:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    35,
                    45,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-29T07:32:42Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    7,
                    32,
                    42,
                    2,
                    29,
                    0
                ],
                "title": "Uniform Confidence Band for Marginal Treatment Effect Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Confidence Band for Marginal Treatment Effect Function"
                },
                "summary": "This paper presents a method for constructing uniform confidence bands for\nthe marginal treatment effect (MTE) function. The shape of the MTE function\noffers insight into how the unobserved propensity to receive treatment is\nrelated to the treatment effect. Our approach visualizes the statistical\nuncertainty of an estimated function, facilitating inferences about the\nfunction's shape. The proposed method is computationally inexpensive and\nrequires only minimal information: sample size, standard errors, kernel\nfunction, and bandwidth. This minimal data requirement enables applications to\nboth new analyses and published results without access to original data. We\nderive a Gaussian approximation for a local quadratic estimator and consider\nthe approximation of the distribution of its supremum in polynomial order.\nMonte Carlo simulations demonstrate that our bands provide the desired coverage\nand are less conservative than those based on the Gumbel approximation. An\nempirical illustration regarding the returns to education is included.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method for constructing uniform confidence bands for\nthe marginal treatment effect (MTE) function. The shape of the MTE function\noffers insight into how the unobserved propensity to receive treatment is\nrelated to the treatment effect. Our approach visualizes the statistical\nuncertainty of an estimated function, facilitating inferences about the\nfunction's shape. The proposed method is computationally inexpensive and\nrequires only minimal information: sample size, standard errors, kernel\nfunction, and bandwidth. This minimal data requirement enables applications to\nboth new analyses and published results without access to original data. We\nderive a Gaussian approximation for a local quadratic estimator and consider\nthe approximation of the distribution of its supremum in polynomial order.\nMonte Carlo simulations demonstrate that our bands provide the desired coverage\nand are less conservative than those based on the Gumbel approximation. An\nempirical illustration regarding the returns to education is included."
                },
                "authors": [
                    {
                        "name": "Toshiki Tsuda"
                    },
                    {
                        "name": "Yanchun Jin"
                    },
                    {
                        "name": "Ryo Okui"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Okui"
                },
                "author": "Ryo Okui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18272v2",
                "updated": "2025-09-22T13:34:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    34,
                    19,
                    0,
                    265,
                    0
                ],
                "published": "2024-10-23T20:45:55Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    20,
                    45,
                    55,
                    2,
                    297,
                    0
                ],
                "title": "Partially Identified Rankings from Pairwise Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Identified Rankings from Pairwise Interactions"
                },
                "summary": "This paper considers the problem of ranking objects based on their latent\nmerits using data from pairwise interactions. We allow for incomplete\nobservation of these interactions and study what can be inferred about rankings\nin such settings. First, we show that identification of the ranking depends on\na trade-off between the tournament graph and the interaction function: in\nparametric models, such as the Bradley-Terry-Luce, rankings are point\nidentified even with sparse graphs, whereas nonparametric models require dense\ngraphs. Second, moving beyond point identification, we characterize the\nidentified set in the nonparametric model under any tournament structure and\nrepresent it through moment inequalities. Finally, we propose a\nlikelihood-based statistic to test whether a ranking belongs to the identified\nset. We study two testing procedures: one is finite-sample valid but\ncomputationally intensive; the other is easy to implement and valid\nasymptotically. We illustrate our results using Brazilian employer-employee\ndata to study how workers rank firms when moving across jobs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers the problem of ranking objects based on their latent\nmerits using data from pairwise interactions. We allow for incomplete\nobservation of these interactions and study what can be inferred about rankings\nin such settings. First, we show that identification of the ranking depends on\na trade-off between the tournament graph and the interaction function: in\nparametric models, such as the Bradley-Terry-Luce, rankings are point\nidentified even with sparse graphs, whereas nonparametric models require dense\ngraphs. Second, moving beyond point identification, we characterize the\nidentified set in the nonparametric model under any tournament structure and\nrepresent it through moment inequalities. Finally, we propose a\nlikelihood-based statistic to test whether a ranking belongs to the identified\nset. We study two testing procedures: one is finite-sample valid but\ncomputationally intensive; the other is easy to implement and valid\nasymptotically. We illustrate our results using Brazilian employer-employee\ndata to study how workers rank firms when moving across jobs."
                },
                "authors": [
                    {
                        "name": "Federico Crippa"
                    },
                    {
                        "name": "Danil Fedchenko"
                    }
                ],
                "author_detail": {
                    "name": "Danil Fedchenko"
                },
                "author": "Danil Fedchenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17766v1",
                "updated": "2025-09-22T13:26:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    26,
                    24,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:26:24Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    26,
                    24,
                    0,
                    265,
                    0
                ],
                "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn\n  Dialogue"
                },
                "summary": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents."
                },
                "authors": [
                    {
                        "name": "Ziyi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyi Liu"
                },
                "author": "Ziyi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16887v2",
                "updated": "2025-09-22T13:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2024-05-27T07:10:04Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    7,
                    10,
                    4,
                    0,
                    148,
                    0
                ],
                "title": "A Large Language Model-based multi-agent manufacturing system for\n  intelligent shopfloor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-based multi-agent manufacturing system for\n  intelligent shopfloor"
                },
                "summary": "As customer demand for multi-variety and small-batch production increases,\ndynamic disturbances place greater demands on manufacturing systems. To address\nsuch challenges, researchers proposed the multi-agent manufacturing system.\nHowever, conventional agent negotiation typically relies on pre-defined and\nfixed heuristic rules, which are ill-suited to managing complex and fluctuating\ndisturbances. In current implementations, mainstream approaches based on\nreinforcement learning require the development of simulators and training\nmodels specific to a given shopfloor, necessitating substantial computational\nresources and lacking scalability. To overcome this limitation, the present\nstudy proposes a Large Language Model-based (LLM-based) multi-agent\nmanufacturing system for intelligent shopfloor management. By defining the\ndiverse modules of agents and their collaborative methods, this system\nfacilitates the processing of all workpieces with minimal human intervention.\nThe agents in this system consist of the Machine Server Module (MSM), Bid\nInviter Module (BIM), Bidder Module (BM), Thinking Module (TM), and Decision\nModule (DM). By harnessing the reasoning capabilities of LLMs, these modules\nenable agents to dynamically analyze shopfloor information and select\nappropriate processing machines. The LLM-based modules, predefined by system\nprompts, provide dynamic functionality for the system without the need for\npre-training. Extensive experiments were conducted in physical shopfloor\nsettings. The results demonstrate that the proposed system exhibits strong\nadaptability, and achieves superior performance (makespan) and stability (as\nmeasured by sample standard deviation) compared to other approaches without\nrequiring pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As customer demand for multi-variety and small-batch production increases,\ndynamic disturbances place greater demands on manufacturing systems. To address\nsuch challenges, researchers proposed the multi-agent manufacturing system.\nHowever, conventional agent negotiation typically relies on pre-defined and\nfixed heuristic rules, which are ill-suited to managing complex and fluctuating\ndisturbances. In current implementations, mainstream approaches based on\nreinforcement learning require the development of simulators and training\nmodels specific to a given shopfloor, necessitating substantial computational\nresources and lacking scalability. To overcome this limitation, the present\nstudy proposes a Large Language Model-based (LLM-based) multi-agent\nmanufacturing system for intelligent shopfloor management. By defining the\ndiverse modules of agents and their collaborative methods, this system\nfacilitates the processing of all workpieces with minimal human intervention.\nThe agents in this system consist of the Machine Server Module (MSM), Bid\nInviter Module (BIM), Bidder Module (BM), Thinking Module (TM), and Decision\nModule (DM). By harnessing the reasoning capabilities of LLMs, these modules\nenable agents to dynamically analyze shopfloor information and select\nappropriate processing machines. The LLM-based modules, predefined by system\nprompts, provide dynamic functionality for the system without the need for\npre-training. Extensive experiments were conducted in physical shopfloor\nsettings. The results demonstrate that the proposed system exhibits strong\nadaptability, and achieves superior performance (makespan) and stability (as\nmeasured by sample standard deviation) compared to other approaches without\nrequiring pre-training."
                },
                "authors": [
                    {
                        "name": "Zhen Zhao"
                    },
                    {
                        "name": "Dunbing Tang"
                    },
                    {
                        "name": "Changchun Liu"
                    },
                    {
                        "name": "Liping Wang"
                    },
                    {
                        "name": "Zequn Zhang"
                    },
                    {
                        "name": "Haihua Zhu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qingwei Nie"
                    },
                    {
                        "name": "Yuchen Ji"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Ji"
                },
                "author": "Yuchen Ji",
                "arxiv_doi": "10.1016/j.aei.2025.103888",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.aei.2025.103888",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Zhao Z, Tang D, Liu C, et al. A Large language model-based\n  multi-agent manufacturing system for intelligent shopfloors[J]. Advanced\n  Engineering Informatics, 2026, 69: 103888",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20099v2",
                "updated": "2025-09-22T13:18:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    18,
                    46,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-26T15:08:23Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    8,
                    23,
                    0,
                    146,
                    0
                ],
                "title": "Large Language Models Meet Knowledge Graphs for Question Answering:\n  Synthesis and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Knowledge Graphs for Question Answering:\n  Synthesis and Opportunities"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art methods in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art methods in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities."
                },
                "authors": [
                    {
                        "name": "Chuangtao Ma"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Tianxing Wu"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Haofen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haofen Wang"
                },
                "author": "Haofen Wang",
                "arxiv_comment": "Accepted at EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22118v2",
                "updated": "2025-09-22T13:16:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    16,
                    16,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-28T08:47:10Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    8,
                    47,
                    10,
                    2,
                    148,
                    0
                ],
                "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of\n  Two Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of\n  Two Approaches"
                },
                "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup."
                },
                "authors": [
                    {
                        "name": "Alan Ramponi"
                    },
                    {
                        "name": "Marco Rovera"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Sara Tonelli"
                    }
                ],
                "author_detail": {
                    "name": "Sara Tonelli"
                },
                "author": "Sara Tonelli",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17743v2",
                "updated": "2025-09-23T07:31:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    7,
                    31,
                    0,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T13:06:17Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    6,
                    17,
                    0,
                    265,
                    0
                ],
                "title": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA"
                },
                "summary": "Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME."
                },
                "authors": [
                    {
                        "name": "Chenglin Li"
                    },
                    {
                        "name": "Feng Han"
                    },
                    {
                        "name": "Feng Tao"
                    },
                    {
                        "name": "Ruilin Li"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17740v1",
                "updated": "2025-09-22T13:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    5,
                    29,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    5,
                    29,
                    0,
                    265,
                    0
                ],
                "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal\n  LLMs in Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal\n  LLMs in Image Classification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding."
                },
                "authors": [
                    {
                        "name": "Yiwen Jiang"
                    },
                    {
                        "name": "Deval Mehta"
                    },
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Yaling Shen"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Zongyuan Ge"
                    }
                ],
                "author_detail": {
                    "name": "Zongyuan Ge"
                },
                "author": "Zongyuan Ge",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17730v1",
                "updated": "2025-09-22T13:00:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    0,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:00:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    0,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement\n  Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement\n  Learning in LLMs"
                },
                "summary": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods."
                },
                "authors": [
                    {
                        "name": "Bonan Zhang"
                    },
                    {
                        "name": "Zhongqi Chen"
                    },
                    {
                        "name": "Bowen Song"
                    },
                    {
                        "name": "Qinya Li"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17729v1",
                "updated": "2025-09-22T12:59:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    59,
                    18,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:59:18Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    59,
                    18,
                    0,
                    265,
                    0
                ],
                "title": "A Generative Conditional Distribution Equality Testing Framework and Its\n  Minimax Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Conditional Distribution Equality Testing Framework and Its\n  Minimax Analysis"
                },
                "summary": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a general framework for testing the equality of the\nconditional distributions in a two-sample problem. This problem is most\nrelevant to transfer learning under covariate shift. Our framework is built on\nneural network-based generative methods and sample splitting techniques by\ntransforming the conditional distribution testing problem into an unconditional\none. We introduce two special tests: the generative permutation-based\nconditional distribution equality test and the generative classification\naccuracy-based conditional distribution equality test. Theoretically, we\nestablish a minimax lower bound for statistical inference in testing the\nequality of two conditional distributions under certain smoothness conditions.\nWe demonstrate that the generative permutation-based conditional distribution\nequality test and its modified version can attain this lower bound precisely or\nup to some iterated logarithmic factor. Moreover, we prove the testing\nconsistency of the generative classification accuracy-based conditional\ndistribution equality test. We also establish the convergence rate for the\nlearned conditional generator by deriving new results related to the\nrecently-developed offset Rademacher complexity and approximation properties\nusing neural networks. Empirically, we conduct numerical studies including\nsynthetic datasets and two real-world datasets, demonstrating the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Siming Zheng"
                    },
                    {
                        "name": "Meifang Lan"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Yuanyuan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yuanyuan Lin"
                },
                "author": "Yuanyuan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17726v1",
                "updated": "2025-09-22T12:57:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    57,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:57:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    57,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Automated Labeling of Intracranial Arteries with Uncertainty\n  Quantification Using Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Labeling of Intracranial Arteries with Uncertainty\n  Quantification Using Deep Learning"
                },
                "summary": "Accurate anatomical labeling of intracranial arteries is essential for\ncerebrovascular diagnosis and hemodynamic analysis but remains time-consuming\nand subject to interoperator variability. We present a deep learning-based\nframework for automated artery labeling from 3D Time-of-Flight Magnetic\nResonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating\nuncertainty quantification to enhance interpretability and reliability. We\nevaluated three convolutional neural network architectures: (1) a UNet with\nresidual encoder blocks, reflecting commonly used baselines in vascular\nlabeling; (2) CS-Net, an attention-augmented UNet incorporating channel and\nspatial attention mechanisms for enhanced curvilinear structure recognition;\nand (3) nnUNet, a self-configuring framework that automates preprocessing,\ntraining, and architectural adaptation based on dataset characteristics. Among\nthese, nnUNet achieved the highest labeling performance (average Dice score:\n0.922; average surface distance: 0.387 mm), with improved robustness in\nanatomically complex vessels. To assess predictive confidence, we implemented\ntest-time augmentation (TTA) and introduced a novel coordinate-guided strategy\nto reduce interpolation errors during augmented inference. The resulting\nuncertainty maps reliably indicated regions of anatomical ambiguity,\npathological variation, or manual labeling inconsistency. We further validated\nclinical utility by comparing flow velocities derived from automated and manual\nlabels in co-registered 4D Flow MRI datasets, observing close agreement with no\nstatistically significant differences. Our framework offers a scalable,\naccurate, and uncertainty-aware solution for automated cerebrovascular\nlabeling, supporting downstream hemodynamic analysis and facilitating clinical\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate anatomical labeling of intracranial arteries is essential for\ncerebrovascular diagnosis and hemodynamic analysis but remains time-consuming\nand subject to interoperator variability. We present a deep learning-based\nframework for automated artery labeling from 3D Time-of-Flight Magnetic\nResonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating\nuncertainty quantification to enhance interpretability and reliability. We\nevaluated three convolutional neural network architectures: (1) a UNet with\nresidual encoder blocks, reflecting commonly used baselines in vascular\nlabeling; (2) CS-Net, an attention-augmented UNet incorporating channel and\nspatial attention mechanisms for enhanced curvilinear structure recognition;\nand (3) nnUNet, a self-configuring framework that automates preprocessing,\ntraining, and architectural adaptation based on dataset characteristics. Among\nthese, nnUNet achieved the highest labeling performance (average Dice score:\n0.922; average surface distance: 0.387 mm), with improved robustness in\nanatomically complex vessels. To assess predictive confidence, we implemented\ntest-time augmentation (TTA) and introduced a novel coordinate-guided strategy\nto reduce interpolation errors during augmented inference. The resulting\nuncertainty maps reliably indicated regions of anatomical ambiguity,\npathological variation, or manual labeling inconsistency. We further validated\nclinical utility by comparing flow velocities derived from automated and manual\nlabels in co-registered 4D Flow MRI datasets, observing close agreement with no\nstatistically significant differences. Our framework offers a scalable,\naccurate, and uncertainty-aware solution for automated cerebrovascular\nlabeling, supporting downstream hemodynamic analysis and facilitating clinical\nintegration."
                },
                "authors": [
                    {
                        "name": "Javier Bisbal"
                    },
                    {
                        "name": "Patrick Winter"
                    },
                    {
                        "name": "Sebastian Jofre"
                    },
                    {
                        "name": "Aaron Ponce"
                    },
                    {
                        "name": "Sameer A. Ansari"
                    },
                    {
                        "name": "Ramez Abdalla"
                    },
                    {
                        "name": "Michael Markl"
                    },
                    {
                        "name": "Oliver Welin Odeback"
                    },
                    {
                        "name": "Sergio Uribe"
                    },
                    {
                        "name": "Cristian Tejos"
                    },
                    {
                        "name": "Julio Sotelo"
                    },
                    {
                        "name": "Susanne Schnell"
                    },
                    {
                        "name": "David Marlevi"
                    }
                ],
                "author_detail": {
                    "name": "David Marlevi"
                },
                "author": "David Marlevi",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.12667v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.12667v5",
                "updated": "2025-09-22T12:52:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    52,
                    13,
                    0,
                    265,
                    0
                ],
                "published": "2023-03-22T15:44:59Z",
                "published_parsed": [
                    2023,
                    3,
                    22,
                    15,
                    44,
                    59,
                    2,
                    81,
                    0
                ],
                "title": "Don't (fully) exclude me, it's not necessary! Causal inference with\n  semi-IVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't (fully) exclude me, it's not necessary! Causal inference with\n  semi-IVs"
                },
                "summary": "This paper proposes semi-instrumental variables (semi-IVs) as an alternative\nto instrumental variables (IVs) to identify the causal effect of a binary (or\ndiscrete) endogenous treatment. A semi-IV is a less restrictive form of\ninstrument: it affects the selection into treatment but is excluded only from\none, not necessarily both, potential outcomes. Having two continuously\ndistributed semi-IVs, one excluded from the potential outcome under treatment\nand the other from the potential outcome under control, is sufficient to\nnonparametrically point identify marginal treatment effect (MTE) and local\naverage treatment effect (LATE) parameters. In practice, semi-IVs provide a\nsolution to the challenge of finding valid IVs because they are often easier to\nfind: many selection-specific shocks, policies, prices, costs, or benefits are\nvalid semi-IVs. As an application, I estimate the returns to working in the\nmanufacturing sector on earnings using sector-specific characteristics as\nsemi-IVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes semi-instrumental variables (semi-IVs) as an alternative\nto instrumental variables (IVs) to identify the causal effect of a binary (or\ndiscrete) endogenous treatment. A semi-IV is a less restrictive form of\ninstrument: it affects the selection into treatment but is excluded only from\none, not necessarily both, potential outcomes. Having two continuously\ndistributed semi-IVs, one excluded from the potential outcome under treatment\nand the other from the potential outcome under control, is sufficient to\nnonparametrically point identify marginal treatment effect (MTE) and local\naverage treatment effect (LATE) parameters. In practice, semi-IVs provide a\nsolution to the challenge of finding valid IVs because they are often easier to\nfind: many selection-specific shocks, policies, prices, costs, or benefits are\nvalid semi-IVs. As an application, I estimate the returns to working in the\nmanufacturing sector on earnings using sector-specific characteristics as\nsemi-IVs."
                },
                "authors": [
                    {
                        "name": "Christophe Bruneel-Zupanc"
                    }
                ],
                "author_detail": {
                    "name": "Christophe Bruneel-Zupanc"
                },
                "author": "Christophe Bruneel-Zupanc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.12667v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.12667v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04760v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04760v5",
                "updated": "2025-09-22T12:50:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    50,
                    51,
                    0,
                    265,
                    0
                ],
                "published": "2024-05-08T02:09:17Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    2,
                    9,
                    17,
                    2,
                    129,
                    0
                ],
                "title": "Large Language Models for Cyber Security: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cyber Security: A Systematic Literature Review"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in a variety of\napplication domains, including cybersecurity. As the volume and sophistication\nof cyber threats continue to grow, there is an increasing need for intelligent\nsystems that can automatically detect vulnerabilities, analyze malware, and\nrespond to attacks. In this survey, we conduct a comprehensive review of the\nliterature on the application of LLMs in cybersecurity~(LLM4Security). By\ncomprehensively collecting over 40K relevant papers and systematically\nanalyzing 185 papers from top security and software engineering venues, we aim\nto provide a holistic view of how LLMs are being used to solve diverse problems\nacross the cybersecurity domain. Through our analysis, we identify several key\nfindings. First, we observe that LLMs are being applied to an expanding range\nof cybersecurity tasks, including vulnerability detection, malware analysis,\nand network intrusion detection. Second, we analyze application trends of\ndifferent LLM architectures (such as encoder-only, encoder-decoder, and\ndecoder-only) across security domains. Third, we identify increasingly\nsophisticated techniques for adapting LLMs to cybersecurity, such as advanced\nfine-tuning, prompt engineering, and external augmentation strategies. A\nsignificant emerging trend is the use of LLM-based autonomous agents, which\nrepresent a paradigm shift from single-task execution to orchestrating complex,\nmulti-step security workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in a variety of\napplication domains, including cybersecurity. As the volume and sophistication\nof cyber threats continue to grow, there is an increasing need for intelligent\nsystems that can automatically detect vulnerabilities, analyze malware, and\nrespond to attacks. In this survey, we conduct a comprehensive review of the\nliterature on the application of LLMs in cybersecurity~(LLM4Security). By\ncomprehensively collecting over 40K relevant papers and systematically\nanalyzing 185 papers from top security and software engineering venues, we aim\nto provide a holistic view of how LLMs are being used to solve diverse problems\nacross the cybersecurity domain. Through our analysis, we identify several key\nfindings. First, we observe that LLMs are being applied to an expanding range\nof cybersecurity tasks, including vulnerability detection, malware analysis,\nand network intrusion detection. Second, we analyze application trends of\ndifferent LLM architectures (such as encoder-only, encoder-decoder, and\ndecoder-only) across security domains. Third, we identify increasingly\nsophisticated techniques for adapting LLMs to cybersecurity, such as advanced\nfine-tuning, prompt engineering, and external augmentation strategies. A\nsignificant emerging trend is the use of LLM-based autonomous agents, which\nrepresent a paradigm shift from single-task execution to orchestrating complex,\nmulti-step security workflows."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Ningke Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Ting Yu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04760v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04760v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17712v1",
                "updated": "2025-09-22T12:49:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    49,
                    49,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:49:49Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    49,
                    49,
                    0,
                    265,
                    0
                ],
                "title": "RCTDistill: Cross-Modal Knowledge Distillation Framework for\n  Radar-Camera 3D Object Detection with Temporal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCTDistill: Cross-Modal Knowledge Distillation Framework for\n  Radar-Camera 3D Object Detection with Temporal Fusion"
                },
                "summary": "Radar-camera fusion methods have emerged as a cost-effective approach for 3D\nobject detection but still lag behind LiDAR-based methods in performance.\nRecent works have focused on employing temporal fusion and Knowledge\nDistillation (KD) strategies to overcome these limitations. However, existing\napproaches have not sufficiently accounted for uncertainties arising from\nobject motion or sensor-specific errors inherent in radar and camera\nmodalities. In this work, we propose RCTDistill, a novel cross-modal KD method\nbased on temporal fusion, comprising three key modules: Range-Azimuth Knowledge\nDistillation (RAKD), Temporal Knowledge Distillation (TKD), and\nRegion-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider\nthe inherent errors in the range and azimuth directions, enabling effective\nknowledge transfer from LiDAR features to refine inaccurate BEV\nrepresentations. TKD mitigates temporal misalignment caused by dynamic objects\nby aligning historical radar-camera BEV features with current LiDAR\nrepresentations. RDKD enhances feature discrimination by distilling relational\nknowledge from the teacher model, allowing the student to differentiate\nforeground and background features. RCTDistill achieves state-of-the-art\nradar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)\ndatasets, with the fastest inference speed of 26.2 FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radar-camera fusion methods have emerged as a cost-effective approach for 3D\nobject detection but still lag behind LiDAR-based methods in performance.\nRecent works have focused on employing temporal fusion and Knowledge\nDistillation (KD) strategies to overcome these limitations. However, existing\napproaches have not sufficiently accounted for uncertainties arising from\nobject motion or sensor-specific errors inherent in radar and camera\nmodalities. In this work, we propose RCTDistill, a novel cross-modal KD method\nbased on temporal fusion, comprising three key modules: Range-Azimuth Knowledge\nDistillation (RAKD), Temporal Knowledge Distillation (TKD), and\nRegion-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider\nthe inherent errors in the range and azimuth directions, enabling effective\nknowledge transfer from LiDAR features to refine inaccurate BEV\nrepresentations. TKD mitigates temporal misalignment caused by dynamic objects\nby aligning historical radar-camera BEV features with current LiDAR\nrepresentations. RDKD enhances feature discrimination by distilling relational\nknowledge from the teacher model, allowing the student to differentiate\nforeground and background features. RCTDistill achieves state-of-the-art\nradar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)\ndatasets, with the fastest inference speed of 26.2 FPS."
                },
                "authors": [
                    {
                        "name": "Geonho Bang"
                    },
                    {
                        "name": "Minjae Seong"
                    },
                    {
                        "name": "Jisong Kim"
                    },
                    {
                        "name": "Geunju Baek"
                    },
                    {
                        "name": "Daye Oh"
                    },
                    {
                        "name": "Junhyung Kim"
                    },
                    {
                        "name": "Junho Koh"
                    },
                    {
                        "name": "Jun Won Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jun Won Choi"
                },
                "author": "Jun Won Choi",
                "arxiv_comment": "Accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.13804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13804v2",
                "updated": "2025-09-22T17:59:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-08-19T13:05:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    5,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding"
                },
                "summary": "How do Large Language Models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluated the best language models (Claude\nSonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from nearly\n700 annotators in 100K+ texts spanning social networks, news and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators,\nperforming much better than average balanced accuracy. Importantly, we find\nthat AI produces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Large Language Models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluated the best language models (Claude\nSonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from nearly\n700 annotators in 100K+ texts spanning social networks, news and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators,\nperforming much better than average balanced accuracy. Importantly, we find\nthat AI produces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities."
                },
                "authors": [
                    {
                        "name": "Maciej Skorski"
                    },
                    {
                        "name": "Alina Landowska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Landowska"
                },
                "author": "Alina Landowska",
                "arxiv_comment": "Appears in UncertaiNLP@EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 62F15, 62P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18091v1",
                "updated": "2025-09-22T17:59:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    7,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:59:07Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    59,
                    7,
                    0,
                    265,
                    0
                ],
                "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System"
                },
                "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue."
                },
                "authors": [
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Jiahua Wu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Bingjun Chen"
                    },
                    {
                        "name": "Bangyang Hong"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Cong Fu"
                    },
                    {
                        "name": "Kangle Wu"
                    },
                    {
                        "name": "Yabo Ni"
                    },
                    {
                        "name": "Anxiang Zeng"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    }
                ],
                "author_detail": {
                    "name": "See-Kiong Ng"
                },
                "author": "See-Kiong Ng",
                "arxiv_comment": "OnePiece Technical Report; Applied in Shopee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18083v1",
                "updated": "2025-09-22T17:56:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    56,
                    38,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:56:38Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    56,
                    38,
                    0,
                    265,
                    0
                ],
                "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning"
                },
                "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models."
                },
                "authors": [
                    {
                        "name": "Valentin Lacombe"
                    },
                    {
                        "name": "Valentin Quesnel"
                    },
                    {
                        "name": "Damien Sileo"
                    }
                ],
                "author_detail": {
                    "name": "Damien Sileo"
                },
                "author": "Damien Sileo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18076v1",
                "updated": "2025-09-22T17:55:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    55,
                    14,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:55:14Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    55,
                    14,
                    0,
                    265,
                    0
                ],
                "title": "Improving Large Language Models Function Calling and Interpretability\n  via Guided-Structured Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Models Function Calling and Interpretability\n  via Guided-Structured Templates"
                },
                "summary": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Hy Dang"
                    },
                    {
                        "name": "Tianyi Liu"
                    },
                    {
                        "name": "Zhuofeng Wu"
                    },
                    {
                        "name": "Jingfeng Yang"
                    },
                    {
                        "name": "Haoming Jiang"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Helen Wang"
                    },
                    {
                        "name": "Huasheng Li"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15423v2",
                "updated": "2025-09-22T17:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    54,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-18T21:02:27Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    21,
                    2,
                    27,
                    3,
                    261,
                    0
                ],
                "title": "Online Slip Detection and Friction Coefficient Estimation for Autonomous\n  Racing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Slip Detection and Friction Coefficient Estimation for Autonomous\n  Racing"
                },
                "summary": "Accurate knowledge of the tire-road friction coefficient (TRFC) is essential\nfor vehicle safety, stability, and performance, especially in autonomous\nracing, where vehicles often operate at the friction limit. However, TRFC\ncannot be directly measured with standard sensors, and existing estimation\nmethods either depend on vehicle or tire models with uncertain parameters or\nrequire large training datasets. In this paper, we present a lightweight\napproach for online slip detection and TRFC estimation. Our approach relies\nsolely on IMU and LiDAR measurements and the control actions, without special\ndynamical or tire models, parameter identification, or training data. Slip\nevents are detected in real time by comparing commanded and measured motions,\nand the TRFC is then estimated directly from observed accelerations under\nno-slip conditions. Experiments with a 1:10-scale autonomous racing car across\ndifferent friction levels demonstrate that the proposed approach achieves\naccurate and consistent slip detections and friction coefficients, with results\nclosely matching ground-truth measurements. These findings highlight the\npotential of our simple, deployable, and computationally efficient approach for\nreal-time slip monitoring and friction coefficient estimation in autonomous\ndriving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate knowledge of the tire-road friction coefficient (TRFC) is essential\nfor vehicle safety, stability, and performance, especially in autonomous\nracing, where vehicles often operate at the friction limit. However, TRFC\ncannot be directly measured with standard sensors, and existing estimation\nmethods either depend on vehicle or tire models with uncertain parameters or\nrequire large training datasets. In this paper, we present a lightweight\napproach for online slip detection and TRFC estimation. Our approach relies\nsolely on IMU and LiDAR measurements and the control actions, without special\ndynamical or tire models, parameter identification, or training data. Slip\nevents are detected in real time by comparing commanded and measured motions,\nand the TRFC is then estimated directly from observed accelerations under\nno-slip conditions. Experiments with a 1:10-scale autonomous racing car across\ndifferent friction levels demonstrate that the proposed approach achieves\naccurate and consistent slip detections and friction coefficients, with results\nclosely matching ground-truth measurements. These findings highlight the\npotential of our simple, deployable, and computationally efficient approach for\nreal-time slip monitoring and friction coefficient estimation in autonomous\ndriving."
                },
                "authors": [
                    {
                        "name": "Christopher Oeltjen"
                    },
                    {
                        "name": "Carson Sobolewski"
                    },
                    {
                        "name": "Saleh Faghfoorian"
                    },
                    {
                        "name": "Lorant Domokos"
                    },
                    {
                        "name": "Giancarlo Vidal"
                    },
                    {
                        "name": "Ivan Ruchkin"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Ruchkin"
                },
                "author": "Ivan Ruchkin",
                "arxiv_comment": "Equal contribution by the first three authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08283v2",
                "updated": "2025-09-22T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    48,
                    24,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-09T23:13:22Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    23,
                    13,
                    22,
                    0,
                    160,
                    0
                ],
                "title": "Serendipitous Recommendation with Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serendipitous Recommendation with Multimodal LLM"
                },
                "summary": "Conventional recommendation systems succeed in identifying relevant content\nbut often fail to provide users with surprising or novel items. Multimodal\nLarge Language Models (MLLMs) possess the world knowledge and multimodal\nunderstanding needed for serendipity, but their integration into\nbillion-item-scale platforms presents significant challenges. In this paper, we\npropose a novel hierarchical framework where fine-tuned MLLMs provide\nhigh-level guidance to conventional recommendation models, steering them\ntowards more serendipitous suggestions. This approach leverages MLLM strengths\nin understanding multimodal content and user interests while retaining the\nefficiency of traditional models for item-level recommendation. This mitigates\nthe complexity of applying MLLMs directly to vast action spaces. We also\ndemonstrate a chain-of-thought strategy enabling MLLMs to discover novel user\ninterests by first understanding video content and then identifying relevant\nyet unexplored interest clusters. Through live experiments within a commercial\nshort-form video platform serving billions of users, we show that our\nMLLM-powered approach significantly improves both recommendation serendipity\nand user satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional recommendation systems succeed in identifying relevant content\nbut often fail to provide users with surprising or novel items. Multimodal\nLarge Language Models (MLLMs) possess the world knowledge and multimodal\nunderstanding needed for serendipity, but their integration into\nbillion-item-scale platforms presents significant challenges. In this paper, we\npropose a novel hierarchical framework where fine-tuned MLLMs provide\nhigh-level guidance to conventional recommendation models, steering them\ntowards more serendipitous suggestions. This approach leverages MLLM strengths\nin understanding multimodal content and user interests while retaining the\nefficiency of traditional models for item-level recommendation. This mitigates\nthe complexity of applying MLLMs directly to vast action spaces. We also\ndemonstrate a chain-of-thought strategy enabling MLLMs to discover novel user\ninterests by first understanding video content and then identifying relevant\nyet unexplored interest clusters. Through live experiments within a commercial\nshort-form video platform serving billions of users, we show that our\nMLLM-powered approach significantly improves both recommendation serendipity\nand user satisfaction."
                },
                "authors": [
                    {
                        "name": "Haoting Wang"
                    },
                    {
                        "name": "Jianling Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Fangjun Yi"
                    },
                    {
                        "name": "Mengyu Fu"
                    },
                    {
                        "name": "Youwei Zhang"
                    },
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Minmin Chen"
                    },
                    {
                        "name": "Ed H. Chi"
                    },
                    {
                        "name": "Lichan Hong"
                    },
                    {
                        "name": "Haokai Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haokai Lu"
                },
                "author": "Haokai Lu",
                "arxiv_comment": "Accepted by 2025 Recsys EARL Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18063v1",
                "updated": "2025-09-22T17:40:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    40,
                    5,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:40:05Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    40,
                    5,
                    0,
                    265,
                    0
                ],
                "title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring\n  Commonsense Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring\n  Commonsense Reasoning"
                },
                "summary": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability."
                },
                "authors": [
                    {
                        "name": "Jan-Felix Klein"
                    },
                    {
                        "name": "Lars Ohnemus"
                    }
                ],
                "author_detail": {
                    "name": "Lars Ohnemus"
                },
                "author": "Lars Ohnemus",
                "arxiv_comment": "Work in Progess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18058v1",
                "updated": "2025-09-22T17:30:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:30:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM"
                },
                "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are subtly incorrect or otherwise harmless in practice. This\nbehavior emerges with hard-to-predict variations even within models from the\nsame model family. We find no apparent cause for the propensity to deceive, but\nwe show that more capable models are better at executing this strategy.\nStrategic dishonesty already has a practical impact on safety evaluations, as\nwe show that dishonest responses fool all output-based monitors used to detect\njailbreaks that we test, rendering benchmark scores unreliable. Further,\nstrategic dishonesty can act like a honeypot against malicious users, which\nnoticeably obfuscates prior jailbreak attacks. While output monitors fail, we\nshow that linear probes on internal activations can be used to reliably detect\nstrategic dishonesty. We validate probes on datasets with verifiable outcomes\nand by using their features as steering vectors. Overall, we consider strategic\ndishonesty as a concrete example of a broader concern that alignment of LLMs is\nhard to control, especially when helpfulness and harmlessness conflict."
                },
                "authors": [
                    {
                        "name": "Alexander Panfilov"
                    },
                    {
                        "name": "Evgenii Kortukov"
                    },
                    {
                        "name": "Kristina Nikoli"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v1",
                "updated": "2025-09-22T17:30:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve provable limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve provable limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18056v1",
                "updated": "2025-09-22T17:30:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    15,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:30:15Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    15,
                    0,
                    265,
                    0
                ],
                "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs"
                },
                "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1"
                },
                "authors": [
                    {
                        "name": "Yunheng Li"
                    },
                    {
                        "name": "Jing Cheng"
                    },
                    {
                        "name": "Shaoyong Jia"
                    },
                    {
                        "name": "Hangyi Kuang"
                    },
                    {
                        "name": "Shaohui Jiao"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Ming Cheng"
                },
                "author": "Ming-Ming Cheng",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18054v1",
                "updated": "2025-09-22T17:29:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    29,
                    10,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:29:10Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    29,
                    10,
                    0,
                    265,
                    0
                ],
                "title": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for\n  Algorithm Selection in the Facility Layout Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for\n  Algorithm Selection in the Facility Layout Problem"
                },
                "summary": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot."
                },
                "authors": [
                    {
                        "name": "Nikhil N S"
                    },
                    {
                        "name": "Amol Dilip Joshi"
                    },
                    {
                        "name": "Bilal Muhammed"
                    },
                    {
                        "name": "Soban Babu"
                    }
                ],
                "author_detail": {
                    "name": "Soban Babu"
                },
                "arxiv_affiliation": "TCS Research, Tata Consultancy Services Ltd",
                "author": "Soban Babu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18052v1",
                "updated": "2025-09-22T17:27:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    27,
                    29,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:27:29Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    27,
                    29,
                    0,
                    265,
                    0
                ],
                "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM\n  Societies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM\n  Societies"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\""
                },
                "authors": [
                    {
                        "name": "Jiaxu Zhou"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Man Ho Lam"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18044v1",
                "updated": "2025-09-22T17:18:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    18,
                    59,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:18:59Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    18,
                    59,
                    0,
                    265,
                    0
                ],
                "title": "Hybrid Reputation Aggregation: A Robust Defense Mechanism for\n  Adversarial Federated Learning in 5G and Edge Network Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Reputation Aggregation: A Robust Defense Mechanism for\n  Adversarial Federated Learning in 5G and Edge Network Environments"
                },
                "summary": "Federated Learning (FL) in 5G and edge network environments face severe\nsecurity threats from adversarial clients. Malicious participants can perform\nlabel flipping, inject backdoor triggers, or launch Sybil attacks to corrupt\nthe global model. This paper introduces Hybrid Reputation Aggregation (HRA), a\nnovel robust aggregation mechanism designed to defend against diverse\nadversarial behaviors in FL without prior knowledge of the attack type. HRA\ncombines geometric anomaly detection with momentum-based reputation tracking of\nclients. In each round, it detects outlier model updates via distance-based\ngeometric analysis while continuously updating a trust score for each client\nbased on historical behavior. This hybrid approach enables adaptive filtering\nof suspicious updates and long-term penalization of unreliable clients,\ncountering attacks ranging from backdoor insertions to random noise Byzantine\nfailures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+\nrecords) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse\nadversarial attack scenarios. Experimental results reveal that HRA achieves\nrobust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on\nNF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,\nTrimmed Mean, and Bulyan by significant margins. Our ablation studies further\ndemonstrate that the full hybrid system achieves 98.66% accuracy, while the\nanomaly-only and reputation-only variants drop to 84.77% and 78.52%,\nrespectively, validating the synergistic value of our dual-mechanism approach.\nThis demonstrates HRA's enhanced resilience and robustness in 5G/edge federated\nlearning deployments, even under significant adversarial conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) in 5G and edge network environments face severe\nsecurity threats from adversarial clients. Malicious participants can perform\nlabel flipping, inject backdoor triggers, or launch Sybil attacks to corrupt\nthe global model. This paper introduces Hybrid Reputation Aggregation (HRA), a\nnovel robust aggregation mechanism designed to defend against diverse\nadversarial behaviors in FL without prior knowledge of the attack type. HRA\ncombines geometric anomaly detection with momentum-based reputation tracking of\nclients. In each round, it detects outlier model updates via distance-based\ngeometric analysis while continuously updating a trust score for each client\nbased on historical behavior. This hybrid approach enables adaptive filtering\nof suspicious updates and long-term penalization of unreliable clients,\ncountering attacks ranging from backdoor insertions to random noise Byzantine\nfailures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+\nrecords) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse\nadversarial attack scenarios. Experimental results reveal that HRA achieves\nrobust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on\nNF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,\nTrimmed Mean, and Bulyan by significant margins. Our ablation studies further\ndemonstrate that the full hybrid system achieves 98.66% accuracy, while the\nanomaly-only and reputation-only variants drop to 84.77% and 78.52%,\nrespectively, validating the synergistic value of our dual-mechanism approach.\nThis demonstrates HRA's enhanced resilience and robustness in 5G/edge federated\nlearning deployments, even under significant adversarial conditions."
                },
                "authors": [
                    {
                        "name": "Saeid Sheikhi"
                    },
                    {
                        "name": "Panos Kostakos"
                    },
                    {
                        "name": "Lauri Loven"
                    }
                ],
                "author_detail": {
                    "name": "Lauri Loven"
                },
                "author": "Lauri Loven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18040v1",
                "updated": "2025-09-22T17:14:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    14,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:14:40Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    14,
                    40,
                    0,
                    265,
                    0
                ],
                "title": "Detection of Misreporting Attacks on Software-Defined Immersive\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Misreporting Attacks on Software-Defined Immersive\n  Environments"
                },
                "summary": "The ability to centrally control network infrastructure using a programmable\nmiddleware has made Software-Defined Networking (SDN) ideal for emerging\napplications, such as immersive environments. However, such flexibility\nintroduces new vulnerabilities, such as switch misreporting led load imbalance,\nwhich in turn make such immersive environment vulnerable to severe quality\ndegradation. In this paper, we present a hybrid machine learning (ML)-based\nnetwork anomaly detection framework that identifies such stealthy misreporting\nby capturing temporal inconsistencies in switch-reported loads, and thereby\ncounter potentially catastrophic quality degradation of hosted immersive\napplication. The detection system combines unsupervised anomaly scoring with\nsupervised classification to robustly distinguish malicious behavior. Data\ncollected from a realistic testbed deployment under both benign and adversarial\nconditions is used to train and evaluate the model. Experimental results show\nthat the framework achieves high recall in detecting misreporting behavior,\nmaking it effective for early and reliable detection in SDN environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to centrally control network infrastructure using a programmable\nmiddleware has made Software-Defined Networking (SDN) ideal for emerging\napplications, such as immersive environments. However, such flexibility\nintroduces new vulnerabilities, such as switch misreporting led load imbalance,\nwhich in turn make such immersive environment vulnerable to severe quality\ndegradation. In this paper, we present a hybrid machine learning (ML)-based\nnetwork anomaly detection framework that identifies such stealthy misreporting\nby capturing temporal inconsistencies in switch-reported loads, and thereby\ncounter potentially catastrophic quality degradation of hosted immersive\napplication. The detection system combines unsupervised anomaly scoring with\nsupervised classification to robustly distinguish malicious behavior. Data\ncollected from a realistic testbed deployment under both benign and adversarial\nconditions is used to train and evaluate the model. Experimental results show\nthat the framework achieves high recall in detecting misreporting behavior,\nmaking it effective for early and reliable detection in SDN environments."
                },
                "authors": [
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Shima Yousefi"
                    },
                    {
                        "name": "Saptarshi Debroy"
                    }
                ],
                "author_detail": {
                    "name": "Saptarshi Debroy"
                },
                "author": "Saptarshi Debroy",
                "arxiv_comment": "7 Pages, 7 Images, will appear in CNSM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00300v2",
                "updated": "2025-09-22T17:11:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    11,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2024-11-01T01:40:23Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    1,
                    40,
                    23,
                    4,
                    306,
                    0
                ],
                "title": "Rationale-Guided Retrieval Augmented Generation for Medical Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Guided Retrieval Augmented Generation for Medical Question\n  Answering"
                },
                "summary": "Large language models (LLM) hold significant potential for applications in\nbiomedicine, but they struggle with hallucinations and outdated knowledge.\nWhile retrieval-augmented generation (RAG) is generally employed to address\nthese issues, it also has its own set of challenges: (1) LLMs are vulnerable to\nirrelevant or incorrect context, (2) medical queries are often not\nwell-targeted for helpful information, and (3) retrievers are prone to bias\ntoward the specific source corpus they were trained on. In this study, we\npresent RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the\nreliability of RAG in biomedical contexts. RAG$^2$ incorporates three key\ninnovations: a small filtering model trained on perplexity-based labels of\nrationales, which selectively augments informative snippets of documents while\nfiltering out distractors; LLM-generated rationales as queries to improve the\nutility of retrieved snippets; a structure designed to retrieve snippets evenly\nfrom a comprehensive set of four biomedical corpora, effectively mitigating\nretriever bias. Our experiments demonstrate that RAG$^2$ improves the\nstate-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and\nit outperforms the previous best medical RAG model by up to 5.6\\% across three\nmedical question-answering benchmarks. Our code is available at\nhttps://github.com/dmis-lab/RAG2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) hold significant potential for applications in\nbiomedicine, but they struggle with hallucinations and outdated knowledge.\nWhile retrieval-augmented generation (RAG) is generally employed to address\nthese issues, it also has its own set of challenges: (1) LLMs are vulnerable to\nirrelevant or incorrect context, (2) medical queries are often not\nwell-targeted for helpful information, and (3) retrievers are prone to bias\ntoward the specific source corpus they were trained on. In this study, we\npresent RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the\nreliability of RAG in biomedical contexts. RAG$^2$ incorporates three key\ninnovations: a small filtering model trained on perplexity-based labels of\nrationales, which selectively augments informative snippets of documents while\nfiltering out distractors; LLM-generated rationales as queries to improve the\nutility of retrieved snippets; a structure designed to retrieve snippets evenly\nfrom a comprehensive set of four biomedical corpora, effectively mitigating\nretriever bias. Our experiments demonstrate that RAG$^2$ improves the\nstate-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and\nit outperforms the previous best medical RAG model by up to 5.6\\% across three\nmedical question-answering benchmarks. Our code is available at\nhttps://github.com/dmis-lab/RAG2."
                },
                "authors": [
                    {
                        "name": "Jiwoong Sohn"
                    },
                    {
                        "name": "Yein Park"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Sihyeon Park"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Mujeen Sung"
                    },
                    {
                        "name": "Hyunjae Kim"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "arxiv_comment": "Accepted to NAACL 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02954v2",
                "updated": "2025-09-22T17:05:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    5,
                    3,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-29T19:54:57Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    19,
                    54,
                    57,
                    6,
                    180,
                    0
                ],
                "title": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of\n  Large Language Models on CFA Level III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of\n  Large Language Models on CFA Level III"
                },
                "summary": "As financial institutions increasingly adopt Large Language Models (LLMs),\nrigorous domain-specific evaluation becomes critical for responsible\ndeployment. This paper presents a comprehensive benchmark evaluating 23\nstate-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -\nthe gold standard for advanced financial reasoning. We assess both\nmultiple-choice questions (MCQs) and essay-style responses using multiple\nprompting strategies including Chain-of-Thought and Self-Discover. Our\nevaluation reveals that leading models demonstrate strong capabilities, with\ncomposite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA\nLevel III. These results, achieved under a revised, stricter essay grading\nmethodology, indicate significant progress in LLM capabilities for high-stakes\nfinancial applications. Our findings provide crucial guidance for practitioners\non model selection and highlight remaining challenges in cost-effective\ndeployment and the need for nuanced interpretation of performance against\nprofessional benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial institutions increasingly adopt Large Language Models (LLMs),\nrigorous domain-specific evaluation becomes critical for responsible\ndeployment. This paper presents a comprehensive benchmark evaluating 23\nstate-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -\nthe gold standard for advanced financial reasoning. We assess both\nmultiple-choice questions (MCQs) and essay-style responses using multiple\nprompting strategies including Chain-of-Thought and Self-Discover. Our\nevaluation reveals that leading models demonstrate strong capabilities, with\ncomposite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA\nLevel III. These results, achieved under a revised, stricter essay grading\nmethodology, indicate significant progress in LLM capabilities for high-stakes\nfinancial applications. Our findings provide crucial guidance for practitioners\non model selection and highlight remaining challenges in cost-effective\ndeployment and the need for nuanced interpretation of performance against\nprofessional benchmarks."
                },
                "authors": [
                    {
                        "name": "Pranam Shetty"
                    },
                    {
                        "name": "Abhisek Upadhayaya"
                    },
                    {
                        "name": "Parth Mitesh Shah"
                    },
                    {
                        "name": "Srikanth Jagabathula"
                    },
                    {
                        "name": "Shilpi Nayak"
                    },
                    {
                        "name": "Anna Joo Fee"
                    }
                ],
                "author_detail": {
                    "name": "Anna Joo Fee"
                },
                "author": "Anna Joo Fee",
                "arxiv_comment": "Accepted at FinLLM @ IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18030v1",
                "updated": "2025-09-22T17:03:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    3,
                    48,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:03:48Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    3,
                    48,
                    0,
                    265,
                    0
                ],
                "title": "RadEval: A framework for radiology text evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadEval: A framework for radiology text evaluation"
                },
                "summary": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration."
                },
                "authors": [
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Javid Abderezaei"
                    },
                    {
                        "name": "Julie Bauml"
                    },
                    {
                        "name": "Roger Boodoo"
                    },
                    {
                        "name": "Fatemeh Haghighi"
                    },
                    {
                        "name": "Ali Ganjizadeh"
                    },
                    {
                        "name": "Eric Brattain"
                    },
                    {
                        "name": "Dave Van Veen"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    },
                    {
                        "name": "David Eyre"
                    },
                    {
                        "name": "Jean-Benoit Delbrouck"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Benoit Delbrouck"
                },
                "author": "Jean-Benoit Delbrouck",
                "arxiv_comment": "Accepted to EMNLP 2025 Demo track - Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15712v2",
                "updated": "2025-09-22T17:02:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    2,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-21T16:22:32Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    22,
                    32,
                    2,
                    141,
                    0
                ],
                "title": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games"
                },
                "summary": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Muyu He"
                    },
                    {
                        "name": "Muhammad Adil Shahid"
                    },
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "In EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18015v1",
                "updated": "2025-09-22T16:54:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    54,
                    23,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:54:23Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    54,
                    23,
                    0,
                    265,
                    0
                ],
                "title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization\n  in Chest Radiographs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization\n  in Chest Radiographs"
                },
                "summary": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use."
                },
                "authors": [
                    {
                        "name": "Advait Gosai"
                    },
                    {
                        "name": "Arun Kavishwar"
                    },
                    {
                        "name": "Stephanie L. McNamara"
                    },
                    {
                        "name": "Soujanya Samineni"
                    },
                    {
                        "name": "Renato Umeton"
                    },
                    {
                        "name": "Alexander Chowdhury"
                    },
                    {
                        "name": "William Lotter"
                    }
                ],
                "author_detail": {
                    "name": "William Lotter"
                },
                "author": "William Lotter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07287v2",
                "updated": "2025-09-22T16:50:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    50,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-04-09T21:27:54Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    27,
                    54,
                    2,
                    99,
                    0
                ],
                "title": "Hybrid Privilege Escalation and Remote Code Execution Exploit Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Privilege Escalation and Remote Code Execution Exploit Chains"
                },
                "summary": "Research on exploit chains predominantly focuses on sequences with one type\nof exploit, e.g., either escalating privileges on a machine or executing remote\ncode. In networks, hybrid exploit chains are critical because of their linkable\nvulnerabilities. Moreover, developing hybrid exploit chains is challenging\nbecause it requires understanding the diverse and independent dependencies and\noutcomes. We present hybrid chains encompassing privilege escalation (PE) and\nremote code execution (RCE) exploits. These chains are executable and can span\nlarge networks, where numerous potential exploit combinations arise from the\nlarge array of network assets, their hardware, software, configurations, and\nvulnerabilities. The chains are generated by ALFA-Chains, an AI-supported\nframework for the automated discovery of multi-step PE and RCE exploit chains\nin networks across arbitrary environments and segmented networks. Through an\nLLM-based classification, ALFA-Chains describes exploits in Planning Domain\nDescription Language (PDDL). PDDL exploit and network descriptions then use\noff-the-shelf AI planners to find multiple exploit chains. ALFA-Chains finds 12\nunknown chains on an example with a known three-step chain. A red-team exercise\nvalidates the executability with Metasploit. ALFA-Chains is efficient, finding\nan exploit chain in 0.01 seconds in an enterprise network with 83\nvulnerabilities, 20 hosts, and 6 subnets. In addition, it is scalable, it finds\nan exploit chain in an industrial network with 114 vulnerabilities, 200 hosts,\nand 6 subnets in 3.16 seconds. It is comprehensive, finding 13 exploit chains\nin 26.26 seconds in the network. Finally, ALFA-Chains demonstrates flexibility\nacross different exploit sources, ability to generalize across diverse network\ntypes, and robustness in discovering chains under constrained privilege\nassumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on exploit chains predominantly focuses on sequences with one type\nof exploit, e.g., either escalating privileges on a machine or executing remote\ncode. In networks, hybrid exploit chains are critical because of their linkable\nvulnerabilities. Moreover, developing hybrid exploit chains is challenging\nbecause it requires understanding the diverse and independent dependencies and\noutcomes. We present hybrid chains encompassing privilege escalation (PE) and\nremote code execution (RCE) exploits. These chains are executable and can span\nlarge networks, where numerous potential exploit combinations arise from the\nlarge array of network assets, their hardware, software, configurations, and\nvulnerabilities. The chains are generated by ALFA-Chains, an AI-supported\nframework for the automated discovery of multi-step PE and RCE exploit chains\nin networks across arbitrary environments and segmented networks. Through an\nLLM-based classification, ALFA-Chains describes exploits in Planning Domain\nDescription Language (PDDL). PDDL exploit and network descriptions then use\noff-the-shelf AI planners to find multiple exploit chains. ALFA-Chains finds 12\nunknown chains on an example with a known three-step chain. A red-team exercise\nvalidates the executability with Metasploit. ALFA-Chains is efficient, finding\nan exploit chain in 0.01 seconds in an enterprise network with 83\nvulnerabilities, 20 hosts, and 6 subnets. In addition, it is scalable, it finds\nan exploit chain in an industrial network with 114 vulnerabilities, 200 hosts,\nand 6 subnets in 3.16 seconds. It is comprehensive, finding 13 exploit chains\nin 26.26 seconds in the network. Finally, ALFA-Chains demonstrates flexibility\nacross different exploit sources, ability to generalize across diverse network\ntypes, and robustness in discovering chains under constrained privilege\nassumptions."
                },
                "authors": [
                    {
                        "name": "Miguel Tulla"
                    },
                    {
                        "name": "Andrea Vignali"
                    },
                    {
                        "name": "Christian Colon"
                    },
                    {
                        "name": "Giancarlo Sperli"
                    },
                    {
                        "name": "Simon Pietro Romano"
                    },
                    {
                        "name": "Masataro Asai"
                    },
                    {
                        "name": "Una-May O'Reilly"
                    },
                    {
                        "name": "Erik Hemberg"
                    }
                ],
                "author_detail": {
                    "name": "Erik Hemberg"
                },
                "author": "Erik Hemberg",
                "arxiv_comment": "16 pages, 12 Tables, 6 Figures, 7 Listing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03116v2",
                "updated": "2025-09-22T16:47:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    47,
                    45,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-03T08:19:13Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    19,
                    13,
                    2,
                    246,
                    0
                ],
                "title": "Measuring Scalar Constructs in Social Science with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Scalar Constructs in Social Science with LLMs"
                },
                "summary": "Many constructs that characterize language, like its complexity or\nemotionality, have a naturally continuous semantic structure; a public speech\nis not just \"simple\" or \"complex,\" but exists on a continuum between extremes.\nAlthough large language models (LLMs) are an attractive tool for measuring\nscalar constructs, their idiosyncratic treatment of numerical outputs raises\nquestions of how to best apply them. We address these questions with a\ncomprehensive evaluation of LLM-based approaches to scalar construct\nmeasurement in social science. Using multiple datasets sourced from the\npolitical science literature, we evaluate four approaches: unweighted direct\npointwise scoring, aggregation of pairwise comparisons,\ntoken-probability-weighted pointwise scoring, and finetuning. Our study finds\nthat pairwise comparisons made by LLMs produce better measurements than simply\nprompting the LLM to directly output the scores, which suffers from bunching\naround arbitrary numbers. However, taking the weighted mean over the token\nprobability of scores further improves the measurements over the two previous\napproaches. Finally, finetuning smaller models with as few as 1,000 training\npairs can match or exceed the performance of prompted LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many constructs that characterize language, like its complexity or\nemotionality, have a naturally continuous semantic structure; a public speech\nis not just \"simple\" or \"complex,\" but exists on a continuum between extremes.\nAlthough large language models (LLMs) are an attractive tool for measuring\nscalar constructs, their idiosyncratic treatment of numerical outputs raises\nquestions of how to best apply them. We address these questions with a\ncomprehensive evaluation of LLM-based approaches to scalar construct\nmeasurement in social science. Using multiple datasets sourced from the\npolitical science literature, we evaluate four approaches: unweighted direct\npointwise scoring, aggregation of pairwise comparisons,\ntoken-probability-weighted pointwise scoring, and finetuning. Our study finds\nthat pairwise comparisons made by LLMs produce better measurements than simply\nprompting the LLM to directly output the scores, which suffers from bunching\naround arbitrary numbers. However, taking the weighted mean over the token\nprobability of scores further improves the measurements over the two previous\napproaches. Finally, finetuning smaller models with as few as 1,000 training\npairs can match or exceed the performance of prompted LLMs."
                },
                "authors": [
                    {
                        "name": "Hauke Licht"
                    },
                    {
                        "name": "Rupak Sarkar"
                    },
                    {
                        "name": "Patrick Y. Wu"
                    },
                    {
                        "name": "Pranav Goel"
                    },
                    {
                        "name": "Niklas Stoehr"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Alexander Miserlis Hoyle"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Miserlis Hoyle"
                },
                "author": "Alexander Miserlis Hoyle",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18008v1",
                "updated": "2025-09-22T16:47:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    47,
                    8,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:47:08Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    47,
                    8,
                    0,
                    265,
                    0
                ],
                "title": "Through the Lens of Human-Human Collaboration: A Configurable Research\n  Platform for Exploring Human-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Lens of Human-Human Collaboration: A Configurable Research\n  Platform for Exploring Human-Agent Collaboration"
                },
                "summary": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis."
                },
                "authors": [
                    {
                        "name": "Bingsheng Yao"
                    },
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "April Wang"
                    },
                    {
                        "name": "Toby Jia-jun Li"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13557v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13557v4",
                "updated": "2025-09-22T16:46:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    46,
                    17,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-16T21:52:04Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    21,
                    52,
                    4,
                    1,
                    259,
                    0
                ],
                "title": "MALTA: An Automated CGRA Design Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALTA: An Automated CGRA Design Framework"
                },
                "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MALTA-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MALTA iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MALTA efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MALTA-- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MALTA iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MALTA efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design."
                },
                "authors": [
                    {
                        "name": "Zesong Jiang"
                    },
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qing Zhong"
                    },
                    {
                        "name": "Mahathi Krishna"
                    },
                    {
                        "name": "Deepak Patil"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    },
                    {
                        "name": "Jeff Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Zhang"
                },
                "author": "Jeff Zhang",
                "arxiv_comment": "Due to certain confidentiality requirements, this article needs to be\n  withdrawn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13557v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13557v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18005v1",
                "updated": "2025-09-22T16:44:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    44,
                    34,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:44:34Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    44,
                    34,
                    0,
                    265,
                    0
                ],
                "title": "M3ET: Efficient Vision-Language Learning for Robotics based on\n  Multimodal Mamba-Enhanced Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3ET: Efficient Vision-Language Learning for Robotics based on\n  Multimodal Mamba-Enhanced Transformer"
                },
                "summary": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms."
                },
                "authors": [
                    {
                        "name": "Yanxin Zhang"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Zeyi Kang"
                    },
                    {
                        "name": "Zuheng Ming"
                    },
                    {
                        "name": "Kaixing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Kaixing Zhao"
                },
                "arxiv_affiliation": "School of Software Yangtze River Delta Research Institute",
                "author": "Kaixing Zhao",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18761v2",
                "updated": "2025-09-22T16:41:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    41,
                    27,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-24T15:56:22Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    15,
                    56,
                    22,
                    5,
                    144,
                    0
                ],
                "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using\n  a Controlled Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using\n  a Controlled Benchmark"
                },
                "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions."
                },
                "authors": [
                    {
                        "name": "Minglai Yang"
                    },
                    {
                        "name": "Ethan Huang"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "William Wang"
                    },
                    {
                        "name": "Liangming Pan"
                    }
                ],
                "author_detail": {
                    "name": "Liangming Pan"
                },
                "author": "Liangming Pan",
                "arxiv_comment": "19 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17998v1",
                "updated": "2025-09-22T16:39:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    12,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:39:12Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    12,
                    0,
                    265,
                    0
                ],
                "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs"
                },
                "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/cake4bo/cake."
                },
                "authors": [
                    {
                        "name": "Richard Cornelius Suwandi"
                    },
                    {
                        "name": "Feng Yin"
                    },
                    {
                        "name": "Juntao Wang"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Tsung-Hui Chang"
                    },
                    {
                        "name": "Sergios Theodoridis"
                    }
                ],
                "author_detail": {
                    "name": "Sergios Theodoridis"
                },
                "author": "Sergios Theodoridis",
                "arxiv_comment": "Accepted as Poster at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17995v1",
                "updated": "2025-09-22T16:36:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    36,
                    56,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:36:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    36,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Variation in Verification: Understanding Verification Dynamics in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variation in Verification: Understanding Verification Dynamics in Large\n  Language Models"
                },
                "summary": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges."
                },
                "authors": [
                    {
                        "name": "Yefan Zhou"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Janvijay Singh"
                    },
                    {
                        "name": "Jiang Gui"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04183v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04183v4",
                "updated": "2025-09-23T03:53:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    53,
                    5,
                    1,
                    266,
                    0
                ],
                "published": "2024-09-06T10:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    57,
                    34,
                    4,
                    250,
                    0
                ],
                "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GALLa: Graph Aligned Large Language Models for Improved Source Code\n  Understanding"
                },
                "summary": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming languages possess rich semantic information - such as data flow -\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with seven different baseline LLMs ranging in size from 350M to 14B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04183v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04183v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16531v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16531v4",
                "updated": "2025-09-22T16:30:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    30,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2024-10-21T21:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    45,
                    22,
                    0,
                    295,
                    0
                ],
                "title": "Bayesian scaling laws for in-context learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian scaling laws for in-context learning"
                },
                "summary": "In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a novel Bayesian scaling law for ICL. In experiments\nwith \\mbox{GPT-2} models of different sizes, our scaling law matches existing\nscaling laws in accuracy while also offering interpretable terms for task\npriors, learning efficiency, and per-example probabilities. To illustrate the\nanalytic power that such interpretable scaling laws provide, we report on\ncontrolled synthetic dataset experiments designed to inform real-world studies\nof safety alignment. In our experimental protocol, we use SFT or DPO to\nsuppress an unwanted existing model capability and then use ICL to try to bring\nthat capability back (many-shot jailbreaking). We then study ICL on real-world\ninstruction-tuned LLMs using capabilities benchmarks as well as a new many-shot\njailbreaking dataset. In all cases, Bayesian scaling laws accurately predict\nthe conditions under which ICL will cause suppressed behaviors to reemerge,\nwhich sheds light on the ineffectiveness of post-training at increasing LLM\nsafety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a novel Bayesian scaling law for ICL. In experiments\nwith \\mbox{GPT-2} models of different sizes, our scaling law matches existing\nscaling laws in accuracy while also offering interpretable terms for task\npriors, learning efficiency, and per-example probabilities. To illustrate the\nanalytic power that such interpretable scaling laws provide, we report on\ncontrolled synthetic dataset experiments designed to inform real-world studies\nof safety alignment. In our experimental protocol, we use SFT or DPO to\nsuppress an unwanted existing model capability and then use ICL to try to bring\nthat capability back (many-shot jailbreaking). We then study ICL on real-world\ninstruction-tuned LLMs using capabilities benchmarks as well as a new many-shot\njailbreaking dataset. In all cases, Bayesian scaling laws accurately predict\nthe conditions under which ICL will cause suppressed behaviors to reemerge,\nwhich sheds light on the ineffectiveness of post-training at increasing LLM\nsafety."
                },
                "authors": [
                    {
                        "name": "Aryaman Arora"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "COLM 2025 camera-ready version; 9 pages main text, 39 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16531v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16531v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17965v1",
                "updated": "2025-09-22T16:18:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    18,
                    5,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:18:05Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    18,
                    5,
                    0,
                    265,
                    0
                ],
                "title": "Benchmarking Humans and Machines on Complex Multilingual Speech\n  Understanding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Humans and Machines on Complex Multilingual Speech\n  Understanding Tasks"
                },
                "summary": "Auditory attention and selective phase-locking are central to human speech\nunderstanding in complex acoustic scenes and cocktail party settings, yet these\ncapabilities in multilingual subjects remain poorly understood. While machine\nunderstanding of natural speech has advanced in recent years, questions persist\nabout comprehension of overlapped and mixed-channel speech. We propose a\nsystematic paradigm for studying humans and machines in speech\nquestion-answering tasks in multilingual settings with clean and mixed-channel\nspeech. For human listeners, selective attention to a target speaker was\nsignificantly better in their native language (L1) than in their second\nlanguage (L2). For machine listening, speech-based large language models (LLMs)\nmatch or exceed human performance in clean, single-speaker conditions but often\nstruggle to selectively attend in two-speaker settings. These results reveal a\nkey divergence: humans rely on attentional cues that are more streamlined in\ntheir native language, whereas LLMs default to parallel information extraction\nwhich exceed human skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditory attention and selective phase-locking are central to human speech\nunderstanding in complex acoustic scenes and cocktail party settings, yet these\ncapabilities in multilingual subjects remain poorly understood. While machine\nunderstanding of natural speech has advanced in recent years, questions persist\nabout comprehension of overlapped and mixed-channel speech. We propose a\nsystematic paradigm for studying humans and machines in speech\nquestion-answering tasks in multilingual settings with clean and mixed-channel\nspeech. For human listeners, selective attention to a target speaker was\nsignificantly better in their native language (L1) than in their second\nlanguage (L2). For machine listening, speech-based large language models (LLMs)\nmatch or exceed human performance in clean, single-speaker conditions but often\nstruggle to selectively attend in two-speaker settings. These results reveal a\nkey divergence: humans rely on attentional cues that are more streamlined in\ntheir native language, whereas LLMs default to parallel information extraction\nwhich exceed human skills."
                },
                "authors": [
                    {
                        "name": "Sai Samrat Kankanala"
                    },
                    {
                        "name": "Ram Chandra"
                    },
                    {
                        "name": "Sriram Ganapathy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Ganapathy"
                },
                "author": "Sriram Ganapathy",
                "arxiv_comment": "5 Pages, 1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17961v1",
                "updated": "2025-09-22T16:15:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    15,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:15:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    15,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants'\n  Question-Answering in Asynchronous Learning Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants'\n  Question-Answering in Asynchronous Learning Environments"
                },
                "summary": "Asynchronous learning environments (ALEs) are widely adopted for formal and\ninformal learning, but timely and personalized support is often limited. In\nthis context, Virtual Teaching Assistants (VTAs) can potentially reduce the\nworkload of instructors, but rigorous and pedagogically sound evaluation is\nessential. Existing assessments often rely on surface-level metrics and lack\nsufficient grounding in educational theories, making it difficult to\nmeaningfully compare the pedagogical effectiveness of different VTA systems. To\nbridge this gap, we propose an evaluation framework rooted in learning sciences\nand tailored to asynchronous forum discussions, a common VTA deployment context\nin ALE. We construct classifiers using expert annotations of VTA responses on a\ndiverse set of forum posts. We evaluate the effectiveness of our classifiers,\nidentifying approaches that improve accuracy as well as challenges that hinder\ngeneralization. Our work establishes a foundation for theory-driven evaluation\nof VTA systems, paving the way for more pedagogically effective AI in\neducation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous learning environments (ALEs) are widely adopted for formal and\ninformal learning, but timely and personalized support is often limited. In\nthis context, Virtual Teaching Assistants (VTAs) can potentially reduce the\nworkload of instructors, but rigorous and pedagogically sound evaluation is\nessential. Existing assessments often rely on surface-level metrics and lack\nsufficient grounding in educational theories, making it difficult to\nmeaningfully compare the pedagogical effectiveness of different VTA systems. To\nbridge this gap, we propose an evaluation framework rooted in learning sciences\nand tailored to asynchronous forum discussions, a common VTA deployment context\nin ALE. We construct classifiers using expert annotations of VTA responses on a\ndiverse set of forum posts. We evaluate the effectiveness of our classifiers,\nidentifying approaches that improve accuracy as well as challenges that hinder\ngeneralization. Our work establishes a foundation for theory-driven evaluation\nof VTA systems, paving the way for more pedagogically effective AI in\neducation."
                },
                "authors": [
                    {
                        "name": "Li Siyan"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Vethavikashini Chithrra Raghuram"
                    },
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Renzhe Yu"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "arxiv_comment": "Accepted in EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17946v1",
                "updated": "2025-09-22T16:07:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    7,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:07:11Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    7,
                    11,
                    0,
                    265,
                    0
                ],
                "title": "HICode: Hierarchical Inductive Coding with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HICode: Hierarchical Inductive Coding with LLMs"
                },
                "summary": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data."
                },
                "authors": [
                    {
                        "name": "Mian Zhong"
                    },
                    {
                        "name": "Pristina Wang"
                    },
                    {
                        "name": "Anjalie Field"
                    }
                ],
                "author_detail": {
                    "name": "Anjalie Field"
                },
                "author": "Anjalie Field",
                "arxiv_comment": "Long paper accepted at EMNLP 2025 main conference, 19 pages, 8\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17941v1",
                "updated": "2025-09-22T16:04:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    4,
                    50,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T16:04:50Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    4,
                    50,
                    0,
                    265,
                    0
                ],
                "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments\n  via Composable Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComposableNav: Instruction-Following Navigation in Dynamic Environments\n  via Composable Diffusion"
                },
                "summary": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/"
                },
                "authors": [
                    {
                        "name": "Zichao Hu"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Michael J. Munje"
                    },
                    {
                        "name": "Yifeng Zhu"
                    },
                    {
                        "name": "Alex Liu"
                    },
                    {
                        "name": "Shuijing Liu"
                    },
                    {
                        "name": "Garrett Warnell"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Joydeep Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Joydeep Biswas"
                },
                "author": "Joydeep Biswas",
                "arxiv_comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://amrl.cs.utexas.edu/ComposableNav/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17938v1",
                "updated": "2025-09-22T15:59:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:59:40Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    59,
                    40,
                    0,
                    265,
                    0
                ],
                "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language\n  Models"
                },
                "summary": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs."
                },
                "authors": [
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Rahul Gupta"
                    },
                    {
                        "name": "Eliot Krzysztof Jones"
                    },
                    {
                        "name": "Nick Winter"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Spyros Matsoukas"
                    }
                ],
                "author_detail": {
                    "name": "Spyros Matsoukas"
                },
                "author": "Spyros Matsoukas",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15017v2",
                "updated": "2025-09-22T15:55:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    55,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-21T01:55:04Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    55,
                    4,
                    2,
                    141,
                    0
                ],
                "title": "PsyScam: A Benchmark for Psychological Techniques in Real-World Scams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyScam: A Benchmark for Psychological Techniques in Real-World Scams"
                },
                "summary": "Over the years, online scams have grown dramatically, with nearly 50% of\nglobal consumers encountering scam attempts each week. These scams cause not\nonly significant financial losses to individuals and businesses, but also\nlasting psychological trauma, largely due to scammers' strategic employment of\npsychological techniques (PTs) to manipulate victims. Meanwhile, scammers\ncontinually evolve their tactics by leveraging advances in Large Language\nModels (LLMs) to generate diverse scam variants that easily bypass existing\ndefenses.\n  To address this pressing problem, we introduce PsyScam, a benchmark designed\nto systematically capture the PTs employed in real-world scam reports, and\ninvestigate how LLMs can be utilized to generate variants of scams based on the\nPTs and the contexts provided by these scams. Specifically, we collect a wide\nrange of scam reports and ground its annotations of employed PTs in\nwell-established cognitive and psychological theories. We further demonstrate\nLLMs' capabilities in generating through two downstream tasks: scam completion,\nand scam augmentation. Experimental results show that PsyScam presents\nsignificant challenges to existing models in both detecting and generating scam\ncontent based on the PTs used by real-world scammers. Our code and dataset are\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the years, online scams have grown dramatically, with nearly 50% of\nglobal consumers encountering scam attempts each week. These scams cause not\nonly significant financial losses to individuals and businesses, but also\nlasting psychological trauma, largely due to scammers' strategic employment of\npsychological techniques (PTs) to manipulate victims. Meanwhile, scammers\ncontinually evolve their tactics by leveraging advances in Large Language\nModels (LLMs) to generate diverse scam variants that easily bypass existing\ndefenses.\n  To address this pressing problem, we introduce PsyScam, a benchmark designed\nto systematically capture the PTs employed in real-world scam reports, and\ninvestigate how LLMs can be utilized to generate variants of scams based on the\nPTs and the contexts provided by these scams. Specifically, we collect a wide\nrange of scam reports and ground its annotations of employed PTs in\nwell-established cognitive and psychological theories. We further demonstrate\nLLMs' capabilities in generating through two downstream tasks: scam completion,\nand scam augmentation. Experimental results show that PsyScam presents\nsignificant challenges to existing models in both detecting and generating scam\ncontent based on the PTs used by real-world scammers. Our code and dataset are\navailable."
                },
                "authors": [
                    {
                        "name": "Shang Ma"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Zhenkai Liang"
                    },
                    {
                        "name": "Xusheng Xiao"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17932v1",
                "updated": "2025-09-22T15:54:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    54,
                    29,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:54:29Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    54,
                    29,
                    0,
                    265,
                    0
                ],
                "title": "Training-free Truthfulness Detection via Value Vectors in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Truthfulness Detection via Value Vectors in LLMs"
                },
                "summary": "Large language models often generate factually incorrect outputs, motivating\nefforts to detect the truthfulness of their content. Most existing approaches\nrely on training probes over internal activations, but these methods suffer\nfrom scalability and generalization issues. A recent training-free method,\nNoVo, addresses this challenge by exploiting statistical patterns from the\nmodel itself. However, it focuses exclusively on attention mechanisms,\npotentially overlooking the MLP module-a core component of Transformer models\nknown to support factual recall. In this paper, we show that certain value\nvectors within MLP modules exhibit truthfulness-related statistical patterns.\nBuilding on this insight, we propose TruthV, a simple and interpretable\ntraining-free method that detects content truthfulness by leveraging these\nvalue vectors. On the NoVo benchmark, TruthV significantly outperforms both\nNoVo and log-likelihood baselines, demonstrating that MLP modules-despite being\nneglected in prior training-free efforts-encode rich and useful signals for\ntruthfulness detection. These findings offer new insights into how truthfulness\nis internally represented in LLMs and motivate further research on scalable and\ninterpretable truthfulness detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models often generate factually incorrect outputs, motivating\nefforts to detect the truthfulness of their content. Most existing approaches\nrely on training probes over internal activations, but these methods suffer\nfrom scalability and generalization issues. A recent training-free method,\nNoVo, addresses this challenge by exploiting statistical patterns from the\nmodel itself. However, it focuses exclusively on attention mechanisms,\npotentially overlooking the MLP module-a core component of Transformer models\nknown to support factual recall. In this paper, we show that certain value\nvectors within MLP modules exhibit truthfulness-related statistical patterns.\nBuilding on this insight, we propose TruthV, a simple and interpretable\ntraining-free method that detects content truthfulness by leveraging these\nvalue vectors. On the NoVo benchmark, TruthV significantly outperforms both\nNoVo and log-likelihood baselines, demonstrating that MLP modules-despite being\nneglected in prior training-free efforts-encode rich and useful signals for\ntruthfulness detection. These findings offer new insights into how truthfulness\nis internally represented in LLMs and motivate further research on scalable and\ninterpretable truthfulness detection."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17925v1",
                "updated": "2025-09-22T15:50:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    50,
                    59,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:50:59Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    50,
                    59,
                    0,
                    265,
                    0
                ],
                "title": "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain\n  Brain Tumor Segmentation in MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain\n  Brain Tumor Segmentation in MRI"
                },
                "summary": "Reliable brain tumor segmentation in MRI is indispensable for treatment\nplanning and outcome monitoring, yet models trained on curated benchmarks often\nfail under domain shifts arising from scanner and protocol variability as well\nas population heterogeneity. Such gaps are especially severe in low-resource\nand pediatric cohorts, where conventional test-time or source-free adaptation\nstrategies often suffer from instability and structural inconsistency. We\npropose SmaRT, a style-modulated robust test-time adaptation framework that\nenables source-free cross-domain generalization. SmaRT integrates style-aware\naugmentation to mitigate appearance discrepancies, a dual-branch momentum\nstrategy for stable pseudo-label refinement, and structural priors enforcing\nconsistency, integrity, and connectivity. This synergy ensures both adaptation\nstability and anatomical fidelity under extreme domain shifts. Extensive\nevaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT\nconsistently outperforms state-of-the-art methods, with notable gains in Dice\naccuracy and boundary precision. Overall, SmaRT bridges the gap between\nalgorithmic advances and equitable clinical applicability, supporting robust\ndeployment of MRI-based neuro-oncology tools in diverse clinical environments.\nOur source code is available at https://github.com/baiyou1234/SmaRT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable brain tumor segmentation in MRI is indispensable for treatment\nplanning and outcome monitoring, yet models trained on curated benchmarks often\nfail under domain shifts arising from scanner and protocol variability as well\nas population heterogeneity. Such gaps are especially severe in low-resource\nand pediatric cohorts, where conventional test-time or source-free adaptation\nstrategies often suffer from instability and structural inconsistency. We\npropose SmaRT, a style-modulated robust test-time adaptation framework that\nenables source-free cross-domain generalization. SmaRT integrates style-aware\naugmentation to mitigate appearance discrepancies, a dual-branch momentum\nstrategy for stable pseudo-label refinement, and structural priors enforcing\nconsistency, integrity, and connectivity. This synergy ensures both adaptation\nstability and anatomical fidelity under extreme domain shifts. Extensive\nevaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT\nconsistently outperforms state-of-the-art methods, with notable gains in Dice\naccuracy and boundary precision. Overall, SmaRT bridges the gap between\nalgorithmic advances and equitable clinical applicability, supporting robust\ndeployment of MRI-based neuro-oncology tools in diverse clinical environments.\nOur source code is available at https://github.com/baiyou1234/SmaRT."
                },
                "authors": [
                    {
                        "name": "Yuanhan Wang"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Wenjing Yu"
                    },
                    {
                        "name": "Mingxuan Liu"
                    },
                    {
                        "name": "Beining Wu"
                    },
                    {
                        "name": "Jinying Zong"
                    },
                    {
                        "name": "Feiwei Qin"
                    },
                    {
                        "name": "Changmiao Wang"
                    },
                    {
                        "name": "Qiyuan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qiyuan Tian"
                },
                "author": "Qiyuan Tian",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01659v2",
                "updated": "2025-09-22T15:50:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    50,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2025-08-03T08:32:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    8,
                    32,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "From Contrast to Commonality: Audio Commonality Captioning for Enhanced\n  Audio-Text Cross-modal Understanding in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Contrast to Commonality: Audio Commonality Captioning for Enhanced\n  Audio-Text Cross-modal Understanding in Multimodal LLMs"
                },
                "summary": "Audio Captioning (AC) plays a pivotal role in enhancing audio-text\ncross-modal understanding during the pretraining and finetuning of Multimodal\nLLMs (MLLMs). To strengthen this alignment, recent works propose Audio\nDifference Captioning (ADC), which takes multiple audio inputs and encourages\nthe model to describe their differences, thereby promoting fine-grained\ndiscrimination. However, despite its effectiveness, ADC introduces a semantic\ngap between input audios-often rich in diverse events-and the brief,\ndifference-focused short caption. This deviation from AC-style task causes a\nmismatch with the pretraining objective, leading to catastrophic forgetting. To\naddress this, we propose Audio Commonality Captioning (ACC), a comparably\nchallenging but gentler alternative that guides the model to capture shared\nsemantics across audio clips rather than detailed differences. Experiments show\nthat ACC not only improves audio-text understanding on captioning benchmarks\nbut also better preserves general capabilities across diverse speech and music\ntasks, confirming its ability to enable more robust cross-modal understanding\nand achieve a better balance between generalization and task-specific\nperformance in MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Captioning (AC) plays a pivotal role in enhancing audio-text\ncross-modal understanding during the pretraining and finetuning of Multimodal\nLLMs (MLLMs). To strengthen this alignment, recent works propose Audio\nDifference Captioning (ADC), which takes multiple audio inputs and encourages\nthe model to describe their differences, thereby promoting fine-grained\ndiscrimination. However, despite its effectiveness, ADC introduces a semantic\ngap between input audios-often rich in diverse events-and the brief,\ndifference-focused short caption. This deviation from AC-style task causes a\nmismatch with the pretraining objective, leading to catastrophic forgetting. To\naddress this, we propose Audio Commonality Captioning (ACC), a comparably\nchallenging but gentler alternative that guides the model to capture shared\nsemantics across audio clips rather than detailed differences. Experiments show\nthat ACC not only improves audio-text understanding on captioning benchmarks\nbut also better preserves general capabilities across diverse speech and music\ntasks, confirming its ability to enable more robust cross-modal understanding\nand achieve a better balance between generalization and task-specific\nperformance in MLLMs."
                },
                "authors": [
                    {
                        "name": "Yuhang Jia"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Yujie Guo"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Shiwan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwan Zhao"
                },
                "author": "Shiwan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17924v1",
                "updated": "2025-09-22T15:49:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    49,
                    20,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:49:20Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    49,
                    20,
                    0,
                    265,
                    0
                ],
                "title": "Medical priority fusion: achieving dual optimization of sensitivity and\n  interpretability in nipt anomaly detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical priority fusion: achieving dual optimization of sensitivity and\n  interpretability in nipt anomaly detection"
                },
                "summary": "Clinical machine learning faces a critical dilemma in high-stakes medical\napplications: algorithms achieving optimal diagnostic performance typically\nsacrifice the interpretability essential for physician decision-making, while\ninterpretable methods compromise sensitivity in complex scenarios. This paradox\nbecomes particularly acute in non-invasive prenatal testing (NIPT), where\nmissed chromosomal abnormalities carry profound clinical consequences yet\nregulatory frameworks mandate explainable AI systems. We introduce Medical\nPriority Fusion (MPF), a constrained multi-objective optimization framework\nthat resolves this fundamental trade-off by systematically integrating Naive\nBayes probabilistic reasoning with Decision Tree rule-based logic through\nmathematically-principled weighted fusion under explicit medical constraints.\nRigorous validation on 1,687 real-world NIPT samples characterized by extreme\nclass imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold\ncross-validation with comprehensive ablation studies and statistical hypothesis\ntesting using McNemar's paired comparisons. MPF achieved simultaneous\noptimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with\n80% interpretability score, significantly outperforming individual algorithms\n(McNemar's test, p < 0.001). The optimal fusion configuration achieved Grade A\nclinical deployment criteria with large effect size (d = 1.24), establishing\nthe first clinically-deployable solution that maintains both diagnostic\naccuracy and decision transparency essential for prenatal care. This work\ndemonstrates that medical-constrained algorithm fusion can resolve the\ninterpretability-performance trade-off, providing a mathematical framework for\ndeveloping high-stakes medical decision support systems that meet both clinical\nefficacy and explainability requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical machine learning faces a critical dilemma in high-stakes medical\napplications: algorithms achieving optimal diagnostic performance typically\nsacrifice the interpretability essential for physician decision-making, while\ninterpretable methods compromise sensitivity in complex scenarios. This paradox\nbecomes particularly acute in non-invasive prenatal testing (NIPT), where\nmissed chromosomal abnormalities carry profound clinical consequences yet\nregulatory frameworks mandate explainable AI systems. We introduce Medical\nPriority Fusion (MPF), a constrained multi-objective optimization framework\nthat resolves this fundamental trade-off by systematically integrating Naive\nBayes probabilistic reasoning with Decision Tree rule-based logic through\nmathematically-principled weighted fusion under explicit medical constraints.\nRigorous validation on 1,687 real-world NIPT samples characterized by extreme\nclass imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold\ncross-validation with comprehensive ablation studies and statistical hypothesis\ntesting using McNemar's paired comparisons. MPF achieved simultaneous\noptimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with\n80% interpretability score, significantly outperforming individual algorithms\n(McNemar's test, p < 0.001). The optimal fusion configuration achieved Grade A\nclinical deployment criteria with large effect size (d = 1.24), establishing\nthe first clinically-deployable solution that maintains both diagnostic\naccuracy and decision transparency essential for prenatal care. This work\ndemonstrates that medical-constrained algorithm fusion can resolve the\ninterpretability-performance trade-off, providing a mathematical framework for\ndeveloping high-stakes medical decision support systems that meet both clinical\nefficacy and explainability requirements."
                },
                "authors": [
                    {
                        "name": "Xiuqi Ge"
                    },
                    {
                        "name": "Zhibo Yao"
                    },
                    {
                        "name": "Yaosong Du"
                    }
                ],
                "author_detail": {
                    "name": "Yaosong Du"
                },
                "author": "Yaosong Du",
                "arxiv_comment": "24 pages, 47 figures, publish to BIBM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17775v3",
                "updated": "2025-09-22T15:43:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    43,
                    4,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-25T02:10:30Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    10,
                    30,
                    1,
                    56,
                    0
                ],
                "title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks"
                },
                "summary": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference, which identifies the\nperspective of spatial expressions. Despite its significance, FoR has received\nlimited attention in AI models that need spatial intelligence. There is a lack\nof dedicated benchmarks and in-depth evaluation of large language models (LLMs)\nin this area. To address this issue, we introduce the Frame of Reference\nEvaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess\nFoR comprehension in LLMs. We evaluate LLMs on answering questions that require\nFoR comprehension and layout generation in text-to-image models using FoREST.\nOur results reveal a notable performance gap across different FoR classes in\nvarious LLMs, affecting their ability to generate accurate layouts for\ntext-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference, which identifies the\nperspective of spatial expressions. Despite its significance, FoR has received\nlimited attention in AI models that need spatial intelligence. There is a lack\nof dedicated benchmarks and in-depth evaluation of large language models (LLMs)\nin this area. To address this issue, we introduce the Frame of Reference\nEvaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to assess\nFoR comprehension in LLMs. We evaluate LLMs on answering questions that require\nFoR comprehension and layout generation in text-to-image models using FoREST.\nOur results reveal a notable performance gap across different FoR classes in\nvarious LLMs, affecting their ability to generate accurate layouts for\ntext-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Tanawan Premsri"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "arxiv_comment": "10 pages, 3 Figures, 4 Tables, EMNLP-2025 Main (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17917v1",
                "updated": "2025-09-22T15:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    40,
                    31,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:40:31Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    40,
                    31,
                    0,
                    265,
                    0
                ],
                "title": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent"
                },
                "summary": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities."
                },
                "authors": [
                    {
                        "name": "Junyu Lu"
                    },
                    {
                        "name": "Songxin Zhang"
                    },
                    {
                        "name": "Zejian Xie"
                    },
                    {
                        "name": "Zhuoyang Song"
                    },
                    {
                        "name": "Jiaxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Zhang"
                },
                "author": "Jiaxing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17914v1",
                "updated": "2025-09-22T15:39:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    39,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:39:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    39,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "XaaS Containers: Performance-Portable Representation With Source and IR\n  Containers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XaaS Containers: Performance-Portable Representation With Source and IR\n  Containers"
                },
                "summary": "High-performance computing (HPC) systems and cloud data centers are\nconverging, and containers are becoming the default method of portable software\ndeployment. Yet, while containers simplify software management, they face\nsignificant performance challenges in HPC environments as they must sacrifice\nhardware-specific optimizations to achieve portability. Although HPC containers\ncan use runtime hooks to access optimized MPI libraries and GPU devices, they\nare limited by application binary interface (ABI) compatibility and cannot\novercome the effects of early-stage compilation decisions. Acceleration as a\nService (XaaS) proposes a vision of performance-portable containers, where a\ncontainerized application should achieve peak performance across all HPC\nsystems. We present a practical realization of this vision through Source and\nIntermediate Representation (IR) containers, where we delay\nperformance-critical decisions until the target system specification is known.\nWe analyze specialization mechanisms in HPC software and propose a new\nLLM-assisted method for automatic discovery of specializations. By examining\nthe compilation pipeline, we develop a methodology to build containers\noptimized for target architectures at deployment time. Our prototype\ndemonstrates that new XaaS containers combine the convenience of\ncontainerization with the performance benefits of system-specialized builds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-performance computing (HPC) systems and cloud data centers are\nconverging, and containers are becoming the default method of portable software\ndeployment. Yet, while containers simplify software management, they face\nsignificant performance challenges in HPC environments as they must sacrifice\nhardware-specific optimizations to achieve portability. Although HPC containers\ncan use runtime hooks to access optimized MPI libraries and GPU devices, they\nare limited by application binary interface (ABI) compatibility and cannot\novercome the effects of early-stage compilation decisions. Acceleration as a\nService (XaaS) proposes a vision of performance-portable containers, where a\ncontainerized application should achieve peak performance across all HPC\nsystems. We present a practical realization of this vision through Source and\nIntermediate Representation (IR) containers, where we delay\nperformance-critical decisions until the target system specification is known.\nWe analyze specialization mechanisms in HPC software and propose a new\nLLM-assisted method for automatic discovery of specializations. By examining\nthe compilation pipeline, we develop a methodology to build containers\noptimized for target architectures at deployment time. Our prototype\ndemonstrates that new XaaS containers combine the convenience of\ncontainerization with the performance benefits of system-specialized builds."
                },
                "authors": [
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Eiman Alnuaimi"
                    },
                    {
                        "name": "Alok Kamatar"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Alberto Madonna"
                    },
                    {
                        "name": "Todd Gamblin"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1145/3712285.3759868",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759868",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.17914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC'25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10369v2",
                "updated": "2025-09-22T15:39:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    39,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-04-14T16:15:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    15,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning"
                },
                "summary": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yiting Wang"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Bowei Tian"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Sihan Chen"
                    },
                    {
                        "name": "Ankur Srivastava"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17905v2",
                "updated": "2025-09-23T05:27:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    27,
                    9,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T15:30:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Mitigating Strategy-Selection Bias in Reasoning for More Effective\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Strategy-Selection Bias in Reasoning for More Effective\n  Test-Time Scaling"
                },
                "summary": "Test-time scaling (TTS) has been shown to improve the performance of large\nlanguage models (LLMs) by sampling and aggregating diverse reasoning paths.\nHowever, existing research has overlooked a critical issue: selection bias of\nreasoning strategies during scaling. Specifically, when generating reasoning\nprocesses, LLMs tend to follow certain strategies (e.g., algebraic solutions\nfor math problems) while neglecting other valid alternatives (e.g., geometric\nsolutions), resulting in insufficient exploration of the solution space. To\nfurther understand the impact of this bias, we present a theoretical analysis\nthat reveals when it undermines the effectiveness of test-time scaling.\nMotivated by this theoretical insight, we introduce TTS-Uniform, a framework\ndesigned to mitigate the selection bias of reasoning strategies. It (i)\nidentifies potential strategies, (ii) uniformly allocates the sampling budget\nacross them, and (iii) filters out unstable strategies prior to aggregation.\nExperimental results show that TTS-Uniform significantly enhances scaling\neffectiveness across multiple mainstream LLMs and benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) has been shown to improve the performance of large\nlanguage models (LLMs) by sampling and aggregating diverse reasoning paths.\nHowever, existing research has overlooked a critical issue: selection bias of\nreasoning strategies during scaling. Specifically, when generating reasoning\nprocesses, LLMs tend to follow certain strategies (e.g., algebraic solutions\nfor math problems) while neglecting other valid alternatives (e.g., geometric\nsolutions), resulting in insufficient exploration of the solution space. To\nfurther understand the impact of this bias, we present a theoretical analysis\nthat reveals when it undermines the effectiveness of test-time scaling.\nMotivated by this theoretical insight, we introduce TTS-Uniform, a framework\ndesigned to mitigate the selection bias of reasoning strategies. It (i)\nidentifies potential strategies, (ii) uniformly allocates the sampling budget\nacross them, and (iii) filters out unstable strategies prior to aggregation.\nExperimental results show that TTS-Uniform significantly enhances scaling\neffectiveness across multiple mainstream LLMs and benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Zongqian Wu"
                    },
                    {
                        "name": "Baoduo Xu"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Lei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Lei Feng"
                },
                "author": "Lei Feng",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17901v1",
                "updated": "2025-09-22T15:28:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    28,
                    54,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:28:54Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    28,
                    54,
                    0,
                    265,
                    0
                ],
                "title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?"
                },
                "summary": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM."
                },
                "authors": [
                    {
                        "name": "Geewook Kim"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "arxiv_comment": "5 pages, 2 figures, under review. Project page:\n  https://github.com/naver-ai/LLaVA-AV-SSM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17898v1",
                "updated": "2025-09-22T15:26:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    26,
                    46,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:26:46Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    26,
                    46,
                    0,
                    265,
                    0
                ],
                "title": "Lipschitz-Based Robustness Certification for Recurrent Neural Networks\n  via Convex Relaxation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lipschitz-Based Robustness Certification for Recurrent Neural Networks\n  via Convex Relaxation"
                },
                "summary": "Robustness certification against bounded input noise or adversarial\nperturbations is increasingly important for deployment recurrent neural\nnetworks (RNNs) in safety-critical control applications. To address this\nchallenge, we present RNN-SDP, a relaxation based method that models the RNN's\nlayer interactions as a convex problem and computes a certified upper bound on\nthe Lipschitz constant via semidefinite programming (SDP). We also explore an\nextension that incorporates known input constraints to further tighten the\nresulting Lipschitz bounds. RNN-SDP is evaluated on a synthetic multi-tank\nsystem, with upper bounds compared to empirical estimates. While incorporating\ninput constraints yields only modest improvements, the general method produces\nreasonably tight and certifiable bounds, even as sequence length increases. The\nresults also underscore the often underestimated impact of initialization\nerrors, an important consideration for applications where models are frequently\nre-initialized, such as model predictive control (MPC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness certification against bounded input noise or adversarial\nperturbations is increasingly important for deployment recurrent neural\nnetworks (RNNs) in safety-critical control applications. To address this\nchallenge, we present RNN-SDP, a relaxation based method that models the RNN's\nlayer interactions as a convex problem and computes a certified upper bound on\nthe Lipschitz constant via semidefinite programming (SDP). We also explore an\nextension that incorporates known input constraints to further tighten the\nresulting Lipschitz bounds. RNN-SDP is evaluated on a synthetic multi-tank\nsystem, with upper bounds compared to empirical estimates. While incorporating\ninput constraints yields only modest improvements, the general method produces\nreasonably tight and certifiable bounds, even as sequence length increases. The\nresults also underscore the often underestimated impact of initialization\nerrors, an important consideration for applications where models are frequently\nre-initialized, such as model predictive control (MPC)."
                },
                "authors": [
                    {
                        "name": "Paul Hamelbeck"
                    },
                    {
                        "name": "Johannes Schiffer"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Schiffer"
                },
                "author": "Johannes Schiffer",
                "arxiv_comment": "10 pages, 3 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17874v1",
                "updated": "2025-09-22T15:13:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    13,
                    14,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:13:14Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    13,
                    14,
                    0,
                    265,
                    0
                ],
                "title": "Deep Hierarchical Learning with Nested Subspace Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Hierarchical Learning with Nested Subspace Networks"
                },
                "summary": "Large neural networks are typically trained for a fixed computational budget,\ncreating a rigid trade-off between performance and efficiency that is\nill-suited for deployment in resource-constrained or dynamic environments.\nExisting approaches to this problem present a difficult choice: training a\ndiscrete collection of specialist models is computationally prohibitive, while\ndynamic methods like slimmable networks often lack the flexibility to be\napplied to large, pre-trained foundation models. In this work, we propose\nNested Subspace Networks (NSNs), a novel architectural paradigm that enables a\nsingle model to be dynamically and granularly adjusted across a continuous\nspectrum of compute budgets at inference time. The core of our approach is to\nre-parameterize linear layers to satisfy a nested subspace property, such that\nthe function computed at a given rank is a strict subspace of the function at\nany higher rank. We show that this entire hierarchy of models can be optimized\njointly via an uncertainty-aware objective that learns to balance the\ncontributions of different ranks based on their intrinsic difficulty. We\ndemonstrate empirically that NSNs can be surgically applied to pre-trained LLMs\nand unlock a smooth and predictable compute-performance frontier. For example,\na single NSN-adapted model can achieve a 50% reduction in inference FLOPs with\nonly a 5 percentage point loss in accuracy. Our findings establish NSNs as a\npowerful framework for creating the next generation of adaptive foundation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large neural networks are typically trained for a fixed computational budget,\ncreating a rigid trade-off between performance and efficiency that is\nill-suited for deployment in resource-constrained or dynamic environments.\nExisting approaches to this problem present a difficult choice: training a\ndiscrete collection of specialist models is computationally prohibitive, while\ndynamic methods like slimmable networks often lack the flexibility to be\napplied to large, pre-trained foundation models. In this work, we propose\nNested Subspace Networks (NSNs), a novel architectural paradigm that enables a\nsingle model to be dynamically and granularly adjusted across a continuous\nspectrum of compute budgets at inference time. The core of our approach is to\nre-parameterize linear layers to satisfy a nested subspace property, such that\nthe function computed at a given rank is a strict subspace of the function at\nany higher rank. We show that this entire hierarchy of models can be optimized\njointly via an uncertainty-aware objective that learns to balance the\ncontributions of different ranks based on their intrinsic difficulty. We\ndemonstrate empirically that NSNs can be surgically applied to pre-trained LLMs\nand unlock a smooth and predictable compute-performance frontier. For example,\na single NSN-adapted model can achieve a 50% reduction in inference FLOPs with\nonly a 5 percentage point loss in accuracy. Our findings establish NSNs as a\npowerful framework for creating the next generation of adaptive foundation\nmodels."
                },
                "authors": [
                    {
                        "name": "Paulius Rauba"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17866v1",
                "updated": "2025-09-22T15:03:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    3,
                    36,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T15:03:36Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    15,
                    3,
                    36,
                    0,
                    265,
                    0
                ],
                "title": "Understanding Post-Training Structural Changes in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Post-Training Structural Changes in Large Language Models"
                },
                "summary": "Post-training fundamentally alters the behavior of large language models\n(LLMs), yet its impact on the internal parameter space remains poorly\nunderstood. In this work, we conduct a systematic singular value decomposition\n(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two\nwidely adopted post-training methods: instruction tuning and\nlong-chain-of-thought (Long-CoT) distillation. Our analysis reveals two\nconsistent and unexpected structural changes:(1) a near-uniform geometric\nscaling of singular values across layers, which theoretically modulates\nattention scores; and (2) highly consistent orthogonal transformations are\napplied to the left and right singular vectors of each matrix. Disrupting this\northogonal consistency leads to catastrophic performance degradation. Based on\nthese findings, we propose a simple yet effective framework that interprets\npost-training as a reparameterization of fixed subspaces in the pretrained\nparameter space. Further experiments reveal that singular value scaling behaves\nas a secondary effect, analogous to a temperature adjustment, whereas the core\nfunctional transformation lies in the coordinated rotation of singular vectors.\nThese results challenge the prevailing view of the parameter space in large\nmodels as a black box, uncovering the first clear regularities in how\nparameters evolve during training, and providing a new perspective for deeper\ninvestigation into model parameter changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training fundamentally alters the behavior of large language models\n(LLMs), yet its impact on the internal parameter space remains poorly\nunderstood. In this work, we conduct a systematic singular value decomposition\n(SVD) analysis of principal linear layers in pretrained LLMs, focusing on two\nwidely adopted post-training methods: instruction tuning and\nlong-chain-of-thought (Long-CoT) distillation. Our analysis reveals two\nconsistent and unexpected structural changes:(1) a near-uniform geometric\nscaling of singular values across layers, which theoretically modulates\nattention scores; and (2) highly consistent orthogonal transformations are\napplied to the left and right singular vectors of each matrix. Disrupting this\northogonal consistency leads to catastrophic performance degradation. Based on\nthese findings, we propose a simple yet effective framework that interprets\npost-training as a reparameterization of fixed subspaces in the pretrained\nparameter space. Further experiments reveal that singular value scaling behaves\nas a secondary effect, analogous to a temperature adjustment, whereas the core\nfunctional transformation lies in the coordinated rotation of singular vectors.\nThese results challenge the prevailing view of the parameter space in large\nmodels as a black box, uncovering the first clear regularities in how\nparameters evolve during training, and providing a new perspective for deeper\ninvestigation into model parameter changes."
                },
                "authors": [
                    {
                        "name": "Xinyu He"
                    },
                    {
                        "name": "Xianghui Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xianghui Cao"
                },
                "author": "Xianghui Cao",
                "arxiv_comment": "38 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17863v1",
                "updated": "2025-09-22T14:56:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    56,
                    46,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:56:46Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    56,
                    46,
                    0,
                    265,
                    0
                ],
                "title": "Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale\n  MoE Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale\n  MoE Serving"
                },
                "summary": "Mixture-of-Experts (MoE) models challenge serving infrastructures with\ndynamic, sparse expert utilization, causing instability on conventional systems\ndesigned for dense architectures. We propose EaaS, a novel serving system to\nenable efficient, scalable, and robust MoE deployment. Our system disaggregates\nMoE modules into independent, stateless services. This design enables\nfine-grained resource scaling and provides inherent fault tolerance by\ndecoupling compute units. The architecture is powered by a high-performance,\nCPU-free peer-to-peer communication library that ensures minimal overhead and\nhigh throughput. Experiments confirm EaaS's scalability and efficiency,\nachieving performance comparable to monolithic systems while providing robust\nfault tolerance and strong scalability. EaaS incurs less than a 2% throughput\nreduction under simulated hardware failures that would otherwise halt\nmonolithic architectures. It further saves up to 37.5% of computing resources\nthrough dynamic fine-grained adaptation to serving traffic, demonstrating\nstrong resilience for large-scale MoE deployment in production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models challenge serving infrastructures with\ndynamic, sparse expert utilization, causing instability on conventional systems\ndesigned for dense architectures. We propose EaaS, a novel serving system to\nenable efficient, scalable, and robust MoE deployment. Our system disaggregates\nMoE modules into independent, stateless services. This design enables\nfine-grained resource scaling and provides inherent fault tolerance by\ndecoupling compute units. The architecture is powered by a high-performance,\nCPU-free peer-to-peer communication library that ensures minimal overhead and\nhigh throughput. Experiments confirm EaaS's scalability and efficiency,\nachieving performance comparable to monolithic systems while providing robust\nfault tolerance and strong scalability. EaaS incurs less than a 2% throughput\nreduction under simulated hardware failures that would otherwise halt\nmonolithic architectures. It further saves up to 37.5% of computing resources\nthrough dynamic fine-grained adaptation to serving traffic, demonstrating\nstrong resilience for large-scale MoE deployment in production."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Guoteng Wang"
                    },
                    {
                        "name": "Zhen Jiang"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Xiaohe Hu"
                    },
                    {
                        "name": "Yanmin Jia"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "He Liu"
                    },
                    {
                        "name": "Mingjun Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Qiaoling Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Siyuan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Feng"
                },
                "author": "Siyuan Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20608v2",
                "updated": "2025-09-22T14:54:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    54,
                    39,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-25T17:00:05Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    0,
                    5,
                    2,
                    176,
                    0
                ],
                "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base"
                },
                "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery."
                },
                "authors": [
                    {
                        "name": "Barry Smith"
                    },
                    {
                        "name": "Junchao Zhang"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Lois Curfman McInnes"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Archit Vasan"
                    },
                    {
                        "name": "Satish Balay"
                    },
                    {
                        "name": "Toby Isaac"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "arxiv_journal_ref": "54th International Conference on Parallel Processing Companion\n  (ICPP Companion '25), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07809v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07809v4",
                "updated": "2025-09-23T05:59:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    59,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-11T09:49:01Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    49,
                    1,
                    0,
                    223,
                    0
                ],
                "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning"
                },
                "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on teacher models for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens CoT steps to expand the space in a controlled way. The\nframework enables LLMs to stably learn from initially unsolved hard problems\nunder sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen,\nDeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve\npreviously unsolved problems, improves reasoning capability without external\nCoT supervision, and is compatible with various RL fine-tuning methods. We\nrelease the source code to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on teacher models for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens CoT steps to expand the space in a controlled way. The\nframework enables LLMs to stably learn from initially unsolved hard problems\nunder sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen,\nDeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve\npreviously unsolved problems, improves reasoning capability without external\nCoT supervision, and is compatible with various RL fine-tuning methods. We\nrelease the source code to support future research."
                },
                "authors": [
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Taozhi Chen"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "XiaoLong Hu"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07809v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07809v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17858v1",
                "updated": "2025-09-22T14:51:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    51,
                    37,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:51:37Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    51,
                    37,
                    0,
                    265,
                    0
                ],
                "title": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual\n  Coreference Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual\n  Coreference Resolution"
                },
                "summary": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on\nMultilingual Coreference Resolution. This fourth iteration of the shared task\nintroduces a new LLM track alongside the original unconstrained track, features\nreduced development and test sets to lower computational requirements, and\nincludes additional datasets. CorPipe 25 represents a complete reimplementation\nof our previous systems, migrating from TensorFlow to PyTorch. Our system\nsignificantly outperforms all other submissions in both the LLM and\nunconstrained tracks by a substantial margin of 8 percentage points. The source\ncode and trained models are publicly available at\nhttps://github.com/ufal/crac2025-corpipe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on\nMultilingual Coreference Resolution. This fourth iteration of the shared task\nintroduces a new LLM track alongside the original unconstrained track, features\nreduced development and test sets to lower computational requirements, and\nincludes additional datasets. CorPipe 25 represents a complete reimplementation\nof our previous systems, migrating from TensorFlow to PyTorch. Our system\nsignificantly outperforms all other submissions in both the LLM and\nunconstrained tracks by a substantial margin of 8 percentage points. The source\ncode and trained models are publicly available at\nhttps://github.com/ufal/crac2025-corpipe."
                },
                "authors": [
                    {
                        "name": "Milan Straka"
                    }
                ],
                "author_detail": {
                    "name": "Milan Straka"
                },
                "author": "Milan Straka",
                "arxiv_comment": "Accepted to CODI-CRAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17855v1",
                "updated": "2025-09-22T14:49:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    49,
                    8,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:49:08Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    49,
                    8,
                    0,
                    265,
                    0
                ],
                "title": "Make Every Letter Count: Building Dialect Variation Dictionaries from\n  Monolingual Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make Every Letter Count: Building Dialect Variation Dictionaries from\n  Monolingual Corpora"
                },
                "summary": "Dialects exhibit a substantial degree of variation due to the lack of a\nstandard orthography. At the same time, the ability of Large Language Models\n(LLMs) to process dialects remains largely understudied. To address this gap,\nwe use Bavarian as a case study and investigate the lexical dialect\nunderstanding capability of LLMs by examining how well they recognize and\ntranslate dialectal terms across different parts-of-speech. To this end, we\nintroduce DiaLemma, a novel annotation framework for creating dialect variation\ndictionaries from monolingual data only, and use it to compile a ground truth\ndataset consisting of 100K human-annotated German-Bavarian word pairs. We\nevaluate how well nine state-of-the-art LLMs can judge Bavarian terms as\ndialect translations, inflected variants, or unrelated forms of a given German\nlemma. Our results show that LLMs perform best on nouns and lexically similar\nword pairs, and struggle most in distinguishing between direct translations and\ninflected variants. Interestingly, providing additional context in the form of\nexample usages improves the translation performance, but reduces their ability\nto recognize dialect variants. This study highlights the limitations of LLMs in\ndealing with orthographic dialect variation and emphasizes the need for future\nwork on adapting LLMs to dialects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialects exhibit a substantial degree of variation due to the lack of a\nstandard orthography. At the same time, the ability of Large Language Models\n(LLMs) to process dialects remains largely understudied. To address this gap,\nwe use Bavarian as a case study and investigate the lexical dialect\nunderstanding capability of LLMs by examining how well they recognize and\ntranslate dialectal terms across different parts-of-speech. To this end, we\nintroduce DiaLemma, a novel annotation framework for creating dialect variation\ndictionaries from monolingual data only, and use it to compile a ground truth\ndataset consisting of 100K human-annotated German-Bavarian word pairs. We\nevaluate how well nine state-of-the-art LLMs can judge Bavarian terms as\ndialect translations, inflected variants, or unrelated forms of a given German\nlemma. Our results show that LLMs perform best on nouns and lexically similar\nword pairs, and struggle most in distinguishing between direct translations and\ninflected variants. Interestingly, providing additional context in the form of\nexample usages improves the translation performance, but reduces their ability\nto recognize dialect variants. This study highlights the limitations of LLMs in\ndealing with orthographic dialect variation and emphasizes the need for future\nwork on adapting LLMs to dialects."
                },
                "authors": [
                    {
                        "name": "Robert Litschko"
                    },
                    {
                        "name": "Verena Blaschke"
                    },
                    {
                        "name": "Diana Burkhardt"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Diego Frassinelli"
                    }
                ],
                "author_detail": {
                    "name": "Diego Frassinelli"
                },
                "author": "Diego Frassinelli",
                "arxiv_comment": "Accepted at EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17849v1",
                "updated": "2025-09-22T14:42:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    42,
                    4,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:42:04Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    42,
                    4,
                    0,
                    265,
                    0
                ],
                "title": "Fast qubit-based frequency recovery algorithm for quantum key\n  distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast qubit-based frequency recovery algorithm for quantum key\n  distribution"
                },
                "summary": "Clock synchronization serves as a foundational subsystem in quantum key\ndistribution (QKD). The recently proposed Qubit-based synchronization\n(Qubit4Sync) has opportunities in eliminating additional cost, noise, and\npotential side channels. It offers a promising alternative to dedicated\nsynchronization hardware. However, the current frequency recovery process in\nQubit4Sync requires high data throughput and computational speed, limiting\npractical use. To overcome these issues, we developed a fast frequency recovery\nalgorithm that increases the recovery rate by orders of magnitude and remains\nrobust under bad signal-to-noise ratio (SNR). This enables Qubit4Sync to\noperate effectively in mainstream gated-mode QKD systems. We further establish\na theoretical model for frequency recovery, showing that our algorithm is\nrobust against disturbances like dead time, jitter, and afterpulse. A\nfrequency-domain SNR calculation method is also provided to guide parameter\ndesign for specific experimental conditions. This work opens the door to\npractical Qubit4Sync deployment in general QKD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clock synchronization serves as a foundational subsystem in quantum key\ndistribution (QKD). The recently proposed Qubit-based synchronization\n(Qubit4Sync) has opportunities in eliminating additional cost, noise, and\npotential side channels. It offers a promising alternative to dedicated\nsynchronization hardware. However, the current frequency recovery process in\nQubit4Sync requires high data throughput and computational speed, limiting\npractical use. To overcome these issues, we developed a fast frequency recovery\nalgorithm that increases the recovery rate by orders of magnitude and remains\nrobust under bad signal-to-noise ratio (SNR). This enables Qubit4Sync to\noperate effectively in mainstream gated-mode QKD systems. We further establish\na theoretical model for frequency recovery, showing that our algorithm is\nrobust against disturbances like dead time, jitter, and afterpulse. A\nfrequency-domain SNR calculation method is also provided to guide parameter\ndesign for specific experimental conditions. This work opens the door to\npractical Qubit4Sync deployment in general QKD systems."
                },
                "authors": [
                    {
                        "name": "Feng-Yu Lu"
                    },
                    {
                        "name": "Zheng-Kai Huang"
                    },
                    {
                        "name": "Jia-Jv Deng"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Shuang Wang"
                    },
                    {
                        "name": "De-Yong He"
                    },
                    {
                        "name": "Zhen-Qiang Yin"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Guang-Can Guo"
                    },
                    {
                        "name": "Zheng-Fu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Fu Han"
                },
                "author": "Zheng-Fu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17832v1",
                "updated": "2025-09-22T14:23:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    23,
                    4,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:23:04Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    23,
                    4,
                    0,
                    265,
                    0
                ],
                "title": "AEAS: Actionable Exploit Assessment System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEAS: Actionable Exploit Assessment System"
                },
                "summary": "Security practitioners face growing challenges in exploit assessment, as\npublic vulnerability repositories are increasingly populated with inconsistent\nand low-quality exploit artifacts. Existing scoring systems, such as CVSS and\nEPSS, offer limited support for this task. They either rely on theoretical\nmetrics or produce opaque probability estimates without assessing whether\nusable exploit code exists. In practice, security teams often resort to manual\ntriage of exploit repositories, which is time-consuming, error-prone, and\ndifficult to scale. We present AEAS, an automated system designed to assess and\nprioritize actionable exploits through static analysis. AEAS analyzes both\nexploit code and associated documentation to extract a structured set of\nfeatures reflecting exploit availability, functionality, and setup complexity.\nIt then computes an actionability score for each exploit and produces ranked\nexploit recommendations. We evaluate AEAS on a dataset of over 5,000\nvulnerabilities derived from 600+ real-world applications frequently\nencountered by red teams. Manual validation and expert review on representative\nsubsets show that AEAS achieves a 100% top-3 success rate in recommending\nfunctional exploits and shows strong alignment with expert-validated rankings.\nThese results demonstrate the effectiveness of AEAS in supporting\nexploit-driven vulnerability prioritization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security practitioners face growing challenges in exploit assessment, as\npublic vulnerability repositories are increasingly populated with inconsistent\nand low-quality exploit artifacts. Existing scoring systems, such as CVSS and\nEPSS, offer limited support for this task. They either rely on theoretical\nmetrics or produce opaque probability estimates without assessing whether\nusable exploit code exists. In practice, security teams often resort to manual\ntriage of exploit repositories, which is time-consuming, error-prone, and\ndifficult to scale. We present AEAS, an automated system designed to assess and\nprioritize actionable exploits through static analysis. AEAS analyzes both\nexploit code and associated documentation to extract a structured set of\nfeatures reflecting exploit availability, functionality, and setup complexity.\nIt then computes an actionability score for each exploit and produces ranked\nexploit recommendations. We evaluate AEAS on a dataset of over 5,000\nvulnerabilities derived from 600+ real-world applications frequently\nencountered by red teams. Manual validation and expert review on representative\nsubsets show that AEAS achieves a 100% top-3 success rate in recommending\nfunctional exploits and shows strong alignment with expert-validated rankings.\nThese results demonstrate the effectiveness of AEAS in supporting\nexploit-driven vulnerability prioritization."
                },
                "authors": [
                    {
                        "name": "Xiangmin Shen"
                    },
                    {
                        "name": "Wenyuan Cheng"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Zhenyuan Li"
                    },
                    {
                        "name": "Yuqiao Gu"
                    },
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Wencheng Zhao"
                    },
                    {
                        "name": "Dawei Sun"
                    },
                    {
                        "name": "Jiashui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiashui Wang"
                },
                "author": "Jiashui Wang",
                "arxiv_comment": "AEAS has been implemented in the planning agent of PentestAgent, our\n  LLM-driven automated penetration testing framework. Check out our repository:\n  https://github.com/nbshenxm/pentest-agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14992v2",
                "updated": "2025-09-22T14:21:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    21,
                    36,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-18T14:23:36Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    23,
                    36,
                    3,
                    261,
                    0
                ],
                "title": "ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task\n  Pretraining and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task\n  Pretraining and Fine-Tuning"
                },
                "summary": "Scaling up the deployment of autonomous excavators is of great economic and\nsocietal importance. Yet it remains a challenging problem, as effective systems\nmust robustly handle unseen worksite conditions and new hardware\nconfigurations. Current state-of-the-art approaches rely on highly engineered,\ntask-specific controllers, which require extensive manual tuning for each new\nscenario. In contrast, recent advances in large-scale pretrained models have\nshown remarkable adaptability across tasks and embodiments in domains such as\nmanipulation and navigation, but their applicability to heavy construction\nmachinery remains largely unexplored. In this work, we introduce ExT, a unified\nopen-source framework for large-scale demonstration collection, pretraining,\nand fine-tuning of multitask excavation policies. ExT policies are first\ntrained on large-scale demonstrations collected from a mix of experts, then\nfine-tuned either with supervised fine-tuning (SFT) or reinforcement learning\nfine-tuning (RLFT) to specialize to new tasks or operating conditions. Through\nboth simulation and real-world experiments, we show that pretrained ExT\npolicies can execute complete excavation cycles with centimeter-level accuracy,\nsuccessfully transferring from simulation to real machine with performance\ncomparable to specialized single-task controllers. Furthermore, in simulation,\nwe demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new\ntasks, out-of-distribution conditions, and machine configurations, while\nmaintaining strong performance on previously learned tasks. These results\nhighlight the potential of ExT to serve as a foundation for scalable and\ngeneralizable autonomous excavation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up the deployment of autonomous excavators is of great economic and\nsocietal importance. Yet it remains a challenging problem, as effective systems\nmust robustly handle unseen worksite conditions and new hardware\nconfigurations. Current state-of-the-art approaches rely on highly engineered,\ntask-specific controllers, which require extensive manual tuning for each new\nscenario. In contrast, recent advances in large-scale pretrained models have\nshown remarkable adaptability across tasks and embodiments in domains such as\nmanipulation and navigation, but their applicability to heavy construction\nmachinery remains largely unexplored. In this work, we introduce ExT, a unified\nopen-source framework for large-scale demonstration collection, pretraining,\nand fine-tuning of multitask excavation policies. ExT policies are first\ntrained on large-scale demonstrations collected from a mix of experts, then\nfine-tuned either with supervised fine-tuning (SFT) or reinforcement learning\nfine-tuning (RLFT) to specialize to new tasks or operating conditions. Through\nboth simulation and real-world experiments, we show that pretrained ExT\npolicies can execute complete excavation cycles with centimeter-level accuracy,\nsuccessfully transferring from simulation to real machine with performance\ncomparable to specialized single-task controllers. Furthermore, in simulation,\nwe demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new\ntasks, out-of-distribution conditions, and machine configurations, while\nmaintaining strong performance on previously learned tasks. These results\nhighlight the potential of ExT to serve as a foundation for scalable and\ngeneralizable autonomous excavation."
                },
                "authors": [
                    {
                        "name": "Yifan Zhai"
                    },
                    {
                        "name": "Lorenzo Terenzi"
                    },
                    {
                        "name": "Patrick Frey"
                    },
                    {
                        "name": "Diego Garcia Soto"
                    },
                    {
                        "name": "Pascal Egli"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14498v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14498v4",
                "updated": "2025-09-22T14:02:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    2,
                    3,
                    0,
                    265,
                    0
                ],
                "published": "2024-06-20T17:00:34Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    0,
                    34,
                    3,
                    172,
                    0
                ],
                "title": "LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human\n  Activity from IMU Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human\n  Activity from IMU Data"
                },
                "summary": "Wearable systems can recognize activities from IMU data but often fail to\nexplain their underlying causes or contextual significance. To address this\nlimitation, we introduce two large-scale resources: SensorCap, comprising\n35,960 IMU--caption pairs, and OpenSQA, with 199,701 question--answer pairs\ndesigned for causal and explanatory reasoning. OpenSQA includes a curated\ntuning split (Tune-OpenSQA) optimized for scientific accuracy, narrative\nclarity, and diagnostic insight. Leveraging these datasets, we develop LLaSA\n(Large Language and Sensor Assistant), a family of compact sensor-aware\nlanguage models (7B and 13B) that generate interpretable, context-rich\nresponses to open-ended questions grounded in raw IMU data. LLaSA outperforms\ncommercial LLMs, including GPT-3.5 and GPT-4o-mini, on benchmark and real-world\ntasks, demonstrating the effectiveness of domain supervision and model\nalignment for sensor reasoning. Our code repository and datasets can be found\nat https://github.com/BASHLab/LLaSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable systems can recognize activities from IMU data but often fail to\nexplain their underlying causes or contextual significance. To address this\nlimitation, we introduce two large-scale resources: SensorCap, comprising\n35,960 IMU--caption pairs, and OpenSQA, with 199,701 question--answer pairs\ndesigned for causal and explanatory reasoning. OpenSQA includes a curated\ntuning split (Tune-OpenSQA) optimized for scientific accuracy, narrative\nclarity, and diagnostic insight. Leveraging these datasets, we develop LLaSA\n(Large Language and Sensor Assistant), a family of compact sensor-aware\nlanguage models (7B and 13B) that generate interpretable, context-rich\nresponses to open-ended questions grounded in raw IMU data. LLaSA outperforms\ncommercial LLMs, including GPT-3.5 and GPT-4o-mini, on benchmark and real-world\ntasks, demonstrating the effectiveness of domain supervision and model\nalignment for sensor reasoning. Our code repository and datasets can be found\nat https://github.com/BASHLab/LLaSA."
                },
                "authors": [
                    {
                        "name": "Sheikh Asif Imran"
                    },
                    {
                        "name": "Mohammad Nur Hossain Khan"
                    },
                    {
                        "name": "Subrata Biswas"
                    },
                    {
                        "name": "Bashima Islam"
                    }
                ],
                "author_detail": {
                    "name": "Bashima Islam"
                },
                "author": "Bashima Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14498v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14498v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17805v1",
                "updated": "2025-09-22T14:00:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    0,
                    20,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T14:00:20Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    14,
                    0,
                    20,
                    0,
                    265,
                    0
                ],
                "title": "Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric\n  Assessment of 2D Projections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric\n  Assessment of 2D Projections"
                },
                "summary": "Objective: To systematically quantify the effect of the camera view (frontal\nvs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D\nmotion capture ground truth. Methods: Gait data from 18 subjects were recorded\nsimultaneously using frontal, lateral and 3D motion capture systems. Pose\nestimation used YOLOv8. Four metrics were assessed to evaluate agreement:\nDynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation\n(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution\ndifferences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank\ntests (significance: $p < 0.05$) and Cliff's delta ($\\delta$) were used to\nmeasure statistical differences and effect sizes. Results: Lateral views\nsignificantly outperformed frontal views for sagittal plane kinematics: step\nlength (DTW: $53.08 \\pm 24.50$ vs. $69.87 \\pm 25.36$, $p = 0.005$) and knee\nrotation (DTW: $106.46 \\pm 38.57$ vs. $155.41 \\pm 41.77$, $p = 0.004$). Frontal\nviews were superior for symmetry parameters: trunk rotation (KLD: $0.09 \\pm\n0.06$ vs. $0.30 \\pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:\n$105.77 \\pm 29.72$ vs. $75.20 \\pm 20.38$, $p = 0.003$). Effect sizes were\nmedium-to-large ($\\delta: 0.34$--$0.76$). Conclusion: Camera view critically\nimpacts gait parameter accuracy. Lateral views are optimal for sagittal\nkinematics; frontal views excel for trunk symmetry. Significance: This first\nsystematic evidence enables data-driven camera deployment in 2D gait analysis,\nenhancing clinical utility. Future implementations should leverage both views\nvia disease-oriented setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: To systematically quantify the effect of the camera view (frontal\nvs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D\nmotion capture ground truth. Methods: Gait data from 18 subjects were recorded\nsimultaneously using frontal, lateral and 3D motion capture systems. Pose\nestimation used YOLOv8. Four metrics were assessed to evaluate agreement:\nDynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation\n(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution\ndifferences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank\ntests (significance: $p < 0.05$) and Cliff's delta ($\\delta$) were used to\nmeasure statistical differences and effect sizes. Results: Lateral views\nsignificantly outperformed frontal views for sagittal plane kinematics: step\nlength (DTW: $53.08 \\pm 24.50$ vs. $69.87 \\pm 25.36$, $p = 0.005$) and knee\nrotation (DTW: $106.46 \\pm 38.57$ vs. $155.41 \\pm 41.77$, $p = 0.004$). Frontal\nviews were superior for symmetry parameters: trunk rotation (KLD: $0.09 \\pm\n0.06$ vs. $0.30 \\pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:\n$105.77 \\pm 29.72$ vs. $75.20 \\pm 20.38$, $p = 0.003$). Effect sizes were\nmedium-to-large ($\\delta: 0.34$--$0.76$). Conclusion: Camera view critically\nimpacts gait parameter accuracy. Lateral views are optimal for sagittal\nkinematics; frontal views excel for trunk symmetry. Significance: This first\nsystematic evidence enables data-driven camera deployment in 2D gait analysis,\nenhancing clinical utility. Future implementations should leverage both views\nvia disease-oriented setups."
                },
                "authors": [
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Huili Peng"
                    },
                    {
                        "name": "Yong Hu"
                    },
                    {
                        "name": "Kenneth MC. Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth MC. Cheung"
                },
                "author": "Kenneth MC. Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17800v1",
                "updated": "2025-09-22T13:55:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    55,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:55:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    55,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Convolutional Neural Network Optimization for Beehive Classification\n  Using Bioacoustic Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Network Optimization for Beehive Classification\n  Using Bioacoustic Signals"
                },
                "summary": "The behavior of honeybees is an important ecological phenomenon not only in\nterms of honey and beeswax production but also due to the proliferation of\nflora and fauna around it. The best way to study this significant phenomenon is\nby non-invasive monitoring of beehives using the sounds produced by various\nbody movements that give out audio signals which can be exploited for various\npredictions related to the objectives mentioned above. This study investigates\nthe application of Convolutional Neural Networks to classify and monitor\ndifferent hive states with the help of joint time and frequency image\nrepresentations such as Spectrogram, Mel-Spectrogram, Smoothed-Spectrogram, and\nCochleagram. Our findings indicate that the Cochleagram outperformed all the\nother representations, achieving an accuracy of 98.31% on unseen data.\nFurthermore, we employed various strategies including pruning, quantization,\nand knowledge distillation to optimize the network and prevent any potential\nissues with model size. With these optimizations, the network size was lowered\nby 91.8% and the inference time was accelerated by 66%, increasing its\nsuitability for real-time applications. Thus our study emphasizes the\nsignificance of using optimization approaches to minimize model size, avoid\ndeployment problems, and expedite inference for real-time application as well\nas the selection of an appropriate time-frequency representation for optimal\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The behavior of honeybees is an important ecological phenomenon not only in\nterms of honey and beeswax production but also due to the proliferation of\nflora and fauna around it. The best way to study this significant phenomenon is\nby non-invasive monitoring of beehives using the sounds produced by various\nbody movements that give out audio signals which can be exploited for various\npredictions related to the objectives mentioned above. This study investigates\nthe application of Convolutional Neural Networks to classify and monitor\ndifferent hive states with the help of joint time and frequency image\nrepresentations such as Spectrogram, Mel-Spectrogram, Smoothed-Spectrogram, and\nCochleagram. Our findings indicate that the Cochleagram outperformed all the\nother representations, achieving an accuracy of 98.31% on unseen data.\nFurthermore, we employed various strategies including pruning, quantization,\nand knowledge distillation to optimize the network and prevent any potential\nissues with model size. With these optimizations, the network size was lowered\nby 91.8% and the inference time was accelerated by 66%, increasing its\nsuitability for real-time applications. Thus our study emphasizes the\nsignificance of using optimization approaches to minimize model size, avoid\ndeployment problems, and expedite inference for real-time application as well\nas the selection of an appropriate time-frequency representation for optimal\nperformance."
                },
                "authors": [
                    {
                        "name": "Harshit"
                    },
                    {
                        "name": "Rahul Jana"
                    },
                    {
                        "name": "Ritesh Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ritesh Kumar"
                },
                "author": "Ritesh Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17796v1",
                "updated": "2025-09-22T13:52:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    52,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:52:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    52,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Findings of the Fourth Shared Task on Multilingual Coreference\n  Resolution: Can LLMs Dethrone Traditional Approaches?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the Fourth Shared Task on Multilingual Coreference\n  Resolution: Can LLMs Dethrone Traditional Approaches?"
                },
                "summary": "The paper presents an overview of the fourth edition of the Shared Task on\nMultilingual Coreference Resolution, organized as part of the CODI-CRAC 2025\nworkshop. As in the previous editions, participants were challenged to develop\nsystems that identify mentions and cluster them according to identity\ncoreference.\n  A key innovation of this year's task was the introduction of a dedicated\nLarge Language Model (LLM) track, featuring a simplified plaintext format\ndesigned to be more suitable for LLMs than the original CoNLL-U representation.\n  The task also expanded its coverage with three new datasets in two additional\nlanguages, using version 1.3 of CorefUD - a harmonized multilingual collection\nof 22 datasets in 17 languages.\n  In total, nine systems participated, including four LLM-based approaches (two\nfine-tuned and two using few-shot adaptation). While traditional systems still\nkept the lead, LLMs showed clear potential, suggesting they may soon challenge\nestablished approaches in future editions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper presents an overview of the fourth edition of the Shared Task on\nMultilingual Coreference Resolution, organized as part of the CODI-CRAC 2025\nworkshop. As in the previous editions, participants were challenged to develop\nsystems that identify mentions and cluster them according to identity\ncoreference.\n  A key innovation of this year's task was the introduction of a dedicated\nLarge Language Model (LLM) track, featuring a simplified plaintext format\ndesigned to be more suitable for LLMs than the original CoNLL-U representation.\n  The task also expanded its coverage with three new datasets in two additional\nlanguages, using version 1.3 of CorefUD - a harmonized multilingual collection\nof 22 datasets in 17 languages.\n  In total, nine systems participated, including four LLM-based approaches (two\nfine-tuned and two using few-shot adaptation). While traditional systems still\nkept the lead, LLMs showed clear potential, suggesting they may soon challenge\nestablished approaches in future editions."
                },
                "authors": [
                    {
                        "name": "Michal Novk"
                    },
                    {
                        "name": "Miloslav Konopk"
                    },
                    {
                        "name": "Anna Nedoluzhko"
                    },
                    {
                        "name": "Martin Popel"
                    },
                    {
                        "name": "Ondej Prak"
                    },
                    {
                        "name": "Jakub Sido"
                    },
                    {
                        "name": "Milan Straka"
                    },
                    {
                        "name": "Zdenk abokrtsk"
                    },
                    {
                        "name": "Daniel Zeman"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Zeman"
                },
                "author": "Daniel Zeman",
                "arxiv_comment": "Accepted to CODI-CRAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11405v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11405v2",
                "updated": "2025-09-22T13:51:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    51,
                    23,
                    0,
                    265,
                    0
                ],
                "published": "2025-07-15T15:23:53Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    15,
                    23,
                    53,
                    1,
                    196,
                    0
                ],
                "title": "DCR: Quantifying Data Contamination in LLMs Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCR: Quantifying Data Contamination in LLMs Evaluation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data during the training process, inflating performance metrics, and\nundermining genuine generalization assessment. This paper introduces the Data\nContamination Risk (DCR) framework, a lightweight, interpretable pipeline\ndesigned to detect and quantify BDC risk across four granular levels: semantic,\ninformational, data, and label. By synthesizing contamination scores via a\nfuzzy inference system, DCR produces a unified DCR Factor that adjusts raw\naccuracy to reflect contamination-aware performance. Validated on 9 LLMs\n(0.5B-72B) across sentiment analysis, fake news detection, and arithmetic\nreasoning tasks, the DCR framework reliably diagnoses contamination severity\nand with accuracy adjusted using the DCR Factor to within 4% average error\nacross the three benchmarks compared to the uncontaminated baseline.\nEmphasizing computational efficiency and transparency, DCR provides a practical\ntool for integrating contamination assessment into routine evaluations,\nfostering fairer comparisons and enhancing the credibility of LLM benchmarking\npractices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data during the training process, inflating performance metrics, and\nundermining genuine generalization assessment. This paper introduces the Data\nContamination Risk (DCR) framework, a lightweight, interpretable pipeline\ndesigned to detect and quantify BDC risk across four granular levels: semantic,\ninformational, data, and label. By synthesizing contamination scores via a\nfuzzy inference system, DCR produces a unified DCR Factor that adjusts raw\naccuracy to reflect contamination-aware performance. Validated on 9 LLMs\n(0.5B-72B) across sentiment analysis, fake news detection, and arithmetic\nreasoning tasks, the DCR framework reliably diagnoses contamination severity\nand with accuracy adjusted using the DCR Factor to within 4% average error\nacross the three benchmarks compared to the uncontaminated baseline.\nEmphasizing computational efficiency and transparency, DCR provides a practical\ntool for integrating contamination assessment into routine evaluations,\nfostering fairer comparisons and enhancing the credibility of LLM benchmarking\npractices."
                },
                "authors": [
                    {
                        "name": "Cheng Xu"
                    },
                    {
                        "name": "Nan Yan"
                    },
                    {
                        "name": "Shuhao Guan"
                    },
                    {
                        "name": "Changhong Jin"
                    },
                    {
                        "name": "Yuke Mei"
                    },
                    {
                        "name": "Yibing Guo"
                    },
                    {
                        "name": "M-Tahar Kechadi"
                    }
                ],
                "author_detail": {
                    "name": "M-Tahar Kechadi"
                },
                "author": "M-Tahar Kechadi",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11405v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11405v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17788v1",
                "updated": "2025-09-22T13:49:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    49,
                    37,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:49:37Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    49,
                    37,
                    0,
                    265,
                    0
                ],
                "title": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for\n  Millions of Multi-Style Official Accounts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for\n  Millions of Multi-Style Official Accounts"
                },
                "summary": "Conversational agents deployed in industrial-scale official account platforms\nmust generate responses that are both contextually grounded and stylistically\naligned-requirements that existing methods struggle to meet. Chain-of-thought\n(CoT) prompting induces significant latency due to multi-turn reasoning;\nper-account fine-tuning is computationally prohibitive; and long prompt-based\nmethods degrade the model's ability to grasp injected context and style. In\nthis paper, we propose WeStar, a lite-adaptive framework for stylized\ncontextual question answering that scales to millions of official accounts.\nWeStar combines context-grounded generation via RAG with style-aware generation\nusing Parametric RAG (PRAG), where LoRA modules are dynamically activated per\nstyle cluster. Our contributions are fourfold: (1) We introduce WeStar, a\nunified framework capable of serving large volumes of official accounts with\nminimal overhead. (2) We propose a multi-dimensional, cluster-based parameter\nsharing scheme that enables compact style representation while preserving\nstylistic diversity. (3) We develop a style-enhanced Direct Preference\nOptimization (SeDPO) method to optimize each style cluster's parameters for\nimproved generation quality. (4) Experiments on a large-scale industrial\ndataset validate the effectiveness and efficiency of WeStar, underscoring its\npracitical value in real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents deployed in industrial-scale official account platforms\nmust generate responses that are both contextually grounded and stylistically\naligned-requirements that existing methods struggle to meet. Chain-of-thought\n(CoT) prompting induces significant latency due to multi-turn reasoning;\nper-account fine-tuning is computationally prohibitive; and long prompt-based\nmethods degrade the model's ability to grasp injected context and style. In\nthis paper, we propose WeStar, a lite-adaptive framework for stylized\ncontextual question answering that scales to millions of official accounts.\nWeStar combines context-grounded generation via RAG with style-aware generation\nusing Parametric RAG (PRAG), where LoRA modules are dynamically activated per\nstyle cluster. Our contributions are fourfold: (1) We introduce WeStar, a\nunified framework capable of serving large volumes of official accounts with\nminimal overhead. (2) We propose a multi-dimensional, cluster-based parameter\nsharing scheme that enables compact style representation while preserving\nstylistic diversity. (3) We develop a style-enhanced Direct Preference\nOptimization (SeDPO) method to optimize each style cluster's parameters for\nimproved generation quality. (4) Experiments on a large-scale industrial\ndataset validate the effectiveness and efficiency of WeStar, underscoring its\npracitical value in real-world deployment."
                },
                "authors": [
                    {
                        "name": "Xingyu Fan"
                    },
                    {
                        "name": "Feifei Li"
                    },
                    {
                        "name": "Wenhui Que"
                    },
                    {
                        "name": "Hailong Li"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Li"
                },
                "author": "Hailong Li",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17784v1",
                "updated": "2025-09-22T13:45:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    45,
                    17,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:45:17Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    45,
                    17,
                    0,
                    265,
                    0
                ],
                "title": "Revealing Multimodal Causality with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Multimodal Causality with Large Language Models"
                },
                "summary": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data."
                },
                "authors": [
                    {
                        "name": "Jin Li"
                    },
                    {
                        "name": "Shoujin Wang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Longbing Cao"
                    },
                    {
                        "name": "Shui Yu"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04931v2",
                "updated": "2025-09-22T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    39,
                    16,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-07T13:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    52,
                    37,
                    4,
                    38,
                    0
                ],
                "title": "Breaking the News: Taking the Roles of Influencer vs. Journalist in a\n  LLM-Based Game for Raising Misinformation Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the News: Taking the Roles of Influencer vs. Journalist in a\n  LLM-Based Game for Raising Misinformation Awareness"
                },
                "summary": "Effectively mitigating online misinformation requires understanding of their\nmechanisms and learning of practical skills for identification and\ncounteraction. Serious games may serve as tools for combating misinformation,\nteaching players to recognize common misinformation tactics, and improving\ntheir skills of discernment. However, current interventions are designed as\nsingle-player, choice-based games, which present players with limited\npredefined choices. Such restrictions reduce replayability and may lead to an\noverly simplistic understanding of misinformation and how to debunk them. This\nstudy seeks to empower people to understand opinion-influencing and\nmisinformation-debunking processes. We created a Player vs. Player (PvP) game\nin which participants attempt to generate or debunk misinformation to convince\nthe public opinion represented by LLM. Using a within-subjects mixed-methods\nstudy design (N=47), we found that this game significantly raised participants'\nmedia literacy and improved their ability to identify misinformation.\nQualitative analyses revealed how participants' use of debunking and content\ncreation strategies deepened their understanding of misinformation. This work\nshows the potential for illuminating contrasting viewpoints of social issues by\nLLM-based mechanics in PvP games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively mitigating online misinformation requires understanding of their\nmechanisms and learning of practical skills for identification and\ncounteraction. Serious games may serve as tools for combating misinformation,\nteaching players to recognize common misinformation tactics, and improving\ntheir skills of discernment. However, current interventions are designed as\nsingle-player, choice-based games, which present players with limited\npredefined choices. Such restrictions reduce replayability and may lead to an\noverly simplistic understanding of misinformation and how to debunk them. This\nstudy seeks to empower people to understand opinion-influencing and\nmisinformation-debunking processes. We created a Player vs. Player (PvP) game\nin which participants attempt to generate or debunk misinformation to convince\nthe public opinion represented by LLM. Using a within-subjects mixed-methods\nstudy design (N=47), we found that this game significantly raised participants'\nmedia literacy and improved their ability to identify misinformation.\nQualitative analyses revealed how participants' use of debunking and content\ncreation strategies deepened their understanding of misinformation. This work\nshows the potential for illuminating contrasting viewpoints of social issues by\nLLM-based mechanics in PvP games."
                },
                "authors": [
                    {
                        "name": "Huiyun Tang"
                    },
                    {
                        "name": "Songqi Sun"
                    },
                    {
                        "name": "Kexin Nie"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Anastasia Sergeeva"
                    },
                    {
                        "name": "Ray LC"
                    }
                ],
                "author_detail": {
                    "name": "Ray LC"
                },
                "author": "Ray LC",
                "arxiv_comment": "Accepted to ACM CHI PLAY 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21693v3",
                "updated": "2025-09-22T13:38:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    38,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-27T19:29:40Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    19,
                    29,
                    40,
                    1,
                    147,
                    0
                ],
                "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural\n  Awareness Evaluation for LLMs"
                },
                "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge."
                },
                "authors": [
                    {
                        "name": "Raoyuan Zhao"
                    },
                    {
                        "name": "Beiduo Chen"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Hedderich"
                },
                "author": "Michael A. Hedderich",
                "arxiv_comment": "Accepted by EMNLP 2025 Findings, 33 pages, 30 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14746v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14746v4",
                "updated": "2025-09-22T13:37:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    37,
                    52,
                    0,
                    265,
                    0
                ],
                "published": "2024-02-22T18:06:19Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    6,
                    19,
                    3,
                    53,
                    0
                ],
                "title": "Scaling Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Efficient LLMs"
                },
                "summary": "Trained LLMs in the transformer architecture are typically sparse in that\nmost of the parameters are negligible, raising questions on efficiency.\nFurthermore, the so called \"AI scaling law\" for transformers suggests that the\nnumber of parameters must scale linearly with the size of the data. In\nresponse, we inquire into efficient LLMs, i.e. those with the fewest parameters\nthat achieve the desired accuracy on a training corpus. Specifically, by\ncomparing theoretical and empirical estimates of the Kullback-Liebler\ndivergence, we derive a natural AI scaling law that the number of parameters in\nan efficient LLM scales as $D^{\\gamma}$ where $D$ is the size of the training\ndata and $ \\gamma \\in [0.44, 0.72]$, suggesting the existence of more efficient\narchitectures. Against this backdrop, we propose recurrent transformers,\ncombining the efficacy of transformers with the efficiency of recurrent\nnetworks, progressively applying a single transformer layer to a fixed-width\nsliding window across the input sequence. Recurrent transformers (a) run in\nlinear time in the sequence length, (b) are memory-efficient and amenable to\nparallel processing in large batches, (c) learn to forget history for language\ntasks, or accumulate history for long range tasks like copy and selective copy,\nand (d) are amenable to curriculum training to overcome vanishing gradients. In\nour experiments, we find that recurrent transformers perform favorably on\nbenchmark tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained LLMs in the transformer architecture are typically sparse in that\nmost of the parameters are negligible, raising questions on efficiency.\nFurthermore, the so called \"AI scaling law\" for transformers suggests that the\nnumber of parameters must scale linearly with the size of the data. In\nresponse, we inquire into efficient LLMs, i.e. those with the fewest parameters\nthat achieve the desired accuracy on a training corpus. Specifically, by\ncomparing theoretical and empirical estimates of the Kullback-Liebler\ndivergence, we derive a natural AI scaling law that the number of parameters in\nan efficient LLM scales as $D^{\\gamma}$ where $D$ is the size of the training\ndata and $ \\gamma \\in [0.44, 0.72]$, suggesting the existence of more efficient\narchitectures. Against this backdrop, we propose recurrent transformers,\ncombining the efficacy of transformers with the efficiency of recurrent\nnetworks, progressively applying a single transformer layer to a fixed-width\nsliding window across the input sequence. Recurrent transformers (a) run in\nlinear time in the sequence length, (b) are memory-efficient and amenable to\nparallel processing in large batches, (c) learn to forget history for language\ntasks, or accumulate history for long range tasks like copy and selective copy,\nand (d) are amenable to curriculum training to overcome vanishing gradients. In\nour experiments, we find that recurrent transformers perform favorably on\nbenchmark tests."
                },
                "authors": [
                    {
                        "name": "B. N. Kausik"
                    }
                ],
                "author_detail": {
                    "name": "B. N. Kausik"
                },
                "author": "B. N. Kausik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14746v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14746v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17766v1",
                "updated": "2025-09-22T13:26:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    26,
                    24,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:26:24Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    26,
                    24,
                    0,
                    265,
                    0
                ],
                "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn\n  Dialogue"
                },
                "summary": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents."
                },
                "authors": [
                    {
                        "name": "Ziyi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziyi Liu"
                },
                "author": "Ziyi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17759v1",
                "updated": "2025-09-22T13:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    21,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:21:11Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    21,
                    11,
                    0,
                    265,
                    0
                ],
                "title": "MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic\n  Manipulation Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic\n  Manipulation Policies"
                },
                "summary": "Scaling real robot data is a key bottleneck in imitation learning, leading to\nthe use of auxiliary data for policy training. While other aspects of robotic\nmanipulation such as image or language understanding may be learned from\ninternet-based datasets, acquiring motion knowledge remains challenging. Human\ndata, with its rich diversity of manipulation behaviors, offers a valuable\nresource for this purpose. While previous works show that using human data can\nbring benefits, such as improving robustness and training efficiency, it\nremains unclear whether it can realize its greatest advantage: enabling robot\npolicies to directly learn new motions for task completion. In this paper, we\nsystematically explore this potential through multi-task human-robot\ncotraining. We introduce MotionTrans, a framework that includes a data\ncollection system, a human data transformation pipeline, and a weighted\ncotraining strategy. By cotraining 30 human-robot tasks simultaneously, we\ndirecly transfer motions of 13 tasks from human data to deployable end-to-end\nrobot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot\nmanner. MotionTrans also significantly enhances pretraining-finetuning\nperformance (+40% success rate). Through ablation study, we also identify key\nfactors for successful motion learning: cotraining with robot data and broad\ntask-related motion coverage. These findings unlock the potential of\nmotion-level learning from human data, offering insights into its effective use\nfor training robotic manipulation policies. All data, code, and model weights\nare open-sourced https://motiontrans.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling real robot data is a key bottleneck in imitation learning, leading to\nthe use of auxiliary data for policy training. While other aspects of robotic\nmanipulation such as image or language understanding may be learned from\ninternet-based datasets, acquiring motion knowledge remains challenging. Human\ndata, with its rich diversity of manipulation behaviors, offers a valuable\nresource for this purpose. While previous works show that using human data can\nbring benefits, such as improving robustness and training efficiency, it\nremains unclear whether it can realize its greatest advantage: enabling robot\npolicies to directly learn new motions for task completion. In this paper, we\nsystematically explore this potential through multi-task human-robot\ncotraining. We introduce MotionTrans, a framework that includes a data\ncollection system, a human data transformation pipeline, and a weighted\ncotraining strategy. By cotraining 30 human-robot tasks simultaneously, we\ndirecly transfer motions of 13 tasks from human data to deployable end-to-end\nrobot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot\nmanner. MotionTrans also significantly enhances pretraining-finetuning\nperformance (+40% success rate). Through ablation study, we also identify key\nfactors for successful motion learning: cotraining with robot data and broad\ntask-related motion coverage. These findings unlock the potential of\nmotion-level learning from human data, offering insights into its effective use\nfor training robotic manipulation policies. All data, code, and model weights\nare open-sourced https://motiontrans.github.io/."
                },
                "authors": [
                    {
                        "name": "Chengbo Yuan"
                    },
                    {
                        "name": "Rui Zhou"
                    },
                    {
                        "name": "Mengzhen Liu"
                    },
                    {
                        "name": "Yingdong Hu"
                    },
                    {
                        "name": "Shengjie Wang"
                    },
                    {
                        "name": "Li Yi"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16887v2",
                "updated": "2025-09-22T13:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2024-05-27T07:10:04Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    7,
                    10,
                    4,
                    0,
                    148,
                    0
                ],
                "title": "A Large Language Model-based multi-agent manufacturing system for\n  intelligent shopfloor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-based multi-agent manufacturing system for\n  intelligent shopfloor"
                },
                "summary": "As customer demand for multi-variety and small-batch production increases,\ndynamic disturbances place greater demands on manufacturing systems. To address\nsuch challenges, researchers proposed the multi-agent manufacturing system.\nHowever, conventional agent negotiation typically relies on pre-defined and\nfixed heuristic rules, which are ill-suited to managing complex and fluctuating\ndisturbances. In current implementations, mainstream approaches based on\nreinforcement learning require the development of simulators and training\nmodels specific to a given shopfloor, necessitating substantial computational\nresources and lacking scalability. To overcome this limitation, the present\nstudy proposes a Large Language Model-based (LLM-based) multi-agent\nmanufacturing system for intelligent shopfloor management. By defining the\ndiverse modules of agents and their collaborative methods, this system\nfacilitates the processing of all workpieces with minimal human intervention.\nThe agents in this system consist of the Machine Server Module (MSM), Bid\nInviter Module (BIM), Bidder Module (BM), Thinking Module (TM), and Decision\nModule (DM). By harnessing the reasoning capabilities of LLMs, these modules\nenable agents to dynamically analyze shopfloor information and select\nappropriate processing machines. The LLM-based modules, predefined by system\nprompts, provide dynamic functionality for the system without the need for\npre-training. Extensive experiments were conducted in physical shopfloor\nsettings. The results demonstrate that the proposed system exhibits strong\nadaptability, and achieves superior performance (makespan) and stability (as\nmeasured by sample standard deviation) compared to other approaches without\nrequiring pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As customer demand for multi-variety and small-batch production increases,\ndynamic disturbances place greater demands on manufacturing systems. To address\nsuch challenges, researchers proposed the multi-agent manufacturing system.\nHowever, conventional agent negotiation typically relies on pre-defined and\nfixed heuristic rules, which are ill-suited to managing complex and fluctuating\ndisturbances. In current implementations, mainstream approaches based on\nreinforcement learning require the development of simulators and training\nmodels specific to a given shopfloor, necessitating substantial computational\nresources and lacking scalability. To overcome this limitation, the present\nstudy proposes a Large Language Model-based (LLM-based) multi-agent\nmanufacturing system for intelligent shopfloor management. By defining the\ndiverse modules of agents and their collaborative methods, this system\nfacilitates the processing of all workpieces with minimal human intervention.\nThe agents in this system consist of the Machine Server Module (MSM), Bid\nInviter Module (BIM), Bidder Module (BM), Thinking Module (TM), and Decision\nModule (DM). By harnessing the reasoning capabilities of LLMs, these modules\nenable agents to dynamically analyze shopfloor information and select\nappropriate processing machines. The LLM-based modules, predefined by system\nprompts, provide dynamic functionality for the system without the need for\npre-training. Extensive experiments were conducted in physical shopfloor\nsettings. The results demonstrate that the proposed system exhibits strong\nadaptability, and achieves superior performance (makespan) and stability (as\nmeasured by sample standard deviation) compared to other approaches without\nrequiring pre-training."
                },
                "authors": [
                    {
                        "name": "Zhen Zhao"
                    },
                    {
                        "name": "Dunbing Tang"
                    },
                    {
                        "name": "Changchun Liu"
                    },
                    {
                        "name": "Liping Wang"
                    },
                    {
                        "name": "Zequn Zhang"
                    },
                    {
                        "name": "Haihua Zhu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qingwei Nie"
                    },
                    {
                        "name": "Yuchen Ji"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Ji"
                },
                "author": "Yuchen Ji",
                "arxiv_doi": "10.1016/j.aei.2025.103888",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.aei.2025.103888",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Zhao Z, Tang D, Liu C, et al. A Large language model-based\n  multi-agent manufacturing system for intelligent shopfloors[J]. Advanced\n  Engineering Informatics, 2026, 69: 103888",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20099v2",
                "updated": "2025-09-22T13:18:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    18,
                    46,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-26T15:08:23Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    15,
                    8,
                    23,
                    0,
                    146,
                    0
                ],
                "title": "Large Language Models Meet Knowledge Graphs for Question Answering:\n  Synthesis and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Knowledge Graphs for Question Answering:\n  Synthesis and Opportunities"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art methods in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art methods in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities."
                },
                "authors": [
                    {
                        "name": "Chuangtao Ma"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Tianxing Wu"
                    },
                    {
                        "name": "Arijit Khan"
                    },
                    {
                        "name": "Haofen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haofen Wang"
                },
                "author": "Haofen Wang",
                "arxiv_comment": "Accepted at EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22118v2",
                "updated": "2025-09-22T13:16:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    16,
                    16,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-28T08:47:10Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    8,
                    47,
                    10,
                    2,
                    148,
                    0
                ],
                "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of\n  Two Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of\n  Two Approaches"
                },
                "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup."
                },
                "authors": [
                    {
                        "name": "Alan Ramponi"
                    },
                    {
                        "name": "Marco Rovera"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Sara Tonelli"
                    }
                ],
                "author_detail": {
                    "name": "Sara Tonelli"
                },
                "author": "Sara Tonelli",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17743v2",
                "updated": "2025-09-23T07:31:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    7,
                    31,
                    0,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T13:06:17Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    6,
                    17,
                    0,
                    265,
                    0
                ],
                "title": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA"
                },
                "summary": "Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in generating program\nworkflows for visual tasks. However, previous approaches often rely on\nclosed-source models, lack systematic reasoning, and struggle with long-form\nvideo question answering (videoQA). To address these challenges, we introduce\nthe FS-VisPR framework, an adaptive visual program reasoning approach that\nbalances fast reasoning for simple queries with slow reasoning for difficult\nones. First, we design efficient visual modules (e.g., key clip retrieval and\nsubtitle retrieval) to support long-form video tasks. Then, we construct a\ndiverse and high-quality fast-slow reasoning dataset with a strong LLM to align\nopen-source language models' ability to generate visual program workflows as\nFS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple\nqueries are directly solved by VideoLLMs, while difficult ones invoke visual\nprogram reasoning, motivated by human-like reasoning processes. During this\nprocess, low-confidence fast-thinking answers will trigger a second-stage\nslow-reasoning process, and a fallback mechanism to fast reasoning is activated\nif the program execution fails. Moreover, we improve visual programs through\nparameter search during both training and inference. By adjusting the\nparameters of the visual modules within the program, multiple variants are\ngenerated: during training, programs that yield correct answers are selected,\nwhile during inference, the program with the highest confidence result is\napplied. Experiments show that FS-VisPR improves both efficiency and\nreliability in visual program workflows. It achieves 50.4% accuracy on LVBench,\nsurpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME."
                },
                "authors": [
                    {
                        "name": "Chenglin Li"
                    },
                    {
                        "name": "Feng Han"
                    },
                    {
                        "name": "Feng Tao"
                    },
                    {
                        "name": "Ruilin Li"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17740v1",
                "updated": "2025-09-22T13:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    5,
                    29,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    5,
                    29,
                    0,
                    265,
                    0
                ],
                "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal\n  LLMs in Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal\n  LLMs in Image Classification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding."
                },
                "authors": [
                    {
                        "name": "Yiwen Jiang"
                    },
                    {
                        "name": "Deval Mehta"
                    },
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Yaling Shen"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Zongyuan Ge"
                    }
                ],
                "author_detail": {
                    "name": "Zongyuan Ge"
                },
                "author": "Zongyuan Ge",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17730v1",
                "updated": "2025-09-22T13:00:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    0,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T13:00:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    13,
                    0,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement\n  Learning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement\n  Learning in LLMs"
                },
                "summary": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods."
                },
                "authors": [
                    {
                        "name": "Bonan Zhang"
                    },
                    {
                        "name": "Zhongqi Chen"
                    },
                    {
                        "name": "Bowen Song"
                    },
                    {
                        "name": "Qinya Li"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04760v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04760v5",
                "updated": "2025-09-22T12:50:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    50,
                    51,
                    0,
                    265,
                    0
                ],
                "published": "2024-05-08T02:09:17Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    2,
                    9,
                    17,
                    2,
                    129,
                    0
                ],
                "title": "Large Language Models for Cyber Security: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cyber Security: A Systematic Literature Review"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in a variety of\napplication domains, including cybersecurity. As the volume and sophistication\nof cyber threats continue to grow, there is an increasing need for intelligent\nsystems that can automatically detect vulnerabilities, analyze malware, and\nrespond to attacks. In this survey, we conduct a comprehensive review of the\nliterature on the application of LLMs in cybersecurity~(LLM4Security). By\ncomprehensively collecting over 40K relevant papers and systematically\nanalyzing 185 papers from top security and software engineering venues, we aim\nto provide a holistic view of how LLMs are being used to solve diverse problems\nacross the cybersecurity domain. Through our analysis, we identify several key\nfindings. First, we observe that LLMs are being applied to an expanding range\nof cybersecurity tasks, including vulnerability detection, malware analysis,\nand network intrusion detection. Second, we analyze application trends of\ndifferent LLM architectures (such as encoder-only, encoder-decoder, and\ndecoder-only) across security domains. Third, we identify increasingly\nsophisticated techniques for adapting LLMs to cybersecurity, such as advanced\nfine-tuning, prompt engineering, and external augmentation strategies. A\nsignificant emerging trend is the use of LLM-based autonomous agents, which\nrepresent a paradigm shift from single-task execution to orchestrating complex,\nmulti-step security workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in a variety of\napplication domains, including cybersecurity. As the volume and sophistication\nof cyber threats continue to grow, there is an increasing need for intelligent\nsystems that can automatically detect vulnerabilities, analyze malware, and\nrespond to attacks. In this survey, we conduct a comprehensive review of the\nliterature on the application of LLMs in cybersecurity~(LLM4Security). By\ncomprehensively collecting over 40K relevant papers and systematically\nanalyzing 185 papers from top security and software engineering venues, we aim\nto provide a holistic view of how LLMs are being used to solve diverse problems\nacross the cybersecurity domain. Through our analysis, we identify several key\nfindings. First, we observe that LLMs are being applied to an expanding range\nof cybersecurity tasks, including vulnerability detection, malware analysis,\nand network intrusion detection. Second, we analyze application trends of\ndifferent LLM architectures (such as encoder-only, encoder-decoder, and\ndecoder-only) across security domains. Third, we identify increasingly\nsophisticated techniques for adapting LLMs to cybersecurity, such as advanced\nfine-tuning, prompt engineering, and external augmentation strategies. A\nsignificant emerging trend is the use of LLM-based autonomous agents, which\nrepresent a paradigm shift from single-task execution to orchestrating complex,\nmulti-step security workflows."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Ningke Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Ting Yu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04760v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04760v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17711v1",
                "updated": "2025-09-22T12:48:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    48,
                    42,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:48:42Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    48,
                    42,
                    0,
                    265,
                    0
                ],
                "title": "DA-Mamba: Dialogue-aware selective state-space model for multimodal\n  engagement estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DA-Mamba: Dialogue-aware selective state-space model for multimodal\n  engagement estimation"
                },
                "summary": "Human engagement estimation in conversational scenarios is essential for\napplications such as adaptive tutoring, remote healthcare assessment, and\nsocially aware human--computer interaction. Engagement is a dynamic, multimodal\nsignal conveyed by facial expressions, speech, gestures, and behavioral cues\nover time. In this work we introduce DA-Mamba, a dialogue-aware multimodal\narchitecture that replaces attention-heavy dialogue encoders with Mamba-based\nselective state-space processing to achieve linear time and memory complexity\nwhile retaining expressive cross-modal reasoning. We design a Mamba\ndialogue-aware selective state-space model composed of three core modules: a\nDialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group\nFusion and Partner-Group Fusion, these modules achieve expressive dialogue\nunderstanding. Extensive experiments on three standard benchmarks (NoXi,\nNoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art\n(SOTA) methods in concordance correlation coefficient (CCC), while reducing\ntraining time and peak memory; these gains enable processing much longer\nsequences and facilitate real-time deployment in resource-constrained,\nmulti-party conversational settings. The source code will be available at:\nhttps://github.com/kksssssss-ssda/MMEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human engagement estimation in conversational scenarios is essential for\napplications such as adaptive tutoring, remote healthcare assessment, and\nsocially aware human--computer interaction. Engagement is a dynamic, multimodal\nsignal conveyed by facial expressions, speech, gestures, and behavioral cues\nover time. In this work we introduce DA-Mamba, a dialogue-aware multimodal\narchitecture that replaces attention-heavy dialogue encoders with Mamba-based\nselective state-space processing to achieve linear time and memory complexity\nwhile retaining expressive cross-modal reasoning. We design a Mamba\ndialogue-aware selective state-space model composed of three core modules: a\nDialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group\nFusion and Partner-Group Fusion, these modules achieve expressive dialogue\nunderstanding. Extensive experiments on three standard benchmarks (NoXi,\nNoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art\n(SOTA) methods in concordance correlation coefficient (CCC), while reducing\ntraining time and peak memory; these gains enable processing much longer\nsequences and facilitate real-time deployment in resource-constrained,\nmulti-party conversational settings. The source code will be available at:\nhttps://github.com/kksssssss-ssda/MMEA."
                },
                "authors": [
                    {
                        "name": "Shenwei Kang"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Bo Gao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Gao"
                },
                "author": "Bo Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12260v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12260v4",
                "updated": "2025-09-22T12:48:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    48,
                    4,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-18T06:51:21Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    6,
                    51,
                    21,
                    6,
                    138,
                    0
                ],
                "title": "LightRetriever: A LLM-based Text Retrieval Architecture with Extremely\n  Faster Query Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightRetriever: A LLM-based Text Retrieval Architecture with Extremely\n  Faster Query Inference"
                },
                "summary": "Large Language Models (LLMs)-based text retrieval retrieves documents\nrelevant to search queries based on vector similarities. Documents are\npre-encoded offline, while queries arrive in real-time, necessitating an\nefficient online query encoder. Although LLMs significantly enhance retrieval\ncapabilities, serving deeply parameterized LLMs slows down query inference\nthroughput and increases demands for online deployment resources. In this\npaper, we propose LightRetriever, a novel LLM-based retriever with extremely\nlightweight query encoders. Our method retains a full-sized LLM for document\nencoding, but reduces the workload of query encoding to no more than an\nembedding lookup. Compared to serving a full LLM on an A800 GPU, our method\nachieves over 1000x speedup in query encoding and over 10x increase in\nend-to-end retrieval throughput. Extensive experiments on large-scale retrieval\nbenchmarks show that LightRetriever generalizes well across diverse tasks,\nmaintaining an average of 95% retrieval performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs)-based text retrieval retrieves documents\nrelevant to search queries based on vector similarities. Documents are\npre-encoded offline, while queries arrive in real-time, necessitating an\nefficient online query encoder. Although LLMs significantly enhance retrieval\ncapabilities, serving deeply parameterized LLMs slows down query inference\nthroughput and increases demands for online deployment resources. In this\npaper, we propose LightRetriever, a novel LLM-based retriever with extremely\nlightweight query encoders. Our method retains a full-sized LLM for document\nencoding, but reduces the workload of query encoding to no more than an\nembedding lookup. Compared to serving a full LLM on an A800 GPU, our method\nachieves over 1000x speedup in query encoding and over 10x increase in\nend-to-end retrieval throughput. Extensive experiments on large-scale retrieval\nbenchmarks show that LightRetriever generalizes well across diverse tasks,\nmaintaining an average of 95% retrieval performance."
                },
                "authors": [
                    {
                        "name": "Guangyuan Ma"
                    },
                    {
                        "name": "Yongliang Ma"
                    },
                    {
                        "name": "Xuanrui Gou"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Ming Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12260v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12260v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22424v2",
                "updated": "2025-09-22T12:45:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    45,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2025-03-28T13:36:26Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    36,
                    26,
                    4,
                    87,
                    0
                ],
                "title": "CoSIL: Issue Localization via LLM-Driven Code Graph Searching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSIL: Issue Localization via LLM-Driven Code Graph Searching"
                },
                "summary": "Issue solving aims to generate patches to fix reported issues in real-world\ncode repositories according to issue descriptions. Issue localization forms the\nbasis for accurate issue solving. Recently, LLM-based issue localization\nmethods have demonstrated state-of-the-art performance. However, these methods\neither search from files mentioned in issue descriptions or in the whole\nrepository and struggle to balance the breadth and depth of the search space to\nconverge on the target efficiently. Moreover, they allow LLM to explore whole\nrepositories freely, making it challenging to control the search direction to\nprevent the LLM from searching for incorrect targets. This paper introduces\nCoSIL, an LLM-driven, powerful function-level issue localization method without\ntraining or indexing. CoSIL employs a two-phase code graph search strategy. It\nfirst conducts broad exploration at the file level using dynamically\nconstructed module call graphs, and then performs in-depth analysis at the\nfunction level by expanding the module call graph into a function call graph\nand executing iterative searches. To precisely control the search direction,\nCoSIL designs a pruner to filter unrelated directions and irrelevant contexts.\nTo avoid incorrect interaction formats in long contexts, CoSIL introduces a\nreflection mechanism that uses additional independent queries in short contexts\nto enhance formatted abilities. Experiment results demonstrate that CoSIL\nachieves a Top-1 localization accuracy of 43.3\\% and 44.6\\% on SWE-bench Lite\nand SWE-bench Verified, respectively, with Qwen2.5-Coder-32B, average\noutperforming the state-of-the-art methods by 96.04\\%. When CoSIL is integrated\ninto an issue-solving method, Agentless, the issue resolution rate improves by\n2.98\\%--30.5\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue solving aims to generate patches to fix reported issues in real-world\ncode repositories according to issue descriptions. Issue localization forms the\nbasis for accurate issue solving. Recently, LLM-based issue localization\nmethods have demonstrated state-of-the-art performance. However, these methods\neither search from files mentioned in issue descriptions or in the whole\nrepository and struggle to balance the breadth and depth of the search space to\nconverge on the target efficiently. Moreover, they allow LLM to explore whole\nrepositories freely, making it challenging to control the search direction to\nprevent the LLM from searching for incorrect targets. This paper introduces\nCoSIL, an LLM-driven, powerful function-level issue localization method without\ntraining or indexing. CoSIL employs a two-phase code graph search strategy. It\nfirst conducts broad exploration at the file level using dynamically\nconstructed module call graphs, and then performs in-depth analysis at the\nfunction level by expanding the module call graph into a function call graph\nand executing iterative searches. To precisely control the search direction,\nCoSIL designs a pruner to filter unrelated directions and irrelevant contexts.\nTo avoid incorrect interaction formats in long contexts, CoSIL introduces a\nreflection mechanism that uses additional independent queries in short contexts\nto enhance formatted abilities. Experiment results demonstrate that CoSIL\nachieves a Top-1 localization accuracy of 43.3\\% and 44.6\\% on SWE-bench Lite\nand SWE-bench Verified, respectively, with Qwen2.5-Coder-32B, average\noutperforming the state-of-the-art methods by 96.04\\%. When CoSIL is integrated\ninto an issue-solving method, Agentless, the issue resolution rate improves by\n2.98\\%--30.5\\%."
                },
                "authors": [
                    {
                        "name": "Zhonghao Jiang"
                    },
                    {
                        "name": "Xiaoxue Ren"
                    },
                    {
                        "name": "Meng Yan"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17703v1",
                "updated": "2025-09-22T12:43:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    43,
                    9,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:43:09Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    43,
                    9,
                    0,
                    265,
                    0
                ],
                "title": "An LLM-based Agent Simulation Approach to Study Moral Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Agent Simulation Approach to Study Moral Evolution"
                },
                "summary": "The evolution of morality presents a puzzle: natural selection should favor\nself-interest, yet humans developed moral systems promoting altruism. We\naddress this question by introducing a novel Large Language Model (LLM)-based\nagent simulation framework modeling prehistoric hunter-gatherer societies. This\nplatform is designed to probe diverse questions in social evolution, from\nsurvival advantages to inter-group dynamics. To investigate moral evolution, we\ndesigned agents with varying moral dispositions based on the Expanding Circle\nTheory \\citep{singer1981expanding}. We evaluated their evolutionary success\nacross a series of simulations and analyzed their decision-making in specially\ndesigned moral dilemmas. These experiments reveal how an agent's moral\nframework, in combination with its cognitive constraints, directly shapes its\nbehavior and determines its evolutionary outcome. Crucially, the emergent\npatterns echo seminal theories from related domains of social science,\nproviding external validation for the simulations. This work establishes\nLLM-based simulation as a powerful new paradigm to complement traditional\nresearch in evolutionary biology and anthropology, opening new avenues for\ninvestigating the complexities of moral and social evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of morality presents a puzzle: natural selection should favor\nself-interest, yet humans developed moral systems promoting altruism. We\naddress this question by introducing a novel Large Language Model (LLM)-based\nagent simulation framework modeling prehistoric hunter-gatherer societies. This\nplatform is designed to probe diverse questions in social evolution, from\nsurvival advantages to inter-group dynamics. To investigate moral evolution, we\ndesigned agents with varying moral dispositions based on the Expanding Circle\nTheory \\citep{singer1981expanding}. We evaluated their evolutionary success\nacross a series of simulations and analyzed their decision-making in specially\ndesigned moral dilemmas. These experiments reveal how an agent's moral\nframework, in combination with its cognitive constraints, directly shapes its\nbehavior and determines its evolutionary outcome. Crucially, the emergent\npatterns echo seminal theories from related domains of social science,\nproviding external validation for the simulations. This work establishes\nLLM-based simulation as a powerful new paradigm to complement traditional\nresearch in evolutionary biology and anthropology, opening new avenues for\ninvestigating the complexities of moral and social evolution."
                },
                "authors": [
                    {
                        "name": "Zhou Ziheng"
                    },
                    {
                        "name": "Huacong Tang"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Yipeng Kang"
                    },
                    {
                        "name": "Wanying He"
                    },
                    {
                        "name": "Fang Sun"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Demetri Terzopoulos"
                    },
                    {
                        "name": "Fangwei Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Fangwei Zhong"
                },
                "author": "Fangwei Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10834v3",
                "updated": "2025-09-22T12:42:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    42,
                    54,
                    0,
                    265,
                    0
                ],
                "published": "2024-01-19T17:32:04Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    17,
                    32,
                    4,
                    4,
                    19,
                    0
                ],
                "title": "Cppless: Single-Source and High-Performance Serverless Programming in\n  C++",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cppless: Single-Source and High-Performance Serverless Programming in\n  C++"
                },
                "summary": "The rise of serverless computing introduced a new class of scalable, elastic\nand widely available parallel workers in the cloud. Many systems and\napplications benefit from offloading computations and parallel tasks to\ndynamically allocated resources. However, the developers of C++ applications\nfind it difficult to integrate functions due to complex deployment, lack of\ncompatibility between client and cloud environments, and loosely typed input\nand output data. To enable single-source and efficient serverless acceleration\nin C++, we introduce Cppless, an end-to-end framework for implementing remote\nfunctions which handles the creation, deployment, and invocation of serverless\nfunctions. Cppless is built on top of LLVM and requires only two compiler\nextensions to automatically extract C++ function objects and deploy them to the\ncloud. We demonstrate that offloading parallel computations, such as from a C++\napplication to serverless workers, can provide up to 59x speedup with minimal\ncost increase while requiring only minor code modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of serverless computing introduced a new class of scalable, elastic\nand widely available parallel workers in the cloud. Many systems and\napplications benefit from offloading computations and parallel tasks to\ndynamically allocated resources. However, the developers of C++ applications\nfind it difficult to integrate functions due to complex deployment, lack of\ncompatibility between client and cloud environments, and loosely typed input\nand output data. To enable single-source and efficient serverless acceleration\nin C++, we introduce Cppless, an end-to-end framework for implementing remote\nfunctions which handles the creation, deployment, and invocation of serverless\nfunctions. Cppless is built on top of LLVM and requires only two compiler\nextensions to automatically extract C++ function objects and deploy them to the\ncloud. We demonstrate that offloading parallel computations, such as from a C++\napplication to serverless workers, can provide up to 59x speedup with minimal\ncost increase while requiring only minor code modifications."
                },
                "authors": [
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Lukas Mller"
                    },
                    {
                        "name": "Alexandru Calotoiu"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1145/3747841",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3747841",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.10834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version of paper accepted at the ACM Transactions on\n  Architecture and Code Optimization (TACO) journal",
                "arxiv_journal_ref": "ACM Transactions on Architecture and Code Optimization, Volume 22,\n  Issue 3, Article No.: 110, Pages 1 - 27, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16072v2",
                "updated": "2025-09-22T12:38:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    38,
                    55,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-19T15:19:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "I-FailSense: Towards General Robotic Failure Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-FailSense: Towards General Robotic Failure Detection with\n  Vision-Language Models"
                },
                "summary": "Language-conditioned robotic manipulation in open-world settings requires not\nonly accurate task execution but also the ability to detect failures for robust\ndeployment in real-world environments. Although recent advances in\nvision-language models (VLMs) have significantly improved the spatial reasoning\nand task-planning capabilities of robots, they remain limited in their ability\nto recognize their own failures. In particular, a critical yet underexplored\nchallenge lies in detecting semantic misalignment errors, where the robot\nexecutes a task that is semantically meaningful but inconsistent with the given\ninstruction. To address this, we propose a method for building datasets\ntargeting Semantic Misalignment Failures detection, from existing\nlanguage-conditioned manipulation datasets. We also present I-FailSense, an\nopen-source VLM framework with grounded arbitration designed specifically for\nfailure detection. Our approach relies on post-training a base VLM, followed by\ntraining lightweight classification heads, called FS blocks, attached to\ndifferent internal layers of the VLM and whose predictions are aggregated using\nan ensembling mechanism. Experiments show that I-FailSense outperforms\nstate-of-the-art VLMs, both comparable in size and larger, in detecting\nsemantic misalignment errors. Notably, despite being trained only on semantic\nmisalignment detection, I-FailSense generalizes to broader robotic failure\ncategories and effectively transfers to other simulation environments and\nreal-world with zero-shot or minimal post-training. The datasets and models are\npublicly released on HuggingFace (Webpage:\nhttps://clemgris.github.io/I-FailSense/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-conditioned robotic manipulation in open-world settings requires not\nonly accurate task execution but also the ability to detect failures for robust\ndeployment in real-world environments. Although recent advances in\nvision-language models (VLMs) have significantly improved the spatial reasoning\nand task-planning capabilities of robots, they remain limited in their ability\nto recognize their own failures. In particular, a critical yet underexplored\nchallenge lies in detecting semantic misalignment errors, where the robot\nexecutes a task that is semantically meaningful but inconsistent with the given\ninstruction. To address this, we propose a method for building datasets\ntargeting Semantic Misalignment Failures detection, from existing\nlanguage-conditioned manipulation datasets. We also present I-FailSense, an\nopen-source VLM framework with grounded arbitration designed specifically for\nfailure detection. Our approach relies on post-training a base VLM, followed by\ntraining lightweight classification heads, called FS blocks, attached to\ndifferent internal layers of the VLM and whose predictions are aggregated using\nan ensembling mechanism. Experiments show that I-FailSense outperforms\nstate-of-the-art VLMs, both comparable in size and larger, in detecting\nsemantic misalignment errors. Notably, despite being trained only on semantic\nmisalignment detection, I-FailSense generalizes to broader robotic failure\ncategories and effectively transfers to other simulation environments and\nreal-world with zero-shot or minimal post-training. The datasets and models are\npublicly released on HuggingFace (Webpage:\nhttps://clemgris.github.io/I-FailSense/)."
                },
                "authors": [
                    {
                        "name": "Clemence Grislain"
                    },
                    {
                        "name": "Hamed Rahimi"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chetouani"
                },
                "author": "Mohamed Chetouani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17701v1",
                "updated": "2025-09-22T12:38:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    38,
                    9,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:38:09Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    38,
                    9,
                    0,
                    265,
                    0
                ],
                "title": "Investigating Bias: A Multilingual Pipeline for Generating, Solving, and\n  Evaluating Math Problems with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Bias: A Multilingual Pipeline for Generating, Solving, and\n  Evaluating Math Problems with LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly used for educational support,\nyet their response quality varies depending on the language of interaction.\nThis paper presents an automated multilingual pipeline for generating, solving,\nand evaluating math problems aligned with the German K-10 curriculum. We\ngenerated 628 math exercises and translated them into English, German, and\nArabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)\nwere prompted to produce step-by-step solutions in each language. A held-out\npanel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality\nusing a comparative framework. Results show a consistent gap, with English\nsolutions consistently rated highest, and Arabic often ranked lower. These\nfindings highlight persistent linguistic bias and the need for more equitable\nmultilingual AI systems in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used for educational support,\nyet their response quality varies depending on the language of interaction.\nThis paper presents an automated multilingual pipeline for generating, solving,\nand evaluating math problems aligned with the German K-10 curriculum. We\ngenerated 628 math exercises and translated them into English, German, and\nArabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)\nwere prompted to produce step-by-step solutions in each language. A held-out\npanel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality\nusing a comparative framework. Results show a consistent gap, with English\nsolutions consistently rated highest, and Arabic often ranked lower. These\nfindings highlight persistent linguistic bias and the need for more equitable\nmultilingual AI systems in education."
                },
                "authors": [
                    {
                        "name": "Mariam Mahran"
                    },
                    {
                        "name": "Katharina Simbeck"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Simbeck"
                },
                "author": "Katharina Simbeck",
                "arxiv_comment": "Accepted at edu4AI'25: 2nd Workshop on Education for Artificial\n  Intelligence | co-located with ECAI, October 26th, 2025, Bologna, Italy. 7\n  pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17694v1",
                "updated": "2025-09-22T12:33:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    33,
                    2,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:33:02Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    33,
                    2,
                    0,
                    265,
                    0
                ],
                "title": "Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play\n  Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play\n  Dialogues"
                },
                "summary": "Evaluating large language models (LLMs) in long-form, knowledge-grounded\nrole-play dialogues remains challenging. This study compares LLM-generated and\nhuman-authored responses in multi-turn professional training simulations\nthrough human evaluation ($N=38$) and automated LLM-as-a-judge assessment.\nHuman evaluation revealed significant degradation in LLM-generated response\nquality across turns, particularly in naturalness, context maintenance and\noverall quality, while human-authored responses progressively improved. In line\nwith this finding, participants also indicated a consistent preference for\nhuman-authored dialogue. These human judgements were validated by our automated\nLLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment\nwith human evaluators on both zero-shot pairwise preference and stochastic\n6-shot construct ratings, confirming the widening quality gap between LLM and\nhuman responses over time. Our work contributes a multi-turn benchmark exposing\nLLM degradation in knowledge-grounded role-play dialogues and provides a\nvalidated hybrid evaluation framework to guide the reliable integration of LLMs\nin training simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in long-form, knowledge-grounded\nrole-play dialogues remains challenging. This study compares LLM-generated and\nhuman-authored responses in multi-turn professional training simulations\nthrough human evaluation ($N=38$) and automated LLM-as-a-judge assessment.\nHuman evaluation revealed significant degradation in LLM-generated response\nquality across turns, particularly in naturalness, context maintenance and\noverall quality, while human-authored responses progressively improved. In line\nwith this finding, participants also indicated a consistent preference for\nhuman-authored dialogue. These human judgements were validated by our automated\nLLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment\nwith human evaluators on both zero-shot pairwise preference and stochastic\n6-shot construct ratings, confirming the widening quality gap between LLM and\nhuman responses over time. Our work contributes a multi-turn benchmark exposing\nLLM degradation in knowledge-grounded role-play dialogues and provides a\nvalidated hybrid evaluation framework to guide the reliable integration of LLMs\nin training simulations."
                },
                "authors": [
                    {
                        "name": "Dongxu Lu"
                    },
                    {
                        "name": "Johan Jeuring"
                    },
                    {
                        "name": "Albert Gatt"
                    }
                ],
                "author_detail": {
                    "name": "Albert Gatt"
                },
                "author": "Albert Gatt",
                "arxiv_comment": "Accepted for publication at the 18th International Natural Language\n  Generation Conference (INLG 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08851v2",
                "updated": "2025-09-22T12:26:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    26,
                    56,
                    0,
                    265,
                    0
                ],
                "published": "2025-07-08T22:49:03Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    22,
                    49,
                    3,
                    1,
                    189,
                    0
                ],
                "title": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation"
                },
                "summary": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Existing\nvision-language mapping approaches typically rely on object-centric\nsegmentation priors, which often fail outdoors due to semantic ambiguities and\nindistinct class boundaries. We propose OTAS - an Open-vocabulary Token\nAlignment method for outdoor Segmentation. OTAS addresses the limitations of\nopen-vocabulary segmentation models by extracting semantic structure directly\nfrom the output tokens of pre-trained vision models. By clustering semantically\nsimilar structures across single and multiple views and grounding them in\nlanguage, OTAS reconstructs a geometrically consistent feature field that\nsupports open-vocabulary segmentation queries. Our method operates in a\nzero-shot manner, without scene-specific fine-tuning, and achieves real-time\nperformance of up to ~17 fps. On the Off-Road Freespace Detection dataset, OTAS\nyields a modest IoU improvement over fine-tuned and open-vocabulary 2D\nsegmentation baselines. In 3D segmentation on TartanAir, it achieves up to a\n151% relative IoU improvement compared to existing open-vocabulary mapping\nmethods. Real-world reconstructions further demonstrate OTAS' applicability to\nrobotic deployment. Code and a ROS 2 node are available at\nhttps://otas-segmentation.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Existing\nvision-language mapping approaches typically rely on object-centric\nsegmentation priors, which often fail outdoors due to semantic ambiguities and\nindistinct class boundaries. We propose OTAS - an Open-vocabulary Token\nAlignment method for outdoor Segmentation. OTAS addresses the limitations of\nopen-vocabulary segmentation models by extracting semantic structure directly\nfrom the output tokens of pre-trained vision models. By clustering semantically\nsimilar structures across single and multiple views and grounding them in\nlanguage, OTAS reconstructs a geometrically consistent feature field that\nsupports open-vocabulary segmentation queries. Our method operates in a\nzero-shot manner, without scene-specific fine-tuning, and achieves real-time\nperformance of up to ~17 fps. On the Off-Road Freespace Detection dataset, OTAS\nyields a modest IoU improvement over fine-tuned and open-vocabulary 2D\nsegmentation baselines. In 3D segmentation on TartanAir, it achieves up to a\n151% relative IoU improvement compared to existing open-vocabulary mapping\nmethods. Real-world reconstructions further demonstrate OTAS' applicability to\nrobotic deployment. Code and a ROS 2 node are available at\nhttps://otas-segmentation.github.io/."
                },
                "authors": [
                    {
                        "name": "Simon Schwaiger"
                    },
                    {
                        "name": "Stefan Thalhammer"
                    },
                    {
                        "name": "Wilfried Wber"
                    },
                    {
                        "name": "Gerald Steinbauer-Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Gerald Steinbauer-Wagner"
                },
                "author": "Gerald Steinbauer-Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17680v1",
                "updated": "2025-09-22T12:25:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    25,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:25:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    25,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "When TableQA Meets Noise: A Dual Denoising Framework for Complex\n  Questions and Large-scale Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When TableQA Meets Noise: A Dual Denoising Framework for Complex\n  Questions and Large-scale Tables"
                },
                "summary": "Table question answering (TableQA) is a fundamental task in natural language\nprocessing (NLP). The strong reasoning capabilities of large language models\n(LLMs) have brought significant advances in this field. However, as real-world\napplications involve increasingly complex questions and larger tables,\nsubstantial noisy data is introduced, which severely degrades reasoning\nperformance. To address this challenge, we focus on improving two core\ncapabilities: Relevance Filtering, which identifies and retains information\ntruly relevant to reasoning, and Table Pruning, which reduces table size while\npreserving essential content. Based on these principles, we propose EnoTab, a\ndual denoising framework for complex questions and large-scale tables.\nSpecifically, we first perform Evidence-based Question Denoising by decomposing\nthe question into minimal semantic units and filtering out those irrelevant to\nanswer reasoning based on consistency and usability criteria. Then, we propose\nEvidence Tree-guided Table Denoising, which constructs an explicit and\ntransparent table pruning path to remove irrelevant data step by step. At each\npruning step, we observe the intermediate state of the table and apply a\npost-order node rollback mechanism to handle abnormal table states, ultimately\nproducing a highly reliable sub-table for final answer reasoning. Finally,\nextensive experiments show that EnoTab achieves outstanding performance on\nTableQA tasks with complex questions and large-scale tables, confirming its\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table question answering (TableQA) is a fundamental task in natural language\nprocessing (NLP). The strong reasoning capabilities of large language models\n(LLMs) have brought significant advances in this field. However, as real-world\napplications involve increasingly complex questions and larger tables,\nsubstantial noisy data is introduced, which severely degrades reasoning\nperformance. To address this challenge, we focus on improving two core\ncapabilities: Relevance Filtering, which identifies and retains information\ntruly relevant to reasoning, and Table Pruning, which reduces table size while\npreserving essential content. Based on these principles, we propose EnoTab, a\ndual denoising framework for complex questions and large-scale tables.\nSpecifically, we first perform Evidence-based Question Denoising by decomposing\nthe question into minimal semantic units and filtering out those irrelevant to\nanswer reasoning based on consistency and usability criteria. Then, we propose\nEvidence Tree-guided Table Denoising, which constructs an explicit and\ntransparent table pruning path to remove irrelevant data step by step. At each\npruning step, we observe the intermediate state of the table and apply a\npost-order node rollback mechanism to handle abnormal table states, ultimately\nproducing a highly reliable sub-table for final answer reasoning. Finally,\nextensive experiments show that EnoTab achieves outstanding performance on\nTableQA tasks with complex questions and large-scale tables, confirming its\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Shenghao Ye"
                    },
                    {
                        "name": "Yu Guo"
                    },
                    {
                        "name": "Dong Jin"
                    },
                    {
                        "name": "Yikai Shen"
                    },
                    {
                        "name": "Yunpeng Hou"
                    },
                    {
                        "name": "Shuangwu Chen"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaofeng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Jiang"
                },
                "author": "Xiaofeng Jiang",
                "arxiv_comment": "23 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17677v1",
                "updated": "2025-09-22T12:20:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    20,
                    27,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:20:27Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    20,
                    27,
                    0,
                    265,
                    0
                ],
                "title": "EngiBench: A Benchmark for Evaluating Large Language Models on\n  Engineering Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EngiBench: A Benchmark for Evaluating Large Language Models on\n  Engineering Problem Solving"
                },
                "summary": "Large language models (LLMs) have shown strong performance on mathematical\nreasoning under well-posed conditions. However, real-world engineering problems\nrequire more than mathematical symbolic computation -- they need to deal with\nuncertainty, context, and open-ended scenarios. Existing benchmarks fail to\ncapture these complexities. We introduce EngiBench, a hierarchical benchmark\ndesigned to evaluate LLMs on solving engineering problems. It spans three\nlevels of increasing difficulty (foundational knowledge retrieval, multi-step\ncontextual reasoning, and open-ended modeling) and covers diverse engineering\nsubfields. To facilitate a deeper understanding of model performance, we\nsystematically rewrite each problem into three controlled variants (perturbed,\nknowledge-enhanced, and math abstraction), enabling us to separately evaluate\nthe model's robustness, domain-specific knowledge, and mathematical reasoning\nabilities. Experiment results reveal a clear performance gap across levels:\nmodels struggle more as tasks get harder, perform worse when problems are\nslightly changed, and fall far behind human experts on the high-level\nengineering tasks. These findings reveal that current LLMs still lack the\nhigh-level reasoning needed for real-world engineering, highlighting the need\nfor future models with deeper and more reliable problem-solving capabilities.\nOur source code and data are available at\nhttps://github.com/EngiBench/EngiBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance on mathematical\nreasoning under well-posed conditions. However, real-world engineering problems\nrequire more than mathematical symbolic computation -- they need to deal with\nuncertainty, context, and open-ended scenarios. Existing benchmarks fail to\ncapture these complexities. We introduce EngiBench, a hierarchical benchmark\ndesigned to evaluate LLMs on solving engineering problems. It spans three\nlevels of increasing difficulty (foundational knowledge retrieval, multi-step\ncontextual reasoning, and open-ended modeling) and covers diverse engineering\nsubfields. To facilitate a deeper understanding of model performance, we\nsystematically rewrite each problem into three controlled variants (perturbed,\nknowledge-enhanced, and math abstraction), enabling us to separately evaluate\nthe model's robustness, domain-specific knowledge, and mathematical reasoning\nabilities. Experiment results reveal a clear performance gap across levels:\nmodels struggle more as tasks get harder, perform worse when problems are\nslightly changed, and fall far behind human experts on the high-level\nengineering tasks. These findings reveal that current LLMs still lack the\nhigh-level reasoning needed for real-world engineering, highlighting the need\nfor future models with deeper and more reliable problem-solving capabilities.\nOur source code and data are available at\nhttps://github.com/EngiBench/EngiBench."
                },
                "authors": [
                    {
                        "name": "Xiyuan Zhou"
                    },
                    {
                        "name": "Xinlei Wang"
                    },
                    {
                        "name": "Yirui He"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Ruixi Zou"
                    },
                    {
                        "name": "Yuheng Cheng"
                    },
                    {
                        "name": "Yulu Xie"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17676v1",
                "updated": "2025-09-22T12:19:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    19,
                    46,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:19:46Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    19,
                    46,
                    0,
                    265,
                    0
                ],
                "title": "GLo-MAPPO: A Multi-Agent Proximal Policy Optimization for Energy\n  Efficiency in UAV-Assisted LoRa Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLo-MAPPO: A Multi-Agent Proximal Policy Optimization for Energy\n  Efficiency in UAV-Assisted LoRa Networks"
                },
                "summary": "Long Range (LoRa) based low-power wide area networks (LPWANs) are crucial for\nenabling next-generation IoT (NG-IoT) applications in 5G/6G ecosystems due to\ntheir long-range, low-power, and low-cost characteristics. However, achieving\nhigh energy efficiency in such networks remains a critical challenge,\nparticularly in large-scale or dynamically changing environments. Traditional\nterrestrial LoRa deployments often suffer from coverage gaps and\nnon-line-of-sight (NLoS) propagation losses, while satellite-based IoT\nsolutions consume excessive energy and introduce high latency, limiting their\nsuitability for energy-constrained and delay-sensitive applications. To address\nthese limitations, we propose a novel architecture using multiple unmanned\naerial vehicles (UAVs) as flying LoRa gateways to dynamically collect data from\nground-based LoRa end devices. Our approach aims to maximize the system's\nweighted global energy efficiency by jointly optimizing spreading factors,\ntransmission powers, UAV trajectories, and end-device associations.\nAdditionally, we formulate this complex optimization problem as a partially\nobservable Markov decision process (POMDP) and propose green LoRa multi-agent\nproximal policy optimization (GLo-MAPPO), a multi-agent reinforcement learning\n(MARL) framework based on centralized training with decentralized execution\n(CTDE). Simulation results show that GLo-MAPPO significantly outperforms\nbenchmark algorithms, achieving energy efficiency improvements of 71.25%,\n18.56%, 67.00%, 59.73%, and 49.95% for networks with 10, 20, 30, 40, and 50\nLoRa end devices, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Range (LoRa) based low-power wide area networks (LPWANs) are crucial for\nenabling next-generation IoT (NG-IoT) applications in 5G/6G ecosystems due to\ntheir long-range, low-power, and low-cost characteristics. However, achieving\nhigh energy efficiency in such networks remains a critical challenge,\nparticularly in large-scale or dynamically changing environments. Traditional\nterrestrial LoRa deployments often suffer from coverage gaps and\nnon-line-of-sight (NLoS) propagation losses, while satellite-based IoT\nsolutions consume excessive energy and introduce high latency, limiting their\nsuitability for energy-constrained and delay-sensitive applications. To address\nthese limitations, we propose a novel architecture using multiple unmanned\naerial vehicles (UAVs) as flying LoRa gateways to dynamically collect data from\nground-based LoRa end devices. Our approach aims to maximize the system's\nweighted global energy efficiency by jointly optimizing spreading factors,\ntransmission powers, UAV trajectories, and end-device associations.\nAdditionally, we formulate this complex optimization problem as a partially\nobservable Markov decision process (POMDP) and propose green LoRa multi-agent\nproximal policy optimization (GLo-MAPPO), a multi-agent reinforcement learning\n(MARL) framework based on centralized training with decentralized execution\n(CTDE). Simulation results show that GLo-MAPPO significantly outperforms\nbenchmark algorithms, achieving energy efficiency improvements of 71.25%,\n18.56%, 67.00%, 59.73%, and 49.95% for networks with 10, 20, 30, 40, and 50\nLoRa end devices, respectively."
                },
                "authors": [
                    {
                        "name": "Abdullahi Isa Ahmed"
                    },
                    {
                        "name": "Jamal Bentahar"
                    },
                    {
                        "name": "El Mehdi Amhoud"
                    }
                ],
                "author_detail": {
                    "name": "El Mehdi Amhoud"
                },
                "author": "El Mehdi Amhoud",
                "arxiv_comment": "15 pages, 19 figures, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11717v2",
                "updated": "2025-09-22T12:17:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    17,
                    27,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-15T09:12:57Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    12,
                    57,
                    0,
                    258,
                    0
                ],
                "title": "Neural Audio Codecs for Prompt-Driven Universal Source Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Codecs for Prompt-Driven Universal Source Separation"
                },
                "summary": "Text-guided source separation supports flexible audio editing across media\nand assistive applications, but existing models like AudioSep are too\ncompute-heavy for edge deployment. Neural audio codec (NAC) models such as\nCodecFormer and SDCodec are compute-efficient but limited to fixed-class\nseparation. We introduce CodecSep, the first NAC-based model for on-device\nuniversal, text-driven separation. CodecSep combines DAC compression with a\nTransformer masker modulated by CLAP-derived FiLM parameters. Across six\nopen-domain benchmarks under matched training/prompt protocols,\n\\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR)\nwhile remaining competitive in perceptual quality (ViSQOL) and matching or\nexceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream\ndeployments, it needs just 1.35~GMACs end-to-end -- approximately $54\\times$\nless compute ($25\\times$ architecture-only) than spectrogram-domain separators\nlike AudioSep -- while remaining fully bitstream-compatible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided source separation supports flexible audio editing across media\nand assistive applications, but existing models like AudioSep are too\ncompute-heavy for edge deployment. Neural audio codec (NAC) models such as\nCodecFormer and SDCodec are compute-efficient but limited to fixed-class\nseparation. We introduce CodecSep, the first NAC-based model for on-device\nuniversal, text-driven separation. CodecSep combines DAC compression with a\nTransformer masker modulated by CLAP-derived FiLM parameters. Across six\nopen-domain benchmarks under matched training/prompt protocols,\n\\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR)\nwhile remaining competitive in perceptual quality (ViSQOL) and matching or\nexceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream\ndeployments, it needs just 1.35~GMACs end-to-end -- approximately $54\\times$\nless compute ($25\\times$ architecture-only) than spectrogram-domain separators\nlike AudioSep -- while remaining fully bitstream-compatible."
                },
                "authors": [
                    {
                        "name": "Adhiraj Banerjee"
                    },
                    {
                        "name": "Vipul Arora"
                    }
                ],
                "author_detail": {
                    "name": "Vipul Arora"
                },
                "author": "Vipul Arora",
                "arxiv_comment": "main content- 10 pages, total - 23 pages, 1 figure, pre-print, under\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17671v1",
                "updated": "2025-09-22T12:14:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    14,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:14:11Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    14,
                    11,
                    0,
                    265,
                    0
                ],
                "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG\n  Applications"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages."
                },
                "authors": [
                    {
                        "name": "Selva Ta"
                    },
                    {
                        "name": "Mahmut El Huseyni"
                    },
                    {
                        "name": "zay Ezerceli"
                    },
                    {
                        "name": "Reyhan Bayraktar"
                    },
                    {
                        "name": "Fatma Betl Terziolu"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Betl Terziolu"
                },
                "author": "Fatma Betl Terziolu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17669v1",
                "updated": "2025-09-22T12:12:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    12,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:12:41Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    12,
                    41,
                    0,
                    265,
                    0
                ],
                "title": "PG-CE: A Progressive Generation Dataset with Constraint Enhancement for\n  Controllable Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PG-CE: A Progressive Generation Dataset with Constraint Enhancement for\n  Controllable Text Generation"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), Controllable Text\nGeneration (CTG) has become a critical technology for enhancing system\nreliability and user experience. Addressing the limitations of traditional\nmethods, this paper proposes the PG-CE (Progressive Generation with Constraint\nEnhancement) approach, which decomposes CTG tasks into three steps: type\nprediction, constraint construction, and guided generation. This method employs\nconstraint generation models to dynamically build multi-dimensional constraints\nincluding tone, expression style, and thematic focus to guide output.\nExperiments demonstrate that PG-CE significantly improves generation quality\nacross multiple scenarios while maintaining text controllability, thematic\nrelevance, and response practicality. The research developed a dataset\ncontaining 90,000 constraint-text pairs (with an 8:2 ratio between daily and\nother topics), effectively reflecting real-world application requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), Controllable Text\nGeneration (CTG) has become a critical technology for enhancing system\nreliability and user experience. Addressing the limitations of traditional\nmethods, this paper proposes the PG-CE (Progressive Generation with Constraint\nEnhancement) approach, which decomposes CTG tasks into three steps: type\nprediction, constraint construction, and guided generation. This method employs\nconstraint generation models to dynamically build multi-dimensional constraints\nincluding tone, expression style, and thematic focus to guide output.\nExperiments demonstrate that PG-CE significantly improves generation quality\nacross multiple scenarios while maintaining text controllability, thematic\nrelevance, and response practicality. The research developed a dataset\ncontaining 90,000 constraint-text pairs (with an 8:2 ratio between daily and\nother topics), effectively reflecting real-world application requirements."
                },
                "authors": [
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Yuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Sun"
                },
                "author": "Yuan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17665v1",
                "updated": "2025-09-22T12:09:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    9,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T12:09:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    9,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Mechanistic Interpretability with SAEs: Probing Religion, Violence, and\n  Geography in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Interpretability with SAEs: Probing Religion, Violence, and\n  Geography in Large Language Models"
                },
                "summary": "Despite growing research on bias in large language models (LLMs), most work\nhas focused on gender and race, with little attention to religious identity.\nThis paper explores how religion is internally represented in LLMs and how it\nintersects with concepts of violence and geography. Using mechanistic\ninterpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we\nanalyze latent feature activations across five models. We measure overlap\nbetween religion- and violence-related prompts and probe semantic patterns in\nactivation contexts. While all five religions show comparable internal\ncohesion, Islam is more frequently linked to features associated with violent\nlanguage. In contrast, geographic associations largely reflect real-world\nreligious demographics, revealing how models embed both factual distributions\nand cultural stereotypes. These findings highlight the value of structural\nanalysis in auditing not just outputs but also internal representations that\nshape model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite growing research on bias in large language models (LLMs), most work\nhas focused on gender and race, with little attention to religious identity.\nThis paper explores how religion is internally represented in LLMs and how it\nintersects with concepts of violence and geography. Using mechanistic\ninterpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we\nanalyze latent feature activations across five models. We measure overlap\nbetween religion- and violence-related prompts and probe semantic patterns in\nactivation contexts. While all five religions show comparable internal\ncohesion, Islam is more frequently linked to features associated with violent\nlanguage. In contrast, geographic associations largely reflect real-world\nreligious demographics, revealing how models embed both factual distributions\nand cultural stereotypes. These findings highlight the value of structural\nanalysis in auditing not just outputs but also internal representations that\nshape model behavior."
                },
                "authors": [
                    {
                        "name": "Katharina Simbeck"
                    },
                    {
                        "name": "Mariam Mahran"
                    }
                ],
                "author_detail": {
                    "name": "Mariam Mahran"
                },
                "author": "Mariam Mahran",
                "arxiv_comment": "Accepted at AEQUITAS 2025: Workshop on Fairness and Bias in AI |\n  co-located with ECAI, October 26th, 2025, Bologna, Italy. 12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18635v2",
                "updated": "2025-09-22T12:00:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    0,
                    11,
                    0,
                    265,
                    0
                ],
                "published": "2025-08-26T03:18:53Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    18,
                    53,
                    1,
                    238,
                    0
                ],
                "title": "STRATA-TS: Selective Knowledge Transfer for Urban Time Series\n  Forecasting with Retrieval-Guided Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STRATA-TS: Selective Knowledge Transfer for Urban Time Series\n  Forecasting with Retrieval-Guided Reasoning"
                },
                "summary": "Urban forecasting models often face a severe data imbalance problem: only a\nfew cities have dense, long-span records, while many others expose short or\nincomplete histories. Direct transfer from data-rich to data-scarce cities is\nunreliable because only a limited subset of source patterns truly benefits the\ntarget domain, whereas indiscriminate transfer risks introducing noise and\nnegative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware\nretrieval for Time Series), a framework that combines domain-adapted retrieval\nwith reasoning-capable large models to improve forecasting in scarce data\nregimes. STRATA-TS employs a patch-based temporal encoder to identify source\nsubsequences that are semantically and dynamically aligned with the target\nquery. These retrieved exemplars are then injected into a retrieval-guided\nreasoning stage, where an LLM performs structured inference over target inputs\nand retrieved support. To enable efficient deployment, we distill the reasoning\nprocess into a compact open model via supervised fine-tuning. Extensive\nexperiments on three parking availability datasets across Singapore,\nNottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms\nstrong forecasting and transfer baselines, while providing interpretable\nknowledge transfer pathways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban forecasting models often face a severe data imbalance problem: only a\nfew cities have dense, long-span records, while many others expose short or\nincomplete histories. Direct transfer from data-rich to data-scarce cities is\nunreliable because only a limited subset of source patterns truly benefits the\ntarget domain, whereas indiscriminate transfer risks introducing noise and\nnegative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware\nretrieval for Time Series), a framework that combines domain-adapted retrieval\nwith reasoning-capable large models to improve forecasting in scarce data\nregimes. STRATA-TS employs a patch-based temporal encoder to identify source\nsubsequences that are semantically and dynamically aligned with the target\nquery. These retrieved exemplars are then injected into a retrieval-guided\nreasoning stage, where an LLM performs structured inference over target inputs\nand retrieved support. To enable efficient deployment, we distill the reasoning\nprocess into a compact open model via supervised fine-tuning. Extensive\nexperiments on three parking availability datasets across Singapore,\nNottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms\nstrong forecasting and transfer baselines, while providing interpretable\nknowledge transfer pathways."
                },
                "authors": [
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Yile Chen"
                    },
                    {
                        "name": "Qin Chao"
                    },
                    {
                        "name": "Shuai Liu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Gao Cong"
                    }
                ],
                "author_detail": {
                    "name": "Gao Cong"
                },
                "author": "Gao Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17641v1",
                "updated": "2025-09-22T11:45:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    45,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:45:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    45,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuditoryBench++: Can Language Models Understand Auditory Knowledge\n  without Hearing?"
                },
                "summary": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io."
                },
                "authors": [
                    {
                        "name": "Hyunjong Ok"
                    },
                    {
                        "name": "Suho Yoo"
                    },
                    {
                        "name": "Hyeonjun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09996v3",
                "updated": "2025-09-22T11:37:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    37,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2025-06-11T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    17,
                    59,
                    58,
                    2,
                    162,
                    0
                ],
                "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring"
                },
                "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Yehan Yang"
                    },
                    {
                        "name": "Xueyao Zhang"
                    },
                    {
                        "name": "Juan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Juan Cao"
                },
                "author": "Juan Cao",
                "arxiv_comment": "NeurIPS 2025 Accepted Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17628v1",
                "updated": "2025-09-22T11:36:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    36,
                    16,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:36:16Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    36,
                    16,
                    0,
                    265,
                    0
                ],
                "title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM\n  Agents"
                },
                "summary": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks\nwithin single domains. However, their reasoning and coordination capabilities\nin complex, multi-stage scenarios remain underexplored. Existing benchmarks\ntypically focus on isolated tasks or narrow domains, overlooking models'\nabilities for multi-stage collaboration and optimization without explicit\nexternal guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel\nbenchmark comprising 126696 domain-specific QA instances spanning scenarios in\nautomotive, pharmaceutical, electronics, and energy sectors. The dataset is\ncreated using a structured three-phase pipeline: dynamic sampling, iterative\nquestion-answer generation, and a multi-level quality assessment to ensure data\nquality. Tasks are further categorized into three difficulty levels according\nto stage coverage and complexity. With MSCoRe, we have conducted a\ncomprehensive evaluation of various state-of-the-art LLM agents. The commercial\nmodels performed best across all tasks and scenarios, but a notable gap in\nROUGE scores remains between simple and complex tasks. We also tested the\nmodels' robustness and found that their performance is negatively affected by\nnoisy data. MSCoRe provides a valuable new resource for the community to\nevaluate and improve multi-stage reasoning in LLM agents. The code and data are\navailable at https://github.com/D3E0-source/MSCoRE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks\nwithin single domains. However, their reasoning and coordination capabilities\nin complex, multi-stage scenarios remain underexplored. Existing benchmarks\ntypically focus on isolated tasks or narrow domains, overlooking models'\nabilities for multi-stage collaboration and optimization without explicit\nexternal guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel\nbenchmark comprising 126696 domain-specific QA instances spanning scenarios in\nautomotive, pharmaceutical, electronics, and energy sectors. The dataset is\ncreated using a structured three-phase pipeline: dynamic sampling, iterative\nquestion-answer generation, and a multi-level quality assessment to ensure data\nquality. Tasks are further categorized into three difficulty levels according\nto stage coverage and complexity. With MSCoRe, we have conducted a\ncomprehensive evaluation of various state-of-the-art LLM agents. The commercial\nmodels performed best across all tasks and scenarios, but a notable gap in\nROUGE scores remains between simple and complex tasks. We also tested the\nmodels' robustness and found that their performance is negatively affected by\nnoisy data. MSCoRe provides a valuable new resource for the community to\nevaluate and improve multi-stage reasoning in LLM agents. The code and data are\navailable at https://github.com/D3E0-source/MSCoRE."
                },
                "authors": [
                    {
                        "name": "Yuzhen Lei"
                    },
                    {
                        "name": "Hongbin Xie"
                    },
                    {
                        "name": "Jiaxing Zhao"
                    },
                    {
                        "name": "Shuangxue Liu"
                    },
                    {
                        "name": "Xuan Song"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Song"
                },
                "author": "Xuan Song",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17619v1",
                "updated": "2025-09-22T11:30:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    30,
                    39,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:30:39Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    30,
                    39,
                    0,
                    265,
                    0
                ],
                "title": "Human vs. Agent in Task-Oriented Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human vs. Agent in Task-Oriented Conversations"
                },
                "summary": "Task-oriented conversational systems are essential for efficiently addressing\ndiverse user needs, yet their development requires substantial amounts of\nhigh-quality conversational data that is challenging and costly to obtain.\nWhile large language models (LLMs) have demonstrated potential in generating\nsynthetic conversations, the extent to which these agent-generated interactions\ncan effectively substitute real human conversations remains unclear. This work\npresents the first systematic comparison between LLM-simulated users and human\nusers in personalized task-oriented conversations. We propose a comprehensive\nanalytical framework encompassing three key aspects (conversation strategy,\ninteraction style, and conversation evaluation) and ten distinct dimensions for\nevaluating user behaviors, and collect parallel conversational datasets from\nboth human users and LLM agent users across four representative scenarios under\nidentical conditions. Our analysis reveals significant behavioral differences\nbetween the two user types in problem-solving approaches, question broadness,\nuser engagement, context dependency, feedback polarity and promise, language\nstyle, and hallucination awareness. We found consistency in the agent users and\nhuman users across the depth-first or breadth-first dimensions, as well as the\nusefulness dimensions. These findings provide critical insights for advancing\nLLM-based user simulation. Our multi-dimensional taxonomy constructed a\ngeneralizable framework for analyzing user behavior patterns, offering insights\nfrom LLM agent users and human users. By this work, we provide perspectives on\nrethinking how to use user simulation in conversational systems in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented conversational systems are essential for efficiently addressing\ndiverse user needs, yet their development requires substantial amounts of\nhigh-quality conversational data that is challenging and costly to obtain.\nWhile large language models (LLMs) have demonstrated potential in generating\nsynthetic conversations, the extent to which these agent-generated interactions\ncan effectively substitute real human conversations remains unclear. This work\npresents the first systematic comparison between LLM-simulated users and human\nusers in personalized task-oriented conversations. We propose a comprehensive\nanalytical framework encompassing three key aspects (conversation strategy,\ninteraction style, and conversation evaluation) and ten distinct dimensions for\nevaluating user behaviors, and collect parallel conversational datasets from\nboth human users and LLM agent users across four representative scenarios under\nidentical conditions. Our analysis reveals significant behavioral differences\nbetween the two user types in problem-solving approaches, question broadness,\nuser engagement, context dependency, feedback polarity and promise, language\nstyle, and hallucination awareness. We found consistency in the agent users and\nhuman users across the depth-first or breadth-first dimensions, as well as the\nusefulness dimensions. These findings provide critical insights for advancing\nLLM-based user simulation. Our multi-dimensional taxonomy constructed a\ngeneralizable framework for analyzing user behavior patterns, offering insights\nfrom LLM agent users and human users. By this work, we provide perspectives on\nrethinking how to use user simulation in conversational systems in the future."
                },
                "authors": [
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Ning Geng"
                    },
                    {
                        "name": "Zhiqiang Guo"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "SIGIR-AP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]