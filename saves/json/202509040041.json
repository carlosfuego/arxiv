[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v1",
                "updated": "2025-08-27T10:11:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstädtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v2",
                "updated": "2025-08-22T03:36:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    36,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v2",
                "updated": "2025-08-21T20:13:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    20,
                    13,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/"
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v1",
                "updated": "2025-08-21T18:40:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15717v1",
                "updated": "2025-08-21T16:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches."
                },
                "authors": [
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15694v1",
                "updated": "2025-08-21T16:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "title": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search"
                },
                "summary": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shengyuan Lin"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shuhao Fan"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "12 pages, 12 figures, this paper is the English version of our\n  Chinese paper accepted for publication in Journal of Software, Vol. 37, No.\n  3, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15647v1",
                "updated": "2025-08-21T15:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing"
                },
                "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shuai Mu"
                    },
                    {
                        "name": "Sebastian Angel"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu",
                "arxiv_doi": "10.14778/3704965.3704969",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3704965.3704969",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.15647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs",
                "arxiv_journal_ref": "PVLDB Volume 17, Issue 13, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v2",
                "updated": "2025-08-21T14:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    58,
                    12,
                    3,
                    233,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15601v1",
                "updated": "2025-08-21T14:24:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T14:24:52Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed-Precision Large Language Model Inference with TurboMind"
                },
                "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Qian Yao"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15583v1",
                "updated": "2025-08-21T13:57:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:57:09Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "title": "Time-Optimal Directed q-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Optimal Directed q-Analysis"
                },
                "summary": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Florian Unger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Unger"
                },
                "author": "Florian Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15545v1",
                "updated": "2025-08-21T13:24:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "title": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation"
                },
                "summary": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value."
                },
                "authors": [
                    {
                        "name": "Mingyang Yu"
                    },
                    {
                        "name": "Haorui Yang"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Desheng Kong"
                    },
                    {
                        "name": "Ji Du"
                    },
                    {
                        "name": "Yulong Fu"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v2",
                "updated": "2025-08-21T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    12,
                    52,
                    11,
                    3,
                    233,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://lukashoel.github.io/3DGS-LM, Video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, Code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14204v3",
                "updated": "2025-08-21T11:43:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    43,
                    48,
                    3,
                    233,
                    0
                ],
                "published": "2024-04-22T14:13:36Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    13,
                    36,
                    0,
                    113,
                    0
                ],
                "title": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading"
                },
                "summary": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "18 pages, 13 figures. Part of this work has been accepted by ICDCS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15290v1",
                "updated": "2025-08-21T06:26:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T06:26:18Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "title": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search"
                },
                "summary": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%."
                },
                "authors": [
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Xiaolu Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15036v1",
                "updated": "2025-08-20T20:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T20:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs"
                },
                "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."
                },
                "authors": [
                    {
                        "name": "Ruyi Ding"
                    },
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Xinyi Shen"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "This paper will appear in CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15033v1",
                "updated": "2025-08-20T19:54:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T19:54:41Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "title": "Rethinking the Potential of Layer Freezing for Efficient DNN Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Layer Freezing for Efficient DNN Training"
                },
                "summary": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training."
                },
                "authors": [
                    {
                        "name": "Chence Yang"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Xulong Tang"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Jinzhen Wang"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19263v1",
                "updated": "2025-08-20T12:46:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    46,
                    50,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:46:50Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    46,
                    50,
                    2,
                    232,
                    0
                ],
                "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints,\n  and K/V Caches in Low-Precision Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless Compression of Neural Network Components: Weights, Checkpoints,\n  and K/V Caches in Low-Precision Formats"
                },
                "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment."
                },
                "authors": [
                    {
                        "name": "Anat Heilper"
                    },
                    {
                        "name": "Doron Singer"
                    }
                ],
                "author_detail": {
                    "name": "Doron Singer"
                },
                "author": "Doron Singer",
                "arxiv_comment": "16 pages 9 images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14468v1",
                "updated": "2025-08-20T06:48:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:48:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Diverse Negative Sampling for Implicit Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Negative Sampling for Implicit Collaborative Filtering"
                },
                "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yueqing Xuan"
                    },
                    {
                        "name": "Kacper Sokol"
                    },
                    {
                        "name": "Mark Sanderson"
                    },
                    {
                        "name": "Jeffrey Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Chan"
                },
                "author": "Jeffrey Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14420v1",
                "updated": "2025-08-20T04:36:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:36:25Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan"
                },
                "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform."
                },
                "authors": [
                    {
                        "name": "Shuli Wang"
                    },
                    {
                        "name": "Yinqiu Huang"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yonggang Liu"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Yinhua Zhu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang",
                "arxiv_doi": "10.1145/3746252.3761539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16653v1",
                "updated": "2025-08-20T03:42:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:42:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Peiyu Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "International Conference on Computer-Aided Design (ICCAD) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel Máté"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel Máté"
                },
                "author": "Tejfel Máté",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. León"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.18688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18688v2",
                "updated": "2025-09-02T12:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    12,
                    20,
                    55,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-26T05:22:48Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    5,
                    22,
                    48,
                    1,
                    238,
                    0
                ],
                "title": "End to End Autoencoder MLP Framework for Sepsis Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End to End Autoencoder MLP Framework for Sepsis Prediction"
                },
                "summary": "Sepsis is a life threatening condition that requires timely detection in\nintensive care settings. Traditional machine learning approaches, including\nNaive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often\nrely on manual feature engineering and struggle with irregular, incomplete\ntime-series data commonly present in electronic health records. We introduce an\nend-to-end deep learning framework integrating an unsupervised autoencoder for\nautomatic feature extraction with a multilayer perceptron classifier for binary\nsepsis risk prediction. To enhance clinical applicability, we implement a\ncustomized down sampling strategy that extracts high information density\nsegments during training and a non-overlapping dynamic sliding window mechanism\nfor real-time inference. Preprocessed time series data are represented as fixed\ndimension vectors with explicit missingness indicators, mitigating bias and\nnoise. We validate our approach on three ICU cohorts. Our end-to-end model\nachieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,\nrespectively, consistently outperforming traditional machine learning\nbaselines. These results demonstrate the framework's superior robustness,\ngeneralizability, and clinical utility for early sepsis detection across\nheterogeneous ICU environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sepsis is a life threatening condition that requires timely detection in\nintensive care settings. Traditional machine learning approaches, including\nNaive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often\nrely on manual feature engineering and struggle with irregular, incomplete\ntime-series data commonly present in electronic health records. We introduce an\nend-to-end deep learning framework integrating an unsupervised autoencoder for\nautomatic feature extraction with a multilayer perceptron classifier for binary\nsepsis risk prediction. To enhance clinical applicability, we implement a\ncustomized down sampling strategy that extracts high information density\nsegments during training and a non-overlapping dynamic sliding window mechanism\nfor real-time inference. Preprocessed time series data are represented as fixed\ndimension vectors with explicit missingness indicators, mitigating bias and\nnoise. We validate our approach on three ICU cohorts. Our end-to-end model\nachieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,\nrespectively, consistently outperforming traditional machine learning\nbaselines. These results demonstrate the framework's superior robustness,\ngeneralizability, and clinical utility for early sepsis detection across\nheterogeneous ICU environments."
                },
                "authors": [
                    {
                        "name": "Hejiang Cai"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Ji Xu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yiziting Zhu"
                    },
                    {
                        "name": "Xin Shu"
                    },
                    {
                        "name": "Yujie Li"
                    },
                    {
                        "name": "Bin Yi"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yi"
                },
                "author": "Bin Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14146v2",
                "updated": "2025-09-02T11:28:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    28,
                    27,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-19T16:37:19Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    37,
                    19,
                    1,
                    231,
                    0
                ],
                "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation"
                },
                "summary": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05995v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05995v4",
                "updated": "2025-09-02T10:52:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    10,
                    52,
                    27,
                    1,
                    245,
                    0
                ],
                "published": "2025-07-08T13:54:22Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    54,
                    22,
                    1,
                    189,
                    0
                ],
                "title": "PromiseTune: Unveiling Causally Promising and Explainable Configuration\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromiseTune: Unveiling Causally Promising and Explainable Configuration\n  Tuning"
                },
                "summary": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with 42% superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with 42% superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Pengzhou Chen"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_doi": "10.1145/3744916.3764552",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764552",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.05995v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05995v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by ICSE26",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10671v2",
                "updated": "2025-09-02T10:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    10,
                    32,
                    26,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-14T14:11:26Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    26,
                    3,
                    226,
                    0
                ],
                "title": "AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space\n  Selection -- A novel semi-automated active space selection workflow for\n  quantum chemistry and quantum computing applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space\n  Selection -- A novel semi-automated active space selection workflow for\n  quantum chemistry and quantum computing applications"
                },
                "summary": "The selection of a balanced active space is a critical step in\nmulti-reference quantum chemistry calculations, particularly for systems with\nstrong electron correlation. Likewise, active space selection is a key to\nunlock the potential of contemporary quantum computing in quantum chemistry.\nAlbeit recent progress, there remains a lack of a unified, robust, and fully\nautomated framework for active space selection that performs reliably across a\nwide range of molecular systems.\n  In this work, we present a novel approach inspired by both the AVAS (Atomic\nValence Active Space) and AutoCAS methods. Our method unifies orbital entropy\nanalysis with atomic orbital projections to guide the construction of\nchemically and physically meaningful active spaces. This integrated scheme\nenables a more consistent and flexible selection of active orbitals while\nretaining automation and scalability. We validate our approach on a set of\nmolecular systems relevant to photodynamic therapy, in particular a set of\nRu(II)-complexes, selected to span increasing levels of electron correlation\nand structural complexity. These molecules serve as challenging test cases due\nto the presence of strong static correlation and the need for highly accurate\nelectronic structure descriptions. Our results demonstrate that the method can\nreliably identify compact, chemically intuitive active spaces that capture the\nessential physics, making it suitable for both classical and quantum\ncomputational frameworks.\n  Furthermore, we have developed this approach in a package that is intuitive\nto use for users and can be interfaced with both standard quantum chemistry and\nquantum computing applications, making it accessible to a broad research\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The selection of a balanced active space is a critical step in\nmulti-reference quantum chemistry calculations, particularly for systems with\nstrong electron correlation. Likewise, active space selection is a key to\nunlock the potential of contemporary quantum computing in quantum chemistry.\nAlbeit recent progress, there remains a lack of a unified, robust, and fully\nautomated framework for active space selection that performs reliably across a\nwide range of molecular systems.\n  In this work, we present a novel approach inspired by both the AVAS (Atomic\nValence Active Space) and AutoCAS methods. Our method unifies orbital entropy\nanalysis with atomic orbital projections to guide the construction of\nchemically and physically meaningful active spaces. This integrated scheme\nenables a more consistent and flexible selection of active orbitals while\nretaining automation and scalability. We validate our approach on a set of\nmolecular systems relevant to photodynamic therapy, in particular a set of\nRu(II)-complexes, selected to span increasing levels of electron correlation\nand structural complexity. These molecules serve as challenging test cases due\nto the presence of strong static correlation and the need for highly accurate\nelectronic structure descriptions. Our results demonstrate that the method can\nreliably identify compact, chemically intuitive active spaces that capture the\nessential physics, making it suitable for both classical and quantum\ncomputational frameworks.\n  Furthermore, we have developed this approach in a package that is intuitive\nto use for users and can be interfaced with both standard quantum chemistry and\nquantum computing applications, making it accessible to a broad research\ncommunity."
                },
                "authors": [
                    {
                        "name": "Fabio Tarocco"
                    },
                    {
                        "name": "Pi A. B. Haase"
                    },
                    {
                        "name": "Fabijan Pavošević"
                    },
                    {
                        "name": "Vijay Krishna"
                    },
                    {
                        "name": "Leonardo Guidoni"
                    },
                    {
                        "name": "Stefan Knecht"
                    },
                    {
                        "name": "Martina Stella"
                    }
                ],
                "author_detail": {
                    "name": "Martina Stella"
                },
                "author": "Martina Stella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01551v2",
                "updated": "2025-09-02T09:50:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    50,
                    44,
                    1,
                    245,
                    0
                ],
                "published": "2025-04-02T09:48:27Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    9,
                    48,
                    27,
                    2,
                    92,
                    0
                ],
                "title": "Identifying Macro Causal Effects in a C-DMG over ADMGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Macro Causal Effects in a C-DMG over ADMGs"
                },
                "summary": "Causal effect identification using causal graphs is a fundamental challenge\nin causal inference. While extensive research has been conducted in this area,\nmost existing methods assume the availability of fully specified directed\nacyclic graphs or acyclic directed mixed graphs. However, in complex domains\nsuch as medicine and epidemiology, complete causal knowledge is often\nunavailable, and only partial information about the system is accessible. This\npaper focuses on causal effect identification within partially specified causal\ngraphs, with particular emphasis on cluster-directed mixed graphs (C-DMGs)\nwhich can represent many different acyclic directed mixed graphs (ADMGs). These\ngraphs provide a higher-level representation of causal relationships by\ngrouping variables into clusters, offering a more practical approach for\nhandling complex systems. Unlike fully specified ADMGs, C-DMGs can contain\ncycles, which complicate their analysis and interpretation. Furthermore, their\ncluster-based nature introduces new challenges, as it gives rise to two\ndistinct types of causal effects: macro causal effects and micro causal\neffects, each with different properties. In this work, we focus on macro causal\neffects, which describe the effects of entire clusters on other clusters. We\nestablish that the do-calculus is both sound and complete for identifying these\neffects in C-DMGs over ADMGs when the cluster sizes are either unknown or of\nsize greater than one. Additionally, we provide a graphical characterization of\nnon-identifiability for macro causal effects in these graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal effect identification using causal graphs is a fundamental challenge\nin causal inference. While extensive research has been conducted in this area,\nmost existing methods assume the availability of fully specified directed\nacyclic graphs or acyclic directed mixed graphs. However, in complex domains\nsuch as medicine and epidemiology, complete causal knowledge is often\nunavailable, and only partial information about the system is accessible. This\npaper focuses on causal effect identification within partially specified causal\ngraphs, with particular emphasis on cluster-directed mixed graphs (C-DMGs)\nwhich can represent many different acyclic directed mixed graphs (ADMGs). These\ngraphs provide a higher-level representation of causal relationships by\ngrouping variables into clusters, offering a more practical approach for\nhandling complex systems. Unlike fully specified ADMGs, C-DMGs can contain\ncycles, which complicate their analysis and interpretation. Furthermore, their\ncluster-based nature introduces new challenges, as it gives rise to two\ndistinct types of causal effects: macro causal effects and micro causal\neffects, each with different properties. In this work, we focus on macro causal\neffects, which describe the effects of entire clusters on other clusters. We\nestablish that the do-calculus is both sound and complete for identifying these\neffects in C-DMGs over ADMGs when the cluster sizes are either unknown or of\nsize greater than one. Additionally, we provide a graphical characterization of\nnon-identifiability for macro causal effects in these graphs."
                },
                "authors": [
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "arxiv_comment": "Accepted to TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19171v2",
                "updated": "2025-09-02T08:53:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    53,
                    3,
                    1,
                    245,
                    0
                ],
                "published": "2025-04-27T09:26:15Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    9,
                    26,
                    15,
                    6,
                    117,
                    0
                ],
                "title": "GPU-Accelerated Parallel Selected Inversion for Structured Matrices\n  Using sTiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Parallel Selected Inversion for Structured Matrices\n  Using sTiles"
                },
                "summary": "Selected inversion is essential for applications such as Bayesian inference,\nelectronic structure calculations, and inverse covariance estimation, where\ncomputing only specific elements of large sparse matrix inverses significantly\nreduces computational and memory overhead. We present an efficient\nimplementation of a two-phase parallel algorithm for computing selected\nelements of the inverse of a sparse symmetric matrix A, which can be expressed\nas A = LL^T through sparse Cholesky factorization. Our approach leverages a\ntile-based structure, focusing on selected dense tiles to optimize\ncomputational efficiency and parallelism. While the focus is on arrowhead\nmatrices, the method can be extended to handle general structured matrices.\nPerformance evaluations on a dual-socket 26-core Intel Xeon CPU server\ndemonstrate that sTiles outperforms state-of-the-art direct solvers such as\nPanua-PARDISO, achieving up to 13X speedup on large-scale structured matrices.\nAdditionally, our GPU implementation using an NVIDIA A100 GPU demonstrates\nsubstantial acceleration over its CPU counterpart, achieving up to 5X speedup\nfor large, high-bandwidth matrices with high computational intensity. These\nresults underscore the robustness and versatility of sTiles, validating its\neffectiveness across various densities and problem configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selected inversion is essential for applications such as Bayesian inference,\nelectronic structure calculations, and inverse covariance estimation, where\ncomputing only specific elements of large sparse matrix inverses significantly\nreduces computational and memory overhead. We present an efficient\nimplementation of a two-phase parallel algorithm for computing selected\nelements of the inverse of a sparse symmetric matrix A, which can be expressed\nas A = LL^T through sparse Cholesky factorization. Our approach leverages a\ntile-based structure, focusing on selected dense tiles to optimize\ncomputational efficiency and parallelism. While the focus is on arrowhead\nmatrices, the method can be extended to handle general structured matrices.\nPerformance evaluations on a dual-socket 26-core Intel Xeon CPU server\ndemonstrate that sTiles outperforms state-of-the-art direct solvers such as\nPanua-PARDISO, achieving up to 13X speedup on large-scale structured matrices.\nAdditionally, our GPU implementation using an NVIDIA A100 GPU demonstrates\nsubstantial acceleration over its CPU counterpart, achieving up to 5X speedup\nfor large, high-bandwidth matrices with high computational intensity. These\nresults underscore the robustness and versatility of sTiles, validating its\neffectiveness across various densities and problem configurations."
                },
                "authors": [
                    {
                        "name": "Esmail Abdul Fattah"
                    },
                    {
                        "name": "Hatem Ltaief"
                    },
                    {
                        "name": "Havard Rue"
                    },
                    {
                        "name": "David Keyes"
                    }
                ],
                "author_detail": {
                    "name": "David Keyes"
                },
                "author": "David Keyes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12122v2",
                "updated": "2025-09-02T08:43:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    43,
                    5,
                    1,
                    245,
                    0
                ],
                "published": "2025-02-17T18:46:29Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    46,
                    29,
                    0,
                    48,
                    0
                ],
                "title": "Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty\n  Quantification for LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty\n  Quantification for LoRA"
                },
                "summary": "Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large\nlanguage models by decomposing weight updates into low-rank matrices,\nsignificantly reducing storage and computational overhead. While effective,\nstandard LoRA lacks mechanisms for uncertainty quantification, leading to\noverconfident and poorly calibrated models. Bayesian variants of LoRA address\nthis limitation, but at the cost of a significantly increased number of\ntrainable parameters, partially offsetting the original efficiency gains.\nAdditionally, these models are harder to train and may suffer from unstable\nconvergence. In this work, we propose a novel parameter-efficient Bayesian LoRA\nvia subspace inference, demonstrating that effective uncertainty quantification\ncan be achieved in very low-dimensional parameter spaces. The proposed method\nachieves strong performance with improved calibration and generalization while\nmaintaining computational efficiency. Our empirical findings show that, with\nthe appropriate projection of the weight space: (1) uncertainty can be\neffectively modeled in a low-dimensional space, and (2) weight covariances\nexhibit low ranks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large\nlanguage models by decomposing weight updates into low-rank matrices,\nsignificantly reducing storage and computational overhead. While effective,\nstandard LoRA lacks mechanisms for uncertainty quantification, leading to\noverconfident and poorly calibrated models. Bayesian variants of LoRA address\nthis limitation, but at the cost of a significantly increased number of\ntrainable parameters, partially offsetting the original efficiency gains.\nAdditionally, these models are harder to train and may suffer from unstable\nconvergence. In this work, we propose a novel parameter-efficient Bayesian LoRA\nvia subspace inference, demonstrating that effective uncertainty quantification\ncan be achieved in very low-dimensional parameter spaces. The proposed method\nachieves strong performance with improved calibration and generalization while\nmaintaining computational efficiency. Our empirical findings show that, with\nthe appropriate projection of the weight space: (1) uncertainty can be\neffectively modeled in a low-dimensional space, and (2) weight covariances\nexhibit low ranks."
                },
                "authors": [
                    {
                        "name": "Patryk Marszałek"
                    },
                    {
                        "name": "Klaudia Bałazy"
                    },
                    {
                        "name": "Jacek Tabor"
                    },
                    {
                        "name": "Tomasz Kuśmierczyk"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kuśmierczyk"
                },
                "author": "Tomasz Kuśmierczyk",
                "arxiv_comment": "Accepted to Findings of the Association for Computational\n  Linguistics: EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11452v2",
                "updated": "2025-09-02T08:20:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    20,
                    59,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-15T13:00:07Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    7,
                    4,
                    227,
                    0
                ],
                "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps"
                },
                "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://www.tbox.cn/about/model-ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://www.tbox.cn/about/model-ranking."
                },
                "authors": [
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Hongliang He"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Ruiqi Liang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Our platform is publicly accessible at\n  https://www.tbox.cn/about/model-ranking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19855v3",
                "updated": "2025-09-03T02:56:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    2,
                    56,
                    22,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-27T13:13:20Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    13,
                    13,
                    20,
                    2,
                    239,
                    0
                ],
                "title": "Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented\n  Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented\n  Complex Reasoning"
                },
                "summary": "Graph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema."
                },
                "authors": [
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Yifei Yu"
                    },
                    {
                        "name": "Qian-Wen Zhang"
                    },
                    {
                        "name": "Linhao Luo"
                    },
                    {
                        "name": "Xiao Huang"
                    },
                    {
                        "name": "Yunsheng Wu"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "19 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14212v3",
                "updated": "2025-09-03T03:06:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    3,
                    6,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-04-19T07:36:02Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    7,
                    36,
                    2,
                    5,
                    109,
                    0
                ],
                "title": "Bias Analysis and Mitigation through Protected Attribute Detection and\n  Regard Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Analysis and Mitigation through Protected Attribute Detection and\n  Regard Classification"
                },
                "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus."
                },
                "authors": [
                    {
                        "name": "Takuma Udagawa"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Hiroshi Kanayama"
                    },
                    {
                        "name": "Bishwaranjan Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Bishwaranjan Bhattacharjee"
                },
                "author": "Bishwaranjan Bhattacharjee",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10180v2",
                "updated": "2025-09-02T07:27:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    7,
                    27,
                    9,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-13T09:08:24Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    9,
                    8,
                    24,
                    3,
                    72,
                    0
                ],
                "title": "Optical+NIR analysis of a Newly Confirmed Einstein ring at z$\\sim$1 from\n  the Kilo-Degree Survey: Dark matter fraction, total and dark matter density\n  slope and IMF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical+NIR analysis of a Newly Confirmed Einstein ring at z$\\sim$1 from\n  the Kilo-Degree Survey: Dark matter fraction, total and dark matter density\n  slope and IMF"
                },
                "summary": "We report the spectroscopic confirmation of a bright blue Einstein ring in\nthe Kilo Degree Survey (KiDS) footprint: the Einstein ``blue eye''.\nSpectroscopic data from X-Shooter at the Very Large Telescope (VLT) show that\nthe lens is a typical early-type galaxy (ETG) at $z_l=0.9906$, while the\nbackground source is a Ly$\\alpha$ emitter at $z_s=2.823$. The reference lens\nmodeling was performed on a high-resolution $Y-$band adaptive-optics image from\nHAWK-I at VLT. Assuming a singular isothermal ellipsoid (SIE) total mass\ndensity profile, we inferred an Einstein radius $R_{Ein}=10.47 \\pm 0.06$ kpc.\nThe average slope of the total mass density inside the Einstein radius, as\ndetermined by a joint analysis of lensing and isotropic Jeans equations is\n$\\gamma_{tot}=2.14^{+0.06}_{-0.07}$, showing no systematic deviation from the\nslopes of lower redshift galaxies, This can be the evidence of ETGs developing\nthrough dry mergers plus moderate dissipationless accretion. Stellar population\nanalysis with 8-band ($gri$ZYJHK$s$) photometries from KiDS and VIKING shows\nthat the total stellar mass of the lens is $M*=(3.95\\pm 0.35)\\times 10^{11}\nM_\\odot$ (Salpeter Initial Mass Function, IMF), implying a dark matter fraction\ninside the effective radius to be $f_{\\rm DM}=0.307\\pm 0.151$. We finally\nexplored the dark matter halo slope and found a strong degeneracy with the\ndynamic stellar mass. Dark matter adiabatic contraction is needed to explain\nthe posterior distribution of the slope unless IMF heavier than Salpeter is\nassumed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the spectroscopic confirmation of a bright blue Einstein ring in\nthe Kilo Degree Survey (KiDS) footprint: the Einstein ``blue eye''.\nSpectroscopic data from X-Shooter at the Very Large Telescope (VLT) show that\nthe lens is a typical early-type galaxy (ETG) at $z_l=0.9906$, while the\nbackground source is a Ly$\\alpha$ emitter at $z_s=2.823$. The reference lens\nmodeling was performed on a high-resolution $Y-$band adaptive-optics image from\nHAWK-I at VLT. Assuming a singular isothermal ellipsoid (SIE) total mass\ndensity profile, we inferred an Einstein radius $R_{Ein}=10.47 \\pm 0.06$ kpc.\nThe average slope of the total mass density inside the Einstein radius, as\ndetermined by a joint analysis of lensing and isotropic Jeans equations is\n$\\gamma_{tot}=2.14^{+0.06}_{-0.07}$, showing no systematic deviation from the\nslopes of lower redshift galaxies, This can be the evidence of ETGs developing\nthrough dry mergers plus moderate dissipationless accretion. Stellar population\nanalysis with 8-band ($gri$ZYJHK$s$) photometries from KiDS and VIKING shows\nthat the total stellar mass of the lens is $M*=(3.95\\pm 0.35)\\times 10^{11}\nM_\\odot$ (Salpeter Initial Mass Function, IMF), implying a dark matter fraction\ninside the effective radius to be $f_{\\rm DM}=0.307\\pm 0.151$. We finally\nexplored the dark matter halo slope and found a strong degeneracy with the\ndynamic stellar mass. Dark matter adiabatic contraction is needed to explain\nthe posterior distribution of the slope unless IMF heavier than Salpeter is\nassumed."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Nicola R. Napolitano"
                    },
                    {
                        "name": "Giuseppe D Ago"
                    },
                    {
                        "name": "Vyacheslav N. Shalyapin"
                    },
                    {
                        "name": "Kai Zhu"
                    },
                    {
                        "name": "Xiaotong Guo"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Leon V. E. Koopmans"
                    },
                    {
                        "name": "Chiara Spiniello"
                    },
                    {
                        "name": "Crescenzo Tortora"
                    },
                    {
                        "name": "Francesco La Barbera"
                    },
                    {
                        "name": "Haicheng Feng"
                    },
                    {
                        "name": "Liang Gao"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Koen Kuijken"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Linghua Xie"
                    },
                    {
                        "name": "Mario Radovich"
                    },
                    {
                        "name": "Alexey Sergeyev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Sergeyev"
                },
                "author": "Alexey Sergeyev",
                "arxiv_doi": "10.3847/2041-8213/ade680",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ade680",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.10180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 4 figures, 2 tables, published by APJL",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06397v2",
                "updated": "2025-09-02T07:24:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    7,
                    24,
                    5,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-09T02:36:31Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    36,
                    31,
                    6,
                    68,
                    0
                ],
                "title": "Removing Averaging: Personalized Lip-Sync Driven Characters Based on\n  Identity Adapter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Removing Averaging: Personalized Lip-Sync Driven Characters Based on\n  Identity Adapter"
                },
                "summary": "Recent advances in diffusion-based lip-syncing generative models have\ndemonstrated their ability to produce highly synchronized talking face videos\nfor visual dubbing. Although these models excel at lip synchronization, they\noften struggle to maintain fine-grained control over facial details in\ngenerated images. In this work, we identify \"lip averaging\" phenomenon where\nthe model fails to preserve subtle facial details when dubbing unseen\nin-the-wild videos. This issue arises because the commonly used UNet backbone\nprimarily integrates audio features into visual representations in the latent\nspace via cross-attention mechanisms and multi-scale fusion, but it struggles\nto retain fine-grained lip details in the generated faces. To address this\nissue, we propose UnAvgLip, which extracts identity embeddings from reference\nvideos to generate highly faithful facial sequences while maintaining accurate\nlip synchronization. Specifically, our method comprises two primary components:\n(1) an Identity Perceiver module that encodes facial embeddings to align with\nconditioned audio features; and (2) an ID-CrossAttn module that injects facial\nembeddings into the generation process, enhancing model's capability of\nidentity retention. Extensive experiments demonstrate that, at a modest\ntraining and inference cost, UnAvgLip effectively mitigates the \"averaging\"\nphenomenon in lip inpainting, significantly preserving unique facial\ncharacteristics while maintaining precise lip synchronization. Compared with\nthe original approach, our method demonstrates significant improvements of 5%\non the identity consistency metric and 2% on the SSIM metric across two\nbenchmark datasets (HDTF and LRW).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based lip-syncing generative models have\ndemonstrated their ability to produce highly synchronized talking face videos\nfor visual dubbing. Although these models excel at lip synchronization, they\noften struggle to maintain fine-grained control over facial details in\ngenerated images. In this work, we identify \"lip averaging\" phenomenon where\nthe model fails to preserve subtle facial details when dubbing unseen\nin-the-wild videos. This issue arises because the commonly used UNet backbone\nprimarily integrates audio features into visual representations in the latent\nspace via cross-attention mechanisms and multi-scale fusion, but it struggles\nto retain fine-grained lip details in the generated faces. To address this\nissue, we propose UnAvgLip, which extracts identity embeddings from reference\nvideos to generate highly faithful facial sequences while maintaining accurate\nlip synchronization. Specifically, our method comprises two primary components:\n(1) an Identity Perceiver module that encodes facial embeddings to align with\nconditioned audio features; and (2) an ID-CrossAttn module that injects facial\nembeddings into the generation process, enhancing model's capability of\nidentity retention. Extensive experiments demonstrate that, at a modest\ntraining and inference cost, UnAvgLip effectively mitigates the \"averaging\"\nphenomenon in lip inpainting, significantly preserving unique facial\ncharacteristics while maintaining precise lip synchronization. Compared with\nthe original approach, our method demonstrates significant improvements of 5%\non the identity consistency metric and 2% on the SSIM metric across two\nbenchmark datasets (HDTF and LRW)."
                },
                "authors": [
                    {
                        "name": "Yanyu Zhu"
                    },
                    {
                        "name": "Lichen Bai"
                    },
                    {
                        "name": "Jintao Xu"
                    },
                    {
                        "name": "Hai-tao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hai-tao Zheng"
                },
                "author": "Hai-tao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04307v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04307v4",
                "updated": "2025-09-02T07:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    7,
                    20,
                    33,
                    1,
                    245,
                    0
                ],
                "published": "2024-12-05T16:26:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark"
                },
                "summary": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding is required to\nreduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three fundamental\ncontributions. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nprovide a foundation for future research and inspire broader engagement in this\nfield. To support a long-term study, all source code and the dataset are made\navailable at\n\\href{https://github.com/chansongoal/LaMoFC}{https://github.com/chansongoal/LaMoFC}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding is required to\nreduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three fundamental\ncontributions. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nprovide a foundation for future research and inspire broader engagement in this\nfield. To support a long-term study, all source code and the dataset are made\navailable at\n\\href{https://github.com/chansongoal/LaMoFC}{https://github.com/chansongoal/LaMoFC}."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Yifan Ma"
                    },
                    {
                        "name": "Qiaoxi Chen"
                    },
                    {
                        "name": "Yenan Xu"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04307v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04307v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14589v2",
                "updated": "2025-09-02T05:09:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    5,
                    9,
                    17,
                    1,
                    245,
                    0
                ],
                "published": "2025-06-17T14:52:50Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    14,
                    52,
                    50,
                    1,
                    168,
                    0
                ],
                "title": "NetRoller: Interfacing General and Specialized Models for End-to-End\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetRoller: Interfacing General and Specialized Models for End-to-End\n  Autonomous Driving"
                },
                "summary": "Integrating General Models (GMs) such as Large Language Models (LLMs), with\nSpecialized Models (SMs) in autonomous driving tasks presents a promising\napproach to mitigating challenges in data diversity and model capacity of\nexisting specialized driving models. However, this integration leads to\nproblems of asynchronous systems, which arise from the distinct characteristics\ninherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an\nadapter that incorporates a set of novel mechanisms to facilitate the seamless\nintegration of GMs and specialized driving models. Specifically, our mechanisms\nfor interfacing the asynchronous GMs and SMs are organized into three key\nstages. NetRoller first harvests semantically rich and computationally\nefficient representations from the reasoning processes of LLMs using an early\nstopping mechanism, which preserves critical insights on driving context while\nmaintaining low overhead. It then applies learnable query embeddings,\nnonsensical embeddings, and positional layer embeddings to facilitate robust\nand efficient cross-modality translation. At last, it employs computationally\nefficient Query Shift and Feature Shift mechanisms to enhance the performance\nof SMs through few-epoch fine-tuning. Based on the mechanisms formalized in\nthese three stages, NetRoller enables specialized driving models to operate at\ntheir native frequencies while maintaining situational awareness of the GM.\nExperiments conducted on the nuScenes dataset demonstrate that integrating GM\nthrough NetRoller significantly improves human similarity and safety in\nplanning tasks, and it also achieves noticeable precision improvements in\ndetection and mapping tasks for end-to-end autonomous driving. The code and\nmodels are available at https://github.com/Rex-sys-hk/NetRoller .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating General Models (GMs) such as Large Language Models (LLMs), with\nSpecialized Models (SMs) in autonomous driving tasks presents a promising\napproach to mitigating challenges in data diversity and model capacity of\nexisting specialized driving models. However, this integration leads to\nproblems of asynchronous systems, which arise from the distinct characteristics\ninherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an\nadapter that incorporates a set of novel mechanisms to facilitate the seamless\nintegration of GMs and specialized driving models. Specifically, our mechanisms\nfor interfacing the asynchronous GMs and SMs are organized into three key\nstages. NetRoller first harvests semantically rich and computationally\nefficient representations from the reasoning processes of LLMs using an early\nstopping mechanism, which preserves critical insights on driving context while\nmaintaining low overhead. It then applies learnable query embeddings,\nnonsensical embeddings, and positional layer embeddings to facilitate robust\nand efficient cross-modality translation. At last, it employs computationally\nefficient Query Shift and Feature Shift mechanisms to enhance the performance\nof SMs through few-epoch fine-tuning. Based on the mechanisms formalized in\nthese three stages, NetRoller enables specialized driving models to operate at\ntheir native frequencies while maintaining situational awareness of the GM.\nExperiments conducted on the nuScenes dataset demonstrate that integrating GM\nthrough NetRoller significantly improves human similarity and safety in\nplanning tasks, and it also achieves noticeable precision improvements in\ndetection and mapping tasks for end-to-end autonomous driving. The code and\nmodels are available at https://github.com/Rex-sys-hk/NetRoller ."
                },
                "authors": [
                    {
                        "name": "Ren Xin"
                    },
                    {
                        "name": "Hongji Liu"
                    },
                    {
                        "name": "Xiaodong Mei"
                    },
                    {
                        "name": "Wenru Liu"
                    },
                    {
                        "name": "Maosheng Ye"
                    },
                    {
                        "name": "Zhili Chen"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16734v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16734v3",
                "updated": "2025-09-02T05:02:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    5,
                    2,
                    54,
                    1,
                    245,
                    0
                ],
                "published": "2025-01-28T06:19:29Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    19,
                    29,
                    1,
                    28,
                    0
                ],
                "title": "Distilling Large Language Models for Network Active Queue Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Large Language Models for Network Active Queue Management"
                },
                "summary": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems."
                },
                "authors": [
                    {
                        "name": "Shiva Raj Pokhrel"
                    },
                    {
                        "name": "Deol Satish"
                    },
                    {
                        "name": "Jonathan Kua"
                    },
                    {
                        "name": "Anwar Walid"
                    }
                ],
                "author_detail": {
                    "name": "Anwar Walid"
                },
                "author": "Anwar Walid",
                "arxiv_journal_ref": "IEEE Trans on Networking, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16734v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16734v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22441v2",
                "updated": "2025-09-02T04:46:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    4,
                    46,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2025-05-28T15:04:46Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    15,
                    4,
                    46,
                    2,
                    148,
                    0
                ],
                "title": "Can NeRFs See without Cameras?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can NeRFs See without Cameras?"
                },
                "summary": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRFs) have been remarkably successful at\nsynthesizing novel views of 3D scenes by optimizing a volumetric scene\nfunction. This scene function models how optical rays bring color information\nfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signals\ncan also be viewed as a vehicle for delivering information about the\nenvironment to a sensor. However, unlike camera pixels, an RF/audio sensor\nreceives a mixture of signals that contain many environmental reflections (also\ncalled \"multipath\"). Is it still possible to infer the environment using such\nmultipath signals? We show that with redesign, NeRFs can be taught to learn\nfrom multipath signals, and thereby \"see\" the environment. As a grounding\napplication, we aim to infer the indoor floorplan of a home from sparse WiFi\nmeasurements made at multiple locations inside the home. Although a difficult\ninverse problem, our implicitly learnt floorplans look promising, and enables\nforward applications, such as indoor signal prediction and basic ray tracing."
                },
                "authors": [
                    {
                        "name": "Chaitanya Amballa"
                    },
                    {
                        "name": "Sattwik Basu"
                    },
                    {
                        "name": "Yu-Lin Wei"
                    },
                    {
                        "name": "Zhijian Yang"
                    },
                    {
                        "name": "Mehmet Ergezer"
                    },
                    {
                        "name": "Romit Roy Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Romit Roy Choudhury"
                },
                "author": "Romit Roy Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15044v2",
                "updated": "2025-09-02T04:31:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    4,
                    31,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-20T20:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    10,
                    56,
                    2,
                    232,
                    0
                ],
                "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner"
                },
                "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Yanran Wu"
                    },
                    {
                        "name": "Xinyu Luo"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07778v2",
                "updated": "2025-09-02T03:57:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    57,
                    28,
                    1,
                    245,
                    0
                ],
                "published": "2025-06-09T13:55:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    13,
                    55,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "A Vision-Language Agent System for Compositional Reasoning with\n  VLM-assisted Script and Executable Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Vision-Language Agent System for Compositional Reasoning with\n  VLM-assisted Script and Executable Generation"
                },
                "summary": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal vision-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date offer poor performance\nfor compositional reasoning. This paper presents VLAgent, a vision-language\nagent system for vision-text compositional reasoning with three novel features.\nFirst, VLAgent leverages a pre-trained LLM with few-shot context learning to\ngenerate the planning script for each compositional reasoning task and provides\na backend engine to generate and perform executable runtime, which maps the\nplanning script into executable code using the VLAgent library for VLAgent\nexecutor. Second, VLAgent introduces the SS-parser, which identifies and\ncorrects logic errors embedded in the LLM-generated planning script, to further\nenhance the quality of script-executable mapping. Third, VLAgent introduces the\ncompositional reasoning output verifier, which validates and refines the output\nof complex compositional reasoning steps, by leveraging complementary reasoning\ntechniques, e.g., ensemble learning and caption analysis. Extensive experiments\nare conducted on six visual benchmarks and compared to a dozen of the SoTA\nvisual reasoning models. The results show that VLAgent outperforms existing\nrepresentative approaches for compositional text-visual reasoning. Our code and\ndatasets with outputs will be made available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal vision-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date offer poor performance\nfor compositional reasoning. This paper presents VLAgent, a vision-language\nagent system for vision-text compositional reasoning with three novel features.\nFirst, VLAgent leverages a pre-trained LLM with few-shot context learning to\ngenerate the planning script for each compositional reasoning task and provides\na backend engine to generate and perform executable runtime, which maps the\nplanning script into executable code using the VLAgent library for VLAgent\nexecutor. Second, VLAgent introduces the SS-parser, which identifies and\ncorrects logic errors embedded in the LLM-generated planning script, to further\nenhance the quality of script-executable mapping. Third, VLAgent introduces the\ncompositional reasoning output verifier, which validates and refines the output\nof complex compositional reasoning steps, by leveraging complementary reasoning\ntechniques, e.g., ensemble learning and caption analysis. Extensive experiments\nare conducted on six visual benchmarks and compared to a dozen of the SoTA\nvisual reasoning models. The results show that VLAgent outperforms existing\nrepresentative approaches for compositional text-visual reasoning. Our code and\ndatasets with outputs will be made available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yichang Xu"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Ramana Rao Kompella"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Fatih Ilhan"
                    },
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Zachary Yahn"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11423v3",
                "updated": "2025-09-02T02:55:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    2,
                    55,
                    17,
                    1,
                    245,
                    0
                ],
                "published": "2025-05-16T16:36:00Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    16,
                    36,
                    0,
                    4,
                    136,
                    0
                ],
                "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following\n  in LLMs"
                },
                "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Xupeng Chen"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Narayanan Sadagopan"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18190v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18190v3",
                "updated": "2025-09-02T02:30:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    2,
                    30,
                    46,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-25T16:48:51Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    48,
                    51,
                    0,
                    237,
                    0
                ],
                "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering"
                },
                "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor."
                },
                "authors": [
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Boyu Niu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Boxiu Li"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18190v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18190v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17967v2",
                "updated": "2025-09-02T02:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    2,
                    28,
                    24,
                    1,
                    245,
                    0
                ],
                "published": "2025-02-25T08:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    41,
                    1,
                    1,
                    56,
                    0
                ],
                "title": "Agent Trading Arena: A Study on Numerical Understanding in LLM-Based\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Trading Arena: A Study on Numerical Understanding in LLM-Based\n  Agents"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language tasks, yet their performance in dynamic, real-world financial\nenvironments remains underexplored. Existing approaches are limited to\nhistorical backtesting, where trading actions cannot influence market prices\nand agents train only on static data. To address this limitation, we present\nthe Agent Trading Arena, a virtual zero-sum stock market in which LLM-based\nagents engage in competitive multi-agent trading and directly impact price\ndynamics. By simulating realistic bid-ask interactions, our platform enables\ntraining in scenarios that closely mirror live markets, thereby narrowing the\ngap between training and evaluation. Experiments reveal that LLMs struggle with\nnumerical reasoning when given plain-text data, often overfitting to local\npatterns and recent values. In contrast, chart-based visualizations\nsignificantly enhance both numerical reasoning and trading performance.\nFurthermore, incorporating a reflection module yields additional improvements,\nespecially with visual inputs. Evaluations on NASDAQ and CSI datasets\ndemonstrate the superiority of our method, particularly under high volatility.\nAll code and data are available at\nhttps://github.com/wekjsdvnm/Agent-Trading-Arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language tasks, yet their performance in dynamic, real-world financial\nenvironments remains underexplored. Existing approaches are limited to\nhistorical backtesting, where trading actions cannot influence market prices\nand agents train only on static data. To address this limitation, we present\nthe Agent Trading Arena, a virtual zero-sum stock market in which LLM-based\nagents engage in competitive multi-agent trading and directly impact price\ndynamics. By simulating realistic bid-ask interactions, our platform enables\ntraining in scenarios that closely mirror live markets, thereby narrowing the\ngap between training and evaluation. Experiments reveal that LLMs struggle with\nnumerical reasoning when given plain-text data, often overfitting to local\npatterns and recent values. In contrast, chart-based visualizations\nsignificantly enhance both numerical reasoning and trading performance.\nFurthermore, incorporating a reflection module yields additional improvements,\nespecially with visual inputs. Evaluations on NASDAQ and CSI datasets\ndemonstrate the superiority of our method, particularly under high volatility.\nAll code and data are available at\nhttps://github.com/wekjsdvnm/Agent-Trading-Arena."
                },
                "authors": [
                    {
                        "name": "Tianmi Ma"
                    },
                    {
                        "name": "Jiawei Du"
                    },
                    {
                        "name": "Wenxin Huang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Xian Zhong"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Joey Tianyi Zhou"
                },
                "author": "Joey Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16124v2",
                "updated": "2025-09-01T23:52:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    23,
                    52,
                    31,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-22T00:36:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    0,
                    36,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making"
                },
                "summary": "While robots have previously utilized rule-based systems or probabilistic\nmodels for user interaction, the rapid evolution of large language models\n(LLMs) presents new opportunities to develop LLM-powered robots for enhanced\nhuman-robot interaction (HRI). To fully realize these capabilities, however,\nrobots need to collect data such as audio, fine-grained images, video, and\nlocations. As a result, LLMs often process sensitive personal information,\nparticularly within private environments, such as homes. Given the tension\nbetween utility and privacy risks, evaluating how current LLMs manage sensitive\ndata is critical. Specifically, we aim to explore the extent to which\nout-of-the-box LLMs are privacy-aware in the context of household robots. In\nthis work, we present a set of privacy-relevant scenarios developed using the\nContextual Integrity (CI) framework. We first surveyed users' privacy\npreferences regarding in-home robot behaviors and then examined how their\nprivacy orientations affected their choices of these behaviors (N = 450). We\nthen provided the same set of scenarios and questions to state-of-the-art LLMs\n(N = 10) and found that the agreement between humans and LLMs was generally\nlow. To further investigate the capabilities of LLMs as potential privacy\ncontrollers, we implemented four additional prompting strategies and compared\ntheir results. We discuss the performance of the evaluated models as well as\nthe implications and potential of AI privacy awareness in human-robot\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While robots have previously utilized rule-based systems or probabilistic\nmodels for user interaction, the rapid evolution of large language models\n(LLMs) presents new opportunities to develop LLM-powered robots for enhanced\nhuman-robot interaction (HRI). To fully realize these capabilities, however,\nrobots need to collect data such as audio, fine-grained images, video, and\nlocations. As a result, LLMs often process sensitive personal information,\nparticularly within private environments, such as homes. Given the tension\nbetween utility and privacy risks, evaluating how current LLMs manage sensitive\ndata is critical. Specifically, we aim to explore the extent to which\nout-of-the-box LLMs are privacy-aware in the context of household robots. In\nthis work, we present a set of privacy-relevant scenarios developed using the\nContextual Integrity (CI) framework. We first surveyed users' privacy\npreferences regarding in-home robot behaviors and then examined how their\nprivacy orientations affected their choices of these behaviors (N = 450). We\nthen provided the same set of scenarios and questions to state-of-the-art LLMs\n(N = 10) and found that the agreement between humans and LLMs was generally\nlow. To further investigate the capabilities of LLMs as potential privacy\ncontrollers, we implemented four additional prompting strategies and compared\ntheir results. We discuss the performance of the evaluated models as well as\nthe implications and potential of AI privacy awareness in human-robot\ninteraction."
                },
                "authors": [
                    {
                        "name": "Dakota Sullivan"
                    },
                    {
                        "name": "Shirley Zhang"
                    },
                    {
                        "name": "Jennica Li"
                    },
                    {
                        "name": "Heather Kirkorian"
                    },
                    {
                        "name": "Bilge Mutlu"
                    },
                    {
                        "name": "Kassem Fawaz"
                    }
                ],
                "author_detail": {
                    "name": "Kassem Fawaz"
                },
                "author": "Kassem Fawaz",
                "arxiv_comment": "18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed\n  equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17686v3",
                "updated": "2025-09-03T03:13:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    3,
                    13,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-23T16:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    51,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Debiased maximum-likelihood estimators for hazard ratios under\n  kernel-based machine-learning adjustment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased maximum-likelihood estimators for hazard ratios under\n  kernel-based machine-learning adjustment"
                },
                "summary": "Previous studies have shown that hazard ratios between treatment groups\nestimated with the Cox model are uninterpretable because the unspecified\nbaseline hazard of the model fails to identify temporal change in the risk set\ncomposition due to treatment assignment and unobserved factors among multiple,\ncontradictory scenarios. To alleviate this problem, especially in studies based\non observational data with uncontrolled dynamic treatment and real-time\nmeasurement of many covariates, we propose abandoning the baseline hazard and\nusing kernel-based machine learning to explicitly model the change in the risk\nset with or without latent variables. For this framework, we clarify the\ncontext in which hazard ratios can be causally interpreted, and then develop a\nmethod based on Neyman orthogonality to compute debiased maximum-likelihood\nestimators of hazard ratios, proving necessary convergence results. Numerical\nsimulations confirm that the proposed method identifies the true hazard ratios\nwith minimal bias. These results lay the foundation for developing a useful,\nalternative method for causal inference with uncontrolled, observational data\nin modern epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies have shown that hazard ratios between treatment groups\nestimated with the Cox model are uninterpretable because the unspecified\nbaseline hazard of the model fails to identify temporal change in the risk set\ncomposition due to treatment assignment and unobserved factors among multiple,\ncontradictory scenarios. To alleviate this problem, especially in studies based\non observational data with uncontrolled dynamic treatment and real-time\nmeasurement of many covariates, we propose abandoning the baseline hazard and\nusing kernel-based machine learning to explicitly model the change in the risk\nset with or without latent variables. For this framework, we clarify the\ncontext in which hazard ratios can be causally interpreted, and then develop a\nmethod based on Neyman orthogonality to compute debiased maximum-likelihood\nestimators of hazard ratios, proving necessary convergence results. Numerical\nsimulations confirm that the proposed method identifies the true hazard ratios\nwith minimal bias. These results lay the foundation for developing a useful,\nalternative method for causal inference with uncontrolled, observational data\nin modern epidemiology."
                },
                "authors": [
                    {
                        "name": "Takashi Hayakawa"
                    },
                    {
                        "name": "Satoshi Asai"
                    }
                ],
                "author_detail": {
                    "name": "Satoshi Asai"
                },
                "author": "Satoshi Asai",
                "arxiv_comment": "Proposition 3 of the first version was wrong. This was fixed by\n  introducing new theoretical results in the second version so that all of the\n  claims of the first version are valid For some reason, in the uploading of\n  the second version, a duplicate of the first version was wrongly publicized,\n  which we think is not our mistake. We therefore upload the updated version as\n  version 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18115v2",
                "updated": "2025-09-01T21:57:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    21,
                    57,
                    12,
                    0,
                    244,
                    0
                ],
                "published": "2024-03-26T21:38:50Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    21,
                    38,
                    50,
                    1,
                    86,
                    0
                ],
                "title": "Assessing Vaccine Effectiveness in Observational Studies via Nested\n  Trial Emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Vaccine Effectiveness in Observational Studies via Nested\n  Trial Emulation"
                },
                "summary": "Observational data are often used to estimate real-world effectiveness and\ndurability of vaccines. A sequence of trials can be emulated to draw inference\nfrom such data while minimizing selection bias, immortal time bias, and\nconfounding. Typically, when nested trial emulation (NTE) is employed, effect\nestimates are pooled across trials. However, such pooled estimates may lack a\nclear interpretation when the treatment effect is heterogeneous across trials.\nFor vaccines against certain viruses, vaccine effectiveness may vary over\ncalendar time due to newly emerging variants of the virus. This manuscript\nconsiders a NTE inverse probability weighted estimator of vaccine effectiveness\nthat may vary over calendar time, time since vaccination, or both. Statistical\ntesting of the trial effect homogeneity assumption is considered. As observed\nchanges in vaccine effectiveness across trials may be attributable to variation\nin covariate distributions across trial-eligible populations, standardization\nof trial-specific inferences is also considered. Simulation studies are\npresented examining the finite-sample performance of the proposed methods under\na variety of scenarios. The methods are used to estimate vaccine effectiveness\nagainst COVID-19 outcomes using observational data on over 110,000 residents of\nAbruzzo, Italy during 2021.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observational data are often used to estimate real-world effectiveness and\ndurability of vaccines. A sequence of trials can be emulated to draw inference\nfrom such data while minimizing selection bias, immortal time bias, and\nconfounding. Typically, when nested trial emulation (NTE) is employed, effect\nestimates are pooled across trials. However, such pooled estimates may lack a\nclear interpretation when the treatment effect is heterogeneous across trials.\nFor vaccines against certain viruses, vaccine effectiveness may vary over\ncalendar time due to newly emerging variants of the virus. This manuscript\nconsiders a NTE inverse probability weighted estimator of vaccine effectiveness\nthat may vary over calendar time, time since vaccination, or both. Statistical\ntesting of the trial effect homogeneity assumption is considered. As observed\nchanges in vaccine effectiveness across trials may be attributable to variation\nin covariate distributions across trial-eligible populations, standardization\nof trial-specific inferences is also considered. Simulation studies are\npresented examining the finite-sample performance of the proposed methods under\na variety of scenarios. The methods are used to estimate vaccine effectiveness\nagainst COVID-19 outcomes using observational data on over 110,000 residents of\nAbruzzo, Italy during 2021."
                },
                "authors": [
                    {
                        "name": "Justin B. DeMonte"
                    },
                    {
                        "name": "Bonnie E. Shook-Sa"
                    },
                    {
                        "name": "Michael G. Hudgens"
                    }
                ],
                "author_detail": {
                    "name": "Michael G. Hudgens"
                },
                "author": "Michael G. Hudgens",
                "arxiv_comment": "42 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07482v2",
                "updated": "2025-09-01T21:27:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    21,
                    27,
                    15,
                    0,
                    244,
                    0
                ],
                "published": "2024-09-03T06:21:26Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    6,
                    21,
                    26,
                    1,
                    247,
                    0
                ],
                "title": "VSLLaVA: a pipeline of large multimodal foundation model for industrial\n  vibration signal analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSLLaVA: a pipeline of large multimodal foundation model for industrial\n  vibration signal analysis"
                },
                "summary": "While Large Multimodal Models (LMMs) excel in general multimodal tasks, they\nlack the domain-specific knowledge for industrial vibration signal analysis.\nThis paper introduces VSLLaVA, a comprehensive pipeline that utilizes expert\nknowledge-guided instruction tuning and evaluation to create an end-to-end LMM\nfor signal analysis. To achieve this, we construct a novel\nSignal-Question-Answer (SQA) dataset using an expert rule-based signal\ngenerator. This dataset facilitates a two-stage learning procedure. The first\nstep is efficient instruction fine-tuning with Low-Rank Adaptation (LoRA),\nwhich imparts specialized signal identification capabilities. Subsequently, we\ndesigned a tailored Group Relative Policy Optimization (GRPO) to refine the\nreasoning capabilities and enhance classification robustness. Then, a dual-mode\nevaluation framework is proposed, combining an LLM referee with expert rules\nfor semantic assessment using quantitative metrics for numerical and textual\naccuracy, which reveals that VSLLaVA significantly improves performance in\nsignal type identification and parameter analysis, and makes progress in the\nidentification and parameter analysis of fault-related signals. This research\ndemonstrates a viable approach for developing specialized foundational models\nfor complex industrial applications and marks a transition from conventional\ntask-specific systems to a cohesive, interactive foundational model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Multimodal Models (LMMs) excel in general multimodal tasks, they\nlack the domain-specific knowledge for industrial vibration signal analysis.\nThis paper introduces VSLLaVA, a comprehensive pipeline that utilizes expert\nknowledge-guided instruction tuning and evaluation to create an end-to-end LMM\nfor signal analysis. To achieve this, we construct a novel\nSignal-Question-Answer (SQA) dataset using an expert rule-based signal\ngenerator. This dataset facilitates a two-stage learning procedure. The first\nstep is efficient instruction fine-tuning with Low-Rank Adaptation (LoRA),\nwhich imparts specialized signal identification capabilities. Subsequently, we\ndesigned a tailored Group Relative Policy Optimization (GRPO) to refine the\nreasoning capabilities and enhance classification robustness. Then, a dual-mode\nevaluation framework is proposed, combining an LLM referee with expert rules\nfor semantic assessment using quantitative metrics for numerical and textual\naccuracy, which reveals that VSLLaVA significantly improves performance in\nsignal type identification and parameter analysis, and makes progress in the\nidentification and parameter analysis of fault-related signals. This research\ndemonstrates a viable approach for developing specialized foundational models\nfor complex industrial applications and marks a transition from conventional\ntask-specific systems to a cohesive, interactive foundational model."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xinran Zhang"
                    },
                    {
                        "name": "Jinfeng Huang"
                    },
                    {
                        "name": "Hongliang He"
                    },
                    {
                        "name": "Feibin Zhang"
                    },
                    {
                        "name": "Zhaoye Qin"
                    },
                    {
                        "name": "Fulei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Fulei Chu"
                },
                "author": "Fulei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14201v2",
                "updated": "2025-09-01T20:02:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    20,
                    2,
                    0,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-14T17:06:26Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    6,
                    26,
                    0,
                    195,
                    0
                ],
                "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation"
                },
                "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!"
                },
                "authors": [
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Mauricio Velazco"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Manuel Raúl Meléndez Luján"
                    },
                    {
                        "name": "Srisuma Movva"
                    },
                    {
                        "name": "Yogesh K Roy"
                    },
                    {
                        "name": "Quang Nguyen"
                    },
                    {
                        "name": "Roberto Rodriguez"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Michael Albada"
                    },
                    {
                        "name": "Julia Kiseleva"
                    },
                    {
                        "name": "Anand Mudgerikar"
                    }
                ],
                "author_detail": {
                    "name": "Anand Mudgerikar"
                },
                "author": "Anand Mudgerikar",
                "arxiv_comment": "Add code link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11614v3",
                "updated": "2025-09-01T18:50:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    50,
                    34,
                    0,
                    244,
                    0
                ],
                "published": "2024-06-17T15:00:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    0,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Intrinsic Test of Unlearning Using Parametric Knowledge Traces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Test of Unlearning Using Parametric Knowledge Traces"
                },
                "summary": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors."
                },
                "authors": [
                    {
                        "name": "Yihuai Hong"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Haiqin Yang"
                    },
                    {
                        "name": "Shauli Ravfogel"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted in EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03275v2",
                "updated": "2025-09-01T18:25:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    25,
                    38,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-05T15:33:00Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    33,
                    0,
                    2,
                    36,
                    0
                ],
                "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks."
                },
                "authors": [
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Yingchen Xu"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Qinqing Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Qinqing Zheng"
                },
                "author": "Qinqing Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20931v2",
                "updated": "2025-09-01T18:05:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    5,
                    6,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-28T15:57:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $τ$-bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $τ$-bench"
                },
                "summary": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Venkatesh Mishra"
                    },
                    {
                        "name": "Amir Saeidi"
                    },
                    {
                        "name": "Satyam Raj"
                    },
                    {
                        "name": "Mutsumi Nakamura"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Chitta Baral"
                    }
                ],
                "author_detail": {
                    "name": "Chitta Baral"
                },
                "author": "Chitta Baral",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12608v3",
                "updated": "2025-09-01T18:01:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    1,
                    37,
                    0,
                    244,
                    0
                ],
                "published": "2025-03-16T18:44:06Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    18,
                    44,
                    6,
                    6,
                    75,
                    0
                ],
                "title": "UniBERT: Adversarial Training for Language-Universal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniBERT: Adversarial Training for Language-Universal Representations"
                },
                "summary": "This paper presents UniBERT, a compact multilingual language model that uses\nan innovative training framework that integrates three components: masked\nlanguage modeling, adversarial training, and knowledge distillation.\nPre-trained on a meticulously curated Wikipedia corpus spanning 107 languages,\nUniBERT is designed to reduce the computational demands of large-scale models\nwhile maintaining competitive performance across various natural language\nprocessing tasks. Comprehensive evaluations on four tasks - named entity\nrecognition, natural language inference, question answering, and semantic\ntextual similarity - demonstrate that our multilingual training strategy\nenhanced by an adversarial objective significantly improves cross-lingual\ngeneralization. Specifically, UniBERT models show an average relative\nimprovement of 7.72% over traditional baselines, which achieved an average\nrelative improvement of only 1.17%, and statistical analysis confirms the\nsignificance of these gains (p-value = 0.0181). This work highlights the\nbenefits of combining adversarial training and knowledge distillation to build\nscalable and robust language models, thus advancing the field of multilingual\nand cross-lingual natural language processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents UniBERT, a compact multilingual language model that uses\nan innovative training framework that integrates three components: masked\nlanguage modeling, adversarial training, and knowledge distillation.\nPre-trained on a meticulously curated Wikipedia corpus spanning 107 languages,\nUniBERT is designed to reduce the computational demands of large-scale models\nwhile maintaining competitive performance across various natural language\nprocessing tasks. Comprehensive evaluations on four tasks - named entity\nrecognition, natural language inference, question answering, and semantic\ntextual similarity - demonstrate that our multilingual training strategy\nenhanced by an adversarial objective significantly improves cross-lingual\ngeneralization. Specifically, UniBERT models show an average relative\nimprovement of 7.72% over traditional baselines, which achieved an average\nrelative improvement of only 1.17%, and statistical analysis confirms the\nsignificance of these gains (p-value = 0.0181). This work highlights the\nbenefits of combining adversarial training and knowledge distillation to build\nscalable and robust language models, thus advancing the field of multilingual\nand cross-lingual natural language processing."
                },
                "authors": [
                    {
                        "name": "Andrei-Marius Avram"
                    },
                    {
                        "name": "Marian Lupaşcu"
                    },
                    {
                        "name": "Dumitru-Clementin Cercel"
                    },
                    {
                        "name": "Ionuţ Mironică"
                    },
                    {
                        "name": "Ştefan Trăuşan-Matu"
                    }
                ],
                "author_detail": {
                    "name": "Ştefan Trăuşan-Matu"
                },
                "author": "Ştefan Trăuşan-Matu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13258v2",
                "updated": "2025-09-01T17:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    17,
                    54,
                    58,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-18T17:41:35Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    41,
                    35,
                    0,
                    230,
                    0
                ],
                "title": "Statistical Inference for Subgraph Frequencies of Exchangeable Hyperedge\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Subgraph Frequencies of Exchangeable Hyperedge\n  Models"
                },
                "summary": "In statistical network analysis, models for binary adjacency matrices\nsatisfying vertex exchangeability are commonly used. However, such models may\nfail to capture key features of the data-generating process when interactions,\nrather than nodes, are fundamental units. We study statistical inference for\nsubgraph counts under an exchangeable hyperedge model. We introduce several\nclasses of subgraph statistics for hypergraphs and develop inferential tools\nfor subgraph frequencies that account for edge multiplicity. We show that a\nsubclass of these subgraph statistics is robust to the deletion of low-degree\nnodes, enabling inference in settings where low-degree nodes are more likely to\nbe missing. We also examine a more traditional notion of subgraph frequency\nthat ignores multiplicity, showing that while inference based on limiting\ndistributions is feasible in some cases, a non-degenerate limiting distribution\nmay not exist in others. Empirically, we assess our methods through simulations\nand newly collected real-world hypergraph data on academic and movie\ncollaborations, where our inferential tools outperform traditional approaches\nbased on binary adjacency matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In statistical network analysis, models for binary adjacency matrices\nsatisfying vertex exchangeability are commonly used. However, such models may\nfail to capture key features of the data-generating process when interactions,\nrather than nodes, are fundamental units. We study statistical inference for\nsubgraph counts under an exchangeable hyperedge model. We introduce several\nclasses of subgraph statistics for hypergraphs and develop inferential tools\nfor subgraph frequencies that account for edge multiplicity. We show that a\nsubclass of these subgraph statistics is robust to the deletion of low-degree\nnodes, enabling inference in settings where low-degree nodes are more likely to\nbe missing. We also examine a more traditional notion of subgraph frequency\nthat ignores multiplicity, showing that while inference based on limiting\ndistributions is feasible in some cases, a non-degenerate limiting distribution\nmay not exist in others. Empirically, we assess our methods through simulations\nand newly collected real-world hypergraph data on academic and movie\ncollaborations, where our inferential tools outperform traditional approaches\nbased on binary adjacency matrices."
                },
                "authors": [
                    {
                        "name": "Ayoushman Bhattacharya"
                    },
                    {
                        "name": "Nilanjan Chakraborty"
                    },
                    {
                        "name": "Robert Lunde"
                    }
                ],
                "author_detail": {
                    "name": "Robert Lunde"
                },
                "author": "Robert Lunde",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10514v2",
                "updated": "2025-09-01T17:09:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    17,
                    9,
                    40,
                    0,
                    244,
                    0
                ],
                "published": "2024-03-15T17:53:54Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    17,
                    53,
                    54,
                    4,
                    75,
                    0
                ],
                "title": "Multilevel functional distributional models with application to\n  continuous glucose monitoring in diabetes clinical trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilevel functional distributional models with application to\n  continuous glucose monitoring in diabetes clinical trials"
                },
                "summary": "Continuous glucose monitoring (CGM) is a minimally invasive technology that\nmeasures blood glucose every few minutes for weeks or months at a time. CGM\ndata are often collected in the free-living environment and is strongly related\nto sleep, physical activity and meal intake. As the timing of these activities\nvaries substantially within- and between-individuals, it is difficult to model\nCGM trajectories as a function of time of day. Therefore, in practice, CGM\ntrajectories are often reduced to one or two scalar summaries of the thousands\nof measurements collected for a study participant. To alleviate the potential\nloss of information, the cumulative distribution function (cdf) of the CGM time\nseries was proposed as an alternative. Here we address the problem of\nconducting inference on cdfs in clinical trials with long follow up and\nfrequent measurements. Our approach provides three major innovations: (1)\nmodeling the entire cdf and preserving its monotonicity; (2) accounting for the\ncdfs correlation (because they are measured on the same individual), continuity\n(results are robust to the choice of the probability grid), and differential\nerror (e.g., medians have lower variability than $0.99$ quantiles); and (3)\npreserving the family-wise error when the observed data are longitudinal\nsamples of cdfs. We focus on modeling data collected by The Juvenile Diabetes\nResearch Foundation Continuous Glucose Monitoring Group in a large clinical\ntrial that collected CGM data every few minutes for 26 weeks. Our basic\nobservation unit is the distribution of CGM observations in a four--week\ninterval. The scientific goals are to: (1) identify and quantify the effects of\nfactors that affect glycaemic control in type 1 diabetes patients (T1D); and\n(2) identify and characterize the patients who respond to treatment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous glucose monitoring (CGM) is a minimally invasive technology that\nmeasures blood glucose every few minutes for weeks or months at a time. CGM\ndata are often collected in the free-living environment and is strongly related\nto sleep, physical activity and meal intake. As the timing of these activities\nvaries substantially within- and between-individuals, it is difficult to model\nCGM trajectories as a function of time of day. Therefore, in practice, CGM\ntrajectories are often reduced to one or two scalar summaries of the thousands\nof measurements collected for a study participant. To alleviate the potential\nloss of information, the cumulative distribution function (cdf) of the CGM time\nseries was proposed as an alternative. Here we address the problem of\nconducting inference on cdfs in clinical trials with long follow up and\nfrequent measurements. Our approach provides three major innovations: (1)\nmodeling the entire cdf and preserving its monotonicity; (2) accounting for the\ncdfs correlation (because they are measured on the same individual), continuity\n(results are robust to the choice of the probability grid), and differential\nerror (e.g., medians have lower variability than $0.99$ quantiles); and (3)\npreserving the family-wise error when the observed data are longitudinal\nsamples of cdfs. We focus on modeling data collected by The Juvenile Diabetes\nResearch Foundation Continuous Glucose Monitoring Group in a large clinical\ntrial that collected CGM data every few minutes for 26 weeks. Our basic\nobservation unit is the distribution of CGM observations in a four--week\ninterval. The scientific goals are to: (1) identify and quantify the effects of\nfactors that affect glycaemic control in type 1 diabetes patients (T1D); and\n(2) identify and characterize the patients who respond to treatment."
                },
                "authors": [
                    {
                        "name": "Marcos Matabuena"
                    },
                    {
                        "name": "Ciprian M. Crainiceanu"
                    }
                ],
                "author_detail": {
                    "name": "Ciprian M. Crainiceanu"
                },
                "author": "Ciprian M. Crainiceanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21484v2",
                "updated": "2025-09-01T17:06:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    17,
                    6,
                    38,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-29T10:10:02Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    10,
                    2,
                    4,
                    241,
                    0
                ],
                "title": "Data-driven Discovery of Digital Twins in Biomedical Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Discovery of Digital Twins in Biomedical Research"
                },
                "summary": "Recent technological advances have expanded the availability of\nhigh-throughput biological datasets, enabling the reliable design of digital\ntwins of biomedical systems or patients. Such computational tools represent key\nreaction networks driving perturbation or drug response and can guide drug\ndiscovery and personalized therapeutics. Yet, their development still relies on\nlaborious data integration by the human modeler, so that automated approaches\nare critically needed. The success of data-driven system discovery in Physics,\nrooted in clean datasets and well-defined governing laws, has fueled interest\nin applying similar techniques in Biology, which presents unique challenges.\nHere, we reviewed methodologies for automatically inferring digital twins from\nbiological time series, which mostly involve symbolic or sparse regression. We\nevaluate algorithms according to eight biological and methodological\nchallenges, associated to noisy/incomplete data, multiple conditions, prior\nknowledge integration, latent variables, high dimensionality, unobserved\nvariable derivatives, candidate library design, and uncertainty quantification.\nUpon these criteria, sparse regression generally outperformed symbolic\nregression, particularly when using Bayesian frameworks. We further highlight\nthe emerging role of deep learning and large language models, which enable\ninnovative prior knowledge integration, though the reliability and consistency\nof such approaches must be improved. While no single method addresses all\nchallenges, we argue that progress in learning digital twins will come from\nhybrid and modular frameworks combining chemical reaction network-based\nmechanistic grounding, Bayesian uncertainty quantification, and the generative\nand knowledge integration capacities of deep learning. To support their\ndevelopment, we further propose a benchmarking framework to evaluate methods\nacross all challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technological advances have expanded the availability of\nhigh-throughput biological datasets, enabling the reliable design of digital\ntwins of biomedical systems or patients. Such computational tools represent key\nreaction networks driving perturbation or drug response and can guide drug\ndiscovery and personalized therapeutics. Yet, their development still relies on\nlaborious data integration by the human modeler, so that automated approaches\nare critically needed. The success of data-driven system discovery in Physics,\nrooted in clean datasets and well-defined governing laws, has fueled interest\nin applying similar techniques in Biology, which presents unique challenges.\nHere, we reviewed methodologies for automatically inferring digital twins from\nbiological time series, which mostly involve symbolic or sparse regression. We\nevaluate algorithms according to eight biological and methodological\nchallenges, associated to noisy/incomplete data, multiple conditions, prior\nknowledge integration, latent variables, high dimensionality, unobserved\nvariable derivatives, candidate library design, and uncertainty quantification.\nUpon these criteria, sparse regression generally outperformed symbolic\nregression, particularly when using Bayesian frameworks. We further highlight\nthe emerging role of deep learning and large language models, which enable\ninnovative prior knowledge integration, though the reliability and consistency\nof such approaches must be improved. While no single method addresses all\nchallenges, we argue that progress in learning digital twins will come from\nhybrid and modular frameworks combining chemical reaction network-based\nmechanistic grounding, Bayesian uncertainty quantification, and the generative\nand knowledge integration capacities of deep learning. To support their\ndevelopment, we further propose a benchmarking framework to evaluate methods\nacross all challenges."
                },
                "authors": [
                    {
                        "name": "Clémence Métayer"
                    },
                    {
                        "name": "Annabelle Ballesta"
                    },
                    {
                        "name": "Julien Martinelli"
                    }
                ],
                "author_detail": {
                    "name": "Julien Martinelli"
                },
                "author": "Julien Martinelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16150v2",
                "updated": "2025-09-01T16:37:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    16,
                    37,
                    58,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-22T07:19:33Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    19,
                    33,
                    4,
                    234,
                    0
                ],
                "title": "Evaluating the Defense Potential of Machine Unlearning against\n  Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Defense Potential of Machine Unlearning against\n  Membership Inference Attacks"
                },
                "summary": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems."
                },
                "authors": [
                    {
                        "name": "Aristeidis Sidiropoulos"
                    },
                    {
                        "name": "Christos Chrysanthos Nikolaidis"
                    },
                    {
                        "name": "Theodoros Tsiolakis"
                    },
                    {
                        "name": "Nikolaos Pavlidis"
                    },
                    {
                        "name": "Vasilis Perifanis"
                    },
                    {
                        "name": "Pavlos S. Efraimidis"
                    }
                ],
                "author_detail": {
                    "name": "Pavlos S. Efraimidis"
                },
                "author": "Pavlos S. Efraimidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03671v3",
                "updated": "2025-09-01T16:10:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    16,
                    10,
                    22,
                    0,
                    244,
                    0
                ],
                "published": "2024-09-05T16:24:42Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "title": "TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling"
                },
                "summary": "We present TRACE-CS, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs)to address contrastive queries in course\nscheduling problems. TRACE-CS leverages logic-based techniques to encode\nscheduling constraints and generate provably correct explanations, while\nutilizing an LLM to process natural language queries and refine logical\nexplanations into user friendly responses. This system showcases how combining\nsymbolic KR methods with LLMs creates explainable AI agents that balance\nlogical correctness with natural language accessibility, addressing a\nfundamental challenge in deployed scheduling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TRACE-CS, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs)to address contrastive queries in course\nscheduling problems. TRACE-CS leverages logic-based techniques to encode\nscheduling constraints and generate provably correct explanations, while\nutilizing an LLM to process natural language queries and refine logical\nexplanations into user friendly responses. This system showcases how combining\nsymbolic KR methods with LLMs creates explainable AI agents that balance\nlogical correctness with natural language accessibility, addressing a\nfundamental challenge in deployed scheduling systems."
                },
                "authors": [
                    {
                        "name": "Stylianos Loukas Vasileiou"
                    },
                    {
                        "name": "William Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "William Yeoh"
                },
                "author": "William Yeoh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17500v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17500v6",
                "updated": "2025-09-01T15:58:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    58,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2024-06-25T12:36:21Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    12,
                    36,
                    21,
                    1,
                    177,
                    0
                ],
                "title": "Using iterated local alignment to aggregate trajectory data into a\n  traffic flow map",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using iterated local alignment to aggregate trajectory data into a\n  traffic flow map"
                },
                "summary": "Vehicle trajectories are a promising GNSS (Global Navigation Satellite\nSystem) data source to compute multi-scale traffic flow maps ranging from the\ncity/regional level to the road level. The main obstacle is that trajectory\ndata are prone to measurement noise. While this is negligible for city level,\nlarge-scale flow aggregation, it poses substantial difficulties for road level,\nsmall-scale aggregation. To overcome these difficulties, we introduce\ninnovative local alignment algorithms, where we infer road segments to serve as\nlocal reference segments, and proceed to align nearby road segments to them. We\ndeploy these algorithms in an iterative workflow to compute locally aligned\nflow maps. By applying this workflow to synthetic and empirical trajectories,\nwe verify that our locally aligned flow maps provide high levels of accuracy\nand spatial resolution of flow aggregation at multiple scales for static and\ninteractive maps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle trajectories are a promising GNSS (Global Navigation Satellite\nSystem) data source to compute multi-scale traffic flow maps ranging from the\ncity/regional level to the road level. The main obstacle is that trajectory\ndata are prone to measurement noise. While this is negligible for city level,\nlarge-scale flow aggregation, it poses substantial difficulties for road level,\nsmall-scale aggregation. To overcome these difficulties, we introduce\ninnovative local alignment algorithms, where we infer road segments to serve as\nlocal reference segments, and proceed to align nearby road segments to them. We\ndeploy these algorithms in an iterative workflow to compute locally aligned\nflow maps. By applying this workflow to synthetic and empirical trajectories,\nwe verify that our locally aligned flow maps provide high levels of accuracy\nand spatial resolution of flow aggregation at multiple scales for static and\ninteractive maps."
                },
                "authors": [
                    {
                        "name": "Tarn Duong"
                    }
                ],
                "author_detail": {
                    "name": "Tarn Duong"
                },
                "author": "Tarn Duong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17500v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17500v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P30, 62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15695v2",
                "updated": "2025-09-01T15:48:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    48,
                    33,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-21T16:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    44,
                    2,
                    141,
                    0
                ],
                "title": "Can Large Language Models be Effective Online Opinion Miners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models be Effective Online Opinion Miners?"
                },
                "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield."
                },
                "authors": [
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "Yongsik Seo"
                    },
                    {
                        "name": "Junseong Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05308v2",
                "updated": "2025-09-01T15:46:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    46,
                    7,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-07T12:07:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    7,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "Identifying Optimal Regression Models For DEM Simulation Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Optimal Regression Models For DEM Simulation Datasets"
                },
                "summary": "Developing fast regression models (surrogate/metamodels) from DEM data is key\nfor practical industrial application to allow real-time evaluations. However,\nbenchmarking different models is often overlooked in particle technology for\nregression tasks, as model selection is frequently not the primary research\nfocus. This can lead to the use of suboptimal models, resulting in subpar\npredictive accuracy, slow evaluations, or poor generalisation, hindering\neffective real-time decision-making and process optimisation. In this work, we\ndiscuss applying k-fold cross-validation to assess regression models for\ntabular DEM datasets and propose a simple framework for readers to follow to\nfind the optimal model for their data. An example demonstrates its application\nto a DEM dataset of packing fractions measured in a simple measuring beaker\nwith varying inter-particle properties, namely, average particle diameter,\ncoefficient of restitution, coefficient of sliding friction, coefficient of\nrolling resistance, and cohesive energy density. Out of 16 different models\ntested, a histogram-based gradient boosting model was found to be optimal,\nproviding a good fit with acceptable training and inference times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing fast regression models (surrogate/metamodels) from DEM data is key\nfor practical industrial application to allow real-time evaluations. However,\nbenchmarking different models is often overlooked in particle technology for\nregression tasks, as model selection is frequently not the primary research\nfocus. This can lead to the use of suboptimal models, resulting in subpar\npredictive accuracy, slow evaluations, or poor generalisation, hindering\neffective real-time decision-making and process optimisation. In this work, we\ndiscuss applying k-fold cross-validation to assess regression models for\ntabular DEM datasets and propose a simple framework for readers to follow to\nfind the optimal model for their data. An example demonstrates its application\nto a DEM dataset of packing fractions measured in a simple measuring beaker\nwith varying inter-particle properties, namely, average particle diameter,\ncoefficient of restitution, coefficient of sliding friction, coefficient of\nrolling resistance, and cohesive energy density. Out of 16 different models\ntested, a histogram-based gradient boosting model was found to be optimal,\nproviding a good fit with acceptable training and inference times."
                },
                "authors": [
                    {
                        "name": "B. D. Jenkins"
                    },
                    {
                        "name": "A. L. Nicusan"
                    },
                    {
                        "name": "A. Neveu"
                    },
                    {
                        "name": "G. Lumay"
                    },
                    {
                        "name": "F. Francqui"
                    },
                    {
                        "name": "J. P. K. Seville"
                    },
                    {
                        "name": "C. R. K. Windows-Yule"
                    }
                ],
                "author_detail": {
                    "name": "C. R. K. Windows-Yule"
                },
                "author": "C. R. K. Windows-Yule",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14880v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14880v3",
                "updated": "2025-09-01T15:33:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    33,
                    47,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-20T17:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    51,
                    20,
                    2,
                    232,
                    0
                ],
                "title": "MedResearcher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedResearcher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework"
                },
                "summary": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts. We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions. Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts. We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions. Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains."
                },
                "authors": [
                    {
                        "name": "Ailing Yu"
                    },
                    {
                        "name": "Lan Yao"
                    },
                    {
                        "name": "Jingnan Liu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Jiajun Yin"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xinhao Liao"
                    },
                    {
                        "name": "Zhiling Ye"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Hansong Xiao"
                    },
                    {
                        "name": "Hualei Zhou"
                    },
                    {
                        "name": "Chunxiao Guo"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14880v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v4",
                "updated": "2025-09-01T15:29:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    29,
                    28,
                    0,
                    244,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17092v2",
                "updated": "2025-09-01T15:29:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    29,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2023-12-28T16:11:07Z",
                "published_parsed": [
                    2023,
                    12,
                    28,
                    16,
                    11,
                    7,
                    3,
                    362,
                    0
                ],
                "title": "Elliptic anisotropy measurement of the f$_0$(980) hadron in proton-lead\n  collisions and evidence for its quark-antiquark composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elliptic anisotropy measurement of the f$_0$(980) hadron in proton-lead\n  collisions and evidence for its quark-antiquark composition"
                },
                "summary": "Despite the f$_0$(980) hadron having been discovered half a century ago, the\nquestion about its quark content has not been settled: it might be an ordinary\nquark-antiquark ($\\mathrm{q\\bar{q}}$) meson, a tetraquark\n($\\mathrm{q\\bar{q}q\\bar{q}}$) exotic state, a kaon-antikaon\n($\\mathrm{K\\bar{K}}$) molecule, or a quark-antiquark-gluon\n($\\mathrm{q\\bar{q}g}$) hybrid. This paper reports strong evidence that the\nf$_0$(980) state is an ordinary $\\mathrm{q\\bar{q}}$ meson, inferred from the\nscaling of elliptic anisotropies ($v_2$) with the number of constituent quarks\n($n_\\mathrm{q}$), as empirically established using conventional hadrons in\nrelativistic heavy ion collisions. The f$_0$(980) state is reconstructed via\nits dominant decay channel f$_0$(980) $\\to$ $\\pi^+\\pi^-$, in proton-lead\ncollisions recorded by the CMS experiment at the LHC, and its $v_2$ is measured\nas a function of transverse momentum ($p_\\mathrm{T}$). It is found that the\n$n_q$ = 2 ($\\mathrm{q\\bar{q}}$ state) hypothesis is favored over $n_q$ = 4\n($\\mathrm{q\\bar{q}q\\bar{q}}$ or $\\mathrm{K\\bar{K}}$ states) by 7.7, 6.3, or 3.1\nstandard deviations in the $p_\\mathrm{T}$ $\\lt$ 10, 8, or 6 GeV/$c$ ranges,\nrespectively, and over $n_\\mathrm{q}$ = 3 ($\\mathrm{q\\bar{q}g}$ hybrid state)\nby 3.5 standard deviations in the $p_\\mathrm{T}$ $\\lt$ 8 GeV/$c$ range. This\nresult represents the first determination of the quark content of the\nf$_0$(980) state, made possible by using a novel approach, and paves the way\nfor similar studies of other exotic hadron candidates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the f$_0$(980) hadron having been discovered half a century ago, the\nquestion about its quark content has not been settled: it might be an ordinary\nquark-antiquark ($\\mathrm{q\\bar{q}}$) meson, a tetraquark\n($\\mathrm{q\\bar{q}q\\bar{q}}$) exotic state, a kaon-antikaon\n($\\mathrm{K\\bar{K}}$) molecule, or a quark-antiquark-gluon\n($\\mathrm{q\\bar{q}g}$) hybrid. This paper reports strong evidence that the\nf$_0$(980) state is an ordinary $\\mathrm{q\\bar{q}}$ meson, inferred from the\nscaling of elliptic anisotropies ($v_2$) with the number of constituent quarks\n($n_\\mathrm{q}$), as empirically established using conventional hadrons in\nrelativistic heavy ion collisions. The f$_0$(980) state is reconstructed via\nits dominant decay channel f$_0$(980) $\\to$ $\\pi^+\\pi^-$, in proton-lead\ncollisions recorded by the CMS experiment at the LHC, and its $v_2$ is measured\nas a function of transverse momentum ($p_\\mathrm{T}$). It is found that the\n$n_q$ = 2 ($\\mathrm{q\\bar{q}}$ state) hypothesis is favored over $n_q$ = 4\n($\\mathrm{q\\bar{q}q\\bar{q}}$ or $\\mathrm{K\\bar{K}}$ states) by 7.7, 6.3, or 3.1\nstandard deviations in the $p_\\mathrm{T}$ $\\lt$ 10, 8, or 6 GeV/$c$ ranges,\nrespectively, and over $n_\\mathrm{q}$ = 3 ($\\mathrm{q\\bar{q}g}$ hybrid state)\nby 3.5 standard deviations in the $p_\\mathrm{T}$ $\\lt$ 8 GeV/$c$ range. This\nresult represents the first determination of the quark content of the\nf$_0$(980) state, made possible by using a novel approach, and paves the way\nfor similar studies of other exotic hadron candidates."
                },
                "authors": [
                    {
                        "name": "CMS Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "CMS Collaboration"
                },
                "author": "CMS Collaboration",
                "arxiv_doi": "10.1038/s41467-025-56200-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41467-025-56200-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.17092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Replaced with the published version. Added the journal reference and\n  the DOI. All the figures and tables can be found at\n  http://cms-results.web.cern.ch/cms-results/public-results/publications/HIN-20-002\n  (CMS Public Pages)",
                "arxiv_journal_ref": "Nature Communications 16 (2025) 7990",
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02353v2",
                "updated": "2025-09-01T14:56:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    14,
                    56,
                    48,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-03T01:14:45Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    14,
                    45,
                    1,
                    154,
                    0
                ],
                "title": "SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for\n  Robot-Assisted Bite Acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for\n  Robot-Assisted Bite Acquisition"
                },
                "summary": "Robot-assisted feeding requires reliable bite acquisition, a challenging task\ndue to the complex interactions between utensils and food with diverse physical\nproperties. These interactions are further complicated by the temporal\nvariability of food properties-for example, steak becomes firm as it cools even\nduring a meal. To address this, we propose SAVOR, a novel approach for learning\nskill affordances for bite acquisition-how suitable a manipulation skill (e.g.,\nskewering, scooping) is for a given utensil-food interaction. In our\nformulation, skill affordances arise from the combination of tool affordances\n(what a utensil can do) and food affordances (what the food allows). Tool\naffordances are learned offline through calibration, where different utensils\ninteract with a variety of foods to model their functional capabilities. Food\naffordances are characterized by physical properties such as softness,\nmoisture, and viscosity, initially inferred through commonsense reasoning using\na visually-conditioned language model and then dynamically refined through\nonline multi-modal visuo-haptic perception using SAVOR-Net during interaction.\nOur method integrates these offline and online estimates to predict skill\naffordances in real time, enabling the robot to select the most appropriate\nskill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild\nmeals, our approach improves bite acquisition success rate by 13% over\nstate-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits).\nThese results highlight the importance of modeling interaction-driven skill\naffordances for generalizable and effective robot-assisted bite acquisition.\nWebsite: https://emprise.cs.cornell.edu/savor/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot-assisted feeding requires reliable bite acquisition, a challenging task\ndue to the complex interactions between utensils and food with diverse physical\nproperties. These interactions are further complicated by the temporal\nvariability of food properties-for example, steak becomes firm as it cools even\nduring a meal. To address this, we propose SAVOR, a novel approach for learning\nskill affordances for bite acquisition-how suitable a manipulation skill (e.g.,\nskewering, scooping) is for a given utensil-food interaction. In our\nformulation, skill affordances arise from the combination of tool affordances\n(what a utensil can do) and food affordances (what the food allows). Tool\naffordances are learned offline through calibration, where different utensils\ninteract with a variety of foods to model their functional capabilities. Food\naffordances are characterized by physical properties such as softness,\nmoisture, and viscosity, initially inferred through commonsense reasoning using\na visually-conditioned language model and then dynamically refined through\nonline multi-modal visuo-haptic perception using SAVOR-Net during interaction.\nOur method integrates these offline and online estimates to predict skill\naffordances in real time, enabling the robot to select the most appropriate\nskill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild\nmeals, our approach improves bite acquisition success rate by 13% over\nstate-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits).\nThese results highlight the importance of modeling interaction-driven skill\naffordances for generalizable and effective robot-assisted bite acquisition.\nWebsite: https://emprise.cs.cornell.edu/savor/"
                },
                "authors": [
                    {
                        "name": "Zhanxin Wu"
                    },
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Tom Silver"
                    },
                    {
                        "name": "Tapomayukh Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Tapomayukh Bhattacharjee"
                },
                "author": "Tapomayukh Bhattacharjee",
                "arxiv_comment": "Conference on Robot Learning, Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v6",
                "updated": "2025-09-01T14:14:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    14,
                    14,
                    18,
                    0,
                    244,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "arxiv_comment": "Accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11109v2",
                "updated": "2025-09-01T14:13:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    14,
                    13,
                    59,
                    0,
                    244,
                    0
                ],
                "published": "2025-04-15T11:56:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    56,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Agent-Q: Fine-Tuning Large Language Models for Quantum Circuit\n  Generation and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Q: Fine-Tuning Large Language Models for Quantum Circuit\n  Generation and Optimization"
                },
                "summary": "Large language models (LLMs) have achieved remarkable outcomes in complex\nproblems, including math, coding, and analyzing large amounts of scientific\nreports. Yet, few works have explored the potential of LLMs in quantum\ncomputing. The most challenging problem is to leverage LLMs to automatically\ngenerate quantum circuits at a large scale. Fundamentally, the existing\npre-trained LLMs lack the knowledge of quantum circuits. In this paper, we\naddress this challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. We describe Agent-Q, an LLM fine-tuning system\nto generate and optimize quantum circuits. In particular, Agent-Q implements\nthe mechanisms to generate training data sets and constructs an end-to-end\npipeline to fine-tune pre-trained LLMs to generate parameterized quantum\ncircuits for various optimization problems. Agent-Q provides 14,000 quantum\ncircuits covering a large spectrum of the quantum optimization landscape: 12\noptimization problem instances and their optimized QAOA, VQE, and adaptive VQE\ncircuits. Based thereon, Agent-Q fine-tunes LLMs and constructs syntactically\ncorrect parametrized quantum circuits in OpenQASM 3.0. We have evaluated the\nquality of the LLM-generated circuits and parameters by comparing them to the\noptimized expectation values and distributions. Experimental results show\nsuperior performance of Agent-Q, compared to several state-of-the-art LLMs and\nbetter parameters than random. Agent-Q can be integrated into an agentic\nworkflow, and the generated parametrized circuits with initial parameters can\nbe used as a starting point for further optimization, e.g., as templates in\nquantum machine learning and as benchmarks for compilers and hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable outcomes in complex\nproblems, including math, coding, and analyzing large amounts of scientific\nreports. Yet, few works have explored the potential of LLMs in quantum\ncomputing. The most challenging problem is to leverage LLMs to automatically\ngenerate quantum circuits at a large scale. Fundamentally, the existing\npre-trained LLMs lack the knowledge of quantum circuits. In this paper, we\naddress this challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. We describe Agent-Q, an LLM fine-tuning system\nto generate and optimize quantum circuits. In particular, Agent-Q implements\nthe mechanisms to generate training data sets and constructs an end-to-end\npipeline to fine-tune pre-trained LLMs to generate parameterized quantum\ncircuits for various optimization problems. Agent-Q provides 14,000 quantum\ncircuits covering a large spectrum of the quantum optimization landscape: 12\noptimization problem instances and their optimized QAOA, VQE, and adaptive VQE\ncircuits. Based thereon, Agent-Q fine-tunes LLMs and constructs syntactically\ncorrect parametrized quantum circuits in OpenQASM 3.0. We have evaluated the\nquality of the LLM-generated circuits and parameters by comparing them to the\noptimized expectation values and distributions. Experimental results show\nsuperior performance of Agent-Q, compared to several state-of-the-art LLMs and\nbetter parameters than random. Agent-Q can be integrated into an agentic\nworkflow, and the generated parametrized circuits with initial parameters can\nbe used as a starting point for further optimization, e.g., as templates in\nquantum machine learning and as benchmarks for compilers and hardware."
                },
                "authors": [
                    {
                        "name": "Linus Jern"
                    },
                    {
                        "name": "Valter Uotila"
                    },
                    {
                        "name": "Cong Yu"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "arxiv_comment": "12 pages, 8 figures, 3 tables, presented at IEEE International\n  Conference on Quantum Computing and Engineering (QCE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01378v2",
                "updated": "2025-09-01T13:53:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    53,
                    3,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-02T05:44:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms"
                },
                "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems."
                },
                "authors": [
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Sizhao Li"
                    },
                    {
                        "name": "Yuming Xiang"
                    },
                    {
                        "name": "Haiping Wang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10551v3",
                "updated": "2025-09-01T13:47:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    47,
                    40,
                    0,
                    244,
                    0
                ],
                "published": "2025-01-17T20:54:09Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    20,
                    54,
                    9,
                    4,
                    17,
                    0
                ],
                "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays"
                },
                "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom."
                },
                "authors": [
                    {
                        "name": "Andrew Jelson"
                    },
                    {
                        "name": "Daniel Manesh"
                    },
                    {
                        "name": "Alice Jang"
                    },
                    {
                        "name": "Daniel Dunlap"
                    },
                    {
                        "name": "Young-Ho Kim"
                    },
                    {
                        "name": "Sang Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Lee"
                },
                "author": "Sang Won Lee",
                "arxiv_comment": "19 pages, 10 figures, 2 tables, final preparation for a TOCHI\n  submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16064v2",
                "updated": "2025-09-01T13:33:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    33,
                    39,
                    0,
                    244,
                    0
                ],
                "published": "2025-04-22T17:41:42Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    41,
                    42,
                    1,
                    112,
                    0
                ],
                "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis"
                },
                "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling. Project page and\ncode: https://representationdiffusion.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling. Project page and\ncode: https://representationdiffusion.github.io"
                },
                "authors": [
                    {
                        "name": "Theodoros Kouzelis"
                    },
                    {
                        "name": "Efstathios Karypidis"
                    },
                    {
                        "name": "Ioannis Kakogeorgiou"
                    },
                    {
                        "name": "Spyros Gidaris"
                    },
                    {
                        "name": "Nikos Komodakis"
                    }
                ],
                "author_detail": {
                    "name": "Nikos Komodakis"
                },
                "author": "Nikos Komodakis",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19525v2",
                "updated": "2025-09-01T13:24:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    24,
                    24,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-27T02:40:50Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    2,
                    40,
                    50,
                    2,
                    239,
                    0
                ],
                "title": "Breaking the Layer Barrier: Remodeling Private Transformer Inference\n  with Hybrid CKKS and MPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Layer Barrier: Remodeling Private Transformer Inference\n  with Hybrid CKKS and MPC"
                },
                "summary": "This paper presents an efficient framework for private Transformer inference\nthat combines Homomorphic Encryption (HE) and Secure Multi-party Computation\n(MPC) to protect data privacy. Existing methods often leverage HE for linear\nlayers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,\nSoftmax activation functions), but the conversion between HE and MPC introduces\nsignificant communication costs. The proposed framework, dubbed BLB, overcomes\nthis by breaking down layers into fine-grained operators and further fusing\nadjacent linear operators, reducing the need for HE/MPC conversions. To manage\nthe increased ciphertext bit width from the fused linear operators, BLB\nproposes the first secure conversion protocol between CKKS and MPC and enables\nCKKS-based computation of the fused operators. Additionally, BLB proposes an\nefficient matrix multiplication protocol for fused computation in Transformers.\nExtensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB\nachieves a $21\\times$ reduction in communication overhead compared to BOLT\n(S\\&P'24) and a $2\\times$ reduction compared to Bumblebee (NDSS'25), along with\nlatency reductions of $13\\times$ and $1.8\\times$, respectively, when leveraging\nGPU acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an efficient framework for private Transformer inference\nthat combines Homomorphic Encryption (HE) and Secure Multi-party Computation\n(MPC) to protect data privacy. Existing methods often leverage HE for linear\nlayers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,\nSoftmax activation functions), but the conversion between HE and MPC introduces\nsignificant communication costs. The proposed framework, dubbed BLB, overcomes\nthis by breaking down layers into fine-grained operators and further fusing\nadjacent linear operators, reducing the need for HE/MPC conversions. To manage\nthe increased ciphertext bit width from the fused linear operators, BLB\nproposes the first secure conversion protocol between CKKS and MPC and enables\nCKKS-based computation of the fused operators. Additionally, BLB proposes an\nefficient matrix multiplication protocol for fused computation in Transformers.\nExtensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB\nachieves a $21\\times$ reduction in communication overhead compared to BOLT\n(S\\&P'24) and a $2\\times$ reduction compared to Bumblebee (NDSS'25), along with\nlatency reductions of $13\\times$ and $1.8\\times$, respectively, when leveraging\nGPU acceleration."
                },
                "authors": [
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Wen-jie Lu"
                    },
                    {
                        "name": "Jiangrui Yu"
                    },
                    {
                        "name": "Chen Yi"
                    },
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10722v2",
                "updated": "2025-09-01T13:06:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    6,
                    47,
                    0,
                    244,
                    0
                ],
                "published": "2024-08-20T10:44:29Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    44,
                    29,
                    1,
                    233,
                    0
                ],
                "title": "MEGen: Generative Backdoor into Large Language Models via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEGen: Generative Backdoor into Large Language Models via Model Editing"
                },
                "summary": "Large language models (LLMs) have exhibited remarkable versatility and\nadaptability, while their widespread adoption across various applications also\nraises critical safety concerns. This paper focuses on the impact of backdoored\nLLMs. Traditional backdoor injection methods are primarily limited to yes-or-no\ndiscriminative tasks, leading users to underestimate the potential risks of\nbackdoored LLMs. Given the inherently generative nature of LLMs, this paper\nreveals that a generative backdoor injected into LLMs can expose the true\nsafety risks in their applications. We propose an editing-based generative\nbackdoor, named MEGen, aiming to expand the backdoor to generative tasks in a\nunified format of any text-to any text, leading to natural generations with a\nspecific intention. Experiments show that MEGen achieves a high attack success\nrate by adjusting only a small set of local parameters with few-shot samples.\nNotably, we show that the backdoored model, when triggered, can freely output\npre-set dangerous information while completing downstream tasks. Our work\nhighlights that MEGen enables backdoors in LLMs to exhibit generative\ncapabilities, causing potential safety risks by altering the generative style.\nThe code is available at https://github.com/MonoQ-hub/MEGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited remarkable versatility and\nadaptability, while their widespread adoption across various applications also\nraises critical safety concerns. This paper focuses on the impact of backdoored\nLLMs. Traditional backdoor injection methods are primarily limited to yes-or-no\ndiscriminative tasks, leading users to underestimate the potential risks of\nbackdoored LLMs. Given the inherently generative nature of LLMs, this paper\nreveals that a generative backdoor injected into LLMs can expose the true\nsafety risks in their applications. We propose an editing-based generative\nbackdoor, named MEGen, aiming to expand the backdoor to generative tasks in a\nunified format of any text-to any text, leading to natural generations with a\nspecific intention. Experiments show that MEGen achieves a high attack success\nrate by adjusting only a small set of local parameters with few-shot samples.\nNotably, we show that the backdoored model, when triggered, can freely output\npre-set dangerous information while completing downstream tasks. Our work\nhighlights that MEGen enables backdoors in LLMs to exhibit generative\ncapabilities, causing potential safety risks by altering the generative style.\nThe code is available at https://github.com/MonoQ-hub/MEGen."
                },
                "authors": [
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qianren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianren Wang"
                },
                "author": "Qianren Wang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07851v2",
                "updated": "2025-09-01T12:48:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    48,
                    19,
                    0,
                    244,
                    0
                ],
                "published": "2024-04-11T15:47:10Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    15,
                    47,
                    10,
                    3,
                    102,
                    0
                ],
                "title": "Guiding Large Language Models to Post-Edit Machine Translation with\n  Error Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Large Language Models to Post-Edit Machine Translation with\n  Error Annotations"
                },
                "summary": "Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "NAACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16682v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16682v3",
                "updated": "2025-09-01T12:36:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    36,
                    48,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-23T18:56:56Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    18,
                    56,
                    56,
                    6,
                    54,
                    0
                ],
                "title": "Automatic Input Rewriting Improves Translation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Input Rewriting Improves Translation with Large Language\n  Models"
                },
                "summary": "Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16682v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16682v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24671v2",
                "updated": "2025-09-01T12:34:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    34,
                    28,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-30T15:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    1,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple LLM Agents Debate for Equitable Cultural Alignment"
                },
                "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters)."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Rachel Rudinger"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "ACL 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08793v2",
                "updated": "2025-09-01T12:33:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    33,
                    47,
                    0,
                    244,
                    0
                ],
                "published": "2024-12-11T21:53:19Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    21,
                    53,
                    19,
                    2,
                    346,
                    0
                ],
                "title": "Joint species distribution modeling of abundance data through latent\n  variable barcodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint species distribution modeling of abundance data through latent\n  variable barcodes"
                },
                "summary": "Accelerating global biodiversity loss has highlighted the role of complex\nrelationships and shared patterns among species in determining their responses\nto environmental changes. The structure of an ecological community, represented\nby patterns of dependence among constituent species, signals its robustness\nmore than individual species distributions. We focus on obtaining\ncommunity-level insights based on underlying patterns in abundances of bird\nspecies in Finland. We propose \\texttt{barcode}, a modeling framework to infer\nlatent binary and continuous features of samples and species, expanding the\nclass of concurrent ordinations. This approach introduces covariates and\nspatial autocorrelation hierarchically to facilitate ecological interpretations\nof the learned features. By analyzing 132 bird species counts, we infer the\ndominant environmental drivers of the community, species clusters and regions\nof common profile. Three of the learned drivers correspond to distinct\nclimactic regions with different dominant forest types. Three further drivers\nare spatially heterogeneous and signal urban, agricultural, and wetland areas,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating global biodiversity loss has highlighted the role of complex\nrelationships and shared patterns among species in determining their responses\nto environmental changes. The structure of an ecological community, represented\nby patterns of dependence among constituent species, signals its robustness\nmore than individual species distributions. We focus on obtaining\ncommunity-level insights based on underlying patterns in abundances of bird\nspecies in Finland. We propose \\texttt{barcode}, a modeling framework to infer\nlatent binary and continuous features of samples and species, expanding the\nclass of concurrent ordinations. This approach introduces covariates and\nspatial autocorrelation hierarchically to facilitate ecological interpretations\nof the learned features. By analyzing 132 bird species counts, we infer the\ndominant environmental drivers of the community, species clusters and regions\nof common profile. Three of the learned drivers correspond to distinct\nclimactic regions with different dominant forest types. Three further drivers\nare spatially heterogeneous and signal urban, agricultural, and wetland areas,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Braden Scherting"
                    },
                    {
                        "name": "Otso Ovaskainen"
                    },
                    {
                        "name": "David B. Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David B. Dunson"
                },
                "author": "David B. Dunson",
                "arxiv_comment": "44 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24683v2",
                "updated": "2025-09-01T12:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-30T15:08:10Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    10,
                    4,
                    150,
                    0
                ],
                "title": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation"
                },
                "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using\n(1) error highlights and (2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through (3) backtranslation and (4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using\n(1) error highlights and (2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through (3) backtranslation and (4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Kevin Duh"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "EMNLP 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02397v2",
                "updated": "2025-09-01T12:27:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    27,
                    7,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-03T03:31:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    31,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation"
                },
                "summary": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1."
                },
                "authors": [
                    {
                        "name": "Shengjia Zhang"
                    },
                    {
                        "name": "Junjie Wu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13430v2",
                "updated": "2025-09-01T11:50:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    50,
                    29,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-19T17:55:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization"
                },
                "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a simple yet effective approach that perturbs\nthe continuous quantization scale for gradient estimation and uses a\ndirectional derivative clipping method to stabilize training. QZO is orthogonal\nto both scalar-based and codebook-based post-training quantization methods.\nCompared to full-parameter fine-tuning in 16 bits, QZO can reduce the total\nmemory cost by more than 18$\\times$ for 4-bit LLMs, and enables fine-tuning\nLlama-2-13B within a single 24GB GPU. Code will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a simple yet effective approach that perturbs\nthe continuous quantization scale for gradient estimation and uses a\ndirectional derivative clipping method to stabilize training. QZO is orthogonal\nto both scalar-based and codebook-based post-training quantization methods.\nCompared to full-parameter fine-tuning in 16 bits, QZO can reduce the total\nmemory cost by more than 18$\\times$ for 4-bit LLMs, and enables fine-tuning\nLlama-2-13B within a single 24GB GPU. Code will be released publicly."
                },
                "authors": [
                    {
                        "name": "Sifeng Shang"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Chenyu Lin"
                    },
                    {
                        "name": "Minxian Li"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02608v2",
                "updated": "2025-09-01T11:38:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    38,
                    29,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-03T13:32:50Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    32,
                    50,
                    3,
                    184,
                    0
                ],
                "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for\n  Physics Emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for\n  Physics Emulation"
                },
                "summary": "The steep computational cost of diffusion models at inference hinders their\nuse as fast physics emulators. In the context of image and video generation,\nthis computational drawback has been addressed by generating in the latent\nspace of an autoencoder instead of the pixel space. In this work, we\ninvestigate whether a similar strategy can be effectively applied to the\nemulation of dynamical systems and at what cost. We find that the accuracy of\nlatent-space emulation is surprisingly robust to a wide range of compression\nrates (up to 1000x). We also show that diffusion-based emulators are\nconsistently more accurate than non-generative counterparts and compensate for\nuncertainty in their predictions with greater diversity. Finally, we cover\npractical design choices, spanning from architectures to optimizers, that we\nfound critical to train latent-space emulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The steep computational cost of diffusion models at inference hinders their\nuse as fast physics emulators. In the context of image and video generation,\nthis computational drawback has been addressed by generating in the latent\nspace of an autoencoder instead of the pixel space. In this work, we\ninvestigate whether a similar strategy can be effectively applied to the\nemulation of dynamical systems and at what cost. We find that the accuracy of\nlatent-space emulation is surprisingly robust to a wide range of compression\nrates (up to 1000x). We also show that diffusion-based emulators are\nconsistently more accurate than non-generative counterparts and compensate for\nuncertainty in their predictions with greater diversity. Finally, we cover\npractical design choices, spanning from architectures to optimizers, that we\nfound critical to train latent-space emulators."
                },
                "authors": [
                    {
                        "name": "François Rozet"
                    },
                    {
                        "name": "Ruben Ohana"
                    },
                    {
                        "name": "Michael McCabe"
                    },
                    {
                        "name": "Gilles Louppe"
                    },
                    {
                        "name": "François Lanusse"
                    },
                    {
                        "name": "Shirley Ho"
                    }
                ],
                "author_detail": {
                    "name": "Shirley Ho"
                },
                "author": "Shirley Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16988v2",
                "updated": "2025-09-01T11:15:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    15,
                    6,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-20T13:37:03Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    37,
                    3,
                    4,
                    171,
                    0
                ],
                "title": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering"
                },
                "summary": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG)\nframework for attributed question answering (QA) with large language models\n(LLMs). With the goal of trustworthy answer generation, RAGentA focuses on\noptimizing answer correctness, defined by coverage and relevance to the\nquestion and faithfulness, which measures the extent to which answers are\ngrounded in retrieved documents. RAGentA uses a multi-agent architecture that\niteratively filters retrieved documents, generates attributed answers with\nin-line citations, and verifies completeness through dynamic refinement.\nCentral to the framework is a hybrid retrieval strategy that combines sparse\nand dense methods, improving Recall@20 by 12.5% compared to the best single\nretrieval model, resulting in more correct and well-supported answers.\nEvaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA\noutperforms standard RAG baselines, achieving gains of 1.09% in correctness and\n10.72% in faithfulness. These results demonstrate the effectiveness of our\nmulti-agent RAG architecture and hybrid retrieval strategy in advancing\ntrustworthy QA with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG)\nframework for attributed question answering (QA) with large language models\n(LLMs). With the goal of trustworthy answer generation, RAGentA focuses on\noptimizing answer correctness, defined by coverage and relevance to the\nquestion and faithfulness, which measures the extent to which answers are\ngrounded in retrieved documents. RAGentA uses a multi-agent architecture that\niteratively filters retrieved documents, generates attributed answers with\nin-line citations, and verifies completeness through dynamic refinement.\nCentral to the framework is a hybrid retrieval strategy that combines sparse\nand dense methods, improving Recall@20 by 12.5% compared to the best single\nretrieval model, resulting in more correct and well-supported answers.\nEvaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA\noutperforms standard RAG baselines, achieving gains of 1.09% in correctness and\n10.72% in faithfulness. These results demonstrate the effectiveness of our\nmulti-agent RAG architecture and hybrid retrieval strategy in advancing\ntrustworthy QA with LLMs."
                },
                "authors": [
                    {
                        "name": "Ines Besrour"
                    },
                    {
                        "name": "Jingbo He"
                    },
                    {
                        "name": "Tobias Schreieder"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber",
                "arxiv_comment": "Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02037v2",
                "updated": "2025-09-01T10:49:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    49,
                    18,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-31T03:50:19Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    3,
                    50,
                    19,
                    5,
                    151,
                    0
                ],
                "title": "FinS-Pilot: A Benchmark for Online Financial RAG System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinS-Pilot: A Benchmark for Online Financial RAG System"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. In the financial field, the stringent demands\nfor professional accuracy and real-time data processing often necessitate the\nuse of retrieval-augmented generation (RAG) techniques. However, the\ndevelopment of financial RAG benchmarks has been constrained by data\nconfidentiality issues and the lack of dynamic data integration. To address\nthis issue, we introduce FinS-Pilot, a novel benchmark for evaluating RAG\nsystems in online financial applications. Constructed from real-world financial\nassistant interactions, our benchmark incorporates both real-time API data and\ntext data, organized through an intent classification framework covering\ncritical financial domains. The benchmark enables comprehensive evaluation of\nfinancial assistants' capabilities in handling both static knowledge and\ntime-sensitive market information.Through systematic experiments with multiple\nChinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying\nmodels suitable for financial applications while addressing the current gap in\nspecialized evaluation tools for the financial domain. Our work contributes\nboth a practical evaluation framework and a curated dataset to advance research\nin financial NLP systems. The code and dataset are accessible on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. In the financial field, the stringent demands\nfor professional accuracy and real-time data processing often necessitate the\nuse of retrieval-augmented generation (RAG) techniques. However, the\ndevelopment of financial RAG benchmarks has been constrained by data\nconfidentiality issues and the lack of dynamic data integration. To address\nthis issue, we introduce FinS-Pilot, a novel benchmark for evaluating RAG\nsystems in online financial applications. Constructed from real-world financial\nassistant interactions, our benchmark incorporates both real-time API data and\ntext data, organized through an intent classification framework covering\ncritical financial domains. The benchmark enables comprehensive evaluation of\nfinancial assistants' capabilities in handling both static knowledge and\ntime-sensitive market information.Through systematic experiments with multiple\nChinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying\nmodels suitable for financial applications while addressing the current gap in\nspecialized evaluation tools for the financial domain. Our work contributes\nboth a practical evaluation framework and a curated dataset to advance research\nin financial NLP systems. The code and dataset are accessible on GitHub."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yiding Sun"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Danqing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Danqing Xu"
                },
                "author": "Danqing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21748v2",
                "updated": "2025-09-01T10:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    5,
                    21,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-29T16:20:50Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    20,
                    50,
                    4,
                    241,
                    0
                ],
                "title": "A direct black hole mass measurement in a Little Red Dot at the Epoch of\n  Reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A direct black hole mass measurement in a Little Red Dot at the Epoch of\n  Reionization"
                },
                "summary": "Recent discoveries of faint active galactic nuclei (AGN) at the redshift\nfrontier have revealed a plethora of broad \\Halpha emitters with optically red\ncontinua, named Little Red Dots (LRDs), which comprise 15-30\\% of the high\nredshift broad line AGN population. Due to their peculiar spectral properties\nand X-ray weakness, modeling LRDs with standard AGN templates has proven\nchallenging. In particular, the validity of single-epoch virial mass estimates\nin determining the black hole (BH) masses of LRDs has been called into\nquestion, with some models claiming that masses might be overestimated by up to\n2 orders of magnitude, and other models claiming that LRDs may be entirely\nstellar in nature. We report the direct, dynamical BH mass measurement in a\nstrongly lensed LRD at $z = 7.04$. The combination of lensing with deep\nspectroscopic data reveals a rotation curve that is inconsistent with a nuclear\nstar cluster, yet can be well explained by Keplerian rotation around a point\nmass of 50 million Solar masses, consistent with virial BH mass estimates from\nthe Balmer lines. The Keplerian rotation leaves little room for any stellar\ncomponent in a host galaxy, as we conservatively infer $M_{\\rm BH}/M_{*}>2$.\nSuch a ''naked'' black hole, together with its near-pristine environment,\nindicates that this LRD is a massive black hole seed caught in its earliest\naccretion phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent discoveries of faint active galactic nuclei (AGN) at the redshift\nfrontier have revealed a plethora of broad \\Halpha emitters with optically red\ncontinua, named Little Red Dots (LRDs), which comprise 15-30\\% of the high\nredshift broad line AGN population. Due to their peculiar spectral properties\nand X-ray weakness, modeling LRDs with standard AGN templates has proven\nchallenging. In particular, the validity of single-epoch virial mass estimates\nin determining the black hole (BH) masses of LRDs has been called into\nquestion, with some models claiming that masses might be overestimated by up to\n2 orders of magnitude, and other models claiming that LRDs may be entirely\nstellar in nature. We report the direct, dynamical BH mass measurement in a\nstrongly lensed LRD at $z = 7.04$. The combination of lensing with deep\nspectroscopic data reveals a rotation curve that is inconsistent with a nuclear\nstar cluster, yet can be well explained by Keplerian rotation around a point\nmass of 50 million Solar masses, consistent with virial BH mass estimates from\nthe Balmer lines. The Keplerian rotation leaves little room for any stellar\ncomponent in a host galaxy, as we conservatively infer $M_{\\rm BH}/M_{*}>2$.\nSuch a ''naked'' black hole, together with its near-pristine environment,\nindicates that this LRD is a massive black hole seed caught in its earliest\naccretion phase."
                },
                "authors": [
                    {
                        "name": "Ignas Juodžbalis"
                    },
                    {
                        "name": "Cosimo Marconcini"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Alessandro Marconi"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Xihan Ji"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Jake S. Bennett"
                    },
                    {
                        "name": "Volker Bromm"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Stéphane Charlot"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Pratika Dayal"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Andrew Fabian"
                    },
                    {
                        "name": "Kohei Inayoshi"
                    },
                    {
                        "name": "Yuki Isobe"
                    },
                    {
                        "name": "Lucy Ivey"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Sophie Koudmani"
                    },
                    {
                        "name": "Nicolas Laporte"
                    },
                    {
                        "name": "Boyuan Liu"
                    },
                    {
                        "name": "Jianwei Lyu"
                    },
                    {
                        "name": "Giovanni Mazzolari"
                    },
                    {
                        "name": "Stephanie Monty"
                    },
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Pablo G. Pérez-González"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Raffaella Schneider"
                    },
                    {
                        "name": "Debora Sijacki"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Alessandro Trinca"
                    },
                    {
                        "name": "Rosa Valiante"
                    },
                    {
                        "name": "Marta Volonteri"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Saiyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Saiyang Zhang"
                },
                "author": "Saiyang Zhang",
                "arxiv_comment": "18 pages, 11 figures. Submitted. Typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14086v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14086v3",
                "updated": "2025-09-01T10:03:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    3,
                    38,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-13T14:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    14,
                    40,
                    52,
                    2,
                    225,
                    0
                ],
                "title": "EEGDM: EEG Representation Learning via Generative Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEGDM: EEG Representation Learning via Generative Diffusion Model"
                },
                "summary": "While electroencephalogram (EEG) has been a crucial tool for monitoring the\nbrain and diagnosing neurological disorders (e.g., epilepsy), learning\nmeaningful representations from raw EEG signals remains challenging due to\nlimited annotations and high signal variability. Recently, EEG foundation\nmodels (FMs) have shown promising potential by adopting transformer\narchitectures and self-supervised pre-training methods from large language\nmodels (e.g., masked prediction) to learn representations from diverse EEG\ndata, followed by fine-tuning on specific EEG tasks. Nonetheless, these large\nmodels often incurred high computational costs during both training and\ninference, with only marginal performance improvements as the model size\nincreases. In this work, we proposed an EEG representation learning framework\nbuilding upon Generative Diffusion Model (EEGDM). Specifically, we developed a\nstructured state-space model for diffusion pretraining (SSMDP) to better\ncapture the temporal dynamics of EEG signals and trained it using Denoising\nDiffusion Probabilistic Model (DDPM) framework. Subsequently, the resulting\nlatent EEG representations were then used for downstream classification tasks\nvia our proposed latent fusion transformer (LFT). To evaluate our method, we\nused multi-event datasets covering both interictal epileptiform discharges\n(TUEV) and seizure (CHB-MIT) detection, and compared EEGDM with current\nstate-of-the-art approaches, including EEG FMs. Empirical results showed that\nour method outperformed the existing methods. These findings suggested that\nEEGDM offered a promising alternative to current FMs. Our source code and\ncheckpoint are available at: https://github.com/jhpuah/EEGDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While electroencephalogram (EEG) has been a crucial tool for monitoring the\nbrain and diagnosing neurological disorders (e.g., epilepsy), learning\nmeaningful representations from raw EEG signals remains challenging due to\nlimited annotations and high signal variability. Recently, EEG foundation\nmodels (FMs) have shown promising potential by adopting transformer\narchitectures and self-supervised pre-training methods from large language\nmodels (e.g., masked prediction) to learn representations from diverse EEG\ndata, followed by fine-tuning on specific EEG tasks. Nonetheless, these large\nmodels often incurred high computational costs during both training and\ninference, with only marginal performance improvements as the model size\nincreases. In this work, we proposed an EEG representation learning framework\nbuilding upon Generative Diffusion Model (EEGDM). Specifically, we developed a\nstructured state-space model for diffusion pretraining (SSMDP) to better\ncapture the temporal dynamics of EEG signals and trained it using Denoising\nDiffusion Probabilistic Model (DDPM) framework. Subsequently, the resulting\nlatent EEG representations were then used for downstream classification tasks\nvia our proposed latent fusion transformer (LFT). To evaluate our method, we\nused multi-event datasets covering both interictal epileptiform discharges\n(TUEV) and seizure (CHB-MIT) detection, and compared EEGDM with current\nstate-of-the-art approaches, including EEG FMs. Empirical results showed that\nour method outperformed the existing methods. These findings suggested that\nEEGDM offered a promising alternative to current FMs. Our source code and\ncheckpoint are available at: https://github.com/jhpuah/EEGDM."
                },
                "authors": [
                    {
                        "name": "Jia Hong Puah"
                    },
                    {
                        "name": "Sim Kuan Goh"
                    },
                    {
                        "name": "Ziwei Zhang"
                    },
                    {
                        "name": "Zixuan Ye"
                    },
                    {
                        "name": "Chow Khuen Chan"
                    },
                    {
                        "name": "Kheng Seang Lim"
                    },
                    {
                        "name": "Si Lei Fong"
                    },
                    {
                        "name": "Kok Sin Woon"
                    },
                    {
                        "name": "Cuntai Guan"
                    }
                ],
                "author_detail": {
                    "name": "Cuntai Guan"
                },
                "author": "Cuntai Guan",
                "arxiv_comment": "EEGDM Preprint 10 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14086v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14086v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01233v2",
                "updated": "2025-09-01T10:02:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    2,
                    53,
                    0,
                    244,
                    0
                ],
                "published": "2024-01-02T14:58:59Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    14,
                    58,
                    59,
                    1,
                    2,
                    0
                ],
                "title": "GEN: A Practical Alternative to Graph Transformers for Long-Range Graph\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN: A Practical Alternative to Graph Transformers for Long-Range Graph\n  Modeling"
                },
                "summary": "Message Passing Neural Networks (MPNNs) model local relations effectively but\nstruggle to propagate information over long distances. Graph Transformers (GTs)\nmitigate this via global self-attention, yet their quadratic cost in the number\nof nodes limits scalability. We propose Graph Elimination Networks (GENs), an\nMPNN variant that approximates GT-like long-range modeling while maintaining\nhigh efficiency. GENs combine edge-wise and hop-wise self-attention in\nparallel; their multiplicative composition yields an attention kernel separable\nacross edge and hop factors within a bounded K-hop receptive field. To enable\nhop-wise attention, we introduce the Graph Elimination Algorithm (GEA), which\nprevents double counting across hops, ensuring that each round injects the\nk-hop incremental contribution exactly once. Taking differences between\nsuccessive rounds recovers the k-hop increment and yields disentangled\nmulti-hop features as inputs for hop-wise attention. This preserves clearer\nstructural distinctions across hop distances and enables more faithful modeling\nof pairwise dependencies between distant nodes within the K-hop neighborhood.\nOn the Long-Range Graph Benchmark (LRGB), GENs outperform strong MPNN baselines\nby 7.7 and 6.0 percentage points (pp) on PascalVOC-SP and COCO-SP, and achieve\nperformance on par with or better than state-of-the-art Graph Transformers. On\nOGBN-Products, GENs support full-batch training/inference, while\nsparse-attention baselines like Exphormer struggle with memory limits under\ncomparable budgets, highlighting GENs as a practical alternative for large,\nsparse graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Neural Networks (MPNNs) model local relations effectively but\nstruggle to propagate information over long distances. Graph Transformers (GTs)\nmitigate this via global self-attention, yet their quadratic cost in the number\nof nodes limits scalability. We propose Graph Elimination Networks (GENs), an\nMPNN variant that approximates GT-like long-range modeling while maintaining\nhigh efficiency. GENs combine edge-wise and hop-wise self-attention in\nparallel; their multiplicative composition yields an attention kernel separable\nacross edge and hop factors within a bounded K-hop receptive field. To enable\nhop-wise attention, we introduce the Graph Elimination Algorithm (GEA), which\nprevents double counting across hops, ensuring that each round injects the\nk-hop incremental contribution exactly once. Taking differences between\nsuccessive rounds recovers the k-hop increment and yields disentangled\nmulti-hop features as inputs for hop-wise attention. This preserves clearer\nstructural distinctions across hop distances and enables more faithful modeling\nof pairwise dependencies between distant nodes within the K-hop neighborhood.\nOn the Long-Range Graph Benchmark (LRGB), GENs outperform strong MPNN baselines\nby 7.7 and 6.0 percentage points (pp) on PascalVOC-SP and COCO-SP, and achieve\nperformance on par with or better than state-of-the-art Graph Transformers. On\nOGBN-Products, GENs support full-batch training/inference, while\nsparse-attention baselines like Exphormer struggle with memory limits under\ncomparable budgets, highlighting GENs as a practical alternative for large,\nsparse graphs."
                },
                "authors": [
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ge Cheng"
                    },
                    {
                        "name": "Yun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Zhang"
                },
                "author": "Yun Zhang",
                "arxiv_comment": "Includes 11 pages of main text and 6 pages of appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10491v2",
                "updated": "2025-09-01T09:21:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    9,
                    21,
                    41,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-12T08:47:40Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    8,
                    47,
                    40,
                    3,
                    163,
                    0
                ],
                "title": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language\n  Models"
                },
                "summary": "Modern language models are trained on large amounts of data. These data\ninevitably include controversial and stereotypical content, which contains all\nsorts of biases related to gender, origin, age, etc. As a result, the models\nexpress biased points of view or produce different results based on the\nassigned personality or the personality of the user. In this paper, we\ninvestigate various proxy measures of bias in large language models (LLMs). We\nfind that evaluating models with pre-prompted personae on a multi-subject\nbenchmark (MMLU) leads to negligible and mostly random differences in scores.\nHowever, if we reformulate the task and ask a model to grade the user's answer,\nthis shows more significant signs of bias. Finally, if we ask the model for\nsalary negotiation advice, we see pronounced bias in the answers. With the\nrecent trend for LLM assistant memory and personalization, these problems open\nup from a different angle: modern LLM users do not need to pre-prompt the\ndescription of their persona since the model already knows their\nsocio-demographics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern language models are trained on large amounts of data. These data\ninevitably include controversial and stereotypical content, which contains all\nsorts of biases related to gender, origin, age, etc. As a result, the models\nexpress biased points of view or produce different results based on the\nassigned personality or the personality of the user. In this paper, we\ninvestigate various proxy measures of bias in large language models (LLMs). We\nfind that evaluating models with pre-prompted personae on a multi-subject\nbenchmark (MMLU) leads to negligible and mostly random differences in scores.\nHowever, if we reformulate the task and ask a model to grade the user's answer,\nthis shows more significant signs of bias. Finally, if we ask the model for\nsalary negotiation advice, we see pronounced bias in the answers. With the\nrecent trend for LLM assistant memory and personalization, these problems open\nup from a different angle: modern LLM users do not need to pre-prompt the\ndescription of their persona since the model already knows their\nsocio-demographics."
                },
                "authors": [
                    {
                        "name": "Aleksandra Sorokovikova"
                    },
                    {
                        "name": "Pavel Chizhov"
                    },
                    {
                        "name": "Iuliia Eremenko"
                    },
                    {
                        "name": "Ivan P. Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan P. Yamshchikov"
                },
                "author": "Ivan P. Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14252v3",
                "updated": "2025-09-01T08:57:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    57,
                    22,
                    0,
                    244,
                    0
                ],
                "published": "2024-11-21T15:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification"
                },
                "summary": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We also propose MINT-CL, a multi-task contrastive learning framework\nfor multi-turn intent classification, which improves performance while reducing\ndependence on large-scale annotated datasets. Empirical results demonstrate\nthat our approach outperforms competitive baselines in dialogue generation\nquality and classification accuracy, particularly in multilingual settings. To\nfacilitate future research, we release MINT-E, a comprehensive, multilingual,\nintent-aware multi-turn dialogue corpus derived from the e-commerce\ndomain\\footnote{The reproduced source code and dataset are available at\nhttps://github.com/junhua/chain-of-intent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We also propose MINT-CL, a multi-task contrastive learning framework\nfor multi-turn intent classification, which improves performance while reducing\ndependence on large-scale annotated datasets. Empirical results demonstrate\nthat our approach outperforms competitive baselines in dialogue generation\nquality and classification accuracy, particularly in multilingual settings. To\nfacilitate future research, we release MINT-E, a comprehensive, multilingual,\nintent-aware multi-turn dialogue corpus derived from the e-commerce\ndomain\\footnote{The reproduced source code and dataset are available at\nhttps://github.com/junhua/chain-of-intent."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "arxiv_comment": "Accepted to Proceedings of CIKM'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16318v2",
                "updated": "2025-09-01T08:35:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    35,
                    27,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-22T11:57:55Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    11,
                    57,
                    55,
                    4,
                    234,
                    0
                ],
                "title": "SATORI: Static Test Oracle Generation for REST APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATORI: Static Test Oracle Generation for REST APIs"
                },
                "summary": "REST API test case generation tools are evolving rapidly, with growing\ncapabilities for the automated generation of complex tests. However, despite\ntheir strengths in test data generation, these tools are constrained by the\ntypes of test oracles they support, often limited to crashes, regressions, and\nnoncompliance with API specifications or design standards. This paper\nintroduces SATORI (Static API Test ORacle Inference), a black-box approach for\ngenerating test oracles for REST APIs by analyzing their OpenAPI Specification.\nSATORI uses large language models to infer the expected behavior of an API by\nanalyzing the properties of the response fields of its operations, such as\ntheir name and descriptions. To foster its adoption, we extended the\nPostmanAssertify tool to automatically convert the test oracles reported by\nSATORI into executable assertions. Evaluation results on 17 operations from 12\nindustrial APIs show that SATORI can automatically generate up to hundreds of\nvalid test oracles per operation. SATORI achieved an F1-score of 74.3%,\noutperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which\nrequires executing the API-when generating comparable oracle types. Moreover,\nour findings show that static and dynamic oracle inference methods are\ncomplementary: together, SATORI and AGORA+ found 90% of the oracles in our\nannotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular\nAPIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)\nleading to documentation updates by the API maintainers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST API test case generation tools are evolving rapidly, with growing\ncapabilities for the automated generation of complex tests. However, despite\ntheir strengths in test data generation, these tools are constrained by the\ntypes of test oracles they support, often limited to crashes, regressions, and\nnoncompliance with API specifications or design standards. This paper\nintroduces SATORI (Static API Test ORacle Inference), a black-box approach for\ngenerating test oracles for REST APIs by analyzing their OpenAPI Specification.\nSATORI uses large language models to infer the expected behavior of an API by\nanalyzing the properties of the response fields of its operations, such as\ntheir name and descriptions. To foster its adoption, we extended the\nPostmanAssertify tool to automatically convert the test oracles reported by\nSATORI into executable assertions. Evaluation results on 17 operations from 12\nindustrial APIs show that SATORI can automatically generate up to hundreds of\nvalid test oracles per operation. SATORI achieved an F1-score of 74.3%,\noutperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which\nrequires executing the API-when generating comparable oracle types. Moreover,\nour findings show that static and dynamic oracle inference methods are\ncomplementary: together, SATORI and AGORA+ found 90% of the oracles in our\nannotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular\nAPIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)\nleading to documentation updates by the API maintainers."
                },
                "authors": [
                    {
                        "name": "Juan C. Alonso"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Sergio Segura"
                    },
                    {
                        "name": "Gabriele Bavota"
                    },
                    {
                        "name": "Antonio Ruiz-Cortés"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Ruiz-Cortés"
                },
                "author": "Antonio Ruiz-Cortés",
                "arxiv_comment": "Accepted for publication at 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16654v2",
                "updated": "2025-09-01T07:54:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    54,
                    4,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-20T05:41:22Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    5,
                    41,
                    22,
                    2,
                    232,
                    0
                ],
                "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning"
                },
                "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL)."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Minghao Zhang"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "19 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14544v2",
                "updated": "2025-09-01T07:38:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    38,
                    3,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-20T08:55:26Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    55,
                    26,
                    2,
                    232,
                    0
                ],
                "title": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty"
                },
                "summary": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately."
                },
                "authors": [
                    {
                        "name": "Zixi Chen"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11419v2",
                "updated": "2025-09-01T07:04:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    4,
                    47,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T04:17:53Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    4,
                    17,
                    53,
                    0,
                    48,
                    0
                ],
                "title": "InsBank: Evolving Instruction Subset for Ongoing Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InsBank: Evolving Instruction Subset for Ongoing Alignment"
                },
                "summary": "Large language models (LLMs) typically undergo instruction tuning to enhance\nalignment. Recent studies emphasize that quality and diversity of instruction\ndata are more crucial than quantity, highlighting the need to select diverse,\nhigh-quality subsets to reduce training costs. However, how to evolve these\nselected subsets alongside the development of new instruction data remains\ninsufficiently explored. To achieve LLMs' ongoing alignment, we introduce\nInstruction Bank (\\textbf{InsBank}), a continuously updated repository that\nintegrates the latest valuable instruction data. We further propose Progressive\nInstruction Bank Evolution (\\textbf{PIBE}), a novel framework designed to\nevolve InsBank effectively and efficiently over time. PIBE employs a gradual\ndata selection strategy to maintain long-term efficiency, leveraging a\nrepresentation-based diversity score to capture relationships between data\npoints and retain historical information for comprehensive diversity\nevaluation. This also allows for flexible combination of diversity and quality\nscores during data selection and ranking. Extensive experiments demonstrate\nthat PIBE significantly outperforms baselines in InsBank evolution and is able\nto extract budget-specific subsets, demonstrating its effectiveness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically undergo instruction tuning to enhance\nalignment. Recent studies emphasize that quality and diversity of instruction\ndata are more crucial than quantity, highlighting the need to select diverse,\nhigh-quality subsets to reduce training costs. However, how to evolve these\nselected subsets alongside the development of new instruction data remains\ninsufficiently explored. To achieve LLMs' ongoing alignment, we introduce\nInstruction Bank (\\textbf{InsBank}), a continuously updated repository that\nintegrates the latest valuable instruction data. We further propose Progressive\nInstruction Bank Evolution (\\textbf{PIBE}), a novel framework designed to\nevolve InsBank effectively and efficiently over time. PIBE employs a gradual\ndata selection strategy to maintain long-term efficiency, leveraging a\nrepresentation-based diversity score to capture relationships between data\npoints and retain historical information for comprehensive diversity\nevaluation. This also allows for flexible combination of diversity and quality\nscores during data selection and ranking. Extensive experiments demonstrate\nthat PIBE significantly outperforms baselines in InsBank evolution and is able\nto extract budget-specific subsets, demonstrating its effectiveness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Huan Ren"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01235v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01235v3",
                "updated": "2025-09-01T06:03:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    6,
                    3,
                    11,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-02T07:22:08Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    7,
                    22,
                    8,
                    5,
                    214,
                    0
                ],
                "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place\n  Exploration"
                },
                "summary": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yaxin Hu"
                    },
                    {
                        "name": "Arissa J. Sato"
                    },
                    {
                        "name": "Jingxin Du"
                    },
                    {
                        "name": "Chenming Ye"
                    },
                    {
                        "name": "Anjun Zhu"
                    },
                    {
                        "name": "Pragathi Praveena"
                    },
                    {
                        "name": "Bilge Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Mutlu"
                },
                "author": "Bilge Mutlu",
                "arxiv_doi": "10.1145/3746059.3747697",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747697",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01235v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01235v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 38th Annual Acm Symposium on User Interface\n  Software and Technology (UIST 2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10029v2",
                "updated": "2025-09-01T05:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    5,
                    19,
                    6,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-14T08:08:55Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    8,
                    55,
                    0,
                    195,
                    0
                ],
                "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via\n  Selective Optimization Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via\n  Selective Optimization Strategies"
                },
                "summary": "Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency."
                },
                "authors": [
                    {
                        "name": "Seokeon Choi"
                    },
                    {
                        "name": "Sunghyun Park"
                    },
                    {
                        "name": "Hyoungwoo Park"
                    },
                    {
                        "name": "Jeongho Kim"
                    },
                    {
                        "name": "Sungrack Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sungrack Yun"
                },
                "author": "Sungrack Yun",
                "arxiv_comment": "Accepted to ICCV 2025 LIMIT Workshop (4-page short paper). Extended\n  version in preparation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05407v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05407v4",
                "updated": "2025-09-01T05:04:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    5,
                    4,
                    34,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-08T01:54:23Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    1,
                    54,
                    23,
                    5,
                    39,
                    0
                ],
                "title": "The Complexity of Learning Sparse Superposed Features with Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Complexity of Learning Sparse Superposed Features with Feedback"
                },
                "summary": "The success of deep networks is crucially attributed to their ability to\ncapture latent features within a representation space. In this work, we\ninvestigate whether the underlying learned features of a model can be\nefficiently retrieved through feedback from an agent, such as a large language\nmodel (LLM), in the form of relative \\tt{triplet comparisons}. These features\nmay represent various constructs, including dictionaries in LLMs or a\ncovariance matrix of Mahalanobis distances. We analyze the feedback complexity\nassociated with learning a feature matrix in sparse settings. Our results\nestablish tight bounds when the agent is permitted to construct activations and\ndemonstrate strong upper bounds in sparse scenarios when the agent's feedback\nis limited to distributional information. We validate our theoretical findings\nthrough experiments on two distinct applications: feature recovery from\nRecursive Feature Machines and dictionary extraction from sparse autoencoders\ntrained on Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of deep networks is crucially attributed to their ability to\ncapture latent features within a representation space. In this work, we\ninvestigate whether the underlying learned features of a model can be\nefficiently retrieved through feedback from an agent, such as a large language\nmodel (LLM), in the form of relative \\tt{triplet comparisons}. These features\nmay represent various constructs, including dictionaries in LLMs or a\ncovariance matrix of Mahalanobis distances. We analyze the feedback complexity\nassociated with learning a feature matrix in sparse settings. Our results\nestablish tight bounds when the agent is permitted to construct activations and\ndemonstrate strong upper bounds in sparse scenarios when the agent's feedback\nis limited to distributional information. We validate our theoretical findings\nthrough experiments on two distinct applications: feature recovery from\nRecursive Feature Machines and dictionary extraction from sparse autoencoders\ntrained on Large Language Models."
                },
                "authors": [
                    {
                        "name": "Akash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Akash Kumar"
                },
                "author": "Akash Kumar",
                "arxiv_comment": "ICML'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05407v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05407v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05790v3",
                "updated": "2025-09-01T04:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    4,
                    51,
                    13,
                    0,
                    244,
                    0
                ],
                "published": "2025-01-10T08:50:38Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    50,
                    38,
                    4,
                    10,
                    0
                ],
                "title": "Understanding Impact of Human Feedback via Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Impact of Human Feedback via Influence Functions"
                },
                "summary": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. Our experiments showcase two key\napplications of influence functions: (1) detecting common labeler biases in\nhuman feedback datasets and (2) guiding labelers in refining their strategies\nto better align with expert feedback. By quantifying the impact of human\nfeedback, we believe that influence functions can enhance feedback\ninterpretability and contribute to scalable oversight in RLHF, helping labelers\nprovide more accurate and consistent feedback. Source code is available at\nhttps://github.com/mintaywon/IF_RLHF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. Our experiments showcase two key\napplications of influence functions: (1) detecting common labeler biases in\nhuman feedback datasets and (2) guiding labelers in refining their strategies\nto better align with expert feedback. By quantifying the impact of human\nfeedback, we believe that influence functions can enhance feedback\ninterpretability and contribute to scalable oversight in RLHF, helping labelers\nprovide more accurate and consistent feedback. Source code is available at\nhttps://github.com/mintaywon/IF_RLHF"
                },
                "authors": [
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Haeone Lee"
                    },
                    {
                        "name": "Yongchan Kwon"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1333",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1333",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACL 2025, Source code:\n  https://github.com/mintaywon/IF_RLHF",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics 63 (2025) 27471-27500",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00134v3",
                "updated": "2025-09-01T04:16:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    4,
                    16,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-28T19:25:04Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    19,
                    25,
                    4,
                    4,
                    59,
                    0
                ],
                "title": "Personalized Causal Graph Reasoning for LLMs: An Implementation for\n  Dietary Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Causal Graph Reasoning for LLMs: An Implementation for\n  Dietary Recommendations"
                },
                "summary": "Large Language Models (LLMs) excel at general-purpose reasoning by leveraging\nbroad commonsense knowledge, but they remain limited in tasks requiring\npersonalized reasoning over multifactorial personal data. This limitation\nconstrains their applicability in domains such as healthcare, where decisions\nmust adapt to individual contexts. We introduce Personalized Causal Graph\nReasoning, a framework that enables LLMs to reason over individual-specific\ncausal graphs constructed from longitudinal data. Each graph encodes how\nuser-specific factors influence targeted outcomes. In response to a query, the\nLLM traverses the graph to identify relevant causal pathways, rank them by\nestimated impact, simulate potential outcomes, and generate tailored responses.\nWe implement this framework in the context of nutrient-oriented dietary\nrecommendations, where variability in metabolic responses demands personalized\nreasoning. Using counterfactual evaluation, we assess the effectiveness of\nLLM-generated food suggestions for glucose control. Our method reduces\npostprandial glucose iAUC across three time windows compared to prior\napproaches. Additional LLM-as-a-judge evaluations further confirm improvements\nin personalization quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at general-purpose reasoning by leveraging\nbroad commonsense knowledge, but they remain limited in tasks requiring\npersonalized reasoning over multifactorial personal data. This limitation\nconstrains their applicability in domains such as healthcare, where decisions\nmust adapt to individual contexts. We introduce Personalized Causal Graph\nReasoning, a framework that enables LLMs to reason over individual-specific\ncausal graphs constructed from longitudinal data. Each graph encodes how\nuser-specific factors influence targeted outcomes. In response to a query, the\nLLM traverses the graph to identify relevant causal pathways, rank them by\nestimated impact, simulate potential outcomes, and generate tailored responses.\nWe implement this framework in the context of nutrient-oriented dietary\nrecommendations, where variability in metabolic responses demands personalized\nreasoning. Using counterfactual evaluation, we assess the effectiveness of\nLLM-generated food suggestions for glucose control. Our method reduces\npostprandial glucose iAUC across three time windows compared to prior\napproaches. Additional LLM-as-a-judge evaluations further confirm improvements\nin personalization quality."
                },
                "authors": [
                    {
                        "name": "Zhongqi Yang"
                    },
                    {
                        "name": "Amir Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Rahmani"
                },
                "author": "Amir Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.09545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.09545v3",
                "updated": "2025-09-01T03:35:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    35,
                    50,
                    0,
                    244,
                    0
                ],
                "published": "2023-09-18T07:42:47Z",
                "published_parsed": [
                    2023,
                    9,
                    18,
                    7,
                    42,
                    47,
                    0,
                    261,
                    0
                ],
                "title": "Convergence and Inference of Stream SGD, with Applications to Queueing\n  Systems and Inventory Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convergence and Inference of Stream SGD, with Applications to Queueing\n  Systems and Inventory Control"
                },
                "summary": "Stream stochastic gradient descent (SGD) is a simple and efficient method for\nsolving online optimization problems in operations research (OR), where data is\ngenerated by parameter-dependent Markov chains. Unlike traditional approaches\nwhich require increasing batch sizes during iterations, stream SGD uses a\nsingle sample per iteration, significantly improving sample efficiency. This\npaper establishes a systematic framework for analyzing stream SGD, leveraging\nthe Poisson equation solution to address gradient bias and statistical\ndependence. We prove optimal O(1/T) convergence rates and the state-of-the-art\nO(log T) regret, while also introducing an online inference method for\nuncertainty quantification and supporting it by a novel functional central\nlimit theorem. We propose a novel Wasserstein-type divergence to describe the\nframework's conditions, which makes the assumptions in question directly\nverified via coupling techniques tailored to underlying OR models. We consider\napplications in queueing systems and inventory management, demonstrating the\npracticality and broad relevance, as well as providing new insights into the\neffectiveness of stream SGD in OR fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream stochastic gradient descent (SGD) is a simple and efficient method for\nsolving online optimization problems in operations research (OR), where data is\ngenerated by parameter-dependent Markov chains. Unlike traditional approaches\nwhich require increasing batch sizes during iterations, stream SGD uses a\nsingle sample per iteration, significantly improving sample efficiency. This\npaper establishes a systematic framework for analyzing stream SGD, leveraging\nthe Poisson equation solution to address gradient bias and statistical\ndependence. We prove optimal O(1/T) convergence rates and the state-of-the-art\nO(log T) regret, while also introducing an online inference method for\nuncertainty quantification and supporting it by a novel functional central\nlimit theorem. We propose a novel Wasserstein-type divergence to describe the\nframework's conditions, which makes the assumptions in question directly\nverified via coupling techniques tailored to underlying OR models. We consider\napplications in queueing systems and inventory management, demonstrating the\npracticality and broad relevance, as well as providing new insights into the\neffectiveness of stream SGD in OR fields."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Jiadong Liang"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Zhihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhihua Zhang"
                },
                "author": "Zhihua Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.09545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.09545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07279v2",
                "updated": "2025-09-01T03:34:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    34,
                    39,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-10T10:33:16Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    10,
                    33,
                    16,
                    6,
                    222,
                    0
                ],
                "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health\n  Screening using Item Response Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health\n  Screening using Item Response Theory"
                },
                "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows."
                },
                "authors": [
                    {
                        "name": "Vasudha Varadarajan"
                    },
                    {
                        "name": "Hui Xu"
                    },
                    {
                        "name": "Rebecca Astrid Boehme"
                    },
                    {
                        "name": "Mariam Marlan Mirstrom"
                    },
                    {
                        "name": "Sverker Sikstrom"
                    },
                    {
                        "name": "H. Andrew Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "H. Andrew Schwartz"
                },
                "author": "H. Andrew Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02730v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02730v3",
                "updated": "2025-09-01T03:33:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    33,
                    43,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-03T17:49:28Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    49,
                    28,
                    3,
                    277,
                    0
                ],
                "title": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision\n  Language Models in Diverse Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision\n  Language Models in Diverse Scenes"
                },
                "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ntasks like visual question answering and document understanding. However, their\npotential to comprehend embodied environments and navigate within them remains\nunderexplored. In this work, we first study the challenge of open-vocabulary\nobject navigation by introducing DivScene, a large-scale dataset with 4,614\nhouses across 81 scene types and 5,707 kinds of target objects. Our dataset\nprovides a much greater diversity of target objects and scene types than\nexisting datasets, enabling a comprehensive task evaluation. We evaluated\nvarious methods with LVLMs and LLMs on our dataset and found that current\nmodels still fall short of open-vocab object navigation ability. Then, we\nfine-tuned LVLMs to predict the next action with CoT explanations. We observe\nthat LVLM's navigation ability can be improved substantially with only\nBFS-generated shortest paths without any human supervision, surpassing GPT-4o\nby over 20% in success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ntasks like visual question answering and document understanding. However, their\npotential to comprehend embodied environments and navigate within them remains\nunderexplored. In this work, we first study the challenge of open-vocabulary\nobject navigation by introducing DivScene, a large-scale dataset with 4,614\nhouses across 81 scene types and 5,707 kinds of target objects. Our dataset\nprovides a much greater diversity of target objects and scene types than\nexisting datasets, enabling a comprehensive task evaluation. We evaluated\nvarious methods with LVLMs and LLMs on our dataset and found that current\nmodels still fall short of open-vocab object navigation ability. Then, we\nfine-tuned LVLMs to predict the next action with CoT explanations. We observe\nthat LVLM's navigation ability can be improved substantially with only\nBFS-generated shortest paths without any human supervision, surpassing GPT-4o\nby over 20% in success rates."
                },
                "authors": [
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Xiaoman Pan"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02730v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02730v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09857v2",
                "updated": "2025-09-01T03:23:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    23,
                    46,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-14T01:44:57Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    1,
                    44,
                    57,
                    4,
                    45,
                    0
                ],
                "title": "Port-LLM: A Port Prediction Method for Fluid Antenna based on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Port-LLM: A Port Prediction Method for Fluid Antenna based on Large\n  Language Models"
                },
                "summary": "The objective of this study is to address the mobility challenges faced by\nuser equipment (UE) through the implementation of fluid antenna (FA) on the UE\nside. This approach aims to maintain the time-varying channel in a relatively\nstable state by strategically relocating the FA to an appropriate port. To the\nbest of our knowledge, this paper introduces, for the first time, the\napplication of large language models (LLMs) in the prediction of FA ports,\npresenting a novel model termed Port-LLM. Our proposed method for predicting\nthe moving port of the FA is a two-step prediction method. To enhance the\nlearning efficacy of our proposed Port-LLM model, we integrate low-rank\nadaptation (LoRA) fine-tuning technology. Additionally, to further exploit the\nnatural language processing capabilities of pre-trained LLMs, we propose a\nframework named Prompt-Port-LLM, which is constructed upon the Port-LLM\narchitecture and incorporates prompt fine-tuning techniques along with a\nspecialized prompt encoder module. The simulation results show that our\nproposed models all exhibit strong generalization ability and robustness under\ndifferent numbers of base station antennas and medium-to-high mobility speeds\nof UE. In comparison to existing methods, the performance of the port predicted\nby our models demonstrates superior efficacy. Moreover, both of our proposed\nmodels achieve millimeter-level inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The objective of this study is to address the mobility challenges faced by\nuser equipment (UE) through the implementation of fluid antenna (FA) on the UE\nside. This approach aims to maintain the time-varying channel in a relatively\nstable state by strategically relocating the FA to an appropriate port. To the\nbest of our knowledge, this paper introduces, for the first time, the\napplication of large language models (LLMs) in the prediction of FA ports,\npresenting a novel model termed Port-LLM. Our proposed method for predicting\nthe moving port of the FA is a two-step prediction method. To enhance the\nlearning efficacy of our proposed Port-LLM model, we integrate low-rank\nadaptation (LoRA) fine-tuning technology. Additionally, to further exploit the\nnatural language processing capabilities of pre-trained LLMs, we propose a\nframework named Prompt-Port-LLM, which is constructed upon the Port-LLM\narchitecture and incorporates prompt fine-tuning techniques along with a\nspecialized prompt encoder module. The simulation results show that our\nproposed models all exhibit strong generalization ability and robustness under\ndifferent numbers of base station antennas and medium-to-high mobility speeds\nof UE. In comparison to existing methods, the performance of the port predicted\nby our models demonstrates superior efficacy. Moreover, both of our proposed\nmodels achieve millimeter-level inference speed."
                },
                "authors": [
                    {
                        "name": "Yali Zhang"
                    },
                    {
                        "name": "Haifan Yin"
                    },
                    {
                        "name": "Weidong Li"
                    },
                    {
                        "name": "Emil Bjornson"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "14 pages, 13 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17009v2",
                "updated": "2025-09-01T02:43:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    43,
                    50,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-23T12:49:08Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    12,
                    49,
                    8,
                    5,
                    235,
                    0
                ],
                "title": "Contrastive Prompt Clustering for Weakly Supervised Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Prompt Clustering for Weakly Supervised Semantic\n  Segmentation"
                },
                "summary": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has\ngained attention for its cost-effectiveness. Most existing methods emphasize\ninter-class separation, often neglecting the shared semantics among related\ncategories and lacking fine-grained discrimination. To address this, we propose\nContrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large\nLanguage Models (LLMs) to derive category clusters that encode intrinsic\ninter-class relationships, and further introduces a class-aware patch-level\ncontrastive loss to enforce intra-class consistency and inter-class separation.\nThis hierarchical design leverages clusters as coarse-grained semantic priors\nwhile preserving fine-grained boundaries, thereby reducing confusion among\nvisually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014\ndemonstrate that CPC surpasses existing state-of-the-art methods in WSSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has\ngained attention for its cost-effectiveness. Most existing methods emphasize\ninter-class separation, often neglecting the shared semantics among related\ncategories and lacking fine-grained discrimination. To address this, we propose\nContrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large\nLanguage Models (LLMs) to derive category clusters that encode intrinsic\ninter-class relationships, and further introduces a class-aware patch-level\ncontrastive loss to enforce intra-class consistency and inter-class separation.\nThis hierarchical design leverages clusters as coarse-grained semantic priors\nwhile preserving fine-grained boundaries, thereby reducing confusion among\nvisually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014\ndemonstrate that CPC surpasses existing state-of-the-art methods in WSSS."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Zhenhong Chen"
                    },
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09889v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09889v4",
                "updated": "2025-09-01T02:41:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    41,
                    50,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-13T15:46:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    46,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA\n  Problem Solving by AWorld",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA\n  Problem Solving by AWorld"
                },
                "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, this reliance introduces new challenges, as\nextended contexts and noisy tool outputs can undermine system reliability. To\naddress this, we propose a dynamic Multi-Agent System (MAS) in our AWorld\nframework, where an Execution Agent is supervised by a Guard Agent that\nprovides on-demand dynamic maneuvering, verifying and correcting the reasoning\nprocess to improve robustness over single-agent systems. To move beyond this\ngeneric supervision, we enhance the architecture with a methodology inspired by\nSystem Identification from control theory. This method first profiles the\nExecution Agent offline on a benchmark dataset to create a \"performance\nfingerprint\" of its unique weaknesses. The Guard Agent then leverages this\nfingerprint online to deliver profile-aware supervision, making targeted\ninterventions based on known failure patterns rather than merely reacting to\nimmediate logical flaws. Extensive experiments on the GAIA dataset demonstrate\nthat this profile-aware MAS significantly improves both effectiveness and\nstability, outperforming not only single-agent systems but also its naive\ncounterpart. This superior performance led our system to achieve first place\namong open-source projects on the prestigious GAIA leaderboard. These findings\nhighlight that building truly trustworthy intelligent systems requires not just\ncollaboration, but a deep, empirically-grounded understanding of each agent's\nunique capabilities and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, this reliance introduces new challenges, as\nextended contexts and noisy tool outputs can undermine system reliability. To\naddress this, we propose a dynamic Multi-Agent System (MAS) in our AWorld\nframework, where an Execution Agent is supervised by a Guard Agent that\nprovides on-demand dynamic maneuvering, verifying and correcting the reasoning\nprocess to improve robustness over single-agent systems. To move beyond this\ngeneric supervision, we enhance the architecture with a methodology inspired by\nSystem Identification from control theory. This method first profiles the\nExecution Agent offline on a benchmark dataset to create a \"performance\nfingerprint\" of its unique weaknesses. The Guard Agent then leverages this\nfingerprint online to deliver profile-aware supervision, making targeted\ninterventions based on known failure patterns rather than merely reacting to\nimmediate logical flaws. Extensive experiments on the GAIA dataset demonstrate\nthat this profile-aware MAS significantly improves both effectiveness and\nstability, outperforming not only single-agent systems but also its naive\ncounterpart. This superior performance led our system to achieve first place\namong open-source projects on the prestigious GAIA leaderboard. These findings\nhighlight that building truly trustworthy intelligent systems requires not just\ncollaboration, but a deep, empirically-grounded understanding of each agent's\nunique capabilities and limitations."
                },
                "authors": [
                    {
                        "name": "Zhitian Xie"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Chengyue Yu"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09889v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09889v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14135v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14135v4",
                "updated": "2025-09-01T02:40:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    40,
                    12,
                    0,
                    244,
                    0
                ],
                "published": "2024-05-23T03:19:02Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    3,
                    19,
                    2,
                    3,
                    144,
                    0
                ],
                "title": "Space-aware Socioeconomic Indicator Inference with Heterogeneous Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space-aware Socioeconomic Indicator Inference with Heterogeneous Graphs"
                },
                "summary": "Regional socioeconomic indicators are critical across various domains, yet\ntheir acquisition can be costly. Inferring global socioeconomic indicators from\na limited number of regional samples is essential for enhancing management and\nsustainability in urban areas and human settlements. Current inference methods\ntypically rely on spatial interpolation based on the assumption of spatial\ncontinuity, which does not adequately address the complex variations present\nwithin regional spaces. In this paper, we present GeoHG, the first space-aware\nsocioeconomic indicator inference method that utilizes a heterogeneous\ngraph-based structure to represent geospace for non-continuous inference.\nExtensive experiments demonstrate the effectiveness of GeoHG in comparison to\nexisting methods, achieving an $R^2$ score exceeding 0.8 under extreme data\nscarcity with a masked ratio of 95\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regional socioeconomic indicators are critical across various domains, yet\ntheir acquisition can be costly. Inferring global socioeconomic indicators from\na limited number of regional samples is essential for enhancing management and\nsustainability in urban areas and human settlements. Current inference methods\ntypically rely on spatial interpolation based on the assumption of spatial\ncontinuity, which does not adequately address the complex variations present\nwithin regional spaces. In this paper, we present GeoHG, the first space-aware\nsocioeconomic indicator inference method that utilizes a heterogeneous\ngraph-based structure to represent geospace for non-continuous inference.\nExtensive experiments demonstrate the effectiveness of GeoHG in comparison to\nexisting methods, achieving an $R^2$ score exceeding 0.8 under extreme data\nscarcity with a masked ratio of 95\\%."
                },
                "authors": [
                    {
                        "name": "Xingchen Zou"
                    },
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Xixuan Hao"
                    },
                    {
                        "name": "Yuhao Yang"
                    },
                    {
                        "name": "Haomin Wen"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "ACM SIGSPATIAL 2025 Full Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14135v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14135v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16044v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16044v4",
                "updated": "2025-09-01T02:39:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    39,
                    51,
                    0,
                    244,
                    0
                ],
                "published": "2024-11-25T02:15:30Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    2,
                    15,
                    30,
                    0,
                    330,
                    0
                ],
                "title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities\n  through Tree-Based Image Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities\n  through Tree-Based Image Exploration"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in vision-language understanding. Recently, with the integration\nof test-time scaling techniques, these models have also shown strong potential\nin visual reasoning. However, most existing reasoning approaches remain\ntext-level in nature: MLLMs are prompted to explore various combinations of\ntextual tokens via their underlying language model, while the visual input\nremains fixed throughout the reasoning process. This paradigm limits the\nmodel's ability to fully exploit rich visual information, particularly when\ndealing with images containing numerous fine-grained elements. In such cases,\nvision-level reasoning becomes crucial - where models dynamically zoom into\nspecific regions of the image to gather detailed visual cues necessary for\naccurate decision-making. In this paper, we propose Zoom Eye, a training-free,\nmodel-agnostic tree search algorithm tailored for vision-level reasoning. Zoom\nEye treats an image as a hierarchical tree structure, where each child node\nrepresents a zoomed-in sub-region of its parent, and the root corresponds to\nthe full image. The algorithm enables MLLMs to simulate human-like zooming\nbehavior by navigating from root to leaf nodes in search of task-relevant\nvisual evidence. We experiment on a series of high-resolution benchmarks and\nthe results demonstrate that Zoom Eye consistently improves the performance of\nmultiple MLLMs by a large margin (e.g., InternVL2.5-8B increases by 15.71% and\n17.69% on HR-Bench) and also enables small 3-8B MLLMs to outperform strong\nlarge models such as GPT-4o. Code: https://github.com/om-ai-lab/ZoomEye",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in vision-language understanding. Recently, with the integration\nof test-time scaling techniques, these models have also shown strong potential\nin visual reasoning. However, most existing reasoning approaches remain\ntext-level in nature: MLLMs are prompted to explore various combinations of\ntextual tokens via their underlying language model, while the visual input\nremains fixed throughout the reasoning process. This paradigm limits the\nmodel's ability to fully exploit rich visual information, particularly when\ndealing with images containing numerous fine-grained elements. In such cases,\nvision-level reasoning becomes crucial - where models dynamically zoom into\nspecific regions of the image to gather detailed visual cues necessary for\naccurate decision-making. In this paper, we propose Zoom Eye, a training-free,\nmodel-agnostic tree search algorithm tailored for vision-level reasoning. Zoom\nEye treats an image as a hierarchical tree structure, where each child node\nrepresents a zoomed-in sub-region of its parent, and the root corresponds to\nthe full image. The algorithm enables MLLMs to simulate human-like zooming\nbehavior by navigating from root to leaf nodes in search of task-relevant\nvisual evidence. We experiment on a series of high-resolution benchmarks and\nthe results demonstrate that Zoom Eye consistently improves the performance of\nmultiple MLLMs by a large margin (e.g., InternVL2.5-8B increases by 15.71% and\n17.69% on HR-Bench) and also enables small 3-8B MLLMs to outperform strong\nlarge models such as GPT-4o. Code: https://github.com/om-ai-lab/ZoomEye"
                },
                "authors": [
                    {
                        "name": "Haozhan Shen"
                    },
                    {
                        "name": "Kangjia Zhao"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Zilun Zhang"
                    },
                    {
                        "name": "Mingwei Zhu"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "Accepted by EMNLP-2025 Main. Project page:\n  https://szhanz.github.io/zoomeye/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16044v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16044v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11548v3",
                "updated": "2025-09-01T02:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    28,
                    23,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-15T08:14:58Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    14,
                    58,
                    3,
                    135,
                    0
                ],
                "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented\n  Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented\n  Generation Systems"
                },
                "summary": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. While previous studies have exposed knowledge\npoisoning risks in RAG systems, existing attack methods suffer from critical\nlimitations: they either require injecting multiple poisoned documents\n(resulting in poor stealthiness) or can only function effectively on simplistic\nqueries (limiting real-world applicability). This paper reveals a more\nrealistic knowledge poisoning attack against RAG systems that achieves\nsuccessful attacks by poisoning only a single document while remaining\neffective for complex multi-hop questions involving complex relationships\nbetween multiple elements. Our proposed AuthChain address three challenges to\nensure the poisoned documents are reliably retrieved and trusted by the LLM,\neven against large knowledge bases and LLM's own knowledge. Extensive\nexperiments across six popular LLMs demonstrate that AuthChain achieves\nsignificantly higher attack success rates while maintaining superior\nstealthiness against RAG defense mechanisms compared to state-of-the-art\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. While previous studies have exposed knowledge\npoisoning risks in RAG systems, existing attack methods suffer from critical\nlimitations: they either require injecting multiple poisoned documents\n(resulting in poor stealthiness) or can only function effectively on simplistic\nqueries (limiting real-world applicability). This paper reveals a more\nrealistic knowledge poisoning attack against RAG systems that achieves\nsuccessful attacks by poisoning only a single document while remaining\neffective for complex multi-hop questions involving complex relationships\nbetween multiple elements. Our proposed AuthChain address three challenges to\nensure the poisoned documents are reliably retrieved and trusted by the LLM,\neven against large knowledge bases and LLM's own knowledge. Extensive\nexperiments across six popular LLMs demonstrate that AuthChain achieves\nsignificantly higher attack success rates while maintaining superior\nstealthiness against RAG defense mechanisms compared to state-of-the-art\nbaselines."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yuekai Huang"
                    },
                    {
                        "name": "Ziyou Jiang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "arxiv_comment": "15pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11727v3",
                "updated": "2025-09-01T02:10:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    10,
                    58,
                    0,
                    244,
                    0
                ],
                "published": "2024-08-21T15:54:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "Efficient Detection of Toxic Prompts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Detection of Toxic Prompts in Large Language Models"
                },
                "summary": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Junzhe Yu"
                    },
                    {
                        "name": "Huijia Sun"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Yuqi Chen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10352v2",
                "updated": "2025-09-01T01:56:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    56,
                    40,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-12T05:19:17Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    5,
                    19,
                    17,
                    3,
                    163,
                    0
                ],
                "title": "History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling\n  of Path-Dependent Materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling\n  of Path-Dependent Materials"
                },
                "summary": "This study presents an end-to-end learning framework for data-driven modeling\nof path-dependent inelastic materials using neural operators. The framework is\nbuilt on the premise that irreversible evolution of material responses,\ngoverned by hidden dynamics, can be inferred from observable data.\n  We develop the History-Aware Neural Operator (HANO), an autoregressive model\nthat predicts path-dependent material responses from short segments of recent\nstrain-stress history without relying on hidden state variables, thereby\novercoming self-consistency issues commonly encountered in recurrent neural\nnetwork (RNN)-based models. Built on a Fourier-based neural operator backbone,\nHANO enables discretization-invariant learning. To enhance its ability to\ncapture both global loading patterns and critical local path dependencies, we\nembed a hierarchical self-attention mechanism that facilitates multiscale\nfeature extraction.\n  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial\nhidden states, a commonly overlooked issue that can lead to instability in\nrecurrent models when applied to generalized loading paths. By modeling\nstress-strain evolution as a continuous operator rather than relying on fixed\ninput-output mappings, HANO naturally accommodates varying path discretizations\nand exhibits robust performance under complex conditions, including irregular\nsampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate\nHANO on two benchmark problems: elastoplasticity with hardening and progressive\nanisotropic damage in brittle solids. Results show that HANO consistently\noutperforms baseline models in predictive accuracy, generalization, and\nrobustness. With its demonstrated capabilities, HANO provides an effective\ndata-driven surrogate for simulating inelastic materials and is well-suited for\nintegration with classical numerical solvers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents an end-to-end learning framework for data-driven modeling\nof path-dependent inelastic materials using neural operators. The framework is\nbuilt on the premise that irreversible evolution of material responses,\ngoverned by hidden dynamics, can be inferred from observable data.\n  We develop the History-Aware Neural Operator (HANO), an autoregressive model\nthat predicts path-dependent material responses from short segments of recent\nstrain-stress history without relying on hidden state variables, thereby\novercoming self-consistency issues commonly encountered in recurrent neural\nnetwork (RNN)-based models. Built on a Fourier-based neural operator backbone,\nHANO enables discretization-invariant learning. To enhance its ability to\ncapture both global loading patterns and critical local path dependencies, we\nembed a hierarchical self-attention mechanism that facilitates multiscale\nfeature extraction.\n  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial\nhidden states, a commonly overlooked issue that can lead to instability in\nrecurrent models when applied to generalized loading paths. By modeling\nstress-strain evolution as a continuous operator rather than relying on fixed\ninput-output mappings, HANO naturally accommodates varying path discretizations\nand exhibits robust performance under complex conditions, including irregular\nsampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate\nHANO on two benchmark problems: elastoplasticity with hardening and progressive\nanisotropic damage in brittle solids. Results show that HANO consistently\noutperforms baseline models in predictive accuracy, generalization, and\nrobustness. With its demonstrated capabilities, HANO provides an effective\ndata-driven surrogate for simulating inelastic materials and is well-suited for\nintegration with classical numerical solvers."
                },
                "authors": [
                    {
                        "name": "Binyao Guo"
                    },
                    {
                        "name": "Zihan Lin"
                    },
                    {
                        "name": "QiZhi He"
                    }
                ],
                "author_detail": {
                    "name": "QiZhi He"
                },
                "author": "QiZhi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19611v2",
                "updated": "2025-09-01T01:38:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    38,
                    20,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-27T06:45:06Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    6,
                    45,
                    6,
                    2,
                    239,
                    0
                ],
                "title": "Instructional Agents: LLM Agents on Automated Course Material Generation\n  for Teaching Faculties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructional Agents: LLM Agents on Automated Course Material Generation\n  for Teaching Faculties"
                },
                "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Wanpeng Xu"
                    },
                    {
                        "name": "Justin Turnau"
                    },
                    {
                        "name": "Nadia Kellam"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21304v2",
                "updated": "2025-09-01T01:33:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    33,
                    50,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-29T01:59:34Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    1,
                    59,
                    34,
                    4,
                    241,
                    0
                ],
                "title": "ORCA: ORchestrating Causal Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORCA: ORchestrating Causal Agent"
                },
                "summary": "Causal inference is essential for decision-making science while the\ncomplexity of the data analysis workflow, ranging from data wrangling to causal\nanalysis, increases substantially as the scale of data grows in complicated\nbusiness environments. Especially, the execution of the workflow in relational\ndatabases by non-experts can result in repetitive bottlenecks which impede\ntimely and responsible business insights. To address this challenge, we propose\nORCA (Orchestrating Causal Agent), an LLM agentic system that can automate\nroutine workflows in RDBMS while preserving expert oversight via human-AI\ninteractions. ORCA orchestrates the full data analysis pipeline: interpreting\nnatural language queries, navigating tables from DB servers, generating proper\nSQL codes, preprocessing data, and configuring modeling processes using causal\ninference libraries. Domain experts still can control the automation through\niterative interactions with ORCA, enabling robust data-driven decision making\nwith less technical expertise in statistical computing. Empirical evaluations\non benchmark and synthetic e-commerce datasets demonstrate competitive\nperformance of ORCA in table understanding, query generation, and cause-effect\nestimation -- achieving over $7\\times$ improvement in estimating average\ntreatment compared to GPT-4o mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference is essential for decision-making science while the\ncomplexity of the data analysis workflow, ranging from data wrangling to causal\nanalysis, increases substantially as the scale of data grows in complicated\nbusiness environments. Especially, the execution of the workflow in relational\ndatabases by non-experts can result in repetitive bottlenecks which impede\ntimely and responsible business insights. To address this challenge, we propose\nORCA (Orchestrating Causal Agent), an LLM agentic system that can automate\nroutine workflows in RDBMS while preserving expert oversight via human-AI\ninteractions. ORCA orchestrates the full data analysis pipeline: interpreting\nnatural language queries, navigating tables from DB servers, generating proper\nSQL codes, preprocessing data, and configuring modeling processes using causal\ninference libraries. Domain experts still can control the automation through\niterative interactions with ORCA, enabling robust data-driven decision making\nwith less technical expertise in statistical computing. Empirical evaluations\non benchmark and synthetic e-commerce datasets demonstrate competitive\nperformance of ORCA in table understanding, query generation, and cause-effect\nestimation -- achieving over $7\\times$ improvement in estimating average\ntreatment compared to GPT-4o mini."
                },
                "authors": [
                    {
                        "name": "Joanie Hayoun Chung"
                    },
                    {
                        "name": "Chaemyung Lim"
                    },
                    {
                        "name": "Sumin Lee"
                    },
                    {
                        "name": "Songseong Kim"
                    },
                    {
                        "name": "Sungbin Lim"
                    }
                ],
                "author_detail": {
                    "name": "Sungbin Lim"
                },
                "author": "Sungbin Lim",
                "arxiv_comment": "24 pages, 17 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19945v2",
                "updated": "2025-08-31T23:36:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    23,
                    36,
                    46,
                    6,
                    243,
                    0
                ],
                "published": "2025-03-25T11:51:21Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    51,
                    21,
                    1,
                    84,
                    0
                ],
                "title": "Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study\n  of Transfer Learning, Resolution Reduction, and Multi-View Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study\n  of Transfer Learning, Resolution Reduction, and Multi-View Classification"
                },
                "summary": "Mammography, an X-ray-based imaging technique, plays a crucial role in the\nearly detection of breast cancer. Its accuracy heavily depends on expert\nradiologists, making it essential to minimize interpretation errors. To support\nradiologists, various computer-aided detection and diagnostic methods have been\nproposed, increasingly leveraging advancements in artificial intelligence and\nmachine learning. Over recent years, mammogram analysis has evolved\nsignificantly - from early patch-based classifiers, which examine only\nlocalized regions of images, to full-image classifiers, and later towards\nmulti-view systems that simultaneously integrate multiple perspectives of the\nmammographic exam for enhanced accuracy. Despite this progression, critical\nquestions remain, such as whether multi-view systems consistently outperform\nsingle-view approaches. In this paper, we systematically evaluate and compare\nthe effectiveness of single-view and multi-view mammogram classification\ntechniques. Our research introduces models that achieve superior performance\nrelative to existing state-of-the-art approaches in both single-view and\ntwo-view classification scenarios. Furthermore, our findings provide valuable\ninsights into optimal model architectures and effective transfer learning\nstrategies, paving the way for more accurate and efficient mammogram\ninterpretation. The inference code and model are available at\nhttps://github.com/dpetrini/multiple-view.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mammography, an X-ray-based imaging technique, plays a crucial role in the\nearly detection of breast cancer. Its accuracy heavily depends on expert\nradiologists, making it essential to minimize interpretation errors. To support\nradiologists, various computer-aided detection and diagnostic methods have been\nproposed, increasingly leveraging advancements in artificial intelligence and\nmachine learning. Over recent years, mammogram analysis has evolved\nsignificantly - from early patch-based classifiers, which examine only\nlocalized regions of images, to full-image classifiers, and later towards\nmulti-view systems that simultaneously integrate multiple perspectives of the\nmammographic exam for enhanced accuracy. Despite this progression, critical\nquestions remain, such as whether multi-view systems consistently outperform\nsingle-view approaches. In this paper, we systematically evaluate and compare\nthe effectiveness of single-view and multi-view mammogram classification\ntechniques. Our research introduces models that achieve superior performance\nrelative to existing state-of-the-art approaches in both single-view and\ntwo-view classification scenarios. Furthermore, our findings provide valuable\ninsights into optimal model architectures and effective transfer learning\nstrategies, paving the way for more accurate and efficient mammogram\ninterpretation. The inference code and model are available at\nhttps://github.com/dpetrini/multiple-view."
                },
                "authors": [
                    {
                        "name": "Daniel G. P. Petrini"
                    },
                    {
                        "name": "Hae Yong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hae Yong Kim"
                },
                "author": "Hae Yong Kim",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08325v2",
                "updated": "2025-08-31T23:18:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    23,
                    18,
                    38,
                    6,
                    243,
                    0
                ],
                "published": "2025-07-11T05:31:35Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    31,
                    35,
                    4,
                    192,
                    0
                ],
                "title": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation"
                },
                "summary": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics."
                },
                "authors": [
                    {
                        "name": "Yinzhu Quan"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12984v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12984v3",
                "updated": "2025-08-31T22:12:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    22,
                    12,
                    47,
                    6,
                    243,
                    0
                ],
                "published": "2025-04-17T14:45:03Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    45,
                    3,
                    3,
                    107,
                    0
                ],
                "title": "Tilus: A Tile-Level GPGPU Programming Language for Low-Precision\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tilus: A Tile-Level GPGPU Programming Language for Low-Precision\n  Computation"
                },
                "summary": "Serving Large Language Models (LLMs) is critical for AI-powered applications,\nyet it demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance\nbecause of high-level GPU programming abstractions. These abstractions restrict\ncritical optimizations, such as fine-grained register management and optimized\nmemory access patterns, that are essential for efficient low-precision\ncomputations. In this paper, we introduce Tilus, a domain-specific language\ndesigned for General-Purpose GPU (GPGPU) computing that supports low-precision\ndata types with arbitrary bit widths from 1 to 8 while maintaining GPU\nprogrammability. Tilus features a thread-block-level programming model, a\nhierarchical memory space, a novel algebraic layout system, and extensive\nsupport for diverse low-precision data types. Tilus programs are compiled into\nhighly efficient GPU programs through automatic vectorization and instruction\nselection. Extensive experiments demonstrate that Tilus efficiently supports a\nfull spectrum of low-precision data types, and outperforms state-of-the-art\nlow-precision kernels. Compared to existing compilers such as Triton and\nLadder, as well as hand-optimized kernels such as QuantLLM and Marlin, Tilus\nachieves performance improvements of: $1.75\\times$, $2.61\\times$, $1.29\\times$\nand $1.03\\times$, respectively. We open-source Tilus at\nhttps://github.com/NVIDIA/tilus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) is critical for AI-powered applications,\nyet it demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance\nbecause of high-level GPU programming abstractions. These abstractions restrict\ncritical optimizations, such as fine-grained register management and optimized\nmemory access patterns, that are essential for efficient low-precision\ncomputations. In this paper, we introduce Tilus, a domain-specific language\ndesigned for General-Purpose GPU (GPGPU) computing that supports low-precision\ndata types with arbitrary bit widths from 1 to 8 while maintaining GPU\nprogrammability. Tilus features a thread-block-level programming model, a\nhierarchical memory space, a novel algebraic layout system, and extensive\nsupport for diverse low-precision data types. Tilus programs are compiled into\nhighly efficient GPU programs through automatic vectorization and instruction\nselection. Extensive experiments demonstrate that Tilus efficiently supports a\nfull spectrum of low-precision data types, and outperforms state-of-the-art\nlow-precision kernels. Compared to existing compilers such as Triton and\nLadder, as well as hand-optimized kernels such as QuantLLM and Marlin, Tilus\nachieves performance improvements of: $1.75\\times$, $2.61\\times$, $1.29\\times$\nand $1.03\\times$, respectively. We open-source Tilus at\nhttps://github.com/NVIDIA/tilus."
                },
                "authors": [
                    {
                        "name": "Yaoyao Ding"
                    },
                    {
                        "name": "Bohan Hou"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Allan Lin"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Cody Yu Hao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_comment": "17 pages, 14 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12984v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12984v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17182v2",
                "updated": "2025-08-31T21:27:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    21,
                    27,
                    41,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-24T01:43:48Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    1,
                    43,
                    48,
                    6,
                    236,
                    0
                ],
                "title": "LLM Assertiveness can be Mechanistically Decomposed into Emotional and\n  Logical Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Assertiveness can be Mechanistically Decomposed into Emotional and\n  Logical Components"
                },
                "summary": "Large Language Models (LLMs) often display overconfidence, presenting\ninformation with unwarranted certainty in high-stakes contexts. We investigate\nthe internal basis of this behavior via mechanistic interpretability. Using\nopen-sourced Llama 3.2 models fine-tuned on human annotated assertiveness\ndatasets, we extract residual activations across all layers, and compute\nsimilarity metrics to localize assertive representations. Our analysis\nidentifies layers most sensitive to assertiveness contrasts and reveals that\nhigh-assertive representations decompose into two orthogonal sub-components of\nemotional and logical clusters-paralleling the dual-route Elaboration\nLikelihood Model in Psychology. Steering vectors derived from these\nsub-components show distinct causal effects: emotional vectors broadly\ninfluence prediction accuracy, while logical vectors exert more localized\neffects. These findings provide mechanistic evidence for the multi-component\nstructure of LLM assertiveness and highlight avenues for mitigating\noverconfident behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often display overconfidence, presenting\ninformation with unwarranted certainty in high-stakes contexts. We investigate\nthe internal basis of this behavior via mechanistic interpretability. Using\nopen-sourced Llama 3.2 models fine-tuned on human annotated assertiveness\ndatasets, we extract residual activations across all layers, and compute\nsimilarity metrics to localize assertive representations. Our analysis\nidentifies layers most sensitive to assertiveness contrasts and reveals that\nhigh-assertive representations decompose into two orthogonal sub-components of\nemotional and logical clusters-paralleling the dual-route Elaboration\nLikelihood Model in Psychology. Steering vectors derived from these\nsub-components show distinct causal effects: emotional vectors broadly\ninfluence prediction accuracy, while logical vectors exert more localized\neffects. These findings provide mechanistic evidence for the multi-component\nstructure of LLM assertiveness and highlight avenues for mitigating\noverconfident behavior."
                },
                "authors": [
                    {
                        "name": "Hikaru Tsujimura"
                    },
                    {
                        "name": "Arush Tagade"
                    }
                ],
                "author_detail": {
                    "name": "Arush Tagade"
                },
                "author": "Arush Tagade",
                "arxiv_comment": "This preprint is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.08149v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.08149v3",
                "updated": "2025-08-31T20:48:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    20,
                    48,
                    21,
                    6,
                    243,
                    0
                ],
                "published": "2022-10-15T00:32:06Z",
                "published_parsed": [
                    2022,
                    10,
                    15,
                    0,
                    32,
                    6,
                    5,
                    288,
                    0
                ],
                "title": "Distance and Kernel-Based Measures for Global and Local Two-Sample\n  Conditional Distribution Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distance and Kernel-Based Measures for Global and Local Two-Sample\n  Conditional Distribution Testing"
                },
                "summary": "Testing the equality of two conditional distributions is crucial in various\nmodern applications, including transfer learning and causal inference. Despite\nits importance, this fundamental problem has received surprisingly little\nattention in the literature, with existing works focusing exclusively on global\ntwo-sample conditional distribution testing. Based on distance and kernel\nmethods, this paper presents the first unified framework for both global and\nlocal two-sample conditional distribution testing. To this end, we introduce\ndistance and kernel-based measures that characterize the homogeneity of two\nconditional distributions. Drawing from the concept of conditional\nU-statistics, we propose consistent estimators for these measures.\nTheoretically, we derive the convergence rates and the asymptotic distributions\nof the estimators under both the null and alternative hypotheses. Utilizing\nthese measures, along with a local bootstrap approach, we develop global and\nlocal tests that can detect discrepancies between two conditional distributions\nat global and local levels, respectively. Our tests demonstrate reliable\nperformance through simulations and real data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing the equality of two conditional distributions is crucial in various\nmodern applications, including transfer learning and causal inference. Despite\nits importance, this fundamental problem has received surprisingly little\nattention in the literature, with existing works focusing exclusively on global\ntwo-sample conditional distribution testing. Based on distance and kernel\nmethods, this paper presents the first unified framework for both global and\nlocal two-sample conditional distribution testing. To this end, we introduce\ndistance and kernel-based measures that characterize the homogeneity of two\nconditional distributions. Drawing from the concept of conditional\nU-statistics, we propose consistent estimators for these measures.\nTheoretically, we derive the convergence rates and the asymptotic distributions\nof the estimators under both the null and alternative hypotheses. Utilizing\nthese measures, along with a local bootstrap approach, we develop global and\nlocal tests that can detect discrepancies between two conditional distributions\nat global and local levels, respectively. Our tests demonstrate reliable\nperformance through simulations and real data analysis."
                },
                "authors": [
                    {
                        "name": "Jian Yan"
                    },
                    {
                        "name": "Zhuoxi Li"
                    },
                    {
                        "name": "Xianyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianyang Zhang"
                },
                "author": "Xianyang Zhang",
                "arxiv_comment": "Extensively revised version. Code available at\n  https://github.com/lizhuoxi-97/TCDT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.08149v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.08149v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17117v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17117v4",
                "updated": "2025-08-31T19:56:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    19,
                    56,
                    10,
                    6,
                    243,
                    0
                ],
                "published": "2025-05-21T16:29:00Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    29,
                    0,
                    2,
                    141,
                    0
                ],
                "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning"
                },
                "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations."
                },
                "authors": [
                    {
                        "name": "Chen Shani"
                    },
                    {
                        "name": "Liron Soffer"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17117v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17117v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13442v3",
                "updated": "2025-08-31T18:53:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    18,
                    53,
                    24,
                    6,
                    243,
                    0
                ],
                "published": "2024-08-24T02:48:40Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    2,
                    48,
                    40,
                    5,
                    237,
                    0
                ],
                "title": "A Law of Next-Token Prediction in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Law of Next-Token Prediction in Large Language Models"
                },
                "summary": "Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, irrespective of their architectures or pre-training\ndata. We demonstrate that this law offers new perspectives and actionable\ninsights to inform and guide practices in LLM development and applications,\nincluding model scaling, pre-training tasks, and interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, irrespective of their architectures or pre-training\ndata. We demonstrate that this law offers new perspectives and actionable\ninsights to inform and guide practices in LLM development and applications,\nincluding model scaling, pre-training tasks, and interpretation."
                },
                "authors": [
                    {
                        "name": "Hangfeng He"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "arxiv_comment": "Transferred for publication to Physical Review E from Physical Review\n  Research (to waive publication charges)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11465v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11465v4",
                "updated": "2025-08-31T16:55:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    55,
                    47,
                    6,
                    243,
                    0
                ],
                "published": "2024-11-18T10:58:46Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    58,
                    46,
                    0,
                    323,
                    0
                ],
                "title": "Re-examining learning linear functions in context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-examining learning linear functions in context"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning."
                },
                "authors": [
                    {
                        "name": "Omar Naim"
                    },
                    {
                        "name": "Guilhem Fouilhé"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "arxiv_doi": "10.1007/978-3-032-02813-6_8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-02813-6_8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.11465v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11465v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "KI 2025: Advances in Artificial Intelligence",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03665v2",
                "updated": "2025-08-31T16:24:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    24,
                    59,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-05T17:24:50Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    24,
                    50,
                    1,
                    217,
                    0
                ],
                "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design"
                },
                "summary": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts."
                },
                "authors": [
                    {
                        "name": "Claudiu Leoveanu-Condrei"
                    }
                ],
                "author_detail": {
                    "name": "Claudiu Leoveanu-Condrei"
                },
                "author": "Claudiu Leoveanu-Condrei",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.2; I.1.2; D.1.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.00025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.00025v3",
                "updated": "2025-08-31T16:11:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    11,
                    1,
                    6,
                    243,
                    0
                ],
                "published": "2022-12-30T19:00:05Z",
                "published_parsed": [
                    2022,
                    12,
                    30,
                    19,
                    0,
                    5,
                    4,
                    364,
                    0
                ],
                "title": "Search for echoes on the edge of quantum black holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search for echoes on the edge of quantum black holes"
                },
                "summary": "I perform a template-based search for stimulated emission of Hawking\nradiation (or Boltzmann echoes) by combining the gravitational wave data from\n47 binary black hole merger events observed by the LIGO-Virgo-KAGRA\ncollaboration. With a Bayesian inference approach, I found no statistically\nsignificant evidence for this signal in either of the 3 Gravitational Wave\nTransient Catalogs GWTC-1, GWTC-2 and GWTC-3. While the data does not provide\ndefinitive evidence against the presence of Boltzmann echoes, the Bayesian\nevidence for most events falls within the range of 0.3-1.6, with the hypothesis\nof a common (non-vanishing) echo amplitude for all mergers being weakly\ndisfavoured at 2:5 odds. The only exception is GW190521, the most massive and\nconfidently detected event ever observed, which shows a positive evidence of\n9.2 for stimulated Hawking radiation. The ''look-elsewhere'' effect for this\noutlier event is assessed by applying two distinct methods to add simulated\nsignals in real data, before and after the event, giving false (true) positive\ndetection probabilities for higher Bayes factors of $1.5^{+1.2}_{-0.9}\\%$,\n$4.4^{+2.0}_{-2.0} \\%$ ($35 \\pm 7 \\%$, $35 \\pm 15 \\%$). An optimal combination\nof posteriors yields an upper limit of $A < 0.4$ (at $90\\%$ confidence level)\nfor a universal echo amplitude, whereas $A \\sim 1$ was predicted in the\ncanonical model. To ensure the robustness of the results, I have employed an\nadditional method to combine the events hierarchically. This approach involves\nusing a target gaussian distribution and extracting the parameters from\nmultiple uncertain observations, which may be affected by selection biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I perform a template-based search for stimulated emission of Hawking\nradiation (or Boltzmann echoes) by combining the gravitational wave data from\n47 binary black hole merger events observed by the LIGO-Virgo-KAGRA\ncollaboration. With a Bayesian inference approach, I found no statistically\nsignificant evidence for this signal in either of the 3 Gravitational Wave\nTransient Catalogs GWTC-1, GWTC-2 and GWTC-3. While the data does not provide\ndefinitive evidence against the presence of Boltzmann echoes, the Bayesian\nevidence for most events falls within the range of 0.3-1.6, with the hypothesis\nof a common (non-vanishing) echo amplitude for all mergers being weakly\ndisfavoured at 2:5 odds. The only exception is GW190521, the most massive and\nconfidently detected event ever observed, which shows a positive evidence of\n9.2 for stimulated Hawking radiation. The ''look-elsewhere'' effect for this\noutlier event is assessed by applying two distinct methods to add simulated\nsignals in real data, before and after the event, giving false (true) positive\ndetection probabilities for higher Bayes factors of $1.5^{+1.2}_{-0.9}\\%$,\n$4.4^{+2.0}_{-2.0} \\%$ ($35 \\pm 7 \\%$, $35 \\pm 15 \\%$). An optimal combination\nof posteriors yields an upper limit of $A < 0.4$ (at $90\\%$ confidence level)\nfor a universal echo amplitude, whereas $A \\sim 1$ was predicted in the\ncanonical model. To ensure the robustness of the results, I have employed an\nadditional method to combine the events hierarchically. This approach involves\nusing a target gaussian distribution and extracting the parameters from\nmultiple uncertain observations, which may be affected by selection biases."
                },
                "authors": [
                    {
                        "name": "Jahed Abedi"
                    }
                ],
                "author_detail": {
                    "name": "Jahed Abedi"
                },
                "author": "Jahed Abedi",
                "arxiv_comment": "Accepted for publication in Classical and Quantum Gravity. 16 pages,\n  6 figures, 1 table plus supplemental. Data available at\n  https://github.com/jahedabedi/Search-for-echoes-on-the-edge-of-quantum-black-holes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.00025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.00025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05702v2",
                "updated": "2025-08-31T15:44:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    44,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-07T01:10:28Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    1,
                    10,
                    28,
                    3,
                    219,
                    0
                ],
                "title": "Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control"
                },
                "summary": "The increasing penetration of Distributed Energy Resources (DERs), widespread\nadoption of Electric Vehicles (EVs), and the growing frequency of extreme\nweather events have significantly increased the complexity of power grid\nplanning, operation, and management. Traditional rule-based systems and\nnumerical optimization approaches often struggle with the scale, dynamics, and\nadaptability required by modern power networks. This paper introduces\nGrid-Agent, an autonomous, AI-driven framework that combines Large Language\nModels (LLMs) with multi-agent reinforcement learning to detect and remediate\ngrid violations in real time. Grid-Agent integrates semantic reasoning with\nnumerical precision through a modular agent architecture: a planning agent\ngenerates coordinated action sequences using numerical power flow solvers,\nwhile a validation agent evaluates system stability and action effectiveness\nvia sandboxed execution with safety rollbacks. To ensure scalability,\nGrid-Agent incorporates an adaptive multiscale network representation that\ndynamically selects optimal encoding schemes based on network size and\ncomplexity. The framework enables coordinated violation resolution through\noptimizing switch configurations, battery deployment, and load curtailment\nstrategies. Experimental results in standard IEEE and CIGRE test systems (IEEE\n69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation\nperformance. Additionally, the framework's built-in data collection and\nlearning capabilities enable continuous learning and adaptation to diverse\nnetwork topologies. The autonomous nature of the framework makes it\nparticularly suitable for modern smart grid applications requiring rapid\nresponse to dynamic operating conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing penetration of Distributed Energy Resources (DERs), widespread\nadoption of Electric Vehicles (EVs), and the growing frequency of extreme\nweather events have significantly increased the complexity of power grid\nplanning, operation, and management. Traditional rule-based systems and\nnumerical optimization approaches often struggle with the scale, dynamics, and\nadaptability required by modern power networks. This paper introduces\nGrid-Agent, an autonomous, AI-driven framework that combines Large Language\nModels (LLMs) with multi-agent reinforcement learning to detect and remediate\ngrid violations in real time. Grid-Agent integrates semantic reasoning with\nnumerical precision through a modular agent architecture: a planning agent\ngenerates coordinated action sequences using numerical power flow solvers,\nwhile a validation agent evaluates system stability and action effectiveness\nvia sandboxed execution with safety rollbacks. To ensure scalability,\nGrid-Agent incorporates an adaptive multiscale network representation that\ndynamically selects optimal encoding schemes based on network size and\ncomplexity. The framework enables coordinated violation resolution through\noptimizing switch configurations, battery deployment, and load curtailment\nstrategies. Experimental results in standard IEEE and CIGRE test systems (IEEE\n69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation\nperformance. Additionally, the framework's built-in data collection and\nlearning capabilities enable continuous learning and adaptation to diverse\nnetwork topologies. The autonomous nature of the framework makes it\nparticularly suitable for modern smart grid applications requiring rapid\nresponse to dynamic operating conditions."
                },
                "authors": [
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.11361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11361v3",
                "updated": "2025-09-02T12:01:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    12,
                    1,
                    24,
                    1,
                    245,
                    0
                ],
                "published": "2025-07-15T14:28:23Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    14,
                    28,
                    23,
                    1,
                    196,
                    0
                ],
                "title": "Adaptive Robust Optimization for European Electricity System Planning\n  Considering Regional Dunkelflaute Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Robust Optimization for European Electricity System Planning\n  Considering Regional Dunkelflaute Events"
                },
                "summary": "This study provides insights on how to plan a weather robust energy system\nconsisting of several interconnected regions. We develop a capacity expansion\nmodel for a fully decarbonized European electricity system using an adaptive\nrobust optimization framework. The model endogenously identifies the worst\nregional Dunkelflaute events, prolonged periods of low wind and solar\navailability, and incorporates multiple extreme weather realizations within a\nsingle optimization run. Results show that system costs rise nonlinearly with\nthe geographic extent of these events: a single worst-case regional disruption\nincreases costs by 9%, but broader disruptions across multiple regions lead to\nmuch sharper increases, up to 51%. As Dunkelflaute conditions extend across\nmost of Europe, additional cost impacts level off, with a maximum increase of\n71%. The optimal technology mix evolves with the severity of weather stress:\nwhile renewables, batteries, and interregional transmission are sufficient to\nmanage localized events, large-scale disruptions require long-term hydrogen\nstorage and load shedding to maintain system resilience. Central European\nregions, especially Germany and France, emerge as systemic bottlenecks, while\nperipheral regions bear the cost of compensatory overbuilding. These findings\nunderscore the need for a coordinated European policy strategy that goes beyond\nnational planning to support cross-border infrastructure investment, scale up\nflexible technologies such as long-duration storage, and promote a\ngeographically balanced deployment of renewables to mitigate systemic risks\nassociated with Dunkelflaute events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides insights on how to plan a weather robust energy system\nconsisting of several interconnected regions. We develop a capacity expansion\nmodel for a fully decarbonized European electricity system using an adaptive\nrobust optimization framework. The model endogenously identifies the worst\nregional Dunkelflaute events, prolonged periods of low wind and solar\navailability, and incorporates multiple extreme weather realizations within a\nsingle optimization run. Results show that system costs rise nonlinearly with\nthe geographic extent of these events: a single worst-case regional disruption\nincreases costs by 9%, but broader disruptions across multiple regions lead to\nmuch sharper increases, up to 51%. As Dunkelflaute conditions extend across\nmost of Europe, additional cost impacts level off, with a maximum increase of\n71%. The optimal technology mix evolves with the severity of weather stress:\nwhile renewables, batteries, and interregional transmission are sufficient to\nmanage localized events, large-scale disruptions require long-term hydrogen\nstorage and load shedding to maintain system resilience. Central European\nregions, especially Germany and France, emerge as systemic bottlenecks, while\nperipheral regions bear the cost of compensatory overbuilding. These findings\nunderscore the need for a coordinated European policy strategy that goes beyond\nnational planning to support cross-border infrastructure investment, scale up\nflexible technologies such as long-duration storage, and promote a\ngeographically balanced deployment of renewables to mitigate systemic risks\nassociated with Dunkelflaute events."
                },
                "authors": [
                    {
                        "name": "Maximilian Bernecker"
                    },
                    {
                        "name": "Smaranda Sgarciu"
                    },
                    {
                        "name": "Xiaoming Kan"
                    },
                    {
                        "name": "Mehrnaz Anvari"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Felix Müsgens"
                    }
                ],
                "author_detail": {
                    "name": "Felix Müsgens"
                },
                "author": "Felix Müsgens",
                "arxiv_comment": "Code and data can be found on github:\n  https://github.com/bernemax/ARO_Dunkelflaute_Europe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14146v2",
                "updated": "2025-09-02T11:28:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    28,
                    27,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-19T16:37:19Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    37,
                    19,
                    1,
                    231,
                    0
                ],
                "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation"
                },
                "summary": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07412v2",
                "updated": "2025-09-02T08:58:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    58,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-06-09T04:16:39Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    4,
                    16,
                    39,
                    0,
                    160,
                    0
                ],
                "title": "Compressed Feature Quality Assessment: Dataset and Baselines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Feature Quality Assessment: Dataset and Baselines"
                },
                "summary": "The widespread deployment of large models in resource-constrained\nenvironments has underscored the need for efficient transmission of\nintermediate feature representations. In this context, feature coding, which\ncompresses features into compact bitstreams, becomes a critical component for\nscenarios involving feature transmission, storage, and reuse. However, this\ncompression process inevitably introduces semantic degradation that is\ndifficult to quantify with traditional metrics. To address this, we formalize\nthe research problem of Compressed Feature Quality Assessment (CFQA), aiming to\nevaluate the semantic fidelity of compressed features. To advance CFQA\nresearch, we propose the first benchmark dataset, comprising 300 original\nfeatures and 12000 compressed features derived from three vision tasks and four\nfeature codecs. Task-specific performance degradation is provided as true\nsemantic distortion for evaluating CFQA metrics. We systematically assess three\nwidely used metrics -- MSE, cosine similarity, and Centered Kernel Alignment\n(CKA) -- in terms of their ability to capture semantic degradation. Our\nfindings demonstrate the representativeness of the proposed dataset while\nunderscoring the need for more sophisticated metrics capable of measuring\nsemantic distortion in compressed features. This work advances the field by\nestablishing a foundational benchmark and providing a critical resource for the\ncommunity to explore CFQA. To foster further research, we release the dataset\nand all associated source code at\nhttps://github.com/chansongoal/Compressed-Feature-Quality-Assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of large models in resource-constrained\nenvironments has underscored the need for efficient transmission of\nintermediate feature representations. In this context, feature coding, which\ncompresses features into compact bitstreams, becomes a critical component for\nscenarios involving feature transmission, storage, and reuse. However, this\ncompression process inevitably introduces semantic degradation that is\ndifficult to quantify with traditional metrics. To address this, we formalize\nthe research problem of Compressed Feature Quality Assessment (CFQA), aiming to\nevaluate the semantic fidelity of compressed features. To advance CFQA\nresearch, we propose the first benchmark dataset, comprising 300 original\nfeatures and 12000 compressed features derived from three vision tasks and four\nfeature codecs. Task-specific performance degradation is provided as true\nsemantic distortion for evaluating CFQA metrics. We systematically assess three\nwidely used metrics -- MSE, cosine similarity, and Centered Kernel Alignment\n(CKA) -- in terms of their ability to capture semantic degradation. Our\nfindings demonstrate the representativeness of the proposed dataset while\nunderscoring the need for more sophisticated metrics capable of measuring\nsemantic distortion in compressed features. This work advances the field by\nestablishing a foundational benchmark and providing a critical resource for the\ncommunity to explore CFQA. To foster further research, we release the dataset\nand all associated source code at\nhttps://github.com/chansongoal/Compressed-Feature-Quality-Assessment."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Guosheng Lin"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16495v2",
                "updated": "2025-09-02T08:47:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    47,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-06-19T17:43:32Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    17,
                    43,
                    32,
                    3,
                    170,
                    0
                ],
                "title": "DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced\n  Distribution Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced\n  Distribution Transformation"
                },
                "summary": "Like image coding in visual data transmission, feature coding is essential\nfor the distributed deployment of large models by significantly reducing\ntransmission and storage burden. However, prior studies have mostly targeted\ntask- or model-specific scenarios, leaving the challenge of universal feature\ncoding across diverse large models largely unexplored. In this paper, we\npresent the first systematic study on universal feature coding for large\nmodels. The key challenge lies in the inherently diverse and distributionally\nincompatible nature of features extracted from different models. For example,\nfeatures from DINOv2 exhibit highly peaky, concentrated distributions, while\nthose from Stable Diffusion 3 (SD3) are more dispersed and uniform. This\ndistributional heterogeneity severely hampers both compression efficiency and\ncross-model generalization. To address this, we propose a learned\npeaky-to-balanced distribution transformation, which reshapes highly skewed\nfeature distributions into a common, balanced target space. This transformation\nis non-uniform, data-driven, and plug-and-play, enabling effective alignment of\nheterogeneous distributions without modifying downstream codecs. With this\nalignment, a universal codec trained on the balanced target distribution can\neffectively generalize to features from different models and tasks. We validate\nour approach on three representative large models (LLaMA3, DINOv2, and SD3)\nacross multiple tasks and modalities. Extensive experiments show that our\nmethod achieves notable improvements in both compression efficiency and\ncross-model generalization over task-specific baselines. All source code has\nbeen made available at https://github.com/chansongoal/DT-UFC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Like image coding in visual data transmission, feature coding is essential\nfor the distributed deployment of large models by significantly reducing\ntransmission and storage burden. However, prior studies have mostly targeted\ntask- or model-specific scenarios, leaving the challenge of universal feature\ncoding across diverse large models largely unexplored. In this paper, we\npresent the first systematic study on universal feature coding for large\nmodels. The key challenge lies in the inherently diverse and distributionally\nincompatible nature of features extracted from different models. For example,\nfeatures from DINOv2 exhibit highly peaky, concentrated distributions, while\nthose from Stable Diffusion 3 (SD3) are more dispersed and uniform. This\ndistributional heterogeneity severely hampers both compression efficiency and\ncross-model generalization. To address this, we propose a learned\npeaky-to-balanced distribution transformation, which reshapes highly skewed\nfeature distributions into a common, balanced target space. This transformation\nis non-uniform, data-driven, and plug-and-play, enabling effective alignment of\nheterogeneous distributions without modifying downstream codecs. With this\nalignment, a universal codec trained on the balanced target distribution can\neffectively generalize to features from different models and tasks. We validate\nour approach on three representative large models (LLaMA3, DINOv2, and SD3)\nacross multiple tasks and modalities. Extensive experiments show that our\nmethod achieves notable improvements in both compression efficiency and\ncross-model generalization over task-specific baselines. All source code has\nbeen made available at https://github.com/chansongoal/DT-UFC."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11452v2",
                "updated": "2025-09-02T08:20:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    20,
                    59,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-15T13:00:07Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    13,
                    0,
                    7,
                    4,
                    227,
                    0
                ],
                "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models\n  with Real-World Apps"
                },
                "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://www.tbox.cn/about/model-ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://www.tbox.cn/about/model-ranking."
                },
                "authors": [
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Hongliang He"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Ruiqi Liang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Our platform is publicly accessible at\n  https://www.tbox.cn/about/model-ranking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19855v3",
                "updated": "2025-09-03T02:56:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    2,
                    56,
                    22,
                    2,
                    246,
                    0
                ],
                "published": "2025-08-27T13:13:20Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    13,
                    13,
                    20,
                    2,
                    239,
                    0
                ],
                "title": "Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented\n  Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented\n  Complex Reasoning"
                },
                "summary": "Graph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema."
                },
                "authors": [
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Yifei Yu"
                    },
                    {
                        "name": "Qian-Wen Zhang"
                    },
                    {
                        "name": "Linhao Luo"
                    },
                    {
                        "name": "Xiao Huang"
                    },
                    {
                        "name": "Yunsheng Wu"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "19 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14212v3",
                "updated": "2025-09-03T03:06:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    3,
                    6,
                    20,
                    2,
                    246,
                    0
                ],
                "published": "2025-04-19T07:36:02Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    7,
                    36,
                    2,
                    5,
                    109,
                    0
                ],
                "title": "Bias Analysis and Mitigation through Protected Attribute Detection and\n  Regard Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Analysis and Mitigation through Protected Attribute Detection and\n  Regard Classification"
                },
                "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus."
                },
                "authors": [
                    {
                        "name": "Takuma Udagawa"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Hiroshi Kanayama"
                    },
                    {
                        "name": "Bishwaranjan Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Bishwaranjan Bhattacharjee"
                },
                "author": "Bishwaranjan Bhattacharjee",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04307v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04307v4",
                "updated": "2025-09-02T07:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    7,
                    20,
                    33,
                    1,
                    245,
                    0
                ],
                "published": "2024-12-05T16:26:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark"
                },
                "summary": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding is required to\nreduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three fundamental\ncontributions. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nprovide a foundation for future research and inspire broader engagement in this\nfield. To support a long-term study, all source code and the dataset are made\navailable at\n\\href{https://github.com/chansongoal/LaMoFC}{https://github.com/chansongoal/LaMoFC}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding is required to\nreduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three fundamental\ncontributions. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nprovide a foundation for future research and inspire broader engagement in this\nfield. To support a long-term study, all source code and the dataset are made\navailable at\n\\href{https://github.com/chansongoal/LaMoFC}{https://github.com/chansongoal/LaMoFC}."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Yifan Ma"
                    },
                    {
                        "name": "Qiaoxi Chen"
                    },
                    {
                        "name": "Yenan Xu"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04307v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04307v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14589v2",
                "updated": "2025-09-02T05:09:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    5,
                    9,
                    17,
                    1,
                    245,
                    0
                ],
                "published": "2025-06-17T14:52:50Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    14,
                    52,
                    50,
                    1,
                    168,
                    0
                ],
                "title": "NetRoller: Interfacing General and Specialized Models for End-to-End\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetRoller: Interfacing General and Specialized Models for End-to-End\n  Autonomous Driving"
                },
                "summary": "Integrating General Models (GMs) such as Large Language Models (LLMs), with\nSpecialized Models (SMs) in autonomous driving tasks presents a promising\napproach to mitigating challenges in data diversity and model capacity of\nexisting specialized driving models. However, this integration leads to\nproblems of asynchronous systems, which arise from the distinct characteristics\ninherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an\nadapter that incorporates a set of novel mechanisms to facilitate the seamless\nintegration of GMs and specialized driving models. Specifically, our mechanisms\nfor interfacing the asynchronous GMs and SMs are organized into three key\nstages. NetRoller first harvests semantically rich and computationally\nefficient representations from the reasoning processes of LLMs using an early\nstopping mechanism, which preserves critical insights on driving context while\nmaintaining low overhead. It then applies learnable query embeddings,\nnonsensical embeddings, and positional layer embeddings to facilitate robust\nand efficient cross-modality translation. At last, it employs computationally\nefficient Query Shift and Feature Shift mechanisms to enhance the performance\nof SMs through few-epoch fine-tuning. Based on the mechanisms formalized in\nthese three stages, NetRoller enables specialized driving models to operate at\ntheir native frequencies while maintaining situational awareness of the GM.\nExperiments conducted on the nuScenes dataset demonstrate that integrating GM\nthrough NetRoller significantly improves human similarity and safety in\nplanning tasks, and it also achieves noticeable precision improvements in\ndetection and mapping tasks for end-to-end autonomous driving. The code and\nmodels are available at https://github.com/Rex-sys-hk/NetRoller .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating General Models (GMs) such as Large Language Models (LLMs), with\nSpecialized Models (SMs) in autonomous driving tasks presents a promising\napproach to mitigating challenges in data diversity and model capacity of\nexisting specialized driving models. However, this integration leads to\nproblems of asynchronous systems, which arise from the distinct characteristics\ninherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an\nadapter that incorporates a set of novel mechanisms to facilitate the seamless\nintegration of GMs and specialized driving models. Specifically, our mechanisms\nfor interfacing the asynchronous GMs and SMs are organized into three key\nstages. NetRoller first harvests semantically rich and computationally\nefficient representations from the reasoning processes of LLMs using an early\nstopping mechanism, which preserves critical insights on driving context while\nmaintaining low overhead. It then applies learnable query embeddings,\nnonsensical embeddings, and positional layer embeddings to facilitate robust\nand efficient cross-modality translation. At last, it employs computationally\nefficient Query Shift and Feature Shift mechanisms to enhance the performance\nof SMs through few-epoch fine-tuning. Based on the mechanisms formalized in\nthese three stages, NetRoller enables specialized driving models to operate at\ntheir native frequencies while maintaining situational awareness of the GM.\nExperiments conducted on the nuScenes dataset demonstrate that integrating GM\nthrough NetRoller significantly improves human similarity and safety in\nplanning tasks, and it also achieves noticeable precision improvements in\ndetection and mapping tasks for end-to-end autonomous driving. The code and\nmodels are available at https://github.com/Rex-sys-hk/NetRoller ."
                },
                "authors": [
                    {
                        "name": "Ren Xin"
                    },
                    {
                        "name": "Hongji Liu"
                    },
                    {
                        "name": "Xiaodong Mei"
                    },
                    {
                        "name": "Wenru Liu"
                    },
                    {
                        "name": "Maosheng Ye"
                    },
                    {
                        "name": "Zhili Chen"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16734v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16734v3",
                "updated": "2025-09-02T05:02:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    5,
                    2,
                    54,
                    1,
                    245,
                    0
                ],
                "published": "2025-01-28T06:19:29Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    19,
                    29,
                    1,
                    28,
                    0
                ],
                "title": "Distilling Large Language Models for Network Active Queue Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Large Language Models for Network Active Queue Management"
                },
                "summary": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of network traffic and demand for ultra-low latency\ncommunication require smarter packet traffic management. Existing Deep\nLearning-based queuing approaches struggle with dynamic network scenarios and\ndemand high engineering effort. We propose AQM-LLM, distilling Large Language\nModels (LLMs) with few-shot learning, contextual understanding, and pattern\nrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimal\nmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,\nand Scalable Throughput (L4S) and our design of AQM-LLM builds on speculative\ndecoding and reinforcement-based distilling of LLM by tackling congestion\nprevention in the L4S architecture using Explicit Congestion Notification (ECN)\n[RFC 9331] and periodic packet dropping. We develop a new open-source\nexperimental platform by executing L4S-AQM on FreeBSD-14, providing\ninteroperable modules to support LLM integration and facilitate IETF\nrecognition through wider testing. Our extensive evaluations show L4S-LLM\nenhances queue management, prevents congestion, reduces latency, and boosts\nnetwork performance, showcasing LLMs' adaptability and efficiency in uplifting\nAQM systems."
                },
                "authors": [
                    {
                        "name": "Shiva Raj Pokhrel"
                    },
                    {
                        "name": "Deol Satish"
                    },
                    {
                        "name": "Jonathan Kua"
                    },
                    {
                        "name": "Anwar Walid"
                    }
                ],
                "author_detail": {
                    "name": "Anwar Walid"
                },
                "author": "Anwar Walid",
                "arxiv_journal_ref": "IEEE Trans on Networking, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16734v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16734v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15044v2",
                "updated": "2025-09-02T04:31:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    4,
                    31,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-20T20:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    10,
                    56,
                    2,
                    232,
                    0
                ],
                "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner"
                },
                "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Yanran Wu"
                    },
                    {
                        "name": "Xinyu Luo"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07778v2",
                "updated": "2025-09-02T03:57:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    57,
                    28,
                    1,
                    245,
                    0
                ],
                "published": "2025-06-09T13:55:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    13,
                    55,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "A Vision-Language Agent System for Compositional Reasoning with\n  VLM-assisted Script and Executable Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Vision-Language Agent System for Compositional Reasoning with\n  VLM-assisted Script and Executable Generation"
                },
                "summary": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal vision-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date offer poor performance\nfor compositional reasoning. This paper presents VLAgent, a vision-language\nagent system for vision-text compositional reasoning with three novel features.\nFirst, VLAgent leverages a pre-trained LLM with few-shot context learning to\ngenerate the planning script for each compositional reasoning task and provides\na backend engine to generate and perform executable runtime, which maps the\nplanning script into executable code using the VLAgent library for VLAgent\nexecutor. Second, VLAgent introduces the SS-parser, which identifies and\ncorrects logic errors embedded in the LLM-generated planning script, to further\nenhance the quality of script-executable mapping. Third, VLAgent introduces the\ncompositional reasoning output verifier, which validates and refines the output\nof complex compositional reasoning steps, by leveraging complementary reasoning\ntechniques, e.g., ensemble learning and caption analysis. Extensive experiments\nare conducted on six visual benchmarks and compared to a dozen of the SoTA\nvisual reasoning models. The results show that VLAgent outperforms existing\nrepresentative approaches for compositional text-visual reasoning. Our code and\ndatasets with outputs will be made available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal vision-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date offer poor performance\nfor compositional reasoning. This paper presents VLAgent, a vision-language\nagent system for vision-text compositional reasoning with three novel features.\nFirst, VLAgent leverages a pre-trained LLM with few-shot context learning to\ngenerate the planning script for each compositional reasoning task and provides\na backend engine to generate and perform executable runtime, which maps the\nplanning script into executable code using the VLAgent library for VLAgent\nexecutor. Second, VLAgent introduces the SS-parser, which identifies and\ncorrects logic errors embedded in the LLM-generated planning script, to further\nenhance the quality of script-executable mapping. Third, VLAgent introduces the\ncompositional reasoning output verifier, which validates and refines the output\nof complex compositional reasoning steps, by leveraging complementary reasoning\ntechniques, e.g., ensemble learning and caption analysis. Extensive experiments\nare conducted on six visual benchmarks and compared to a dozen of the SoTA\nvisual reasoning models. The results show that VLAgent outperforms existing\nrepresentative approaches for compositional text-visual reasoning. Our code and\ndatasets with outputs will be made available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yichang Xu"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Ramana Rao Kompella"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Fatih Ilhan"
                    },
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Zachary Yahn"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11423v3",
                "updated": "2025-09-02T02:55:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    2,
                    55,
                    17,
                    1,
                    245,
                    0
                ],
                "published": "2025-05-16T16:36:00Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    16,
                    36,
                    0,
                    4,
                    136,
                    0
                ],
                "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following\n  in LLMs"
                },
                "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Xupeng Chen"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Narayanan Sadagopan"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18190v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18190v3",
                "updated": "2025-09-02T02:30:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    2,
                    30,
                    46,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-25T16:48:51Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    48,
                    51,
                    0,
                    237,
                    0
                ],
                "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering"
                },
                "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor."
                },
                "authors": [
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Boyu Niu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Boxiu Li"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18190v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18190v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17967v2",
                "updated": "2025-09-02T02:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    2,
                    28,
                    24,
                    1,
                    245,
                    0
                ],
                "published": "2025-02-25T08:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    41,
                    1,
                    1,
                    56,
                    0
                ],
                "title": "Agent Trading Arena: A Study on Numerical Understanding in LLM-Based\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Trading Arena: A Study on Numerical Understanding in LLM-Based\n  Agents"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language tasks, yet their performance in dynamic, real-world financial\nenvironments remains underexplored. Existing approaches are limited to\nhistorical backtesting, where trading actions cannot influence market prices\nand agents train only on static data. To address this limitation, we present\nthe Agent Trading Arena, a virtual zero-sum stock market in which LLM-based\nagents engage in competitive multi-agent trading and directly impact price\ndynamics. By simulating realistic bid-ask interactions, our platform enables\ntraining in scenarios that closely mirror live markets, thereby narrowing the\ngap between training and evaluation. Experiments reveal that LLMs struggle with\nnumerical reasoning when given plain-text data, often overfitting to local\npatterns and recent values. In contrast, chart-based visualizations\nsignificantly enhance both numerical reasoning and trading performance.\nFurthermore, incorporating a reflection module yields additional improvements,\nespecially with visual inputs. Evaluations on NASDAQ and CSI datasets\ndemonstrate the superiority of our method, particularly under high volatility.\nAll code and data are available at\nhttps://github.com/wekjsdvnm/Agent-Trading-Arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language tasks, yet their performance in dynamic, real-world financial\nenvironments remains underexplored. Existing approaches are limited to\nhistorical backtesting, where trading actions cannot influence market prices\nand agents train only on static data. To address this limitation, we present\nthe Agent Trading Arena, a virtual zero-sum stock market in which LLM-based\nagents engage in competitive multi-agent trading and directly impact price\ndynamics. By simulating realistic bid-ask interactions, our platform enables\ntraining in scenarios that closely mirror live markets, thereby narrowing the\ngap between training and evaluation. Experiments reveal that LLMs struggle with\nnumerical reasoning when given plain-text data, often overfitting to local\npatterns and recent values. In contrast, chart-based visualizations\nsignificantly enhance both numerical reasoning and trading performance.\nFurthermore, incorporating a reflection module yields additional improvements,\nespecially with visual inputs. Evaluations on NASDAQ and CSI datasets\ndemonstrate the superiority of our method, particularly under high volatility.\nAll code and data are available at\nhttps://github.com/wekjsdvnm/Agent-Trading-Arena."
                },
                "authors": [
                    {
                        "name": "Tianmi Ma"
                    },
                    {
                        "name": "Jiawei Du"
                    },
                    {
                        "name": "Wenxin Huang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Xian Zhong"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Joey Tianyi Zhou"
                },
                "author": "Joey Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16124v2",
                "updated": "2025-09-01T23:52:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    23,
                    52,
                    31,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-22T00:36:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    0,
                    36,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making"
                },
                "summary": "While robots have previously utilized rule-based systems or probabilistic\nmodels for user interaction, the rapid evolution of large language models\n(LLMs) presents new opportunities to develop LLM-powered robots for enhanced\nhuman-robot interaction (HRI). To fully realize these capabilities, however,\nrobots need to collect data such as audio, fine-grained images, video, and\nlocations. As a result, LLMs often process sensitive personal information,\nparticularly within private environments, such as homes. Given the tension\nbetween utility and privacy risks, evaluating how current LLMs manage sensitive\ndata is critical. Specifically, we aim to explore the extent to which\nout-of-the-box LLMs are privacy-aware in the context of household robots. In\nthis work, we present a set of privacy-relevant scenarios developed using the\nContextual Integrity (CI) framework. We first surveyed users' privacy\npreferences regarding in-home robot behaviors and then examined how their\nprivacy orientations affected their choices of these behaviors (N = 450). We\nthen provided the same set of scenarios and questions to state-of-the-art LLMs\n(N = 10) and found that the agreement between humans and LLMs was generally\nlow. To further investigate the capabilities of LLMs as potential privacy\ncontrollers, we implemented four additional prompting strategies and compared\ntheir results. We discuss the performance of the evaluated models as well as\nthe implications and potential of AI privacy awareness in human-robot\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While robots have previously utilized rule-based systems or probabilistic\nmodels for user interaction, the rapid evolution of large language models\n(LLMs) presents new opportunities to develop LLM-powered robots for enhanced\nhuman-robot interaction (HRI). To fully realize these capabilities, however,\nrobots need to collect data such as audio, fine-grained images, video, and\nlocations. As a result, LLMs often process sensitive personal information,\nparticularly within private environments, such as homes. Given the tension\nbetween utility and privacy risks, evaluating how current LLMs manage sensitive\ndata is critical. Specifically, we aim to explore the extent to which\nout-of-the-box LLMs are privacy-aware in the context of household robots. In\nthis work, we present a set of privacy-relevant scenarios developed using the\nContextual Integrity (CI) framework. We first surveyed users' privacy\npreferences regarding in-home robot behaviors and then examined how their\nprivacy orientations affected their choices of these behaviors (N = 450). We\nthen provided the same set of scenarios and questions to state-of-the-art LLMs\n(N = 10) and found that the agreement between humans and LLMs was generally\nlow. To further investigate the capabilities of LLMs as potential privacy\ncontrollers, we implemented four additional prompting strategies and compared\ntheir results. We discuss the performance of the evaluated models as well as\nthe implications and potential of AI privacy awareness in human-robot\ninteraction."
                },
                "authors": [
                    {
                        "name": "Dakota Sullivan"
                    },
                    {
                        "name": "Shirley Zhang"
                    },
                    {
                        "name": "Jennica Li"
                    },
                    {
                        "name": "Heather Kirkorian"
                    },
                    {
                        "name": "Bilge Mutlu"
                    },
                    {
                        "name": "Kassem Fawaz"
                    }
                ],
                "author_detail": {
                    "name": "Kassem Fawaz"
                },
                "author": "Kassem Fawaz",
                "arxiv_comment": "18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed\n  equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07482v2",
                "updated": "2025-09-01T21:27:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    21,
                    27,
                    15,
                    0,
                    244,
                    0
                ],
                "published": "2024-09-03T06:21:26Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    6,
                    21,
                    26,
                    1,
                    247,
                    0
                ],
                "title": "VSLLaVA: a pipeline of large multimodal foundation model for industrial\n  vibration signal analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSLLaVA: a pipeline of large multimodal foundation model for industrial\n  vibration signal analysis"
                },
                "summary": "While Large Multimodal Models (LMMs) excel in general multimodal tasks, they\nlack the domain-specific knowledge for industrial vibration signal analysis.\nThis paper introduces VSLLaVA, a comprehensive pipeline that utilizes expert\nknowledge-guided instruction tuning and evaluation to create an end-to-end LMM\nfor signal analysis. To achieve this, we construct a novel\nSignal-Question-Answer (SQA) dataset using an expert rule-based signal\ngenerator. This dataset facilitates a two-stage learning procedure. The first\nstep is efficient instruction fine-tuning with Low-Rank Adaptation (LoRA),\nwhich imparts specialized signal identification capabilities. Subsequently, we\ndesigned a tailored Group Relative Policy Optimization (GRPO) to refine the\nreasoning capabilities and enhance classification robustness. Then, a dual-mode\nevaluation framework is proposed, combining an LLM referee with expert rules\nfor semantic assessment using quantitative metrics for numerical and textual\naccuracy, which reveals that VSLLaVA significantly improves performance in\nsignal type identification and parameter analysis, and makes progress in the\nidentification and parameter analysis of fault-related signals. This research\ndemonstrates a viable approach for developing specialized foundational models\nfor complex industrial applications and marks a transition from conventional\ntask-specific systems to a cohesive, interactive foundational model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Multimodal Models (LMMs) excel in general multimodal tasks, they\nlack the domain-specific knowledge for industrial vibration signal analysis.\nThis paper introduces VSLLaVA, a comprehensive pipeline that utilizes expert\nknowledge-guided instruction tuning and evaluation to create an end-to-end LMM\nfor signal analysis. To achieve this, we construct a novel\nSignal-Question-Answer (SQA) dataset using an expert rule-based signal\ngenerator. This dataset facilitates a two-stage learning procedure. The first\nstep is efficient instruction fine-tuning with Low-Rank Adaptation (LoRA),\nwhich imparts specialized signal identification capabilities. Subsequently, we\ndesigned a tailored Group Relative Policy Optimization (GRPO) to refine the\nreasoning capabilities and enhance classification robustness. Then, a dual-mode\nevaluation framework is proposed, combining an LLM referee with expert rules\nfor semantic assessment using quantitative metrics for numerical and textual\naccuracy, which reveals that VSLLaVA significantly improves performance in\nsignal type identification and parameter analysis, and makes progress in the\nidentification and parameter analysis of fault-related signals. This research\ndemonstrates a viable approach for developing specialized foundational models\nfor complex industrial applications and marks a transition from conventional\ntask-specific systems to a cohesive, interactive foundational model."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xinran Zhang"
                    },
                    {
                        "name": "Jinfeng Huang"
                    },
                    {
                        "name": "Hongliang He"
                    },
                    {
                        "name": "Feibin Zhang"
                    },
                    {
                        "name": "Zhaoye Qin"
                    },
                    {
                        "name": "Fulei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Fulei Chu"
                },
                "author": "Fulei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14201v2",
                "updated": "2025-09-01T20:02:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    20,
                    2,
                    0,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-14T17:06:26Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    6,
                    26,
                    0,
                    195,
                    0
                ],
                "title": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation"
                },
                "summary": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on\nthe task of Cyber Threat Investigation through security questions derived from\ninvestigation graphs. Real-world security analysts must sift through a large\nnumber of heterogeneous alert signals and security logs, follow multi-hop\nchains of evidence, and compile an incident report. With the developments of\nLLMs, building LLM-based agents for automatic thread investigation is a\npromising direction. To assist the development and evaluation of LLM agents, we\nconstruct a dataset from a controlled Azure tenant that covers 8 simulated\nreal-world multi-step attacks, 57 log tables from Microsoft Sentinel and\nrelated services, and 589 automatically generated questions. We leverage\nsecurity logs extracted with expert-crafted detection logic to build threat\ninvestigation graphs, and then generate questions with LLMs using paired nodes\non the graph, taking the start node as background context and the end node as\nanswer. Anchoring each question to these explicit nodes and edges not only\nprovides automatic, explainable ground truth answers but also makes the\npipeline reusable and readily extensible to new logs. This also enables the\nautomatic generation of procedural tasks with verifiable rewards, which can be\nnaturally extended to training agents via reinforcement learning. Our\ncomprehensive experiments with different models confirm the difficulty of the\ntask: with the base setting, the average reward across all evaluated models is\n0.249, and the best achieved is 0.368, leaving substantial headroom for future\nresearch. Code and data are coming soon!"
                },
                "authors": [
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Mauricio Velazco"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Manuel Raúl Meléndez Luján"
                    },
                    {
                        "name": "Srisuma Movva"
                    },
                    {
                        "name": "Yogesh K Roy"
                    },
                    {
                        "name": "Quang Nguyen"
                    },
                    {
                        "name": "Roberto Rodriguez"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Michael Albada"
                    },
                    {
                        "name": "Julia Kiseleva"
                    },
                    {
                        "name": "Anand Mudgerikar"
                    }
                ],
                "author_detail": {
                    "name": "Anand Mudgerikar"
                },
                "author": "Anand Mudgerikar",
                "arxiv_comment": "Add code link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11614v3",
                "updated": "2025-09-01T18:50:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    50,
                    34,
                    0,
                    244,
                    0
                ],
                "published": "2024-06-17T15:00:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    0,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Intrinsic Test of Unlearning Using Parametric Knowledge Traces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrinsic Test of Unlearning Using Parametric Knowledge Traces"
                },
                "summary": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of \"unlearning\" certain concepts in large language models (LLMs) has\nattracted immense attention recently, due to its importance in mitigating\nundesirable model behaviours, such as the generation of harmful, private, or\nincorrect information. Current protocols to evaluate unlearning methods largely\nrely on behavioral tests, without monitoring the presence of unlearned\nknowledge within the model's parameters. This residual knowledge can be\nadversarially exploited to recover the erased information post-unlearning. We\nargue that unlearning should also be evaluated internally, by considering\nchanges in the parametric knowledge traces of the unlearned concepts. To this\nend, we propose a general evaluation methodology that leverages vocabulary\nprojections to inspect concepts encoded in model parameters. We use this\napproach to localize \"concept vectors\" - parameter vectors that encode concrete\nconcepts - and construct ConceptVectors, a benchmark dataset containing\nhundreds of common concepts and their parametric knowledge traces within two\nopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearning\nmethods minimally impact concept vectors and mostly suppress them during\ninference, while directly ablating these vectors demonstrably removes the\nassociated knowledge and significantly reduces the model's susceptibility to\nadversarial manipulation. Our results highlight limitations in behavioral-based\nunlearning evaluations and call for future work to include parameter-based\nevaluations. To support this, we release our code and benchmark at\nhttps://github.com/yihuaihong/ConceptVectors."
                },
                "authors": [
                    {
                        "name": "Yihuai Hong"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Haiqin Yang"
                    },
                    {
                        "name": "Shauli Ravfogel"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted in EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05287v2",
                "updated": "2025-09-01T18:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    49,
                    59,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-08T14:29:00Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    29,
                    0,
                    3,
                    128,
                    0
                ],
                "title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous\n  Bimanual Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphologically Symmetric Reinforcement Learning for Ambidextrous\n  Bimanual Manipulation"
                },
                "summary": "Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Zechu Li"
                    },
                    {
                        "name": "Yufeng Jin"
                    },
                    {
                        "name": "Daniel Ordonez Apraez"
                    },
                    {
                        "name": "Claudio Semini"
                    },
                    {
                        "name": "Puze Liu"
                    },
                    {
                        "name": "Georgia Chalvatzaki"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Chalvatzaki"
                },
                "author": "Georgia Chalvatzaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03275v2",
                "updated": "2025-09-01T18:25:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    25,
                    38,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-05T15:33:00Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    33,
                    0,
                    2,
                    36,
                    0
                ],
                "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks."
                },
                "authors": [
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Yingchen Xu"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Qinqing Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Qinqing Zheng"
                },
                "author": "Qinqing Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20931v2",
                "updated": "2025-09-01T18:05:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    18,
                    5,
                    6,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-28T15:57:33Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    15,
                    57,
                    33,
                    3,
                    240,
                    0
                ],
                "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $τ$-bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $τ$-bench"
                },
                "summary": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Venkatesh Mishra"
                    },
                    {
                        "name": "Amir Saeidi"
                    },
                    {
                        "name": "Satyam Raj"
                    },
                    {
                        "name": "Mutsumi Nakamura"
                    },
                    {
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Chitta Baral"
                    }
                ],
                "author_detail": {
                    "name": "Chitta Baral"
                },
                "author": "Chitta Baral",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03671v3",
                "updated": "2025-09-01T16:10:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    16,
                    10,
                    22,
                    0,
                    244,
                    0
                ],
                "published": "2024-09-05T16:24:42Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "title": "TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling"
                },
                "summary": "We present TRACE-CS, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs)to address contrastive queries in course\nscheduling problems. TRACE-CS leverages logic-based techniques to encode\nscheduling constraints and generate provably correct explanations, while\nutilizing an LLM to process natural language queries and refine logical\nexplanations into user friendly responses. This system showcases how combining\nsymbolic KR methods with LLMs creates explainable AI agents that balance\nlogical correctness with natural language accessibility, addressing a\nfundamental challenge in deployed scheduling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TRACE-CS, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs)to address contrastive queries in course\nscheduling problems. TRACE-CS leverages logic-based techniques to encode\nscheduling constraints and generate provably correct explanations, while\nutilizing an LLM to process natural language queries and refine logical\nexplanations into user friendly responses. This system showcases how combining\nsymbolic KR methods with LLMs creates explainable AI agents that balance\nlogical correctness with natural language accessibility, addressing a\nfundamental challenge in deployed scheduling systems."
                },
                "authors": [
                    {
                        "name": "Stylianos Loukas Vasileiou"
                    },
                    {
                        "name": "William Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "William Yeoh"
                },
                "author": "William Yeoh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17481v2",
                "updated": "2025-09-01T16:04:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    16,
                    4,
                    7,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-24T18:13:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    18,
                    13,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Cybersecurity Assessment of Humanoid Ecosystem"
                },
                "summary": "Humanoids are progressing toward practical deployment across healthcare,\nindustrial, defense, and service sectors. While typically considered\ncyber-physical systems (CPSs), their dependence on traditional networked\nsoftware stacks (e.g., Linux operating systems), robot operating system (ROS)\nmiddleware, and over-the-air update channels, creates a distinct security\nprofile that exposes them to vulnerabilities conventional CPS models do not\nfully address. Prior studies have mainly examined specific threats, such as\nLiDAR spoofing or adversarial machine learning (AML). This narrow focus\noverlooks how an attack targeting one component can cascade harm throughout the\nrobot's interconnected systems. We address this gap through a systematization\nof knowledge (SoK) that takes a comprehensive approach, consolidating\nfragmented research from robotics, CPS, and network security domains. We\nintroduce a seven-layer security model for humanoid robots, organizing 39 known\nattacks and 35 defenses across the humanoid ecosystem-from hardware to\nhuman-robot interaction. Building on this security model, we develop a\nquantitative 39x35 attack-defense matrix with risk-weighted scoring, validated\nthrough Monte Carlo analysis. We demonstrate our method by evaluating three\nreal-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed\nvarying security maturity levels, with scores ranging from 39.9% to 79.5%\nacross the platforms. This work introduces a structured, evidence-based\nassessment method that enables systematic security evaluation, supports\ncross-platform benchmarking, and guides prioritization of security investments\nin humanoid robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoids are progressing toward practical deployment across healthcare,\nindustrial, defense, and service sectors. While typically considered\ncyber-physical systems (CPSs), their dependence on traditional networked\nsoftware stacks (e.g., Linux operating systems), robot operating system (ROS)\nmiddleware, and over-the-air update channels, creates a distinct security\nprofile that exposes them to vulnerabilities conventional CPS models do not\nfully address. Prior studies have mainly examined specific threats, such as\nLiDAR spoofing or adversarial machine learning (AML). This narrow focus\noverlooks how an attack targeting one component can cascade harm throughout the\nrobot's interconnected systems. We address this gap through a systematization\nof knowledge (SoK) that takes a comprehensive approach, consolidating\nfragmented research from robotics, CPS, and network security domains. We\nintroduce a seven-layer security model for humanoid robots, organizing 39 known\nattacks and 35 defenses across the humanoid ecosystem-from hardware to\nhuman-robot interaction. Building on this security model, we develop a\nquantitative 39x35 attack-defense matrix with risk-weighted scoring, validated\nthrough Monte Carlo analysis. We demonstrate our method by evaluating three\nreal-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed\nvarying security maturity levels, with scores ranging from 39.9% to 79.5%\nacross the platforms. This work introduces a structured, evidence-based\nassessment method that enables systematic security evaluation, supports\ncross-platform benchmarking, and guides prioritization of security investments\nin humanoid robotics."
                },
                "authors": [
                    {
                        "name": "Priyanka Prakash Surve"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Yuval Elovici"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Elovici"
                },
                "author": "Yuval Elovici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15695v2",
                "updated": "2025-09-01T15:48:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    48,
                    33,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-21T16:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    44,
                    2,
                    141,
                    0
                ],
                "title": "Can Large Language Models be Effective Online Opinion Miners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models be Effective Online Opinion Miners?"
                },
                "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield."
                },
                "authors": [
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "Yongsik Seo"
                    },
                    {
                        "name": "Junseong Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14880v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14880v3",
                "updated": "2025-09-01T15:33:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    33,
                    47,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-20T17:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    51,
                    20,
                    2,
                    232,
                    0
                ],
                "title": "MedResearcher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedResearcher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework"
                },
                "summary": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts. We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions. Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts. We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions. Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains."
                },
                "authors": [
                    {
                        "name": "Ailing Yu"
                    },
                    {
                        "name": "Lan Yao"
                    },
                    {
                        "name": "Jingnan Liu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Jiajun Yin"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xinhao Liao"
                    },
                    {
                        "name": "Zhiling Ye"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Hansong Xiao"
                    },
                    {
                        "name": "Hualei Zhou"
                    },
                    {
                        "name": "Chunxiao Guo"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14880v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14880v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v4",
                "updated": "2025-09-01T15:29:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    29,
                    28,
                    0,
                    244,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v6",
                "updated": "2025-09-01T14:14:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    14,
                    14,
                    18,
                    0,
                    244,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "arxiv_comment": "Accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11109v2",
                "updated": "2025-09-01T14:13:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    14,
                    13,
                    59,
                    0,
                    244,
                    0
                ],
                "published": "2025-04-15T11:56:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    56,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Agent-Q: Fine-Tuning Large Language Models for Quantum Circuit\n  Generation and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Q: Fine-Tuning Large Language Models for Quantum Circuit\n  Generation and Optimization"
                },
                "summary": "Large language models (LLMs) have achieved remarkable outcomes in complex\nproblems, including math, coding, and analyzing large amounts of scientific\nreports. Yet, few works have explored the potential of LLMs in quantum\ncomputing. The most challenging problem is to leverage LLMs to automatically\ngenerate quantum circuits at a large scale. Fundamentally, the existing\npre-trained LLMs lack the knowledge of quantum circuits. In this paper, we\naddress this challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. We describe Agent-Q, an LLM fine-tuning system\nto generate and optimize quantum circuits. In particular, Agent-Q implements\nthe mechanisms to generate training data sets and constructs an end-to-end\npipeline to fine-tune pre-trained LLMs to generate parameterized quantum\ncircuits for various optimization problems. Agent-Q provides 14,000 quantum\ncircuits covering a large spectrum of the quantum optimization landscape: 12\noptimization problem instances and their optimized QAOA, VQE, and adaptive VQE\ncircuits. Based thereon, Agent-Q fine-tunes LLMs and constructs syntactically\ncorrect parametrized quantum circuits in OpenQASM 3.0. We have evaluated the\nquality of the LLM-generated circuits and parameters by comparing them to the\noptimized expectation values and distributions. Experimental results show\nsuperior performance of Agent-Q, compared to several state-of-the-art LLMs and\nbetter parameters than random. Agent-Q can be integrated into an agentic\nworkflow, and the generated parametrized circuits with initial parameters can\nbe used as a starting point for further optimization, e.g., as templates in\nquantum machine learning and as benchmarks for compilers and hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable outcomes in complex\nproblems, including math, coding, and analyzing large amounts of scientific\nreports. Yet, few works have explored the potential of LLMs in quantum\ncomputing. The most challenging problem is to leverage LLMs to automatically\ngenerate quantum circuits at a large scale. Fundamentally, the existing\npre-trained LLMs lack the knowledge of quantum circuits. In this paper, we\naddress this challenge by fine-tuning LLMs and injecting the domain-specific\nknowledge of quantum computing. We describe Agent-Q, an LLM fine-tuning system\nto generate and optimize quantum circuits. In particular, Agent-Q implements\nthe mechanisms to generate training data sets and constructs an end-to-end\npipeline to fine-tune pre-trained LLMs to generate parameterized quantum\ncircuits for various optimization problems. Agent-Q provides 14,000 quantum\ncircuits covering a large spectrum of the quantum optimization landscape: 12\noptimization problem instances and their optimized QAOA, VQE, and adaptive VQE\ncircuits. Based thereon, Agent-Q fine-tunes LLMs and constructs syntactically\ncorrect parametrized quantum circuits in OpenQASM 3.0. We have evaluated the\nquality of the LLM-generated circuits and parameters by comparing them to the\noptimized expectation values and distributions. Experimental results show\nsuperior performance of Agent-Q, compared to several state-of-the-art LLMs and\nbetter parameters than random. Agent-Q can be integrated into an agentic\nworkflow, and the generated parametrized circuits with initial parameters can\nbe used as a starting point for further optimization, e.g., as templates in\nquantum machine learning and as benchmarks for compilers and hardware."
                },
                "authors": [
                    {
                        "name": "Linus Jern"
                    },
                    {
                        "name": "Valter Uotila"
                    },
                    {
                        "name": "Cong Yu"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "arxiv_comment": "12 pages, 8 figures, 3 tables, presented at IEEE International\n  Conference on Quantum Computing and Engineering (QCE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01378v2",
                "updated": "2025-09-01T13:53:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    53,
                    3,
                    0,
                    244,
                    0
                ],
                "published": "2025-07-02T05:44:17Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    44,
                    17,
                    2,
                    183,
                    0
                ],
                "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms"
                },
                "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems."
                },
                "authors": [
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Sizhao Li"
                    },
                    {
                        "name": "Yuming Xiang"
                    },
                    {
                        "name": "Haiping Wang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10551v3",
                "updated": "2025-09-01T13:47:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    47,
                    40,
                    0,
                    244,
                    0
                ],
                "published": "2025-01-17T20:54:09Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    20,
                    54,
                    9,
                    4,
                    17,
                    0
                ],
                "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study to Understand How Students Use ChatGPT for Writing\n  Essays"
                },
                "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom."
                },
                "authors": [
                    {
                        "name": "Andrew Jelson"
                    },
                    {
                        "name": "Daniel Manesh"
                    },
                    {
                        "name": "Alice Jang"
                    },
                    {
                        "name": "Daniel Dunlap"
                    },
                    {
                        "name": "Young-Ho Kim"
                    },
                    {
                        "name": "Sang Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Lee"
                },
                "author": "Sang Won Lee",
                "arxiv_comment": "19 pages, 10 figures, 2 tables, final preparation for a TOCHI\n  submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07169v2",
                "updated": "2025-09-01T13:40:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    40,
                    8,
                    0,
                    244,
                    0
                ],
                "published": "2025-03-10T10:47:27Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    47,
                    27,
                    0,
                    69,
                    0
                ],
                "title": "Reducing Friction in Cloud Migration of Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Friction in Cloud Migration of Services"
                },
                "summary": "Public cloud services are integral to modern software development, offering\nscalability and flexibility to organizations. Based on customer requests, a\nlarge product development organization considered migrating the\nmicroservice-based product deployments of a large customer to a public cloud\nprovider.\n  We conducted an exploratory single-case study, utilizing quantitative and\nqualitative data analysis to understand how and why deployment costs would\nchange when transitioning the product from a private to a public cloud\nenvironment while preserving the software architecture. We also isolated the\nmajor factors driving the changes in deployment costs.\n  We found that switching to the customer-chosen public cloud provider would\nincrease costs by up to 50\\%, even when sharing some resources between\ndeployments, and limiting the use of expensive cloud services such as security\nlog analyzers. A large part of the cost was related to the sizing and license\ncosts of the existing relational database, which was running on Virtual\nMachines in the cloud. We also found that existing system integrators, using\nthe product via its API, were likely to use the product inefficiently, in many\ncases causing at least 10\\% more load to the system than needed.\n  From a deployment cost perspective, successful migration to a public cloud\nrequires considering the entire system architecture, including services like\nrelational databases, value-added cloud services, and enabled product features.\nOur study highlights the importance of leveraging end-to-end usage data to\nassess and manage these cost drivers effectively, especially in environments\nwith elastic costs, such as public cloud deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public cloud services are integral to modern software development, offering\nscalability and flexibility to organizations. Based on customer requests, a\nlarge product development organization considered migrating the\nmicroservice-based product deployments of a large customer to a public cloud\nprovider.\n  We conducted an exploratory single-case study, utilizing quantitative and\nqualitative data analysis to understand how and why deployment costs would\nchange when transitioning the product from a private to a public cloud\nenvironment while preserving the software architecture. We also isolated the\nmajor factors driving the changes in deployment costs.\n  We found that switching to the customer-chosen public cloud provider would\nincrease costs by up to 50\\%, even when sharing some resources between\ndeployments, and limiting the use of expensive cloud services such as security\nlog analyzers. A large part of the cost was related to the sizing and license\ncosts of the existing relational database, which was running on Virtual\nMachines in the cloud. We also found that existing system integrators, using\nthe product via its API, were likely to use the product inefficiently, in many\ncases causing at least 10\\% more load to the system than needed.\n  From a deployment cost perspective, successful migration to a public cloud\nrequires considering the entire system architecture, including services like\nrelational databases, value-added cloud services, and enabled product features.\nOur study highlights the importance of leveraging end-to-end usage data to\nassess and manage these cost drivers effectively, especially in environments\nwith elastic costs, such as public cloud deployments."
                },
                "authors": [
                    {
                        "name": "Anders Sundelin"
                    },
                    {
                        "name": "Javier Gonzalez-Huerta"
                    },
                    {
                        "name": "Krzysztof Wnuk"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Wnuk"
                },
                "author": "Krzysztof Wnuk",
                "arxiv_comment": "Submitted to JSS In-Practice track Mars 8, 2025. Revision 1\n  resubmitted July 29, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10722v2",
                "updated": "2025-09-01T13:06:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    13,
                    6,
                    47,
                    0,
                    244,
                    0
                ],
                "published": "2024-08-20T10:44:29Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    10,
                    44,
                    29,
                    1,
                    233,
                    0
                ],
                "title": "MEGen: Generative Backdoor into Large Language Models via Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEGen: Generative Backdoor into Large Language Models via Model Editing"
                },
                "summary": "Large language models (LLMs) have exhibited remarkable versatility and\nadaptability, while their widespread adoption across various applications also\nraises critical safety concerns. This paper focuses on the impact of backdoored\nLLMs. Traditional backdoor injection methods are primarily limited to yes-or-no\ndiscriminative tasks, leading users to underestimate the potential risks of\nbackdoored LLMs. Given the inherently generative nature of LLMs, this paper\nreveals that a generative backdoor injected into LLMs can expose the true\nsafety risks in their applications. We propose an editing-based generative\nbackdoor, named MEGen, aiming to expand the backdoor to generative tasks in a\nunified format of any text-to any text, leading to natural generations with a\nspecific intention. Experiments show that MEGen achieves a high attack success\nrate by adjusting only a small set of local parameters with few-shot samples.\nNotably, we show that the backdoored model, when triggered, can freely output\npre-set dangerous information while completing downstream tasks. Our work\nhighlights that MEGen enables backdoors in LLMs to exhibit generative\ncapabilities, causing potential safety risks by altering the generative style.\nThe code is available at https://github.com/MonoQ-hub/MEGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited remarkable versatility and\nadaptability, while their widespread adoption across various applications also\nraises critical safety concerns. This paper focuses on the impact of backdoored\nLLMs. Traditional backdoor injection methods are primarily limited to yes-or-no\ndiscriminative tasks, leading users to underestimate the potential risks of\nbackdoored LLMs. Given the inherently generative nature of LLMs, this paper\nreveals that a generative backdoor injected into LLMs can expose the true\nsafety risks in their applications. We propose an editing-based generative\nbackdoor, named MEGen, aiming to expand the backdoor to generative tasks in a\nunified format of any text-to any text, leading to natural generations with a\nspecific intention. Experiments show that MEGen achieves a high attack success\nrate by adjusting only a small set of local parameters with few-shot samples.\nNotably, we show that the backdoored model, when triggered, can freely output\npre-set dangerous information while completing downstream tasks. Our work\nhighlights that MEGen enables backdoors in LLMs to exhibit generative\ncapabilities, causing potential safety risks by altering the generative style.\nThe code is available at https://github.com/MonoQ-hub/MEGen."
                },
                "authors": [
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qianren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianren Wang"
                },
                "author": "Qianren Wang",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07851v2",
                "updated": "2025-09-01T12:48:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    48,
                    19,
                    0,
                    244,
                    0
                ],
                "published": "2024-04-11T15:47:10Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    15,
                    47,
                    10,
                    3,
                    102,
                    0
                ],
                "title": "Guiding Large Language Models to Post-Edit Machine Translation with\n  Error Annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding Large Language Models to Post-Edit Machine Translation with\n  Error Annotations"
                },
                "summary": "Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Translation (MT) remains one of the last NLP tasks where large\nlanguage models (LLMs) have not yet replaced dedicated supervised systems. This\nwork exploits the complementary strengths of LLMs and supervised MT by guiding\nLLMs to automatically post-edit MT with external feedback on its quality,\nderived from Multidimensional Quality Metric (MQM) annotations. Working with\nLLaMA-2 models, we consider prompting strategies varying the nature of feedback\nprovided and then fine-tune the LLM to improve its ability to exploit the\nprovided guidance. Through experiments on Chinese-English, English-German, and\nEnglish-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT\nimproves TER, BLEU and COMET scores, although the benefits of fine-grained\nfeedback are not clear. Fine-tuning helps integrate fine-grained feedback more\neffectively and further improves translation quality based on both automatic\nand human evaluation."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "NAACL 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16682v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16682v3",
                "updated": "2025-09-01T12:36:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    36,
                    48,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-23T18:56:56Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    18,
                    56,
                    56,
                    6,
                    54,
                    0
                ],
                "title": "Automatic Input Rewriting Improves Translation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Input Rewriting Improves Translation with Large Language\n  Models"
                },
                "summary": "Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16682v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16682v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24671v2",
                "updated": "2025-09-01T12:34:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    34,
                    28,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-30T15:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    1,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple LLM Agents Debate for Equitable Cultural Alignment"
                },
                "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters)."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Rachel Rudinger"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "ACL 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24683v2",
                "updated": "2025-09-01T12:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-30T15:08:10Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    8,
                    10,
                    4,
                    150,
                    0
                ],
                "title": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should I Share this Translation? Evaluating Quality Feedback for User\n  Reliance on Machine Translation"
                },
                "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using\n(1) error highlights and (2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through (3) backtranslation and (4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using\n(1) error highlights and (2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through (3) backtranslation and (4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden."
                },
                "authors": [
                    {
                        "name": "Dayeon Ki"
                    },
                    {
                        "name": "Kevin Duh"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "arxiv_comment": "EMNLP 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02397v2",
                "updated": "2025-09-01T12:27:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    12,
                    27,
                    7,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-03T03:31:30Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    31,
                    30,
                    1,
                    154,
                    0
                ],
                "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation"
                },
                "summary": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1."
                },
                "authors": [
                    {
                        "name": "Shengjia Zhang"
                    },
                    {
                        "name": "Junjie Wu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Xingyu Lou"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13430v2",
                "updated": "2025-09-01T11:50:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    50,
                    29,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-19T17:55:15Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    55,
                    15,
                    0,
                    139,
                    0
                ],
                "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization"
                },
                "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a simple yet effective approach that perturbs\nthe continuous quantization scale for gradient estimation and uses a\ndirectional derivative clipping method to stabilize training. QZO is orthogonal\nto both scalar-based and codebook-based post-training quantization methods.\nCompared to full-parameter fine-tuning in 16 bits, QZO can reduce the total\nmemory cost by more than 18$\\times$ for 4-bit LLMs, and enables fine-tuning\nLlama-2-13B within a single 24GB GPU. Code will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a simple yet effective approach that perturbs\nthe continuous quantization scale for gradient estimation and uses a\ndirectional derivative clipping method to stabilize training. QZO is orthogonal\nto both scalar-based and codebook-based post-training quantization methods.\nCompared to full-parameter fine-tuning in 16 bits, QZO can reduce the total\nmemory cost by more than 18$\\times$ for 4-bit LLMs, and enables fine-tuning\nLlama-2-13B within a single 24GB GPU. Code will be released publicly."
                },
                "authors": [
                    {
                        "name": "Sifeng Shang"
                    },
                    {
                        "name": "Jiayi Zhou"
                    },
                    {
                        "name": "Chenyu Lin"
                    },
                    {
                        "name": "Minxian Li"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyang Zhou"
                },
                "author": "Kaiyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15413v2",
                "updated": "2025-09-01T11:28:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    28,
                    3,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-21T10:08:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    10,
                    8,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "Bridging Generalization and Personalization in Wearable Human Activity\n  Recognition via On-Device Few-Shot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Generalization and Personalization in Wearable Human Activity\n  Recognition via On-Device Few-Shot Learning"
                },
                "summary": "Human Activity Recognition (HAR) with wearable devices requires both strong\ngeneralization across diverse users and efficient personalization for\nindividuals. However, conventional HAR models often fail to generalize when\nfaced with user-specific variations, leading to degraded performance. To\naddress this challenge, we propose a novel on-device few-shot learning\nframework that bridges generalization and personalization in wearable HAR. Our\nmethod first trains a generalizable representation across users and then\nrapidly adapts to new users with only a few labeled samples, updating\nlightweight classifier layers directly on resource-constrained devices. This\napproach achieves robust on-device learning with minimal computation and memory\ncost, making it practical for real-world deployment. We implement our framework\non the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three\nbenchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these\nscenarios, post-deployment adaptation improves accuracy by 3.73%, 17.38%, and\n3.70%, respectively. These results demonstrate that few-shot on-device learning\nenables scalable, user-aware, and energy-efficient wearable human activity\nrecognition by seamlessly uniting generalization and personalization\n\\footnote{https://github.com/kangpx/onlineTiny2023}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Activity Recognition (HAR) with wearable devices requires both strong\ngeneralization across diverse users and efficient personalization for\nindividuals. However, conventional HAR models often fail to generalize when\nfaced with user-specific variations, leading to degraded performance. To\naddress this challenge, we propose a novel on-device few-shot learning\nframework that bridges generalization and personalization in wearable HAR. Our\nmethod first trains a generalizable representation across users and then\nrapidly adapts to new users with only a few labeled samples, updating\nlightweight classifier layers directly on resource-constrained devices. This\napproach achieves robust on-device learning with minimal computation and memory\ncost, making it practical for real-world deployment. We implement our framework\non the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three\nbenchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these\nscenarios, post-deployment adaptation improves accuracy by 3.73%, 17.38%, and\n3.70%, respectively. These results demonstrate that few-shot on-device learning\nenables scalable, user-aware, and energy-efficient wearable human activity\nrecognition by seamlessly uniting generalization and personalization\n\\footnote{https://github.com/kangpx/onlineTiny2023}."
                },
                "authors": [
                    {
                        "name": "Pixi Kang"
                    },
                    {
                        "name": "Julian Moosmann"
                    },
                    {
                        "name": "Mengxi Liu"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Paul Lukowicz"
                    },
                    {
                        "name": "Sizhen Bian"
                    }
                ],
                "author_detail": {
                    "name": "Sizhen Bian"
                },
                "author": "Sizhen Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16988v2",
                "updated": "2025-09-01T11:15:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    15,
                    6,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-20T13:37:03Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    37,
                    3,
                    4,
                    171,
                    0
                ],
                "title": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed\n  Question Answering"
                },
                "summary": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG)\nframework for attributed question answering (QA) with large language models\n(LLMs). With the goal of trustworthy answer generation, RAGentA focuses on\noptimizing answer correctness, defined by coverage and relevance to the\nquestion and faithfulness, which measures the extent to which answers are\ngrounded in retrieved documents. RAGentA uses a multi-agent architecture that\niteratively filters retrieved documents, generates attributed answers with\nin-line citations, and verifies completeness through dynamic refinement.\nCentral to the framework is a hybrid retrieval strategy that combines sparse\nand dense methods, improving Recall@20 by 12.5% compared to the best single\nretrieval model, resulting in more correct and well-supported answers.\nEvaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA\noutperforms standard RAG baselines, achieving gains of 1.09% in correctness and\n10.72% in faithfulness. These results demonstrate the effectiveness of our\nmulti-agent RAG architecture and hybrid retrieval strategy in advancing\ntrustworthy QA with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG)\nframework for attributed question answering (QA) with large language models\n(LLMs). With the goal of trustworthy answer generation, RAGentA focuses on\noptimizing answer correctness, defined by coverage and relevance to the\nquestion and faithfulness, which measures the extent to which answers are\ngrounded in retrieved documents. RAGentA uses a multi-agent architecture that\niteratively filters retrieved documents, generates attributed answers with\nin-line citations, and verifies completeness through dynamic refinement.\nCentral to the framework is a hybrid retrieval strategy that combines sparse\nand dense methods, improving Recall@20 by 12.5% compared to the best single\nretrieval model, resulting in more correct and well-supported answers.\nEvaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA\noutperforms standard RAG baselines, achieving gains of 1.09% in correctness and\n10.72% in faithfulness. These results demonstrate the effectiveness of our\nmulti-agent RAG architecture and hybrid retrieval strategy in advancing\ntrustworthy QA with LLMs."
                },
                "authors": [
                    {
                        "name": "Ines Besrour"
                    },
                    {
                        "name": "Jingbo He"
                    },
                    {
                        "name": "Tobias Schreieder"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber",
                "arxiv_comment": "Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02037v2",
                "updated": "2025-09-01T10:49:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    49,
                    18,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-31T03:50:19Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    3,
                    50,
                    19,
                    5,
                    151,
                    0
                ],
                "title": "FinS-Pilot: A Benchmark for Online Financial RAG System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinS-Pilot: A Benchmark for Online Financial RAG System"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. In the financial field, the stringent demands\nfor professional accuracy and real-time data processing often necessitate the\nuse of retrieval-augmented generation (RAG) techniques. However, the\ndevelopment of financial RAG benchmarks has been constrained by data\nconfidentiality issues and the lack of dynamic data integration. To address\nthis issue, we introduce FinS-Pilot, a novel benchmark for evaluating RAG\nsystems in online financial applications. Constructed from real-world financial\nassistant interactions, our benchmark incorporates both real-time API data and\ntext data, organized through an intent classification framework covering\ncritical financial domains. The benchmark enables comprehensive evaluation of\nfinancial assistants' capabilities in handling both static knowledge and\ntime-sensitive market information.Through systematic experiments with multiple\nChinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying\nmodels suitable for financial applications while addressing the current gap in\nspecialized evaluation tools for the financial domain. Our work contributes\nboth a practical evaluation framework and a curated dataset to advance research\nin financial NLP systems. The code and dataset are accessible on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. In the financial field, the stringent demands\nfor professional accuracy and real-time data processing often necessitate the\nuse of retrieval-augmented generation (RAG) techniques. However, the\ndevelopment of financial RAG benchmarks has been constrained by data\nconfidentiality issues and the lack of dynamic data integration. To address\nthis issue, we introduce FinS-Pilot, a novel benchmark for evaluating RAG\nsystems in online financial applications. Constructed from real-world financial\nassistant interactions, our benchmark incorporates both real-time API data and\ntext data, organized through an intent classification framework covering\ncritical financial domains. The benchmark enables comprehensive evaluation of\nfinancial assistants' capabilities in handling both static knowledge and\ntime-sensitive market information.Through systematic experiments with multiple\nChinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying\nmodels suitable for financial applications while addressing the current gap in\nspecialized evaluation tools for the financial domain. Our work contributes\nboth a practical evaluation framework and a curated dataset to advance research\nin financial NLP systems. The code and dataset are accessible on GitHub."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yiding Sun"
                    },
                    {
                        "name": "Jiaxin Mao"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Danqing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Danqing Xu"
                },
                "author": "Danqing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10491v2",
                "updated": "2025-09-01T09:21:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    9,
                    21,
                    41,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-12T08:47:40Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    8,
                    47,
                    40,
                    3,
                    163,
                    0
                ],
                "title": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language\n  Models"
                },
                "summary": "Modern language models are trained on large amounts of data. These data\ninevitably include controversial and stereotypical content, which contains all\nsorts of biases related to gender, origin, age, etc. As a result, the models\nexpress biased points of view or produce different results based on the\nassigned personality or the personality of the user. In this paper, we\ninvestigate various proxy measures of bias in large language models (LLMs). We\nfind that evaluating models with pre-prompted personae on a multi-subject\nbenchmark (MMLU) leads to negligible and mostly random differences in scores.\nHowever, if we reformulate the task and ask a model to grade the user's answer,\nthis shows more significant signs of bias. Finally, if we ask the model for\nsalary negotiation advice, we see pronounced bias in the answers. With the\nrecent trend for LLM assistant memory and personalization, these problems open\nup from a different angle: modern LLM users do not need to pre-prompt the\ndescription of their persona since the model already knows their\nsocio-demographics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern language models are trained on large amounts of data. These data\ninevitably include controversial and stereotypical content, which contains all\nsorts of biases related to gender, origin, age, etc. As a result, the models\nexpress biased points of view or produce different results based on the\nassigned personality or the personality of the user. In this paper, we\ninvestigate various proxy measures of bias in large language models (LLMs). We\nfind that evaluating models with pre-prompted personae on a multi-subject\nbenchmark (MMLU) leads to negligible and mostly random differences in scores.\nHowever, if we reformulate the task and ask a model to grade the user's answer,\nthis shows more significant signs of bias. Finally, if we ask the model for\nsalary negotiation advice, we see pronounced bias in the answers. With the\nrecent trend for LLM assistant memory and personalization, these problems open\nup from a different angle: modern LLM users do not need to pre-prompt the\ndescription of their persona since the model already knows their\nsocio-demographics."
                },
                "authors": [
                    {
                        "name": "Aleksandra Sorokovikova"
                    },
                    {
                        "name": "Pavel Chizhov"
                    },
                    {
                        "name": "Iuliia Eremenko"
                    },
                    {
                        "name": "Ivan P. Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan P. Yamshchikov"
                },
                "author": "Ivan P. Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14252v3",
                "updated": "2025-09-01T08:57:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    57,
                    22,
                    0,
                    244,
                    0
                ],
                "published": "2024-11-21T15:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification"
                },
                "summary": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We also propose MINT-CL, a multi-task contrastive learning framework\nfor multi-turn intent classification, which improves performance while reducing\ndependence on large-scale annotated datasets. Empirical results demonstrate\nthat our approach outperforms competitive baselines in dialogue generation\nquality and classification accuracy, particularly in multilingual settings. To\nfacilitate future research, we release MINT-E, a comprehensive, multilingual,\nintent-aware multi-turn dialogue corpus derived from the e-commerce\ndomain\\footnote{The reproduced source code and dataset are available at\nhttps://github.com/junhua/chain-of-intent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We also propose MINT-CL, a multi-task contrastive learning framework\nfor multi-turn intent classification, which improves performance while reducing\ndependence on large-scale annotated datasets. Empirical results demonstrate\nthat our approach outperforms competitive baselines in dialogue generation\nquality and classification accuracy, particularly in multilingual settings. To\nfacilitate future research, we release MINT-E, a comprehensive, multilingual,\nintent-aware multi-turn dialogue corpus derived from the e-commerce\ndomain\\footnote{The reproduced source code and dataset are available at\nhttps://github.com/junhua/chain-of-intent."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "arxiv_comment": "Accepted to Proceedings of CIKM'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20435v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20435v3",
                "updated": "2025-09-01T08:37:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    37,
                    20,
                    0,
                    244,
                    0
                ],
                "published": "2023-10-31T13:14:43Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    13,
                    14,
                    43,
                    1,
                    304,
                    0
                ],
                "title": "Assessing the Sustainability and Trustworthiness of Federated Learning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Sustainability and Trustworthiness of Federated Learning\n  Models"
                },
                "summary": "Artificial intelligence (AI) increasingly influences critical decision-making\nacross sectors. Federated Learning (FL), as a privacy-preserving collaborative\nAI paradigm, not only enhances data protection but also holds significant\npromise for intelligent network management, including distributed monitoring,\nadaptive control, and edge intelligence. Although the trustworthiness of FL\nsystems has received growing attention, the sustainability dimension remains\ninsufficiently explored, despite its importance for scalable real-world\ndeployment. To address this gap, this work introduces sustainability as a\ndistinct pillar within a comprehensive trustworthy FL taxonomy, consistent with\nAI-HLEG guidelines. This pillar includes three key aspects: hardware\nefficiency, federation complexity, and the carbon intensity of energy sources.\nExperiments using the FederatedScope framework under diverse scenarios,\nincluding varying participants, system complexity, hardware, and energy\nconfigurations, validate the practicality of the approach. Results show that\nincorporating sustainability into FL evaluation supports environmentally\nresponsible deployment, enabling more efficient, adaptive, and trustworthy\nnetwork services and management AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) increasingly influences critical decision-making\nacross sectors. Federated Learning (FL), as a privacy-preserving collaborative\nAI paradigm, not only enhances data protection but also holds significant\npromise for intelligent network management, including distributed monitoring,\nadaptive control, and edge intelligence. Although the trustworthiness of FL\nsystems has received growing attention, the sustainability dimension remains\ninsufficiently explored, despite its importance for scalable real-world\ndeployment. To address this gap, this work introduces sustainability as a\ndistinct pillar within a comprehensive trustworthy FL taxonomy, consistent with\nAI-HLEG guidelines. This pillar includes three key aspects: hardware\nefficiency, federation complexity, and the carbon intensity of energy sources.\nExperiments using the FederatedScope framework under diverse scenarios,\nincluding varying participants, system complexity, hardware, and energy\nconfigurations, validate the practicality of the approach. Results show that\nincorporating sustainability into FL evaluation supports environmentally\nresponsible deployment, enabling more efficient, adaptive, and trustworthy\nnetwork services and management AI models."
                },
                "authors": [
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Alberto Huertas Celdran"
                    },
                    {
                        "name": "Pedro Miguel Sanchez Sanchez"
                    },
                    {
                        "name": "Lynn Zumtaugwald"
                    },
                    {
                        "name": "Gerome Bovet"
                    },
                    {
                        "name": "Burkhard Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Burkhard Stiller"
                },
                "author": "Burkhard Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.20435v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20435v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05568v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05568v6",
                "updated": "2025-09-01T08:13:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    13,
                    0,
                    0,
                    244,
                    0
                ],
                "published": "2024-06-08T20:19:35Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    20,
                    19,
                    35,
                    5,
                    160,
                    0
                ],
                "title": "SAMM: Sharded Automated Market Maker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAMM: Sharded Automated Market Maker"
                },
                "summary": "Automated Market Makers (AMMs) are a cornerstone of decentralized finance.\nThey are smart contracts (stateful programs) running on blockchains. They\nenable virtual token exchange: traders swap tokens with the AMM for a fee,\nwhile liquidity providers supply liquidity and receive these fees. Demand for\nAMMs is growing rapidly, but our experiment-based estimates show that current\narchitectures cannot meet the projected demand by 2029. This is because the\nexecution of existing AMMs is non-parallelizable.\n  We present SAMM, an AMM comprising multiple shards. All shards are AMMs\nrunning on the same chain, but their independence enables parallel execution.\nThe security of SAMM, unlike in classical sharding solutions, relies on\nincentive compatibility. Therefore, SAMM introduces a novel fee design. Through\nanalysis of Subgame-Perfect Nash Equilibria (SPNE), we show that SAMM\nincentivizes the desired behavior: liquidity providers balance liquidity among\nall shards, overcoming destabilization attacks, and trades are evenly\ndistributed. We validate our game-theoretic analysis with a simulation using\nreal-world data.\n  We evaluate SAMM by implementing and deploying it on local testnets of the\nSui and Solana blockchains. To our knowledge, this is the first quantification\nof high-demand-contract performance. SAMM improves throughput by 5x and 16x,\nrespectively, potentially more with better parallelization of the underlying\nblockchains. It is directly deployable, mitigating the upcoming scaling\nbottleneck.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Market Makers (AMMs) are a cornerstone of decentralized finance.\nThey are smart contracts (stateful programs) running on blockchains. They\nenable virtual token exchange: traders swap tokens with the AMM for a fee,\nwhile liquidity providers supply liquidity and receive these fees. Demand for\nAMMs is growing rapidly, but our experiment-based estimates show that current\narchitectures cannot meet the projected demand by 2029. This is because the\nexecution of existing AMMs is non-parallelizable.\n  We present SAMM, an AMM comprising multiple shards. All shards are AMMs\nrunning on the same chain, but their independence enables parallel execution.\nThe security of SAMM, unlike in classical sharding solutions, relies on\nincentive compatibility. Therefore, SAMM introduces a novel fee design. Through\nanalysis of Subgame-Perfect Nash Equilibria (SPNE), we show that SAMM\nincentivizes the desired behavior: liquidity providers balance liquidity among\nall shards, overcoming destabilization attacks, and trades are evenly\ndistributed. We validate our game-theoretic analysis with a simulation using\nreal-world data.\n  We evaluate SAMM by implementing and deploying it on local testnets of the\nSui and Solana blockchains. To our knowledge, this is the first quantification\nof high-demand-contract performance. SAMM improves throughput by 5x and 16x,\nrespectively, potentially more with better parallelization of the underlying\nblockchains. It is directly deployable, mitigating the upcoming scaling\nbottleneck."
                },
                "authors": [
                    {
                        "name": "Hongyin Chen"
                    },
                    {
                        "name": "Amit Vaisman"
                    },
                    {
                        "name": "Ittay Eyal"
                    }
                ],
                "author_detail": {
                    "name": "Ittay Eyal"
                },
                "author": "Ittay Eyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05568v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05568v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16654v2",
                "updated": "2025-09-01T07:54:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    54,
                    4,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-20T05:41:22Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    5,
                    41,
                    22,
                    2,
                    232,
                    0
                ],
                "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning"
                },
                "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL)."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Minghao Zhang"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "19 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14544v2",
                "updated": "2025-09-01T07:38:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    38,
                    3,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-20T08:55:26Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    55,
                    26,
                    2,
                    232,
                    0
                ],
                "title": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptively Robust LLM Inference Optimization under Prediction\n  Uncertainty"
                },
                "summary": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately."
                },
                "authors": [
                    {
                        "name": "Zixi Chen"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11419v2",
                "updated": "2025-09-01T07:04:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    4,
                    47,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T04:17:53Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    4,
                    17,
                    53,
                    0,
                    48,
                    0
                ],
                "title": "InsBank: Evolving Instruction Subset for Ongoing Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InsBank: Evolving Instruction Subset for Ongoing Alignment"
                },
                "summary": "Large language models (LLMs) typically undergo instruction tuning to enhance\nalignment. Recent studies emphasize that quality and diversity of instruction\ndata are more crucial than quantity, highlighting the need to select diverse,\nhigh-quality subsets to reduce training costs. However, how to evolve these\nselected subsets alongside the development of new instruction data remains\ninsufficiently explored. To achieve LLMs' ongoing alignment, we introduce\nInstruction Bank (\\textbf{InsBank}), a continuously updated repository that\nintegrates the latest valuable instruction data. We further propose Progressive\nInstruction Bank Evolution (\\textbf{PIBE}), a novel framework designed to\nevolve InsBank effectively and efficiently over time. PIBE employs a gradual\ndata selection strategy to maintain long-term efficiency, leveraging a\nrepresentation-based diversity score to capture relationships between data\npoints and retain historical information for comprehensive diversity\nevaluation. This also allows for flexible combination of diversity and quality\nscores during data selection and ranking. Extensive experiments demonstrate\nthat PIBE significantly outperforms baselines in InsBank evolution and is able\nto extract budget-specific subsets, demonstrating its effectiveness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically undergo instruction tuning to enhance\nalignment. Recent studies emphasize that quality and diversity of instruction\ndata are more crucial than quantity, highlighting the need to select diverse,\nhigh-quality subsets to reduce training costs. However, how to evolve these\nselected subsets alongside the development of new instruction data remains\ninsufficiently explored. To achieve LLMs' ongoing alignment, we introduce\nInstruction Bank (\\textbf{InsBank}), a continuously updated repository that\nintegrates the latest valuable instruction data. We further propose Progressive\nInstruction Bank Evolution (\\textbf{PIBE}), a novel framework designed to\nevolve InsBank effectively and efficiently over time. PIBE employs a gradual\ndata selection strategy to maintain long-term efficiency, leveraging a\nrepresentation-based diversity score to capture relationships between data\npoints and retain historical information for comprehensive diversity\nevaluation. This also allows for flexible combination of diversity and quality\nscores during data selection and ranking. Extensive experiments demonstrate\nthat PIBE significantly outperforms baselines in InsBank evolution and is able\nto extract budget-specific subsets, demonstrating its effectiveness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Huan Ren"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01235v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01235v3",
                "updated": "2025-09-01T06:03:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    6,
                    3,
                    11,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-02T07:22:08Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    7,
                    22,
                    8,
                    5,
                    214,
                    0
                ],
                "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place\n  Exploration"
                },
                "summary": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments."
                },
                "authors": [
                    {
                        "name": "Yaxin Hu"
                    },
                    {
                        "name": "Arissa J. Sato"
                    },
                    {
                        "name": "Jingxin Du"
                    },
                    {
                        "name": "Chenming Ye"
                    },
                    {
                        "name": "Anjun Zhu"
                    },
                    {
                        "name": "Pragathi Praveena"
                    },
                    {
                        "name": "Bilge Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Mutlu"
                },
                "author": "Bilge Mutlu",
                "arxiv_doi": "10.1145/3746059.3747697",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746059.3747697",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01235v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01235v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 38th Annual Acm Symposium on User Interface\n  Software and Technology (UIST 2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01992v2",
                "updated": "2025-09-01T05:11:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    5,
                    11,
                    22,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-04T02:19:38Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    19,
                    38,
                    0,
                    216,
                    0
                ],
                "title": "Toward Efficient Spiking Transformers: Synapse Pruning Meets Synergistic\n  Learning-Based Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient Spiking Transformers: Synapse Pruning Meets Synergistic\n  Learning-Based Compensation"
                },
                "summary": "As a foundational architecture of artificial intelligence models, Transformer\nhas been recently adapted to spiking neural networks with promising performance\nacross various tasks. However, existing spiking Transformer (ST)-based models\nrequire a substantial number of parameters and incur high computational costs,\nthus limiting their deployment in resource-constrained environments. To address\nthese challenges, we propose combining synapse pruning with a synergistic\nlearning-based compensation strategy to derive lightweight ST-based models.\nSpecifically, two types of tailored pruning strategies are introduced to reduce\nredundancy in the weight matrices of ST blocks: an unstructured\n$\\mathrm{L_{1}P}$ method to induce sparse representations, and a structured DSP\nmethod to induce low-rank representations. In addition, we propose an enhanced\nspiking neuron model, termed the synergistic leaky integrate-and-fire (sLIF)\nneuron, to effectively compensate for model pruning through synergistic\nlearning between synaptic and intrinsic plasticity mechanisms. Extensive\nexperiments on benchmark datasets demonstrate that the proposed methods\nsignificantly reduce model size and computational overhead while maintaining\ncompetitive performance. These results validate the effectiveness of the\nproposed pruning and compensation strategies in constructing efficient and\nhigh-performing ST-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a foundational architecture of artificial intelligence models, Transformer\nhas been recently adapted to spiking neural networks with promising performance\nacross various tasks. However, existing spiking Transformer (ST)-based models\nrequire a substantial number of parameters and incur high computational costs,\nthus limiting their deployment in resource-constrained environments. To address\nthese challenges, we propose combining synapse pruning with a synergistic\nlearning-based compensation strategy to derive lightweight ST-based models.\nSpecifically, two types of tailored pruning strategies are introduced to reduce\nredundancy in the weight matrices of ST blocks: an unstructured\n$\\mathrm{L_{1}P}$ method to induce sparse representations, and a structured DSP\nmethod to induce low-rank representations. In addition, we propose an enhanced\nspiking neuron model, termed the synergistic leaky integrate-and-fire (sLIF)\nneuron, to effectively compensate for model pruning through synergistic\nlearning between synaptic and intrinsic plasticity mechanisms. Extensive\nexperiments on benchmark datasets demonstrate that the proposed methods\nsignificantly reduce model size and computational overhead while maintaining\ncompetitive performance. These results validate the effectiveness of the\nproposed pruning and compensation strategies in constructing efficient and\nhigh-performing ST-based models."
                },
                "authors": [
                    {
                        "name": "Hongze Sun"
                    },
                    {
                        "name": "Wuque Cai"
                    },
                    {
                        "name": "Duo Chen"
                    },
                    {
                        "name": "Shifeng Mao"
                    },
                    {
                        "name": "Jiayi He"
                    },
                    {
                        "name": "Zhenxing Wang"
                    },
                    {
                        "name": "Dezhong Yao"
                    },
                    {
                        "name": "Daqing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Daqing Guo"
                },
                "author": "Daqing Guo",
                "arxiv_comment": "This manuscript has been submitted for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05407v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05407v4",
                "updated": "2025-09-01T05:04:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    5,
                    4,
                    34,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-08T01:54:23Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    1,
                    54,
                    23,
                    5,
                    39,
                    0
                ],
                "title": "The Complexity of Learning Sparse Superposed Features with Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Complexity of Learning Sparse Superposed Features with Feedback"
                },
                "summary": "The success of deep networks is crucially attributed to their ability to\ncapture latent features within a representation space. In this work, we\ninvestigate whether the underlying learned features of a model can be\nefficiently retrieved through feedback from an agent, such as a large language\nmodel (LLM), in the form of relative \\tt{triplet comparisons}. These features\nmay represent various constructs, including dictionaries in LLMs or a\ncovariance matrix of Mahalanobis distances. We analyze the feedback complexity\nassociated with learning a feature matrix in sparse settings. Our results\nestablish tight bounds when the agent is permitted to construct activations and\ndemonstrate strong upper bounds in sparse scenarios when the agent's feedback\nis limited to distributional information. We validate our theoretical findings\nthrough experiments on two distinct applications: feature recovery from\nRecursive Feature Machines and dictionary extraction from sparse autoencoders\ntrained on Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of deep networks is crucially attributed to their ability to\ncapture latent features within a representation space. In this work, we\ninvestigate whether the underlying learned features of a model can be\nefficiently retrieved through feedback from an agent, such as a large language\nmodel (LLM), in the form of relative \\tt{triplet comparisons}. These features\nmay represent various constructs, including dictionaries in LLMs or a\ncovariance matrix of Mahalanobis distances. We analyze the feedback complexity\nassociated with learning a feature matrix in sparse settings. Our results\nestablish tight bounds when the agent is permitted to construct activations and\ndemonstrate strong upper bounds in sparse scenarios when the agent's feedback\nis limited to distributional information. We validate our theoretical findings\nthrough experiments on two distinct applications: feature recovery from\nRecursive Feature Machines and dictionary extraction from sparse autoencoders\ntrained on Large Language Models."
                },
                "authors": [
                    {
                        "name": "Akash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Akash Kumar"
                },
                "author": "Akash Kumar",
                "arxiv_comment": "ICML'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05407v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05407v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05790v3",
                "updated": "2025-09-01T04:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    4,
                    51,
                    13,
                    0,
                    244,
                    0
                ],
                "published": "2025-01-10T08:50:38Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    8,
                    50,
                    38,
                    4,
                    10,
                    0
                ],
                "title": "Understanding Impact of Human Feedback via Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Impact of Human Feedback via Influence Functions"
                },
                "summary": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. Our experiments showcase two key\napplications of influence functions: (1) detecting common labeler biases in\nhuman feedback datasets and (2) guiding labelers in refining their strategies\nto better align with expert feedback. By quantifying the impact of human\nfeedback, we believe that influence functions can enhance feedback\ninterpretability and contribute to scalable oversight in RLHF, helping labelers\nprovide more accurate and consistent feedback. Source code is available at\nhttps://github.com/mintaywon/IF_RLHF",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn\nsuitable reward models from human feedback to align large language models\n(LLMs) with human intentions. However, human feedback can often be noisy,\ninconsistent, or biased, especially when evaluating complex responses. Such\nfeedback can lead to misaligned reward signals, potentially causing unintended\nside effects during the RLHF process. To address these challenges, we explore\nthe use of influence functions to measure the impact of human feedback on the\nperformance of reward models. We propose a compute-efficient approximation\nmethod that enables the application of influence functions to LLM-based reward\nmodels and large-scale preference datasets. Our experiments showcase two key\napplications of influence functions: (1) detecting common labeler biases in\nhuman feedback datasets and (2) guiding labelers in refining their strategies\nto better align with expert feedback. By quantifying the impact of human\nfeedback, we believe that influence functions can enhance feedback\ninterpretability and contribute to scalable oversight in RLHF, helping labelers\nprovide more accurate and consistent feedback. Source code is available at\nhttps://github.com/mintaywon/IF_RLHF"
                },
                "authors": [
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Haeone Lee"
                    },
                    {
                        "name": "Yongchan Kwon"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_doi": "10.18653/v1/2025.acl-long.1333",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.1333",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACL 2025, Source code:\n  https://github.com/mintaywon/IF_RLHF",
                "arxiv_journal_ref": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics 63 (2025) 27471-27500",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00134v3",
                "updated": "2025-09-01T04:16:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    4,
                    16,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-28T19:25:04Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    19,
                    25,
                    4,
                    4,
                    59,
                    0
                ],
                "title": "Personalized Causal Graph Reasoning for LLMs: An Implementation for\n  Dietary Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Causal Graph Reasoning for LLMs: An Implementation for\n  Dietary Recommendations"
                },
                "summary": "Large Language Models (LLMs) excel at general-purpose reasoning by leveraging\nbroad commonsense knowledge, but they remain limited in tasks requiring\npersonalized reasoning over multifactorial personal data. This limitation\nconstrains their applicability in domains such as healthcare, where decisions\nmust adapt to individual contexts. We introduce Personalized Causal Graph\nReasoning, a framework that enables LLMs to reason over individual-specific\ncausal graphs constructed from longitudinal data. Each graph encodes how\nuser-specific factors influence targeted outcomes. In response to a query, the\nLLM traverses the graph to identify relevant causal pathways, rank them by\nestimated impact, simulate potential outcomes, and generate tailored responses.\nWe implement this framework in the context of nutrient-oriented dietary\nrecommendations, where variability in metabolic responses demands personalized\nreasoning. Using counterfactual evaluation, we assess the effectiveness of\nLLM-generated food suggestions for glucose control. Our method reduces\npostprandial glucose iAUC across three time windows compared to prior\napproaches. Additional LLM-as-a-judge evaluations further confirm improvements\nin personalization quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at general-purpose reasoning by leveraging\nbroad commonsense knowledge, but they remain limited in tasks requiring\npersonalized reasoning over multifactorial personal data. This limitation\nconstrains their applicability in domains such as healthcare, where decisions\nmust adapt to individual contexts. We introduce Personalized Causal Graph\nReasoning, a framework that enables LLMs to reason over individual-specific\ncausal graphs constructed from longitudinal data. Each graph encodes how\nuser-specific factors influence targeted outcomes. In response to a query, the\nLLM traverses the graph to identify relevant causal pathways, rank them by\nestimated impact, simulate potential outcomes, and generate tailored responses.\nWe implement this framework in the context of nutrient-oriented dietary\nrecommendations, where variability in metabolic responses demands personalized\nreasoning. Using counterfactual evaluation, we assess the effectiveness of\nLLM-generated food suggestions for glucose control. Our method reduces\npostprandial glucose iAUC across three time windows compared to prior\napproaches. Additional LLM-as-a-judge evaluations further confirm improvements\nin personalization quality."
                },
                "authors": [
                    {
                        "name": "Zhongqi Yang"
                    },
                    {
                        "name": "Amir Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Rahmani"
                },
                "author": "Amir Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07279v2",
                "updated": "2025-09-01T03:34:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    34,
                    39,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-10T10:33:16Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    10,
                    33,
                    16,
                    6,
                    222,
                    0
                ],
                "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health\n  Screening using Item Response Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health\n  Screening using Item Response Theory"
                },
                "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows."
                },
                "authors": [
                    {
                        "name": "Vasudha Varadarajan"
                    },
                    {
                        "name": "Hui Xu"
                    },
                    {
                        "name": "Rebecca Astrid Boehme"
                    },
                    {
                        "name": "Mariam Marlan Mirstrom"
                    },
                    {
                        "name": "Sverker Sikstrom"
                    },
                    {
                        "name": "H. Andrew Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "H. Andrew Schwartz"
                },
                "author": "H. Andrew Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02730v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02730v3",
                "updated": "2025-09-01T03:33:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    33,
                    43,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-03T17:49:28Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    49,
                    28,
                    3,
                    277,
                    0
                ],
                "title": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision\n  Language Models in Diverse Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision\n  Language Models in Diverse Scenes"
                },
                "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ntasks like visual question answering and document understanding. However, their\npotential to comprehend embodied environments and navigate within them remains\nunderexplored. In this work, we first study the challenge of open-vocabulary\nobject navigation by introducing DivScene, a large-scale dataset with 4,614\nhouses across 81 scene types and 5,707 kinds of target objects. Our dataset\nprovides a much greater diversity of target objects and scene types than\nexisting datasets, enabling a comprehensive task evaluation. We evaluated\nvarious methods with LVLMs and LLMs on our dataset and found that current\nmodels still fall short of open-vocab object navigation ability. Then, we\nfine-tuned LVLMs to predict the next action with CoT explanations. We observe\nthat LVLM's navigation ability can be improved substantially with only\nBFS-generated shortest paths without any human supervision, surpassing GPT-4o\nby over 20% in success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ntasks like visual question answering and document understanding. However, their\npotential to comprehend embodied environments and navigate within them remains\nunderexplored. In this work, we first study the challenge of open-vocabulary\nobject navigation by introducing DivScene, a large-scale dataset with 4,614\nhouses across 81 scene types and 5,707 kinds of target objects. Our dataset\nprovides a much greater diversity of target objects and scene types than\nexisting datasets, enabling a comprehensive task evaluation. We evaluated\nvarious methods with LVLMs and LLMs on our dataset and found that current\nmodels still fall short of open-vocab object navigation ability. Then, we\nfine-tuned LVLMs to predict the next action with CoT explanations. We observe\nthat LVLM's navigation ability can be improved substantially with only\nBFS-generated shortest paths without any human supervision, surpassing GPT-4o\nby over 20% in success rates."
                },
                "authors": [
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Xiaoman Pan"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02730v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02730v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09857v2",
                "updated": "2025-09-01T03:23:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    23,
                    46,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-14T01:44:57Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    1,
                    44,
                    57,
                    4,
                    45,
                    0
                ],
                "title": "Port-LLM: A Port Prediction Method for Fluid Antenna based on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Port-LLM: A Port Prediction Method for Fluid Antenna based on Large\n  Language Models"
                },
                "summary": "The objective of this study is to address the mobility challenges faced by\nuser equipment (UE) through the implementation of fluid antenna (FA) on the UE\nside. This approach aims to maintain the time-varying channel in a relatively\nstable state by strategically relocating the FA to an appropriate port. To the\nbest of our knowledge, this paper introduces, for the first time, the\napplication of large language models (LLMs) in the prediction of FA ports,\npresenting a novel model termed Port-LLM. Our proposed method for predicting\nthe moving port of the FA is a two-step prediction method. To enhance the\nlearning efficacy of our proposed Port-LLM model, we integrate low-rank\nadaptation (LoRA) fine-tuning technology. Additionally, to further exploit the\nnatural language processing capabilities of pre-trained LLMs, we propose a\nframework named Prompt-Port-LLM, which is constructed upon the Port-LLM\narchitecture and incorporates prompt fine-tuning techniques along with a\nspecialized prompt encoder module. The simulation results show that our\nproposed models all exhibit strong generalization ability and robustness under\ndifferent numbers of base station antennas and medium-to-high mobility speeds\nof UE. In comparison to existing methods, the performance of the port predicted\nby our models demonstrates superior efficacy. Moreover, both of our proposed\nmodels achieve millimeter-level inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The objective of this study is to address the mobility challenges faced by\nuser equipment (UE) through the implementation of fluid antenna (FA) on the UE\nside. This approach aims to maintain the time-varying channel in a relatively\nstable state by strategically relocating the FA to an appropriate port. To the\nbest of our knowledge, this paper introduces, for the first time, the\napplication of large language models (LLMs) in the prediction of FA ports,\npresenting a novel model termed Port-LLM. Our proposed method for predicting\nthe moving port of the FA is a two-step prediction method. To enhance the\nlearning efficacy of our proposed Port-LLM model, we integrate low-rank\nadaptation (LoRA) fine-tuning technology. Additionally, to further exploit the\nnatural language processing capabilities of pre-trained LLMs, we propose a\nframework named Prompt-Port-LLM, which is constructed upon the Port-LLM\narchitecture and incorporates prompt fine-tuning techniques along with a\nspecialized prompt encoder module. The simulation results show that our\nproposed models all exhibit strong generalization ability and robustness under\ndifferent numbers of base station antennas and medium-to-high mobility speeds\nof UE. In comparison to existing methods, the performance of the port predicted\nby our models demonstrates superior efficacy. Moreover, both of our proposed\nmodels achieve millimeter-level inference speed."
                },
                "authors": [
                    {
                        "name": "Yali Zhang"
                    },
                    {
                        "name": "Haifan Yin"
                    },
                    {
                        "name": "Weidong Li"
                    },
                    {
                        "name": "Emil Bjornson"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "14 pages, 13 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17009v2",
                "updated": "2025-09-01T02:43:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    43,
                    50,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-23T12:49:08Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    12,
                    49,
                    8,
                    5,
                    235,
                    0
                ],
                "title": "Contrastive Prompt Clustering for Weakly Supervised Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Prompt Clustering for Weakly Supervised Semantic\n  Segmentation"
                },
                "summary": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has\ngained attention for its cost-effectiveness. Most existing methods emphasize\ninter-class separation, often neglecting the shared semantics among related\ncategories and lacking fine-grained discrimination. To address this, we propose\nContrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large\nLanguage Models (LLMs) to derive category clusters that encode intrinsic\ninter-class relationships, and further introduces a class-aware patch-level\ncontrastive loss to enforce intra-class consistency and inter-class separation.\nThis hierarchical design leverages clusters as coarse-grained semantic priors\nwhile preserving fine-grained boundaries, thereby reducing confusion among\nvisually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014\ndemonstrate that CPC surpasses existing state-of-the-art methods in WSSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has\ngained attention for its cost-effectiveness. Most existing methods emphasize\ninter-class separation, often neglecting the shared semantics among related\ncategories and lacking fine-grained discrimination. To address this, we propose\nContrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large\nLanguage Models (LLMs) to derive category clusters that encode intrinsic\ninter-class relationships, and further introduces a class-aware patch-level\ncontrastive loss to enforce intra-class consistency and inter-class separation.\nThis hierarchical design leverages clusters as coarse-grained semantic priors\nwhile preserving fine-grained boundaries, thereby reducing confusion among\nvisually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014\ndemonstrate that CPC surpasses existing state-of-the-art methods in WSSS."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Zhenhong Chen"
                    },
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09889v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09889v4",
                "updated": "2025-09-01T02:41:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    41,
                    50,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-13T15:46:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    15,
                    46,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA\n  Problem Solving by AWorld",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA\n  Problem Solving by AWorld"
                },
                "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, this reliance introduces new challenges, as\nextended contexts and noisy tool outputs can undermine system reliability. To\naddress this, we propose a dynamic Multi-Agent System (MAS) in our AWorld\nframework, where an Execution Agent is supervised by a Guard Agent that\nprovides on-demand dynamic maneuvering, verifying and correcting the reasoning\nprocess to improve robustness over single-agent systems. To move beyond this\ngeneric supervision, we enhance the architecture with a methodology inspired by\nSystem Identification from control theory. This method first profiles the\nExecution Agent offline on a benchmark dataset to create a \"performance\nfingerprint\" of its unique weaknesses. The Guard Agent then leverages this\nfingerprint online to deliver profile-aware supervision, making targeted\ninterventions based on known failure patterns rather than merely reacting to\nimmediate logical flaws. Extensive experiments on the GAIA dataset demonstrate\nthat this profile-aware MAS significantly improves both effectiveness and\nstability, outperforming not only single-agent systems but also its naive\ncounterpart. This superior performance led our system to achieve first place\namong open-source projects on the prestigious GAIA leaderboard. These findings\nhighlight that building truly trustworthy intelligent systems requires not just\ncollaboration, but a deep, empirically-grounded understanding of each agent's\nunique capabilities and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, this reliance introduces new challenges, as\nextended contexts and noisy tool outputs can undermine system reliability. To\naddress this, we propose a dynamic Multi-Agent System (MAS) in our AWorld\nframework, where an Execution Agent is supervised by a Guard Agent that\nprovides on-demand dynamic maneuvering, verifying and correcting the reasoning\nprocess to improve robustness over single-agent systems. To move beyond this\ngeneric supervision, we enhance the architecture with a methodology inspired by\nSystem Identification from control theory. This method first profiles the\nExecution Agent offline on a benchmark dataset to create a \"performance\nfingerprint\" of its unique weaknesses. The Guard Agent then leverages this\nfingerprint online to deliver profile-aware supervision, making targeted\ninterventions based on known failure patterns rather than merely reacting to\nimmediate logical flaws. Extensive experiments on the GAIA dataset demonstrate\nthat this profile-aware MAS significantly improves both effectiveness and\nstability, outperforming not only single-agent systems but also its naive\ncounterpart. This superior performance led our system to achieve first place\namong open-source projects on the prestigious GAIA leaderboard. These findings\nhighlight that building truly trustworthy intelligent systems requires not just\ncollaboration, but a deep, empirically-grounded understanding of each agent's\nunique capabilities and limitations."
                },
                "authors": [
                    {
                        "name": "Zhitian Xie"
                    },
                    {
                        "name": "Qintong Wu"
                    },
                    {
                        "name": "Chengyue Yu"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09889v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09889v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16044v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16044v4",
                "updated": "2025-09-01T02:39:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    39,
                    51,
                    0,
                    244,
                    0
                ],
                "published": "2024-11-25T02:15:30Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    2,
                    15,
                    30,
                    0,
                    330,
                    0
                ],
                "title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities\n  through Tree-Based Image Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities\n  through Tree-Based Image Exploration"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in vision-language understanding. Recently, with the integration\nof test-time scaling techniques, these models have also shown strong potential\nin visual reasoning. However, most existing reasoning approaches remain\ntext-level in nature: MLLMs are prompted to explore various combinations of\ntextual tokens via their underlying language model, while the visual input\nremains fixed throughout the reasoning process. This paradigm limits the\nmodel's ability to fully exploit rich visual information, particularly when\ndealing with images containing numerous fine-grained elements. In such cases,\nvision-level reasoning becomes crucial - where models dynamically zoom into\nspecific regions of the image to gather detailed visual cues necessary for\naccurate decision-making. In this paper, we propose Zoom Eye, a training-free,\nmodel-agnostic tree search algorithm tailored for vision-level reasoning. Zoom\nEye treats an image as a hierarchical tree structure, where each child node\nrepresents a zoomed-in sub-region of its parent, and the root corresponds to\nthe full image. The algorithm enables MLLMs to simulate human-like zooming\nbehavior by navigating from root to leaf nodes in search of task-relevant\nvisual evidence. We experiment on a series of high-resolution benchmarks and\nthe results demonstrate that Zoom Eye consistently improves the performance of\nmultiple MLLMs by a large margin (e.g., InternVL2.5-8B increases by 15.71% and\n17.69% on HR-Bench) and also enables small 3-8B MLLMs to outperform strong\nlarge models such as GPT-4o. Code: https://github.com/om-ai-lab/ZoomEye",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in vision-language understanding. Recently, with the integration\nof test-time scaling techniques, these models have also shown strong potential\nin visual reasoning. However, most existing reasoning approaches remain\ntext-level in nature: MLLMs are prompted to explore various combinations of\ntextual tokens via their underlying language model, while the visual input\nremains fixed throughout the reasoning process. This paradigm limits the\nmodel's ability to fully exploit rich visual information, particularly when\ndealing with images containing numerous fine-grained elements. In such cases,\nvision-level reasoning becomes crucial - where models dynamically zoom into\nspecific regions of the image to gather detailed visual cues necessary for\naccurate decision-making. In this paper, we propose Zoom Eye, a training-free,\nmodel-agnostic tree search algorithm tailored for vision-level reasoning. Zoom\nEye treats an image as a hierarchical tree structure, where each child node\nrepresents a zoomed-in sub-region of its parent, and the root corresponds to\nthe full image. The algorithm enables MLLMs to simulate human-like zooming\nbehavior by navigating from root to leaf nodes in search of task-relevant\nvisual evidence. We experiment on a series of high-resolution benchmarks and\nthe results demonstrate that Zoom Eye consistently improves the performance of\nmultiple MLLMs by a large margin (e.g., InternVL2.5-8B increases by 15.71% and\n17.69% on HR-Bench) and also enables small 3-8B MLLMs to outperform strong\nlarge models such as GPT-4o. Code: https://github.com/om-ai-lab/ZoomEye"
                },
                "authors": [
                    {
                        "name": "Haozhan Shen"
                    },
                    {
                        "name": "Kangjia Zhao"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Zilun Zhang"
                    },
                    {
                        "name": "Mingwei Zhu"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "Accepted by EMNLP-2025 Main. Project page:\n  https://szhanz.github.io/zoomeye/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16044v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16044v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11548v3",
                "updated": "2025-09-01T02:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    28,
                    23,
                    0,
                    244,
                    0
                ],
                "published": "2025-05-15T08:14:58Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    14,
                    58,
                    3,
                    135,
                    0
                ],
                "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented\n  Generation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented\n  Generation Systems"
                },
                "summary": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. While previous studies have exposed knowledge\npoisoning risks in RAG systems, existing attack methods suffer from critical\nlimitations: they either require injecting multiple poisoned documents\n(resulting in poor stealthiness) or can only function effectively on simplistic\nqueries (limiting real-world applicability). This paper reveals a more\nrealistic knowledge poisoning attack against RAG systems that achieves\nsuccessful attacks by poisoning only a single document while remaining\neffective for complex multi-hop questions involving complex relationships\nbetween multiple elements. Our proposed AuthChain address three challenges to\nensure the poisoned documents are reliably retrieved and trusted by the LLM,\neven against large knowledge bases and LLM's own knowledge. Extensive\nexperiments across six popular LLMs demonstrate that AuthChain achieves\nsignificantly higher attack success rates while maintaining superior\nstealthiness against RAG defense mechanisms compared to state-of-the-art\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. While previous studies have exposed knowledge\npoisoning risks in RAG systems, existing attack methods suffer from critical\nlimitations: they either require injecting multiple poisoned documents\n(resulting in poor stealthiness) or can only function effectively on simplistic\nqueries (limiting real-world applicability). This paper reveals a more\nrealistic knowledge poisoning attack against RAG systems that achieves\nsuccessful attacks by poisoning only a single document while remaining\neffective for complex multi-hop questions involving complex relationships\nbetween multiple elements. Our proposed AuthChain address three challenges to\nensure the poisoned documents are reliably retrieved and trusted by the LLM,\neven against large knowledge bases and LLM's own knowledge. Extensive\nexperiments across six popular LLMs demonstrate that AuthChain achieves\nsignificantly higher attack success rates while maintaining superior\nstealthiness against RAG defense mechanisms compared to state-of-the-art\nbaselines."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yuekai Huang"
                    },
                    {
                        "name": "Ziyou Jiang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "arxiv_comment": "15pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07129v2",
                "updated": "2025-09-01T02:23:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    23,
                    59,
                    0,
                    244,
                    0
                ],
                "published": "2025-06-08T13:15:34Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    13,
                    15,
                    34,
                    6,
                    159,
                    0
                ],
                "title": "Energy Efficiency Maximization for Movable Antenna Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Efficiency Maximization for Movable Antenna Communication Systems"
                },
                "summary": "This paper investigates energy efficiency maximization for movable antenna\n(MA)-aided multi-user uplink communication systems by considering the time\ndelay and energy consumption incurred by practical antenna movement. We first\nexamine the special case with a single user and propose an optimization\nalgorithm based on the one-dimensional (1D) exhaustive search to maximize the\nuser's energy efficiency. Moreover, we derive an upper bound on the energy\nefficiency and analyze the conditions required to achieve this performance\nbound under different numbers of channel paths. Then, for the general\nmulti-user scenario, we propose an iterative algorithm to fairly maximize the\nminimum energy efficiency among all users. Simulation results demonstrate the\neffectiveness of the proposed scheme in improving energy efficiency compared to\nexisting MA schemes that do not account for movement-related costs, as well as\nthe conventional fixed-position antenna (FPA) scheme. In addition, the results\nshow the robustness of the proposed scheme to imperfect channel state\ninformation (CSI) and provide valuable insights for practical system\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates energy efficiency maximization for movable antenna\n(MA)-aided multi-user uplink communication systems by considering the time\ndelay and energy consumption incurred by practical antenna movement. We first\nexamine the special case with a single user and propose an optimization\nalgorithm based on the one-dimensional (1D) exhaustive search to maximize the\nuser's energy efficiency. Moreover, we derive an upper bound on the energy\nefficiency and analyze the conditions required to achieve this performance\nbound under different numbers of channel paths. Then, for the general\nmulti-user scenario, we propose an iterative algorithm to fairly maximize the\nminimum energy efficiency among all users. Simulation results demonstrate the\neffectiveness of the proposed scheme in improving energy efficiency compared to\nexisting MA schemes that do not account for movement-related costs, as well as\nthe conventional fixed-position antenna (FPA) scheme. In addition, the results\nshow the robustness of the proposed scheme to imperfect channel state\ninformation (CSI) and provide valuable insights for practical system\ndeployment."
                },
                "authors": [
                    {
                        "name": "Jingze Ding"
                    },
                    {
                        "name": "Zijian Zhou"
                    },
                    {
                        "name": "Lipeng Zhu"
                    },
                    {
                        "name": "Yuping Zhao"
                    },
                    {
                        "name": "Bingli Jiao"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_doi": "10.1109/TWC.2025.3597735",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TWC.2025.3597735",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.07129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by IEEE Transactions on Wireless\n  Communications",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11727v3",
                "updated": "2025-09-01T02:10:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    10,
                    58,
                    0,
                    244,
                    0
                ],
                "published": "2024-08-21T15:54:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    54,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "Efficient Detection of Toxic Prompts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Detection of Toxic Prompts in Large Language Models"
                },
                "summary": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Junzhe Yu"
                    },
                    {
                        "name": "Huijia Sun"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Yuqi Chen"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10049v3",
                "updated": "2025-09-01T02:08:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    2,
                    8,
                    42,
                    0,
                    244,
                    0
                ],
                "published": "2024-09-16T07:21:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    21,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation"
                },
                "summary": "This paper proposes a lightweight systematic solution for multi-robot\ncoordinated navigation with decentralized cooperative perception. An\ninformation flow is first created to facilitate real-time observation sharing\nover unreliable ad-hoc networks. Then, the environmental uncertainties of each\nrobot are reduced by interaction fields that deliver complementary information.\nFinally, path optimization is achieved, enabling self-organized coordination\nwith effective convergence, divergence, and collision avoidance. Our method is\nfully interpretable and ready for deployment without gaps. Comprehensive\nsimulations and real-world experiments demonstrate reduced path redundancy,\nrobust performance across various tasks, and minimal demands on computation and\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a lightweight systematic solution for multi-robot\ncoordinated navigation with decentralized cooperative perception. An\ninformation flow is first created to facilitate real-time observation sharing\nover unreliable ad-hoc networks. Then, the environmental uncertainties of each\nrobot are reduced by interaction fields that deliver complementary information.\nFinally, path optimization is achieved, enabling self-organized coordination\nwith effective convergence, divergence, and collision avoidance. Our method is\nfully interpretable and ready for deployment without gaps. Comprehensive\nsimulations and real-world experiments demonstrate reduced path redundancy,\nrobust performance across various tasks, and minimal demands on computation and\ncommunication."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Weining Lu"
                    },
                    {
                        "name": "Qingquan Lin"
                    },
                    {
                        "name": "Litong Meng"
                    },
                    {
                        "name": "Haolu Li"
                    },
                    {
                        "name": "Bin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Liang"
                },
                "author": "Bin Liang",
                "arxiv_doi": "10.1109/TASE.2025.3604178",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TASE.2025.3604178",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.10049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 figures, accepted in IEEE Transactions on Automation\n  Science and Engineering",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19611v2",
                "updated": "2025-09-01T01:38:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    38,
                    20,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-27T06:45:06Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    6,
                    45,
                    6,
                    2,
                    239,
                    0
                ],
                "title": "Instructional Agents: LLM Agents on Automated Course Material Generation\n  for Teaching Faculties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructional Agents: LLM Agents on Automated Course Material Generation\n  for Teaching Faculties"
                },
                "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Wanpeng Xu"
                    },
                    {
                        "name": "Justin Turnau"
                    },
                    {
                        "name": "Nadia Kellam"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21304v2",
                "updated": "2025-09-01T01:33:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    33,
                    50,
                    0,
                    244,
                    0
                ],
                "published": "2025-08-29T01:59:34Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    1,
                    59,
                    34,
                    4,
                    241,
                    0
                ],
                "title": "ORCA: ORchestrating Causal Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORCA: ORchestrating Causal Agent"
                },
                "summary": "Causal inference is essential for decision-making science while the\ncomplexity of the data analysis workflow, ranging from data wrangling to causal\nanalysis, increases substantially as the scale of data grows in complicated\nbusiness environments. Especially, the execution of the workflow in relational\ndatabases by non-experts can result in repetitive bottlenecks which impede\ntimely and responsible business insights. To address this challenge, we propose\nORCA (Orchestrating Causal Agent), an LLM agentic system that can automate\nroutine workflows in RDBMS while preserving expert oversight via human-AI\ninteractions. ORCA orchestrates the full data analysis pipeline: interpreting\nnatural language queries, navigating tables from DB servers, generating proper\nSQL codes, preprocessing data, and configuring modeling processes using causal\ninference libraries. Domain experts still can control the automation through\niterative interactions with ORCA, enabling robust data-driven decision making\nwith less technical expertise in statistical computing. Empirical evaluations\non benchmark and synthetic e-commerce datasets demonstrate competitive\nperformance of ORCA in table understanding, query generation, and cause-effect\nestimation -- achieving over $7\\times$ improvement in estimating average\ntreatment compared to GPT-4o mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference is essential for decision-making science while the\ncomplexity of the data analysis workflow, ranging from data wrangling to causal\nanalysis, increases substantially as the scale of data grows in complicated\nbusiness environments. Especially, the execution of the workflow in relational\ndatabases by non-experts can result in repetitive bottlenecks which impede\ntimely and responsible business insights. To address this challenge, we propose\nORCA (Orchestrating Causal Agent), an LLM agentic system that can automate\nroutine workflows in RDBMS while preserving expert oversight via human-AI\ninteractions. ORCA orchestrates the full data analysis pipeline: interpreting\nnatural language queries, navigating tables from DB servers, generating proper\nSQL codes, preprocessing data, and configuring modeling processes using causal\ninference libraries. Domain experts still can control the automation through\niterative interactions with ORCA, enabling robust data-driven decision making\nwith less technical expertise in statistical computing. Empirical evaluations\non benchmark and synthetic e-commerce datasets demonstrate competitive\nperformance of ORCA in table understanding, query generation, and cause-effect\nestimation -- achieving over $7\\times$ improvement in estimating average\ntreatment compared to GPT-4o mini."
                },
                "authors": [
                    {
                        "name": "Joanie Hayoun Chung"
                    },
                    {
                        "name": "Chaemyung Lim"
                    },
                    {
                        "name": "Sumin Lee"
                    },
                    {
                        "name": "Songseong Kim"
                    },
                    {
                        "name": "Sungbin Lim"
                    }
                ],
                "author_detail": {
                    "name": "Sungbin Lim"
                },
                "author": "Sungbin Lim",
                "arxiv_comment": "24 pages, 17 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06794v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06794v4",
                "updated": "2025-08-31T23:37:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    23,
                    37,
                    11,
                    6,
                    243,
                    0
                ],
                "published": "2025-03-09T22:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    22,
                    16,
                    48,
                    6,
                    68,
                    0
                ],
                "title": "Does Acceleration Cause Hidden Instability in Vision Language Models?\n  Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Acceleration Cause Hidden Instability in Vision Language Models?\n  Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study"
                },
                "summary": "Vision-Language Models (VLMs) are powerful yet computationally intensive for\nwidespread practical deployments. To address such challenge without costly\nre-training, post-training acceleration techniques like quantization and token\nreduction are extensively explored. However, current acceleration evaluations\nprimarily target minimal overall performance degradation, overlooking a crucial\nquestion: does the accelerated model still give the same answers to the same\nquestions as it did before acceleration? This is vital for stability-centered\nindustrial applications where consistently correct answers for specific, known\nsituations are paramount, such as in AI-based disease diagnosis. We\nsystematically investigate this for accelerated VLMs, testing four leading\nmodels (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration\nmethods on ten multi-modal benchmarks. Our findings are stark: despite minimal\naggregate performance drops, accelerated models changed original answers up to\n20% of the time. Critically, up to 6.5% of these changes converted correct\nanswers to incorrect. Input perturbations magnified these inconsistencies, and\nthe trend is confirmed by case studies with the medical VLM LLaVA-Med. This\nresearch reveals a significant oversight in VLM acceleration, stressing an\nurgent need for instance-level stability checks to ensure trustworthy\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are powerful yet computationally intensive for\nwidespread practical deployments. To address such challenge without costly\nre-training, post-training acceleration techniques like quantization and token\nreduction are extensively explored. However, current acceleration evaluations\nprimarily target minimal overall performance degradation, overlooking a crucial\nquestion: does the accelerated model still give the same answers to the same\nquestions as it did before acceleration? This is vital for stability-centered\nindustrial applications where consistently correct answers for specific, known\nsituations are paramount, such as in AI-based disease diagnosis. We\nsystematically investigate this for accelerated VLMs, testing four leading\nmodels (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration\nmethods on ten multi-modal benchmarks. Our findings are stark: despite minimal\naggregate performance drops, accelerated models changed original answers up to\n20% of the time. Critically, up to 6.5% of these changes converted correct\nanswers to incorrect. Input perturbations magnified these inconsistencies, and\nthe trend is confirmed by case studies with the medical VLM LLaVA-Med. This\nresearch reveals a significant oversight in VLM acceleration, stressing an\nurgent need for instance-level stability checks to ensure trustworthy\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Yizheng Sun"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Hongpeng Zhou"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Jingyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Sun"
                },
                "author": "Jingyuan Sun",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06794v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06794v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08325v2",
                "updated": "2025-08-31T23:18:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    23,
                    18,
                    38,
                    6,
                    243,
                    0
                ],
                "published": "2025-07-11T05:31:35Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    31,
                    35,
                    4,
                    192,
                    0
                ],
                "title": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation"
                },
                "summary": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics."
                },
                "authors": [
                    {
                        "name": "Yinzhu Quan"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16839v3",
                "updated": "2025-08-31T22:39:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    22,
                    39,
                    41,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-22T23:34:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    23,
                    34,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "Route-and-Execute: Auditable Model-Card Matching and Specialty-Level\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Route-and-Execute: Auditable Model-Card Matching and Specialty-Level\n  Deployment"
                },
                "summary": "Clinical workflows are fragmented as a patchwork of scripts and task-specific\nnetworks that often handle triage, task selection, and model deployment. These\npipelines are rarely streamlined for data science pipeline, reducing efficiency\nand raising operational costs. Workflows also lack data-driven model\nidentification (from imaging/tabular inputs) and standardized delivery of model\noutputs. In response, we present a practical, healthcare-first framework that\nuses a single vision-language model (VLM) in two complementary roles. First\n(Solution 1), the VLM acts as an aware model-card matcher that routes an\nincoming image to the appropriate specialist model via a three-stage workflow\n(modality -> primary abnormality -> model-card id). Checks are provided by (i)\nstagewise prompts that allow early exit via None/Normal/Other and (ii) a\nstagewise answer selector that arbitrates between the top-2 candidates at each\nstage, reducing the chance of an incorrect selection and aligning the workflow\nwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on\nspecialty-specific datasets ensuring a single model covers multiple downstream\ntasks within each specialty, maintaining performance while simplifying\ndeployment. Across gastroenterology, hematology, ophthalmology, and pathology,\nour single-model deployment matches or approaches specialized baselines.\n  Compared with pipelines composed of many task-specific agents, this approach\nshows that one VLM can both decide and do. It may reduce effort by data\nscientists, shorten monitoring, increase the transparency of model selection\n(with per-stage justifications), and lower integration overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical workflows are fragmented as a patchwork of scripts and task-specific\nnetworks that often handle triage, task selection, and model deployment. These\npipelines are rarely streamlined for data science pipeline, reducing efficiency\nand raising operational costs. Workflows also lack data-driven model\nidentification (from imaging/tabular inputs) and standardized delivery of model\noutputs. In response, we present a practical, healthcare-first framework that\nuses a single vision-language model (VLM) in two complementary roles. First\n(Solution 1), the VLM acts as an aware model-card matcher that routes an\nincoming image to the appropriate specialist model via a three-stage workflow\n(modality -> primary abnormality -> model-card id). Checks are provided by (i)\nstagewise prompts that allow early exit via None/Normal/Other and (ii) a\nstagewise answer selector that arbitrates between the top-2 candidates at each\nstage, reducing the chance of an incorrect selection and aligning the workflow\nwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on\nspecialty-specific datasets ensuring a single model covers multiple downstream\ntasks within each specialty, maintaining performance while simplifying\ndeployment. Across gastroenterology, hematology, ophthalmology, and pathology,\nour single-model deployment matches or approaches specialized baselines.\n  Compared with pipelines composed of many task-specific agents, this approach\nshows that one VLM can both decide and do. It may reduce effort by data\nscientists, shorten monitoring, increase the transparency of model selection\n(with per-stage justifications), and lower integration overhead."
                },
                "authors": [
                    {
                        "name": "Shayan Vassef"
                    },
                    {
                        "name": "Soorya Ram Shimegekar"
                    },
                    {
                        "name": "Abhay Goyal"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Pi Zonooz"
                    },
                    {
                        "name": "Navin Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Navin Kumar"
                },
                "author": "Navin Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12984v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12984v3",
                "updated": "2025-08-31T22:12:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    22,
                    12,
                    47,
                    6,
                    243,
                    0
                ],
                "published": "2025-04-17T14:45:03Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    45,
                    3,
                    3,
                    107,
                    0
                ],
                "title": "Tilus: A Tile-Level GPGPU Programming Language for Low-Precision\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tilus: A Tile-Level GPGPU Programming Language for Low-Precision\n  Computation"
                },
                "summary": "Serving Large Language Models (LLMs) is critical for AI-powered applications,\nyet it demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance\nbecause of high-level GPU programming abstractions. These abstractions restrict\ncritical optimizations, such as fine-grained register management and optimized\nmemory access patterns, that are essential for efficient low-precision\ncomputations. In this paper, we introduce Tilus, a domain-specific language\ndesigned for General-Purpose GPU (GPGPU) computing that supports low-precision\ndata types with arbitrary bit widths from 1 to 8 while maintaining GPU\nprogrammability. Tilus features a thread-block-level programming model, a\nhierarchical memory space, a novel algebraic layout system, and extensive\nsupport for diverse low-precision data types. Tilus programs are compiled into\nhighly efficient GPU programs through automatic vectorization and instruction\nselection. Extensive experiments demonstrate that Tilus efficiently supports a\nfull spectrum of low-precision data types, and outperforms state-of-the-art\nlow-precision kernels. Compared to existing compilers such as Triton and\nLadder, as well as hand-optimized kernels such as QuantLLM and Marlin, Tilus\nachieves performance improvements of: $1.75\\times$, $2.61\\times$, $1.29\\times$\nand $1.03\\times$, respectively. We open-source Tilus at\nhttps://github.com/NVIDIA/tilus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) is critical for AI-powered applications,\nyet it demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance\nbecause of high-level GPU programming abstractions. These abstractions restrict\ncritical optimizations, such as fine-grained register management and optimized\nmemory access patterns, that are essential for efficient low-precision\ncomputations. In this paper, we introduce Tilus, a domain-specific language\ndesigned for General-Purpose GPU (GPGPU) computing that supports low-precision\ndata types with arbitrary bit widths from 1 to 8 while maintaining GPU\nprogrammability. Tilus features a thread-block-level programming model, a\nhierarchical memory space, a novel algebraic layout system, and extensive\nsupport for diverse low-precision data types. Tilus programs are compiled into\nhighly efficient GPU programs through automatic vectorization and instruction\nselection. Extensive experiments demonstrate that Tilus efficiently supports a\nfull spectrum of low-precision data types, and outperforms state-of-the-art\nlow-precision kernels. Compared to existing compilers such as Triton and\nLadder, as well as hand-optimized kernels such as QuantLLM and Marlin, Tilus\nachieves performance improvements of: $1.75\\times$, $2.61\\times$, $1.29\\times$\nand $1.03\\times$, respectively. We open-source Tilus at\nhttps://github.com/NVIDIA/tilus."
                },
                "authors": [
                    {
                        "name": "Yaoyao Ding"
                    },
                    {
                        "name": "Bohan Hou"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Allan Lin"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Cody Yu Hao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_comment": "17 pages, 14 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12984v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12984v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17182v2",
                "updated": "2025-08-31T21:27:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    21,
                    27,
                    41,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-24T01:43:48Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    1,
                    43,
                    48,
                    6,
                    236,
                    0
                ],
                "title": "LLM Assertiveness can be Mechanistically Decomposed into Emotional and\n  Logical Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Assertiveness can be Mechanistically Decomposed into Emotional and\n  Logical Components"
                },
                "summary": "Large Language Models (LLMs) often display overconfidence, presenting\ninformation with unwarranted certainty in high-stakes contexts. We investigate\nthe internal basis of this behavior via mechanistic interpretability. Using\nopen-sourced Llama 3.2 models fine-tuned on human annotated assertiveness\ndatasets, we extract residual activations across all layers, and compute\nsimilarity metrics to localize assertive representations. Our analysis\nidentifies layers most sensitive to assertiveness contrasts and reveals that\nhigh-assertive representations decompose into two orthogonal sub-components of\nemotional and logical clusters-paralleling the dual-route Elaboration\nLikelihood Model in Psychology. Steering vectors derived from these\nsub-components show distinct causal effects: emotional vectors broadly\ninfluence prediction accuracy, while logical vectors exert more localized\neffects. These findings provide mechanistic evidence for the multi-component\nstructure of LLM assertiveness and highlight avenues for mitigating\noverconfident behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often display overconfidence, presenting\ninformation with unwarranted certainty in high-stakes contexts. We investigate\nthe internal basis of this behavior via mechanistic interpretability. Using\nopen-sourced Llama 3.2 models fine-tuned on human annotated assertiveness\ndatasets, we extract residual activations across all layers, and compute\nsimilarity metrics to localize assertive representations. Our analysis\nidentifies layers most sensitive to assertiveness contrasts and reveals that\nhigh-assertive representations decompose into two orthogonal sub-components of\nemotional and logical clusters-paralleling the dual-route Elaboration\nLikelihood Model in Psychology. Steering vectors derived from these\nsub-components show distinct causal effects: emotional vectors broadly\ninfluence prediction accuracy, while logical vectors exert more localized\neffects. These findings provide mechanistic evidence for the multi-component\nstructure of LLM assertiveness and highlight avenues for mitigating\noverconfident behavior."
                },
                "authors": [
                    {
                        "name": "Hikaru Tsujimura"
                    },
                    {
                        "name": "Arush Tagade"
                    }
                ],
                "author_detail": {
                    "name": "Arush Tagade"
                },
                "author": "Arush Tagade",
                "arxiv_comment": "This preprint is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17117v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17117v4",
                "updated": "2025-08-31T19:56:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    19,
                    56,
                    10,
                    6,
                    243,
                    0
                ],
                "published": "2025-05-21T16:29:00Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    29,
                    0,
                    2,
                    141,
                    0
                ],
                "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning"
                },
                "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations."
                },
                "authors": [
                    {
                        "name": "Chen Shani"
                    },
                    {
                        "name": "Liron Soffer"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17117v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17117v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13442v3",
                "updated": "2025-08-31T18:53:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    18,
                    53,
                    24,
                    6,
                    243,
                    0
                ],
                "published": "2024-08-24T02:48:40Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    2,
                    48,
                    40,
                    5,
                    237,
                    0
                ],
                "title": "A Law of Next-Token Prediction in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Law of Next-Token Prediction in Large Language Models"
                },
                "summary": "Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, irrespective of their architectures or pre-training\ndata. We demonstrate that this law offers new perspectives and actionable\ninsights to inform and guide practices in LLM development and applications,\nincluding model scaling, pre-training tasks, and interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely employed across various\napplication domains, yet their black-box nature poses significant challenges to\nunderstanding how these models process input data internally to make\npredictions. In this paper, we introduce a precise and quantitative law that\ngoverns the learning of contextualized token embeddings through intermediate\nlayers in pre-trained LLMs for next-token prediction. Our findings reveal that\neach layer contributes equally to enhancing prediction accuracy, from the\nlowest to the highest layer -- a universal phenomenon observed across a diverse\narray of open-source LLMs, irrespective of their architectures or pre-training\ndata. We demonstrate that this law offers new perspectives and actionable\ninsights to inform and guide practices in LLM development and applications,\nincluding model scaling, pre-training tasks, and interpretation."
                },
                "authors": [
                    {
                        "name": "Hangfeng He"
                    },
                    {
                        "name": "Weijie J. Su"
                    }
                ],
                "author_detail": {
                    "name": "Weijie J. Su"
                },
                "author": "Weijie J. Su",
                "arxiv_comment": "Transferred for publication to Physical Review E from Physical Review\n  Research (to waive publication charges)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11465v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11465v4",
                "updated": "2025-08-31T16:55:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    55,
                    47,
                    6,
                    243,
                    0
                ],
                "published": "2024-11-18T10:58:46Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    10,
                    58,
                    46,
                    0,
                    323,
                    0
                ],
                "title": "Re-examining learning linear functions in context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-examining learning linear functions in context"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful paradigm for easily\nadapting Large Language Models (LLMs) to various tasks. However, our\nunderstanding of how ICL works remains limited. We explore a simple model of\nICL in a controlled setup with synthetic training data to investigate ICL of\nunivariate linear functions. We experiment with a range of GPT-2-like\ntransformer models trained from scratch. Our findings challenge the prevailing\nnarrative that transformers adopt algorithmic approaches like linear regression\nto learn a linear function in-context. These models fail to generalize beyond\ntheir training distribution, highlighting fundamental limitations in their\ncapacity to infer abstract task structures. Our experiments lead us to propose\na mathematically precise hypothesis of what the model might be learning."
                },
                "authors": [
                    {
                        "name": "Omar Naim"
                    },
                    {
                        "name": "Guilhem Fouilhé"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "arxiv_doi": "10.1007/978-3-032-02813-6_8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-02813-6_8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.11465v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11465v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "KI 2025: Advances in Artificial Intelligence",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03665v2",
                "updated": "2025-08-31T16:24:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    24,
                    59,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-05T17:24:50Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    17,
                    24,
                    50,
                    1,
                    217,
                    0
                ],
                "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design"
                },
                "summary": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts."
                },
                "authors": [
                    {
                        "name": "Claudiu Leoveanu-Condrei"
                    }
                ],
                "author_detail": {
                    "name": "Claudiu Leoveanu-Condrei"
                },
                "author": "Claudiu Leoveanu-Condrei",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.2; I.1.2; D.1.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05702v2",
                "updated": "2025-08-31T15:44:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    44,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-07T01:10:28Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    1,
                    10,
                    28,
                    3,
                    219,
                    0
                ],
                "title": "Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grid-Agent: An LLM-Powered Multi-Agent System for Power Grid Control"
                },
                "summary": "The increasing penetration of Distributed Energy Resources (DERs), widespread\nadoption of Electric Vehicles (EVs), and the growing frequency of extreme\nweather events have significantly increased the complexity of power grid\nplanning, operation, and management. Traditional rule-based systems and\nnumerical optimization approaches often struggle with the scale, dynamics, and\nadaptability required by modern power networks. This paper introduces\nGrid-Agent, an autonomous, AI-driven framework that combines Large Language\nModels (LLMs) with multi-agent reinforcement learning to detect and remediate\ngrid violations in real time. Grid-Agent integrates semantic reasoning with\nnumerical precision through a modular agent architecture: a planning agent\ngenerates coordinated action sequences using numerical power flow solvers,\nwhile a validation agent evaluates system stability and action effectiveness\nvia sandboxed execution with safety rollbacks. To ensure scalability,\nGrid-Agent incorporates an adaptive multiscale network representation that\ndynamically selects optimal encoding schemes based on network size and\ncomplexity. The framework enables coordinated violation resolution through\noptimizing switch configurations, battery deployment, and load curtailment\nstrategies. Experimental results in standard IEEE and CIGRE test systems (IEEE\n69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation\nperformance. Additionally, the framework's built-in data collection and\nlearning capabilities enable continuous learning and adaptation to diverse\nnetwork topologies. The autonomous nature of the framework makes it\nparticularly suitable for modern smart grid applications requiring rapid\nresponse to dynamic operating conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing penetration of Distributed Energy Resources (DERs), widespread\nadoption of Electric Vehicles (EVs), and the growing frequency of extreme\nweather events have significantly increased the complexity of power grid\nplanning, operation, and management. Traditional rule-based systems and\nnumerical optimization approaches often struggle with the scale, dynamics, and\nadaptability required by modern power networks. This paper introduces\nGrid-Agent, an autonomous, AI-driven framework that combines Large Language\nModels (LLMs) with multi-agent reinforcement learning to detect and remediate\ngrid violations in real time. Grid-Agent integrates semantic reasoning with\nnumerical precision through a modular agent architecture: a planning agent\ngenerates coordinated action sequences using numerical power flow solvers,\nwhile a validation agent evaluates system stability and action effectiveness\nvia sandboxed execution with safety rollbacks. To ensure scalability,\nGrid-Agent incorporates an adaptive multiscale network representation that\ndynamically selects optimal encoding schemes based on network size and\ncomplexity. The framework enables coordinated violation resolution through\noptimizing switch configurations, battery deployment, and load curtailment\nstrategies. Experimental results in standard IEEE and CIGRE test systems (IEEE\n69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation\nperformance. Additionally, the framework's built-in data collection and\nlearning capabilities enable continuous learning and adaptation to diverse\nnetwork topologies. The autonomous nature of the framework makes it\nparticularly suitable for modern smart grid applications requiring rapid\nresponse to dynamic operating conditions."
                },
                "authors": [
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07612v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07612v3",
                "updated": "2025-08-31T15:19:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    19,
                    7,
                    6,
                    243,
                    0
                ],
                "published": "2025-04-10T10:03:29Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    3,
                    29,
                    3,
                    100,
                    0
                ],
                "title": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline\n  Dataset"
                },
                "summary": "The primary goal of a news headline is to summarize an event in as few words\nas possible. Depending on the media outlet, a headline can serve as a means to\nobjectively deliver a summary or improve its visibility. For the latter,\nspecific publications may employ stylistic approaches that incorporate the use\nof sarcasm, irony, and exaggeration, key elements of a satirical approach. As\nsuch, even the headline must reflect the tone of the satirical main content.\nCurrent approaches for the Romanian language tend to detect the\nnon-conventional tone (i.e., satire and clickbait) of the news content by\ncombining both the main article and the headline. Because we consider a\nheadline to be merely a brief summary of the main article, we investigate in\nthis paper the presence of satirical tone in headlines alone, testing multiple\nbaselines ranging from standard machine learning algorithms to deep learning\nmodels. Our experiments show that Bidirectional Transformer models outperform\nboth standard machine-learning approaches and Large Language Models (LLMs),\nparticularly when the meta-learning Reptile approach is employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The primary goal of a news headline is to summarize an event in as few words\nas possible. Depending on the media outlet, a headline can serve as a means to\nobjectively deliver a summary or improve its visibility. For the latter,\nspecific publications may employ stylistic approaches that incorporate the use\nof sarcasm, irony, and exaggeration, key elements of a satirical approach. As\nsuch, even the headline must reflect the tone of the satirical main content.\nCurrent approaches for the Romanian language tend to detect the\nnon-conventional tone (i.e., satire and clickbait) of the news content by\ncombining both the main article and the headline. Because we consider a\nheadline to be merely a brief summary of the main article, we investigate in\nthis paper the presence of satirical tone in headlines alone, testing multiple\nbaselines ranging from standard machine learning algorithms to deep learning\nmodels. Our experiments show that Bidirectional Transformer models outperform\nboth standard machine-learning approaches and Large Language Models (LLMs),\nparticularly when the meta-learning Reptile approach is employed."
                },
                "authors": [
                    {
                        "name": "Mihnea-Alexandru Vîrlan"
                    },
                    {
                        "name": "Răzvan-Alexandru Smădu"
                    },
                    {
                        "name": "Dumitru-Clementin Cercel"
                    },
                    {
                        "name": "Florin Pop"
                    },
                    {
                        "name": "Mihaela-Claudia Cercel"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela-Claudia Cercel"
                },
                "author": "Mihaela-Claudia Cercel",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07612v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07612v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07407v2",
                "updated": "2025-08-31T14:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    55,
                    5,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-10T16:07:32Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    16,
                    7,
                    32,
                    6,
                    222,
                    0
                ],
                "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm\n  Bridging Foundation Models and Lifelong Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm\n  Bridging Foundation Models and Lifelong Agentic Systems"
                },
                "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems."
                },
                "authors": [
                    {
                        "name": "Jinyuan Fang"
                    },
                    {
                        "name": "Yanwen Peng"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Xinhao Yi"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Nikos Aletras"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqiao Meng"
                },
                "author": "Zaiqiao Meng",
                "arxiv_comment": "Github Repo:\n  https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09701v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09701v5",
                "updated": "2025-09-03T08:35:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    35,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-14T04:01:25Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    4,
                    1,
                    25,
                    4,
                    166,
                    0
                ],
                "title": "Towards Explainable Vulnerability Detection with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Vulnerability Detection with Large Language Models"
                },
                "summary": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security."
                },
                "authors": [
                    {
                        "name": "Qiheng Mao"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Jianling Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianling Sun"
                },
                "author": "Jianling Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09701v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09701v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17761v2",
                "updated": "2025-08-31T14:54:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    54,
                    18,
                    6,
                    243,
                    0
                ],
                "published": "2025-06-21T16:58:30Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    16,
                    58,
                    30,
                    5,
                    172,
                    0
                ],
                "title": "Towards a Unified Textual Graph Framework for Spectral Reasoning via\n  Physical and Chemical Information Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified Textual Graph Framework for Spectral Reasoning via\n  Physical and Chemical Information Fusion"
                },
                "summary": "Motivated by the limitations of current spectral analysis methods-such as\nreliance on single-modality data, limited generalizability, and poor\ninterpretability-we propose a novel multi-modal spectral analysis framework\nthat integrates prior knowledge graphs with Large Language Models. Our method\nexplicitly bridges physical spectral measurements and chemical structural\nsemantics by representing them in a unified Textual Graph format, enabling\nflexible, interpretable, and generalizable spectral understanding. Raw spectra\nare first transformed into TAGs, where nodes and edges are enriched with\ntextual attributes describing both spectral properties and chemical context.\nThese are then merged with relevant prior knowledge-including functional groups\nand molecular graphs-to form a Task Graph that incorporates \"Prompt Nodes\"\nsupporting LLM-based contextual reasoning. A Graph Neural Network further\nprocesses this structure to complete downstream tasks. This unified design\nenables seamless multi-modal integration and automated feature decoding with\nminimal manual annotation. Our framework achieves consistently high performance\nacross multiple spectral analysis tasks, including node-level, edge-level, and\ngraph-level classification. It demonstrates robust generalization in both\nzero-shot and few-shot settings, highlighting its effectiveness in learning\nfrom limited data and supporting in-context reasoning. This work establishes a\nscalable and interpretable foundation for LLM-driven spectral analysis,\nunifying physical and chemical modalities for scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the limitations of current spectral analysis methods-such as\nreliance on single-modality data, limited generalizability, and poor\ninterpretability-we propose a novel multi-modal spectral analysis framework\nthat integrates prior knowledge graphs with Large Language Models. Our method\nexplicitly bridges physical spectral measurements and chemical structural\nsemantics by representing them in a unified Textual Graph format, enabling\nflexible, interpretable, and generalizable spectral understanding. Raw spectra\nare first transformed into TAGs, where nodes and edges are enriched with\ntextual attributes describing both spectral properties and chemical context.\nThese are then merged with relevant prior knowledge-including functional groups\nand molecular graphs-to form a Task Graph that incorporates \"Prompt Nodes\"\nsupporting LLM-based contextual reasoning. A Graph Neural Network further\nprocesses this structure to complete downstream tasks. This unified design\nenables seamless multi-modal integration and automated feature decoding with\nminimal manual annotation. Our framework achieves consistently high performance\nacross multiple spectral analysis tasks, including node-level, edge-level, and\ngraph-level classification. It demonstrates robust generalization in both\nzero-shot and few-shot settings, highlighting its effectiveness in learning\nfrom limited data and supporting in-context reasoning. This work establishes a\nscalable and interpretable foundation for LLM-driven spectral analysis,\nunifying physical and chemical modalities for scientific applications."
                },
                "authors": [
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Ziru Yu"
                    },
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Yuchen Guo"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Xiangyang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Yu"
                },
                "author": "Xiangyang Yu",
                "arxiv_comment": "We need to further modify and supplement the experiment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04078v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04078v3",
                "updated": "2025-08-31T14:41:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    41,
                    6,
                    6,
                    243,
                    0
                ],
                "published": "2025-06-04T15:43:14Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    15,
                    43,
                    14,
                    2,
                    155,
                    0
                ],
                "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with\n  Physician Validation"
                },
                "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Huayu Sha"
                    },
                    {
                        "name": "Binze Hu"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Chenhao Huang"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04078v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04078v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17671v2",
                "updated": "2025-08-31T14:32:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    32,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-06-21T10:06:07Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    10,
                    6,
                    7,
                    5,
                    172,
                    0
                ],
                "title": "TPTT: Transforming Pretrained Transformers into Titans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPTT: Transforming Pretrained Transformers into Titans"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved strong\nperformance across many natural language processing tasks. Nonetheless, their\nquadratic computational and memory requirements, particularly in self-attention\nlayers, pose challenges for efficient inference on long contexts and for\ndeployment in resource-limited environments. We present TPTT (Transforming\nPretrained Transformers into Titans), a framework designed to augment\npretrained Transformers with linearized attention (LiZA) and internal memory\ngating via Memory as Gate (MaG), applied without full retraining. TPTT supports\nparameter-efficient fine-tuning (LoRA) and integrates with standard toolkits\nsuch as Hugging Face Transformers. We evaluated TPTT on several pretrained\nmodels, including Llama-1B, OlMoE-1B-7B, Qwen2.5-1.5B, Gemma3-270m,\nOpenELM-1.3B, and Mistral-7B, in order to assess applicability across\narchitectures of different scales. Experiments on models with approximately 1\nbillion parameters, evaluated primarily on the MMLU benchmark, suggest\npotential improvements in both efficiency and accuracy compared to baseline\nmodels. For example, Titans-Llama-1B exhibited up to a 20\\% relative increase\nin Exact Match scores in one-shot evaluation. An additional finding is that it\nis possible to convert a quadratic-attention model into a purely\nlinear-attention model using the DeltaProduct mechanism. All training runs were\ncarried out with modest computational resources. These preliminary findings\nindicate that TPTT may help adapt pretrained LLMs for long-context tasks with\nlimited overhead. Further studies on larger models and a broader set of\nbenchmarks will be necessary to evaluate the generality and robustness of the\nframework. Code is available at https://github.com/fabienfrfr/tptt . Python\npackage at https://pypi.org/project/tptt/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved strong\nperformance across many natural language processing tasks. Nonetheless, their\nquadratic computational and memory requirements, particularly in self-attention\nlayers, pose challenges for efficient inference on long contexts and for\ndeployment in resource-limited environments. We present TPTT (Transforming\nPretrained Transformers into Titans), a framework designed to augment\npretrained Transformers with linearized attention (LiZA) and internal memory\ngating via Memory as Gate (MaG), applied without full retraining. TPTT supports\nparameter-efficient fine-tuning (LoRA) and integrates with standard toolkits\nsuch as Hugging Face Transformers. We evaluated TPTT on several pretrained\nmodels, including Llama-1B, OlMoE-1B-7B, Qwen2.5-1.5B, Gemma3-270m,\nOpenELM-1.3B, and Mistral-7B, in order to assess applicability across\narchitectures of different scales. Experiments on models with approximately 1\nbillion parameters, evaluated primarily on the MMLU benchmark, suggest\npotential improvements in both efficiency and accuracy compared to baseline\nmodels. For example, Titans-Llama-1B exhibited up to a 20\\% relative increase\nin Exact Match scores in one-shot evaluation. An additional finding is that it\nis possible to convert a quadratic-attention model into a purely\nlinear-attention model using the DeltaProduct mechanism. All training runs were\ncarried out with modest computational resources. These preliminary findings\nindicate that TPTT may help adapt pretrained LLMs for long-context tasks with\nlimited overhead. Further studies on larger models and a broader set of\nbenchmarks will be necessary to evaluate the generality and robustness of the\nframework. Code is available at https://github.com/fabienfrfr/tptt . Python\npackage at https://pypi.org/project/tptt/ ."
                },
                "authors": [
                    {
                        "name": "Fabien Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Fabien Furfaro"
                },
                "author": "Fabien Furfaro",
                "arxiv_comment": "14 pages, 2 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04660v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04660v3",
                "updated": "2025-08-31T14:19:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    19,
                    8,
                    6,
                    243,
                    0
                ],
                "published": "2024-02-07T08:49:33Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    8,
                    49,
                    33,
                    2,
                    38,
                    0
                ],
                "title": "Redesigning Traffic Signs to Mitigate Machine-Learning Patch Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redesigning Traffic Signs to Mitigate Machine-Learning Patch Attacks"
                },
                "summary": "Traffic-Sign Recognition (TSR) is a critical safety component for autonomous\ndriving. Unfortunately, however, past work has highlighted the vulnerability of\nTSR models to physical-world attacks, through low-cost, easily deployable\nadversarial patches leading to misclassification. To mitigate these threats,\nmost defenses focus on altering the training process or modifying the inference\nprocedure. Still, while these approaches improve adversarial robustness, TSR\nremains susceptible to attacks attaining substantial success rates.\n  To further the adversarial robustness of TSR, this work offers a novel\napproach that redefines traffic-sign designs to create signs that promote\nrobustness while remaining interpretable to humans. Our framework takes three\ninputs: (1) A traffic-sign standard along with modifiable features and\nassociated constraints; (2) A state-of-the-art adversarial training method; and\n(3) A function for efficiently synthesizing realistic traffic-sign images.\nUsing these user-defined inputs, the framework emits an optimized traffic-sign\nstandard such that traffic signs generated per this standard enable training\nTSR models with increased adversarial robustness.\n  We evaluate the effectiveness of our framework via a concrete implementation,\nwhere we allow modifying the pictograms (i.e., symbols) and colors of traffic\nsigns. The results show substantial improvements in robustness -- with gains of\nup to 16.33%--24.58% in robust accuracy over state-of-the-art methods -- while\nbenign accuracy is even improved. Importantly, a user study also confirms that\nthe redesigned traffic signs remain easily recognizable and to human observers.\nOverall, the results highlight that carefully redesigning traffic signs can\nsignificantly enhance TSR system robustness without compromising human\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic-Sign Recognition (TSR) is a critical safety component for autonomous\ndriving. Unfortunately, however, past work has highlighted the vulnerability of\nTSR models to physical-world attacks, through low-cost, easily deployable\nadversarial patches leading to misclassification. To mitigate these threats,\nmost defenses focus on altering the training process or modifying the inference\nprocedure. Still, while these approaches improve adversarial robustness, TSR\nremains susceptible to attacks attaining substantial success rates.\n  To further the adversarial robustness of TSR, this work offers a novel\napproach that redefines traffic-sign designs to create signs that promote\nrobustness while remaining interpretable to humans. Our framework takes three\ninputs: (1) A traffic-sign standard along with modifiable features and\nassociated constraints; (2) A state-of-the-art adversarial training method; and\n(3) A function for efficiently synthesizing realistic traffic-sign images.\nUsing these user-defined inputs, the framework emits an optimized traffic-sign\nstandard such that traffic signs generated per this standard enable training\nTSR models with increased adversarial robustness.\n  We evaluate the effectiveness of our framework via a concrete implementation,\nwhere we allow modifying the pictograms (i.e., symbols) and colors of traffic\nsigns. The results show substantial improvements in robustness -- with gains of\nup to 16.33%--24.58% in robust accuracy over state-of-the-art methods -- while\nbenign accuracy is even improved. Importantly, a user study also confirms that\nthe redesigned traffic signs remain easily recognizable and to human observers.\nOverall, the results highlight that carefully redesigning traffic signs can\nsignificantly enhance TSR system robustness without compromising human\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Tsufit Shua"
                    },
                    {
                        "name": "Liron David"
                    },
                    {
                        "name": "Mahmood Sharif"
                    }
                ],
                "author_detail": {
                    "name": "Mahmood Sharif"
                },
                "author": "Mahmood Sharif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04660v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04660v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12345v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12345v3",
                "updated": "2025-08-31T14:11:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    11,
                    57,
                    6,
                    243,
                    0
                ],
                "published": "2025-03-16T03:51:06Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    3,
                    51,
                    6,
                    6,
                    75,
                    0
                ],
                "title": "General Table Question Answering via Answer-Formula Joint Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Table Question Answering via Answer-Formula Joint Generation"
                },
                "summary": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperation, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nthe Formula as the executable representation for solving complex reasoning on\ntables with different structures. Specifically, we construct\n\\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing\ndatasets. In addition, we propose \\texttt{TabAF}, a general table answering\nframework to solve multiple types of tasks over multiple types of tables\nsimultaneously, which decodes answers and Formulas with a single LLM backbone.\nExtensive experiments demonstrate the versatility and generalization of\n\\texttt{TabAF}. Under the same model size, \\texttt{TabAF} achieves new\nstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperation, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nthe Formula as the executable representation for solving complex reasoning on\ntables with different structures. Specifically, we construct\n\\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing\ndatasets. In addition, we propose \\texttt{TabAF}, a general table answering\nframework to solve multiple types of tasks over multiple types of tables\nsimultaneously, which decodes answers and Formulas with a single LLM backbone.\nExtensive experiments demonstrate the versatility and generalization of\n\\texttt{TabAF}. Under the same model size, \\texttt{TabAF} achieves new\nstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact."
                },
                "authors": [
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Hangyu Mao"
                    }
                ],
                "author_detail": {
                    "name": "Hangyu Mao"
                },
                "author": "Hangyu Mao",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12345v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12345v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01085v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01085v4",
                "updated": "2025-08-31T13:44:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    13,
                    44,
                    38,
                    6,
                    243,
                    0
                ],
                "published": "2024-07-01T08:37:41Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    8,
                    37,
                    41,
                    0,
                    183,
                    0
                ],
                "title": "Explaining Length Bias in LLM-Based Preference Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Length Bias in LLM-Based Preference Evaluations"
                },
                "summary": "The use of large language models (LLMs) as judges, particularly in preference\ncomparisons, has become widespread, but this reveals a notable bias towards\nlonger responses, undermining the reliability of such evaluations. To better\nunderstand such bias, we propose to decompose the preference evaluation metric,\nspecifically the win rate, into two key components: desirability and\ninformation mass, where the former is length-independent and related to\ntrustworthiness such as correctness, toxicity, and consistency, and the latter\nis length-dependent and represents the amount of information in the response.\nWe empirically demonstrated the decomposition through controlled experiments\nand found that response length impacts evaluations by influencing information\nmass. To derive a reliable evaluation metric that assesses content quality\nwithout being confounded by response length, we propose AdapAlpaca, a simple\nyet effective adjustment to win rate measurement. Specifically, AdapAlpaca\nensures a fair comparison of response quality by aligning the lengths of\nreference and test model responses under equivalent length intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) as judges, particularly in preference\ncomparisons, has become widespread, but this reveals a notable bias towards\nlonger responses, undermining the reliability of such evaluations. To better\nunderstand such bias, we propose to decompose the preference evaluation metric,\nspecifically the win rate, into two key components: desirability and\ninformation mass, where the former is length-independent and related to\ntrustworthiness such as correctness, toxicity, and consistency, and the latter\nis length-dependent and represents the amount of information in the response.\nWe empirically demonstrated the decomposition through controlled experiments\nand found that response length impacts evaluations by influencing information\nmass. To derive a reliable evaluation metric that assesses content quality\nwithout being confounded by response length, we propose AdapAlpaca, a simple\nyet effective adjustment to win rate measurement. Specifically, AdapAlpaca\nensures a fair comparison of response quality by aligning the lengths of\nreference and test model responses under equivalent length intervals."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Linxin Song"
                    },
                    {
                        "name": "Jieyu Zhang"
                    },
                    {
                        "name": "Zheyuan Xiao"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Zhengyu Chen"
                    },
                    {
                        "name": "Nicholas Jing Yuan"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01085v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01085v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11277v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11277v4",
                "updated": "2025-08-31T11:44:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    11,
                    44,
                    41,
                    6,
                    243,
                    0
                ],
                "published": "2025-05-16T14:11:29Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    11,
                    29,
                    4,
                    136,
                    0
                ],
                "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs"
                },
                "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."
                },
                "authors": [
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Chang Wu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Hengxing Cai"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11277v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11277v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19338v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19338v3",
                "updated": "2025-08-31T10:28:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    10,
                    28,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-03-25T04:11:47Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    4,
                    11,
                    47,
                    1,
                    84,
                    0
                ],
                "title": "Membership Inference Attacks on Large-Scale Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks on Large-Scale Models: A Survey"
                },
                "summary": "As large-scale models such as Large Language Models (LLMs) and Large\nMultimodal Models (LMMs) see increasing deployment, their privacy risks remain\nunderexplored. Membership Inference Attacks (MIAs), which reveal whether a data\npoint was used in training the target model, are an important technique for\nexposing or assessing privacy risks and have been shown to be effective across\ndiverse machine learning algorithms. However, despite extensive studies on MIAs\nin classic models, there remains a lack of systematic surveys addressing their\neffectiveness and limitations in large-scale models. To address this gap, we\nprovide the first comprehensive review of MIAs targeting LLMs and LMMs,\nanalyzing attacks by model type, adversarial knowledge, and strategy. Unlike\nprior surveys, we further examine MIAs across multiple stages of the model\npipeline, including pre-training, fine-tuning, alignment, and\nRetrieval-Augmented Generation (RAG). Finally, we identify open challenges and\npropose future research directions for strengthening privacy resilience in\nlarge-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large-scale models such as Large Language Models (LLMs) and Large\nMultimodal Models (LMMs) see increasing deployment, their privacy risks remain\nunderexplored. Membership Inference Attacks (MIAs), which reveal whether a data\npoint was used in training the target model, are an important technique for\nexposing or assessing privacy risks and have been shown to be effective across\ndiverse machine learning algorithms. However, despite extensive studies on MIAs\nin classic models, there remains a lack of systematic surveys addressing their\neffectiveness and limitations in large-scale models. To address this gap, we\nprovide the first comprehensive review of MIAs targeting LLMs and LMMs,\nanalyzing attacks by model type, adversarial knowledge, and strategy. Unlike\nprior surveys, we further examine MIAs across multiple stages of the model\npipeline, including pre-training, fine-tuning, alignment, and\nRetrieval-Augmented Generation (RAG). Finally, we identify open challenges and\npropose future research directions for strengthening privacy resilience in\nlarge-scale models."
                },
                "authors": [
                    {
                        "name": "Hengyu Wu"
                    },
                    {
                        "name": "Yang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Cao"
                },
                "author": "Yang Cao",
                "arxiv_comment": "Preprint. Submitted for peer review. The final version may differ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19338v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19338v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13647v2",
                "updated": "2025-08-31T10:23:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    10,
                    23,
                    48,
                    6,
                    243,
                    0
                ],
                "published": "2025-02-19T11:44:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    44,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "Instruction Tuning on Public Government and Cultural Data for\n  Low-Resource Language: a Case Study in Kazakh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning on Public Government and Cultural Data for\n  Low-Resource Language: a Case Study in Kazakh"
                },
                "summary": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages."
                },
                "authors": [
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Rituraj Joshi"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18798v2",
                "updated": "2025-08-31T09:26:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    9,
                    26,
                    44,
                    6,
                    243,
                    0
                ],
                "published": "2024-10-24T14:50:42Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    50,
                    42,
                    3,
                    298,
                    0
                ],
                "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs"
                },
                "summary": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs), including recognizing key\ninformation from visual inputs and conducting reasoning over it. While\nfine-tuning MLLMs for reasoning is critical, collecting and annotating charts\nand questions is expensive, hard to scale, and often results in low-quality\nannotations. To address this, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and scalable data synthesis method for\ndistilling visual reasoning abilities from LLMs to MLLMs. The code serves as an\nintermediary that translates visual chart representations into textual\nrepresentations, enabling language models to understand cross-modal information\nand generate reasoning chains accordingly. In this way, we can employ\ntext-based synthesizing techniques to expand chart-plotting code and generate\nhigh-quality Q&A pairs for training models. This produces ReachQA, a dataset\ncontaining 3k reasoning-intensive charts and 20k Q&A pairs to enhance both\nrecognition and reasoning abilities of MLLMs. Experiments show that models\nfine-tuned with ReachQA not only perform well on chart-related tasks but also\nshow performance gains on general reasoning benchmarks. The code and dataset\nare publicly available at https://github.com/hewei2001/ReachQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs), including recognizing key\ninformation from visual inputs and conducting reasoning over it. While\nfine-tuning MLLMs for reasoning is critical, collecting and annotating charts\nand questions is expensive, hard to scale, and often results in low-quality\nannotations. To address this, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and scalable data synthesis method for\ndistilling visual reasoning abilities from LLMs to MLLMs. The code serves as an\nintermediary that translates visual chart representations into textual\nrepresentations, enabling language models to understand cross-modal information\nand generate reasoning chains accordingly. In this way, we can employ\ntext-based synthesizing techniques to expand chart-plotting code and generate\nhigh-quality Q&A pairs for training models. This produces ReachQA, a dataset\ncontaining 3k reasoning-intensive charts and 20k Q&A pairs to enhance both\nrecognition and reasoning abilities of MLLMs. Experiments show that models\nfine-tuned with ReachQA not only perform well on chart-related tasks but also\nshow performance gains on general reasoning benchmarks. The code and dataset\nare publicly available at https://github.com/hewei2001/ReachQA."
                },
                "authors": [
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Wanxu Zhao"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings. The code and dataset are publicly\n  available at https://github.com/hewei2001/ReachQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12928v2",
                "updated": "2025-08-31T09:17:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    9,
                    17,
                    53,
                    6,
                    243,
                    0
                ],
                "published": "2024-12-17T14:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    7,
                    1,
                    1,
                    352,
                    0
                ],
                "title": "Truthful Text Sanitization Guided by Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthful Text Sanitization Guided by Inference Attacks"
                },
                "summary": "Text sanitization aims to rewrite parts of a document to prevent disclosure\nof personal information. The central challenge of text sanitization is to\nstrike a balance between privacy protection (avoiding the leakage of personal\ninformation) and utility preservation (retaining as much as possible of the\ndocument's original content). To this end, we introduce a novel text\nsanitization method based on generalizations, that is, broader but still\ninformative terms that subsume the semantic content of the original text spans.\nThe approach relies on the use of instruction-tuned large language models\n(LLMs) and is divided into two stages. Given a document including text spans\nexpressing personally identifiable information (PII), the LLM is first applied\nto obtain truth-preserving replacement candidates for each text span and rank\nthose according to their abstraction level. Those candidates are then evaluated\nfor their ability to protect privacy by conducting inference attacks with the\nLLM. Finally, the system selects the most informative replacement candidate\nshown to be resistant to those attacks. This two-stage process produces\nreplacements that effectively balance privacy and utility.\n  We also present novel metrics to evaluate these two aspects without needing\nto manually annotate documents. Results on the Text Anonymization Benchmark\nshow that the proposed approach, implemented with Mistral 7B Instruct, leads to\nenhanced utility, with only a marginal (< 1 p.p.) increase in re-identification\nrisk compared to fully suppressing the original spans. Furthermore, our\napproach is shown to be more truth-preserving than existing methods such as\nMicrosoft Presidio's synthetic replacements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text sanitization aims to rewrite parts of a document to prevent disclosure\nof personal information. The central challenge of text sanitization is to\nstrike a balance between privacy protection (avoiding the leakage of personal\ninformation) and utility preservation (retaining as much as possible of the\ndocument's original content). To this end, we introduce a novel text\nsanitization method based on generalizations, that is, broader but still\ninformative terms that subsume the semantic content of the original text spans.\nThe approach relies on the use of instruction-tuned large language models\n(LLMs) and is divided into two stages. Given a document including text spans\nexpressing personally identifiable information (PII), the LLM is first applied\nto obtain truth-preserving replacement candidates for each text span and rank\nthose according to their abstraction level. Those candidates are then evaluated\nfor their ability to protect privacy by conducting inference attacks with the\nLLM. Finally, the system selects the most informative replacement candidate\nshown to be resistant to those attacks. This two-stage process produces\nreplacements that effectively balance privacy and utility.\n  We also present novel metrics to evaluate these two aspects without needing\nto manually annotate documents. Results on the Text Anonymization Benchmark\nshow that the proposed approach, implemented with Mistral 7B Instruct, leads to\nenhanced utility, with only a marginal (< 1 p.p.) increase in re-identification\nrisk compared to fully suppressing the original spans. Furthermore, our\napproach is shown to be more truth-preserving than existing methods such as\nMicrosoft Presidio's synthetic replacements."
                },
                "authors": [
                    {
                        "name": "Ildikó Pilán"
                    },
                    {
                        "name": "Benet Manzanares-Salor"
                    },
                    {
                        "name": "David Sánchez"
                    },
                    {
                        "name": "Pierre Lison"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Lison"
                },
                "author": "Pierre Lison",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18517v2",
                "updated": "2025-08-31T08:34:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    8,
                    34,
                    15,
                    6,
                    243,
                    0
                ],
                "published": "2025-02-23T02:52:23Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    2,
                    52,
                    23,
                    6,
                    54,
                    0
                ],
                "title": "RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via\n  Reward Driven Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via\n  Reward Driven Data Synthesis"
                },
                "summary": "The success of large language models (LLMs) has attracted many individuals to\nfine-tune them for domain-specific tasks by uploading their data. However, in\nsensitive areas like healthcare and finance, privacy concerns often arise. One\npromising solution is to generate synthetic data with Differential Privacy (DP)\nguarantees to replace private data. However, these synthetic data contain\nsignificant flawed data, which are considered as noise. Existing solutions\ntypically rely on naive filtering by comparing ROUGE-L scores or embedding\nsimilarities, which are ineffective in addressing the noise. To address this\nissue, we propose \\textit{RewardDS}, a novel privacy-preserving framework that\nfine-tunes a reward proxy model and uses reward signals to guide the synthetic\ndata generation. Our \\textit{RewardDS} introduces two key modules, Reward\nGuided Filtering and Self-Optimizing Refinement, to both filter and refine the\nsynthetic data, effectively mitigating the noise. Extensive experiments across\nmedical, financial, and code generation domains demonstrate the effectiveness\nof our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) has attracted many individuals to\nfine-tune them for domain-specific tasks by uploading their data. However, in\nsensitive areas like healthcare and finance, privacy concerns often arise. One\npromising solution is to generate synthetic data with Differential Privacy (DP)\nguarantees to replace private data. However, these synthetic data contain\nsignificant flawed data, which are considered as noise. Existing solutions\ntypically rely on naive filtering by comparing ROUGE-L scores or embedding\nsimilarities, which are ineffective in addressing the noise. To address this\nissue, we propose \\textit{RewardDS}, a novel privacy-preserving framework that\nfine-tunes a reward proxy model and uses reward signals to guide the synthetic\ndata generation. Our \\textit{RewardDS} introduces two key modules, Reward\nGuided Filtering and Self-Optimizing Refinement, to both filter and refine the\nsynthetic data, effectively mitigating the noise. Extensive experiments across\nmedical, financial, and code generation domains demonstrate the effectiveness\nof our method."
                },
                "authors": [
                    {
                        "name": "Jianwei Wang"
                    },
                    {
                        "name": "Chengming Shi"
                    },
                    {
                        "name": "Junyao Yang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "Ziqian Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Ziqian Zeng"
                },
                "author": "Ziqian Zeng",
                "arxiv_comment": "Accepted by EMNLP2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12597v2",
                "updated": "2025-08-31T08:15:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    8,
                    15,
                    18,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-18T03:14:44Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    3,
                    14,
                    44,
                    0,
                    230,
                    0
                ],
                "title": "UAV Individual Identification via Distilled RF Fingerprints-Based LLM in\n  ISAC Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV Individual Identification via Distilled RF Fingerprints-Based LLM in\n  ISAC Networks"
                },
                "summary": "Unmanned aerial vehicle (UAV) individual (ID) identification is a critical\nsecurity surveillance strategy in low-altitude integrated sensing and\ncommunication (ISAC) networks. In this paper, we propose a novel dynamic\nknowledge distillation (KD)-enabled wireless radio frequency fingerprint large\nlanguage model (RFF-LLM) framework for UAV ID identification. First, we propose\nan RFF-LLM framework based on the modified GPT-2 model to improve the\nidentification accuracy in complex outdoor environments. Then, considering the\nparameter overhead of the RFF-LLM, we design a dynamic KD strategy to compress\nthe model. Specifically, the proximal policy optimization (PPO) algorithm is\nemployed to dynamically adjust the distillation temperature, overcoming the\nlocal optimum dilemma inherent in static KD. As a next step, the knowledge of\nthe RFF-LLM is adequately transferred to the lightweight Lite-HRNet model.\nFinally, our experiments are conducted based on the self-built drone RFF\ndataset of Release one, namely DRFF-R1, by collecting the I/Q signals of 20\ncommercial UAVs in channel 149. The experiment results show that the proposed\nframework achieves 98.38% ID identification accuracy with merely 0.15 million\nparameters and 2.74 ms response time, which outperforms the benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicle (UAV) individual (ID) identification is a critical\nsecurity surveillance strategy in low-altitude integrated sensing and\ncommunication (ISAC) networks. In this paper, we propose a novel dynamic\nknowledge distillation (KD)-enabled wireless radio frequency fingerprint large\nlanguage model (RFF-LLM) framework for UAV ID identification. First, we propose\nan RFF-LLM framework based on the modified GPT-2 model to improve the\nidentification accuracy in complex outdoor environments. Then, considering the\nparameter overhead of the RFF-LLM, we design a dynamic KD strategy to compress\nthe model. Specifically, the proximal policy optimization (PPO) algorithm is\nemployed to dynamically adjust the distillation temperature, overcoming the\nlocal optimum dilemma inherent in static KD. As a next step, the knowledge of\nthe RFF-LLM is adequately transferred to the lightweight Lite-HRNet model.\nFinally, our experiments are conducted based on the self-built drone RFF\ndataset of Release one, namely DRFF-R1, by collecting the I/Q signals of 20\ncommercial UAVs in channel 149. The experiment results show that the proposed\nframework achieves 98.38% ID identification accuracy with merely 0.15 million\nparameters and 2.74 ms response time, which outperforms the benchmarks."
                },
                "authors": [
                    {
                        "name": "Haolin Zheng"
                    },
                    {
                        "name": "Ning Gao"
                    },
                    {
                        "name": "Donghong Cai"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Michail Matthaiou"
                    }
                ],
                "author_detail": {
                    "name": "Michail Matthaiou"
                },
                "author": "Michail Matthaiou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19030v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19030v3",
                "updated": "2025-08-31T08:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    8,
                    1,
                    12,
                    6,
                    243,
                    0
                ],
                "published": "2025-05-25T08:31:08Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    8,
                    31,
                    8,
                    6,
                    145,
                    0
                ],
                "title": "RECAST: Strengthening LLMs' Complex Instruction Following with\n  Constraint-Verifiable Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECAST: Strengthening LLMs' Complex Instruction Following with\n  Constraint-Verifiable Data"
                },
                "summary": "Large language models (LLMs) are increasingly expected to tackle complex\ntasks, driven by their expanding applications and users' growing proficiency in\ncrafting sophisticated prompts. However, as the number of explicitly stated\nrequirements increases (particularly more than 10 constraints), LLMs often\nstruggle to accurately follow such complex instructions. To address this\nchallenge, we propose RECAST, a novel framework for synthesizing datasets where\neach example incorporates far more constraints than those in existing\nbenchmarks. These constraints are extracted from real-world prompt-response\npairs to ensure practical relevance. RECAST enables automatic verification of\nconstraint satisfaction via rule-based validators for quantitative constraints\nand LLM-based validators for qualitative ones. Using this framework, we\nconstruct RECAST-30K, a large-scale, high-quality dataset comprising 30k\ninstances spanning 15 constraint types. Experimental results demonstrate that\nmodels fine-tuned on RECAST-30K show substantial improvements in following\ncomplex instructions. Moreover, the verifiability provided by RECAST enables\nthe design of reward functions for reinforcement learning, which further boosts\nmodel performance on complex and challenging tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly expected to tackle complex\ntasks, driven by their expanding applications and users' growing proficiency in\ncrafting sophisticated prompts. However, as the number of explicitly stated\nrequirements increases (particularly more than 10 constraints), LLMs often\nstruggle to accurately follow such complex instructions. To address this\nchallenge, we propose RECAST, a novel framework for synthesizing datasets where\neach example incorporates far more constraints than those in existing\nbenchmarks. These constraints are extracted from real-world prompt-response\npairs to ensure practical relevance. RECAST enables automatic verification of\nconstraint satisfaction via rule-based validators for quantitative constraints\nand LLM-based validators for qualitative ones. Using this framework, we\nconstruct RECAST-30K, a large-scale, high-quality dataset comprising 30k\ninstances spanning 15 constraint types. Experimental results demonstrate that\nmodels fine-tuned on RECAST-30K show substantial improvements in following\ncomplex instructions. Moreover, the verifiability provided by RECAST enables\nthe design of reward functions for reinforcement learning, which further boosts\nmodel performance on complex and challenging tasks."
                },
                "authors": [
                    {
                        "name": "Zhengkang Guo"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Mingchen Xie"
                    },
                    {
                        "name": "Jingwen Xu"
                    },
                    {
                        "name": "Zisu Huang"
                    },
                    {
                        "name": "Muzhao Tian"
                    },
                    {
                        "name": "Jianhan Xu"
                    },
                    {
                        "name": "Muling Wu"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Changze Lv"
                    },
                    {
                        "name": "He-Da Wang"
                    },
                    {
                        "name": "Hu Yao"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19030v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19030v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10743v2",
                "updated": "2025-08-31T07:48:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    7,
                    48,
                    59,
                    6,
                    243,
                    0
                ],
                "published": "2024-10-14T17:21:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    21,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph\n  Structure into Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Anchors to Answers: A Novel Node Tokenizer for Integrating Graph\n  Structure into Large Language Models"
                },
                "summary": "Enabling large language models (LLMs) to effectively process and reason with\ngraph-structured data remains a significant challenge despite their remarkable\nsuccess in natural language tasks. Current approaches either convert graph\nstructures into verbose textual descriptions, consuming substantial\ncomputational resources, or employ complex graph neural networks as tokenizers,\nwhich introduce significant training overhead. To bridge this gap, we present\nNT-LLM, a novel framework with an anchor-based positional encoding scheme for\ngraph representation. Our approach strategically selects reference nodes as\nanchors and encodes each node's position relative to these anchors, capturing\nessential topological information without the computational burden of existing\nmethods. Notably, we identify and address a fundamental issue: the inherent\nmisalignment between discrete hop-based distances in graphs and continuous\ndistances in embedding spaces. By implementing a rank-preserving objective for\npositional encoding pretraining, NT-LLM achieves superior performance across\ndiverse graph tasks ranging from basic structural analysis to complex reasoning\nscenarios. Our comprehensive evaluation demonstrates that this lightweight yet\npowerful approach effectively enhances LLMs' ability to understand and reason\nwith graph-structured information, offering an efficient solution for\ngraph-based applications of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling large language models (LLMs) to effectively process and reason with\ngraph-structured data remains a significant challenge despite their remarkable\nsuccess in natural language tasks. Current approaches either convert graph\nstructures into verbose textual descriptions, consuming substantial\ncomputational resources, or employ complex graph neural networks as tokenizers,\nwhich introduce significant training overhead. To bridge this gap, we present\nNT-LLM, a novel framework with an anchor-based positional encoding scheme for\ngraph representation. Our approach strategically selects reference nodes as\nanchors and encodes each node's position relative to these anchors, capturing\nessential topological information without the computational burden of existing\nmethods. Notably, we identify and address a fundamental issue: the inherent\nmisalignment between discrete hop-based distances in graphs and continuous\ndistances in embedding spaces. By implementing a rank-preserving objective for\npositional encoding pretraining, NT-LLM achieves superior performance across\ndiverse graph tasks ranging from basic structural analysis to complex reasoning\nscenarios. Our comprehensive evaluation demonstrates that this lightweight yet\npowerful approach effectively enhances LLMs' ability to understand and reason\nwith graph-structured information, offering an efficient solution for\ngraph-based applications of language models."
                },
                "authors": [
                    {
                        "name": "Yanbiao Ji"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Dan Luo"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Wenqing Lin"
                    },
                    {
                        "name": "Hongtao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Lu"
                },
                "author": "Hongtao Lu",
                "arxiv_doi": "10.1145/3746252.3761167",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761167",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.10743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21471v2",
                "updated": "2025-08-31T04:55:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    4,
                    55,
                    1,
                    6,
                    243,
                    0
                ],
                "published": "2025-07-29T03:20:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    20,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "LUMIR: an LLM-Driven Unified Agent Framework for Multi-task Infrared\n  Spectroscopy Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUMIR: an LLM-Driven Unified Agent Framework for Multi-task Infrared\n  Spectroscopy Reasoning"
                },
                "summary": "Infrared spectroscopy enables rapid, non destructive analysis of chemical and\nmaterial properties, yet high dimensional signals and overlapping bands hinder\nconventional chemometric methods. Large language models (LLMs), with strong\ngeneralization and reasoning capabilities, offer new opportunities for\nautomated spectral interpretation, but their potential in this domain remains\nlargely untapped. This study introduces LUMIR (LLM-driven Unified agent\nframework for Multi-task Infrared spectroscopy Reasoning), an agent based\nframework designed to achieve accurate infrared spectral analysis under low\ndata conditions. LUMIR integrates a structured literature knowledge base,\nautomated preprocessing, feature extraction, and predictive modeling into a\nunified pipeline. By mining peer reviewed spectroscopy studies, it identifies\nvalidated preprocessing and feature derivation strategies, transforms spectra\ninto low dimensional representations, and applies few-shot prompts for\nclassification, regression, and anomaly detection. The framework was validated\non diverse datasets, including the publicly available Milk near-infrared\ndataset, Chinese medicinal herbs, Citri Reticulatae Pericarpium(CRP) with\ndifferent storage durations, an industrial wastewater COD dataset, and two\nadditional public benchmarks, Tecator and Corn. Across these tasks, LUMIR\nachieved performance comparable to or surpassing established machine learning\nand deep learning models, particularly in resource limited settings. This work\ndemonstrates that combining structured literature guidance with few-shot\nlearning enables robust, scalable, and automated spectral interpretation. LUMIR\nestablishes a new paradigm for applying LLMs to infrared spectroscopy, offering\nhigh accuracy with minimal labeled data and broad applicability across\nscientific and industrial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrared spectroscopy enables rapid, non destructive analysis of chemical and\nmaterial properties, yet high dimensional signals and overlapping bands hinder\nconventional chemometric methods. Large language models (LLMs), with strong\ngeneralization and reasoning capabilities, offer new opportunities for\nautomated spectral interpretation, but their potential in this domain remains\nlargely untapped. This study introduces LUMIR (LLM-driven Unified agent\nframework for Multi-task Infrared spectroscopy Reasoning), an agent based\nframework designed to achieve accurate infrared spectral analysis under low\ndata conditions. LUMIR integrates a structured literature knowledge base,\nautomated preprocessing, feature extraction, and predictive modeling into a\nunified pipeline. By mining peer reviewed spectroscopy studies, it identifies\nvalidated preprocessing and feature derivation strategies, transforms spectra\ninto low dimensional representations, and applies few-shot prompts for\nclassification, regression, and anomaly detection. The framework was validated\non diverse datasets, including the publicly available Milk near-infrared\ndataset, Chinese medicinal herbs, Citri Reticulatae Pericarpium(CRP) with\ndifferent storage durations, an industrial wastewater COD dataset, and two\nadditional public benchmarks, Tecator and Corn. Across these tasks, LUMIR\nachieved performance comparable to or surpassing established machine learning\nand deep learning models, particularly in resource limited settings. This work\ndemonstrates that combining structured literature guidance with few-shot\nlearning enables robust, scalable, and automated spectral interpretation. LUMIR\nestablishes a new paradigm for applying LLMs to infrared spectroscopy, offering\nhigh accuracy with minimal labeled data and broad applicability across\nscientific and industrial domains."
                },
                "authors": [
                    {
                        "name": "Zujie Xie"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Jiheng Liang"
                    },
                    {
                        "name": "Xiangyang Yu"
                    },
                    {
                        "name": "Ziru Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ziru Yu"
                },
                "author": "Ziru Yu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17966v2",
                "updated": "2025-08-31T04:44:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    4,
                    44,
                    3,
                    6,
                    243,
                    0
                ],
                "published": "2025-06-22T09:53:21Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    9,
                    53,
                    21,
                    6,
                    173,
                    0
                ],
                "title": "LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential\n  Recommendation"
                },
                "summary": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences and capturing both intra- and inter-sequence\nitem relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain\nSequential Recommendation (LLM-EMF), a novel and advanced approach that\nenhances textual information with Large Language Models (LLM) knowledge and\nsignificantly improves recommendation performance through the fusion of visual\nand textual data. Using the frozen CLIP model, we generate image and text\nembeddings, thereby enriching item representations with multimodal data. A\nmultiple attention mechanism jointly learns both single-domain and cross-domain\npreferences, effectively capturing and understanding complex user interests\nacross diverse domains. Evaluations conducted on four e-commerce datasets\ndemonstrate that LLM-EMF consistently outperforms existing methods in modeling\ncross-domain user preferences, thereby highlighting the effectiveness of\nmultimodal data integration and its advantages in enhancing sequential\nrecommendation systems. Our source code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences and capturing both intra- and inter-sequence\nitem relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain\nSequential Recommendation (LLM-EMF), a novel and advanced approach that\nenhances textual information with Large Language Models (LLM) knowledge and\nsignificantly improves recommendation performance through the fusion of visual\nand textual data. Using the frozen CLIP model, we generate image and text\nembeddings, thereby enriching item representations with multimodal data. A\nmultiple attention mechanism jointly learns both single-domain and cross-domain\npreferences, effectively capturing and understanding complex user interests\nacross diverse domains. Evaluations conducted on four e-commerce datasets\ndemonstrate that LLM-EMF consistently outperforms existing methods in modeling\ncross-domain user preferences, thereby highlighting the effectiveness of\nmultimodal data integration and its advantages in enhancing sequential\nrecommendation systems. Our source code will be released."
                },
                "authors": [
                    {
                        "name": "Wangyu Wu"
                    },
                    {
                        "name": "Zhenhong Chen"
                    },
                    {
                        "name": "Xianglin Qiu"
                    },
                    {
                        "name": "Siqi Song"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jimin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Xiao"
                },
                "author": "Jimin Xiao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2504.15085",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]