[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v1",
                "updated": "2025-03-12T17:43:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Vctor Gutirrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v1",
                "updated": "2025-03-11T17:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v3",
                "updated": "2025-03-11T16:35:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    59,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v1",
                "updated": "2025-03-11T13:10:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher R"
                    }
                ],
                "author_detail": {
                    "name": "Christopher R"
                },
                "author": "Christopher R",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Mller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "ric de la Clergerie"
                    },
                    {
                        "name": "Benot Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benot Sagot"
                },
                "author": "Benot Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tams Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tams Jursonovics"
                },
                "author": "Tams Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Cte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.11651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11651v1",
                "updated": "2025-03-14T17:59:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    47,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:59:47Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    47,
                    4,
                    73,
                    0
                ],
                "title": "VGGT: Visual Geometry Grounded Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGGT: Visual Geometry Grounded Transformer"
                },
                "summary": "We present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt."
                },
                "authors": [
                    {
                        "name": "Jianyuan Wang"
                    },
                    {
                        "name": "Minghao Chen"
                    },
                    {
                        "name": "Nikita Karaev"
                    },
                    {
                        "name": "Andrea Vedaldi"
                    },
                    {
                        "name": "Christian Rupprecht"
                    },
                    {
                        "name": "David Novotny"
                    }
                ],
                "author_detail": {
                    "name": "David Novotny"
                },
                "author": "David Novotny",
                "arxiv_comment": "CVPR 2025, Project Page: https://vgg-t.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11650v1",
                "updated": "2025-03-14T17:59:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    41,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:59:41Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    41,
                    4,
                    73,
                    0
                ],
                "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training"
                },
                "summary": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels."
                },
                "authors": [
                    {
                        "name": "Chonghao Sima"
                    },
                    {
                        "name": "Kashyap Chitta"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Andreas Geiger"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvarez"
                },
                "author": "Jose M. Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01974v2",
                "updated": "2025-03-14T17:58:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    58,
                    33,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-04T10:50:37Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    10,
                    50,
                    37,
                    0,
                    309,
                    0
                ],
                "title": "On the phase diagram of extensive-rank symmetric matrix denoising beyond\n  rotational invariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the phase diagram of extensive-rank symmetric matrix denoising beyond\n  rotational invariance"
                },
                "summary": "Matrix denoising is central to signal processing and machine learning. Its\nstatistical analysis when the matrix to infer has a factorised structure with a\nrank growing proportionally to its dimension remains a challenge, except when\nit is rotationally invariant. In this case the information theoretic limits and\nan efficient Bayes-optimal denoising algorithm, called rotational invariant\nestimator [1,2], are known. Beyond this setting few results can be found. The\nreason is that the model is not a usual spin system because of the growing rank\ndimension, nor a matrix model (as appearing in high-energy physics) due to the\nlack of rotation symmetry, but rather a hybrid between the two. Here we make\nprogress towards the understanding of Bayesian matrix denoising when the signal\nis a factored matrix $XX^\\intercal$ that is not rotationally invariant. Monte\nCarlo simulations suggest the existence of a \\emph{denoising-factorisation\ntransition} separating a phase where denoising using the rotational invariant\nestimator remains Bayes-optimal due to universality properties of the same\nnature as in random matrix theory, from one where universality breaks down and\nbetter denoising is possible, though algorithmically hard. We argue that it is\nonly beyond the transition that factorisation, i.e., estimating $X$ itself,\nbecomes possible up to irresolvable ambiguities. On the theory side, we combine\nmean-field techniques in an interpretable multiscale fashion in order to access\nthe minimum mean-square error and mutual information. Interestingly, our\nalternative method yields equations reproducible by the replica approach of\n[3]. Using numerical insights, we delimit the portion of phase diagram where we\nconjecture the mean-field theory to be exact, and correct it using universality\nwhen it is not. Our complete ansatz matches well the numerics in the whole\nphase diagram when considering finite size effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix denoising is central to signal processing and machine learning. Its\nstatistical analysis when the matrix to infer has a factorised structure with a\nrank growing proportionally to its dimension remains a challenge, except when\nit is rotationally invariant. In this case the information theoretic limits and\nan efficient Bayes-optimal denoising algorithm, called rotational invariant\nestimator [1,2], are known. Beyond this setting few results can be found. The\nreason is that the model is not a usual spin system because of the growing rank\ndimension, nor a matrix model (as appearing in high-energy physics) due to the\nlack of rotation symmetry, but rather a hybrid between the two. Here we make\nprogress towards the understanding of Bayesian matrix denoising when the signal\nis a factored matrix $XX^\\intercal$ that is not rotationally invariant. Monte\nCarlo simulations suggest the existence of a \\emph{denoising-factorisation\ntransition} separating a phase where denoising using the rotational invariant\nestimator remains Bayes-optimal due to universality properties of the same\nnature as in random matrix theory, from one where universality breaks down and\nbetter denoising is possible, though algorithmically hard. We argue that it is\nonly beyond the transition that factorisation, i.e., estimating $X$ itself,\nbecomes possible up to irresolvable ambiguities. On the theory side, we combine\nmean-field techniques in an interpretable multiscale fashion in order to access\nthe minimum mean-square error and mutual information. Interestingly, our\nalternative method yields equations reproducible by the replica approach of\n[3]. Using numerical insights, we delimit the portion of phase diagram where we\nconjecture the mean-field theory to be exact, and correct it using universality\nwhen it is not. Our complete ansatz matches well the numerics in the whole\nphase diagram when considering finite size effects."
                },
                "authors": [
                    {
                        "name": "Jean Barbier"
                    },
                    {
                        "name": "Francesco Camilli"
                    },
                    {
                        "name": "Justin Ko"
                    },
                    {
                        "name": "Koki Okajima"
                    }
                ],
                "author_detail": {
                    "name": "Koki Okajima"
                },
                "author": "Koki Okajima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17686v3",
                "updated": "2025-03-14T17:56:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    56,
                    9,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-26T18:53:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    53,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "Filter, Correlate, Compress: Training-Free Token Reduction for MLLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filter, Correlate, Compress: Training-Free Token Reduction for MLLM\n  Acceleration"
                },
                "summary": "The quadratic complexity of Multimodal Large Language Models (MLLMs) with\nrespect to sequence length poses significant computational and memory\nchallenges, hindering their real-world deployment. While existing training-free\ntoken reduction methods aim to address these inefficiencies, how to precisely\nidentify redundant visual tokens and recover the essential information from the\ndiscarded tokens remain unclear. In this paper, we propose a\n''filter-correlate-compress'' framework that decomposes the token reduction\ninto three stages: filtering redundant tokens, correlating discarded\ninformation to preserved tokens, and compressing tokens to minimize redundancy.\nFollowing the framework, we propose a solution FiCoCo to identify limitations\nin single redundancy assessment, propose adaptive strategies to retain critical\ninformation from discarded tokens, and mitigate semantic dilution during token\nfusion. Two specialized variants, FiCoCo-V (for vision encoders) and FiCoCo-L\n(for LLM decoders), further optimize efficiency across MLLM architectures.\nExtensive experiments demonstrate that FiCoCo achieves up to 5.7x/14.7x FLOPs\nreduction with 92.8%/93.6% performance retention on LLaVA-1.5-7B/LLaVA-NeXT-7B.\nOur methods consistently outperform state-of-the-art training-free approaches,\nshowcasing effectiveness and generalizability across model architectures,\nsizes, and tasks without requiring retraining. Our project page is at\nhttps://ficoco-accelerate.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of Multimodal Large Language Models (MLLMs) with\nrespect to sequence length poses significant computational and memory\nchallenges, hindering their real-world deployment. While existing training-free\ntoken reduction methods aim to address these inefficiencies, how to precisely\nidentify redundant visual tokens and recover the essential information from the\ndiscarded tokens remain unclear. In this paper, we propose a\n''filter-correlate-compress'' framework that decomposes the token reduction\ninto three stages: filtering redundant tokens, correlating discarded\ninformation to preserved tokens, and compressing tokens to minimize redundancy.\nFollowing the framework, we propose a solution FiCoCo to identify limitations\nin single redundancy assessment, propose adaptive strategies to retain critical\ninformation from discarded tokens, and mitigate semantic dilution during token\nfusion. Two specialized variants, FiCoCo-V (for vision encoders) and FiCoCo-L\n(for LLM decoders), further optimize efficiency across MLLM architectures.\nExtensive experiments demonstrate that FiCoCo achieves up to 5.7x/14.7x FLOPs\nreduction with 92.8%/93.6% performance retention on LLaVA-1.5-7B/LLaVA-NeXT-7B.\nOur methods consistently outperform state-of-the-art training-free approaches,\nshowcasing effectiveness and generalizability across model architectures,\nsizes, and tasks without requiring retraining. Our project page is at\nhttps://ficoco-accelerate.github.io/."
                },
                "authors": [
                    {
                        "name": "Yuhang Han"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Honggang Chen"
                    },
                    {
                        "name": "Qingsen Yan"
                    },
                    {
                        "name": "Siteng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siteng Huang"
                },
                "author": "Siteng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11637v1",
                "updated": "2025-03-14T17:55:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    55,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:55:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    55,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Gradient-bridged Posterior: Bayesian Inference for Models with Implicit\n  Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient-bridged Posterior: Bayesian Inference for Models with Implicit\n  Functions"
                },
                "summary": "Many statistical problems include model parameters that are defined as the\nsolutions to optimization sub-problems. These include classical approaches such\nas profile likelihood as well as modern applications involving flow networks or\nProcrustes distances. In such cases, the likelihood of the data involves an\nimplicit function, often complicating inferential procedures and entailing\nprohibitive computational cost. In this article, we propose an intuitive and\ntractable posterior inference approach for this setting. We introduce a class\nof continuous models that handle implicit function values using the first-order\noptimality of the sub-problems. Specifically, we apply a shrinkage kernel to\nthe gradient norm, which retains a probabilistic interpretation within a\ngenerative model. This can be understood as a generalization of the Gibbs\nposterior framework to newly enable concentration around partial minimizers in\na subset of the parameters. We show that this method, termed the\ngradient-bridged posterior, is amenable to efficient posterior computation, and\nenjoys theoretical guarantees, establishing a Bernstein--von Mises theorem for\nasymptotic normality. The advantages of our approach are highlighted on a\nsynthetic flow network experiment and an application to data integration using\nProcrustes distances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many statistical problems include model parameters that are defined as the\nsolutions to optimization sub-problems. These include classical approaches such\nas profile likelihood as well as modern applications involving flow networks or\nProcrustes distances. In such cases, the likelihood of the data involves an\nimplicit function, often complicating inferential procedures and entailing\nprohibitive computational cost. In this article, we propose an intuitive and\ntractable posterior inference approach for this setting. We introduce a class\nof continuous models that handle implicit function values using the first-order\noptimality of the sub-problems. Specifically, we apply a shrinkage kernel to\nthe gradient norm, which retains a probabilistic interpretation within a\ngenerative model. This can be understood as a generalization of the Gibbs\nposterior framework to newly enable concentration around partial minimizers in\na subset of the parameters. We show that this method, termed the\ngradient-bridged posterior, is amenable to efficient posterior computation, and\nenjoys theoretical guarantees, establishing a Bernstein--von Mises theorem for\nasymptotic normality. The advantages of our approach are highlighted on a\nsynthetic flow network experiment and an application to data integration using\nProcrustes distances."
                },
                "authors": [
                    {
                        "name": "Cheng Zeng"
                    },
                    {
                        "name": "Yaozhi Yang"
                    },
                    {
                        "name": "Jason Xu"
                    },
                    {
                        "name": "Leo L Duan"
                    }
                ],
                "author_detail": {
                    "name": "Leo L Duan"
                },
                "author": "Leo L Duan",
                "arxiv_comment": "31 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11636v1",
                "updated": "2025-03-14T17:54:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    54,
                    39,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:54:39Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    54,
                    39,
                    4,
                    73,
                    0
                ],
                "title": "Towards Markov-State Holography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Markov-State Holography"
                },
                "summary": "Experiments, in particular on biological systems, typically probe\nlower-dimensional observables which are projections of high-dimensional\ndynamics. In order to infer consistent models capturing the relevant dynamics\nof the system, it is important to detect and account for the memory in the\ndynamics. We develop a method to infer the presence of hidden states and\ntransition pathways based on observable transition probabilities conditioned on\nhistory sequences for projected (i.e. observed) dynamics of Markov processes.\nHistograms conditioned on histories reveal information on the transition\nprobabilities of hidden paths locally between any specific pair of observed\nstates. The convergence rate of these histograms towards a stationary\ndistribution provides a local quantification of the duration of memory, which\nreflects how distinct microscopic paths projecting onto the same observed\ntransition decorrelate in path space. This provides insight about the hidden\ntopology of microscopic paths in a holography-like fashion. The method can be\nused to test for the local Markov property of observables. The information\nextracted is also helpful in inferring relevant hidden transitions which are\nnot captured by a Markov-state model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments, in particular on biological systems, typically probe\nlower-dimensional observables which are projections of high-dimensional\ndynamics. In order to infer consistent models capturing the relevant dynamics\nof the system, it is important to detect and account for the memory in the\ndynamics. We develop a method to infer the presence of hidden states and\ntransition pathways based on observable transition probabilities conditioned on\nhistory sequences for projected (i.e. observed) dynamics of Markov processes.\nHistograms conditioned on histories reveal information on the transition\nprobabilities of hidden paths locally between any specific pair of observed\nstates. The convergence rate of these histograms towards a stationary\ndistribution provides a local quantification of the duration of memory, which\nreflects how distinct microscopic paths projecting onto the same observed\ntransition decorrelate in path space. This provides insight about the hidden\ntopology of microscopic paths in a holography-like fashion. The method can be\nused to test for the local Markov property of observables. The information\nextracted is also helpful in inferring relevant hidden transitions which are\nnot captured by a Markov-state model."
                },
                "authors": [
                    {
                        "name": "Xizhu Zhao"
                    },
                    {
                        "name": "Dmitrii E. Makarov"
                    },
                    {
                        "name": "Alja Godec"
                    }
                ],
                "author_detail": {
                    "name": "Alja Godec"
                },
                "author": "Alja Godec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20076v2",
                "updated": "2025-03-14T17:48:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    48,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-27T13:33:27Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    13,
                    33,
                    27,
                    3,
                    58,
                    0
                ],
                "title": "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework"
                },
                "summary": "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears."
                },
                "authors": [
                    {
                        "name": "Angus McCowan"
                    }
                ],
                "author_detail": {
                    "name": "Angus McCowan"
                },
                "author": "Angus McCowan",
                "arxiv_comment": "Revised to add re-analysis using the subject report's implicit\n  upstream diversity, to add description of imbalance in filtering of early\n  lineages and samples, to clarify assumptions in the modeled diversity, and to\n  rewrite summary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17037v2",
                "updated": "2025-03-14T17:43:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    43,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2024-07-24T06:50:08Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    6,
                    50,
                    8,
                    2,
                    206,
                    0
                ],
                "title": "Robust Comparative Statics with Misspecified Bayesian Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Comparative Statics with Misspecified Bayesian Learning"
                },
                "summary": "We present novel monotone comparative statics results for steady-state\nbehavior in a dynamic optimization environment with misspecified Bayesian\nlearning. Building on \\cite{ep21a}, we analyze a Bayesian learner whose prior\nis over parameterized transition models but is misspecified in the sense that\nthe true process does not belong to this set. We characterize conditions that\nensure monotonicity in the steady-state distribution over states, actions, and\ninferred models. Additionally, we provide a new monotonicity-based proof of\nsteady-state existence, derive an upper bound on the cost of misspecification,\nand illustrate the applicability of our results to several environments of\ngeneral interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present novel monotone comparative statics results for steady-state\nbehavior in a dynamic optimization environment with misspecified Bayesian\nlearning. Building on \\cite{ep21a}, we analyze a Bayesian learner whose prior\nis over parameterized transition models but is misspecified in the sense that\nthe true process does not belong to this set. We characterize conditions that\nensure monotonicity in the steady-state distribution over states, actions, and\ninferred models. Additionally, we provide a new monotonicity-based proof of\nsteady-state existence, derive an upper bound on the cost of misspecification,\nand illustrate the applicability of our results to several environments of\ngeneral interest."
                },
                "authors": [
                    {
                        "name": "Aniruddha Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Aniruddha Ghosh"
                },
                "author": "Aniruddha Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11621v1",
                "updated": "2025-03-14T17:41:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    41,
                    8,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:41:08Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    41,
                    8,
                    4,
                    73,
                    0
                ],
                "title": "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space"
                },
                "summary": "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the \\textit{Euclid}\\xspace photometric survey will allow\nfor precise studies of galaxy clustering from a single survey, over a large\nrange of redshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts\nto extract the baryon acoustic oscillation signal (BAO) from the Flagship\ngalaxy mock catalogue with a tomographic approach to constrain the evolution of\nthe Universe and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the \\textit{Euclid}\\xspace photometric survey will allow\nfor precise studies of galaxy clustering from a single survey, over a large\nrange of redshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts\nto extract the baryon acoustic oscillation signal (BAO) from the Flagship\ngalaxy mock catalogue with a tomographic approach to constrain the evolution of\nthe Universe and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "V. Duret"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "W. Gillard"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "N. Aghanim"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "L. Amendola"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "A. Caillat"
                    },
                    {
                        "name": "G. Caas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "A. Ealet"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "S. Fotopoulou"
                    },
                    {
                        "name": "N. Fourmanoit"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihnen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "M. Kilbinger"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "S. Maurogordato"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "B. Morin"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "R. C. Nichol"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "J. A. Schewtschenko"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. -L. Starck"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "F. M. Zerbi"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "M. Calabrese"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pntinen"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "A. Balaguera-Antolinez"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "H. Bhringer"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "S. Contarini"
                    },
                    {
                        "name": "T. Contini"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "F. De Paolis"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Daz-Snchez"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "A. G. Ferrari"
                    },
                    {
                        "name": "P. G. Ferreira"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. Garca-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "A. Gregorio"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "M. Lattanzi"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "S. J. Liu"
                    },
                    {
                        "name": "A. Loureiro"
                    },
                    {
                        "name": "G. Maggio"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "J. Martn-Fleitas"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "C. Murray"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "P. -F. Rocci"
                    },
                    {
                        "name": "M. Sahln"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "A. Silvestri"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "C. Tao"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "C. Valieri"
                    },
                    {
                        "name": "A. Venhola"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "F. Vernizzi"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    },
                    {
                        "name": "N. A. Walton"
                    }
                ],
                "author_detail": {
                    "name": "N. A. Walton"
                },
                "author": "N. A. Walton",
                "arxiv_affiliation": "Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK",
                "arxiv_comment": "18 pages, 12 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16958v2",
                "updated": "2025-03-14T17:40:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    40,
                    35,
                    4,
                    73,
                    0
                ],
                "published": "2025-01-28T14:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    14,
                    0,
                    7,
                    1,
                    28,
                    0
                ],
                "title": "Estimating the Causal Effect of Redlining on Present-day Air Pollution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the Causal Effect of Redlining on Present-day Air Pollution"
                },
                "summary": "Recent studies have shown associations between redlining policies (1935-1974)\nand present-day fine particulate matter (PM$_{2.5}$) and nitrogen dioxide\n(NO$_2$) air pollution concentrations. In this paper, we reevaluate these\nassociations using spatial causal inference. Redlining policies enacted in the\n1930s, so there is very limited documentation of pre-treatment covariates.\nConsequently, traditional methods fails to sufficiently account for unmeasured\nconfounders, potentially biasing causal interpretations. By integrating\nhistorical redlining data with 2010 PM$_{2.5}$ and NO$_2$ concentrations, our\nstudy aims to discern whether a causal link exists. Our study addresses\nchallenges with a novel spatial and non-spatial latent factor framework, using\nthe unemployment rate, house rent and percentage of Black population in 1940\nU.S. Census as proxies to reconstruct pre-treatment latent socio-economic\nstatus. We establish identification of a causal effect under broad assumptions,\nand use Bayesian Markov Chain Monte Carlo to quantify uncertainty. Our analysis\nindicates that historically redlined neighborhoods are exposed to notably\nhigher NO$_2$ concentration. In contrast, the disparities in PM$_{2.5}$ between\nthese neighborhoods are less pronounced. Among the cities analyzed, Los\nAngeles, CA, and Atlanta, GA, demonstrate the most significant effects for both\nNO$_2$ and PM$_{2.5}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown associations between redlining policies (1935-1974)\nand present-day fine particulate matter (PM$_{2.5}$) and nitrogen dioxide\n(NO$_2$) air pollution concentrations. In this paper, we reevaluate these\nassociations using spatial causal inference. Redlining policies enacted in the\n1930s, so there is very limited documentation of pre-treatment covariates.\nConsequently, traditional methods fails to sufficiently account for unmeasured\nconfounders, potentially biasing causal interpretations. By integrating\nhistorical redlining data with 2010 PM$_{2.5}$ and NO$_2$ concentrations, our\nstudy aims to discern whether a causal link exists. Our study addresses\nchallenges with a novel spatial and non-spatial latent factor framework, using\nthe unemployment rate, house rent and percentage of Black population in 1940\nU.S. Census as proxies to reconstruct pre-treatment latent socio-economic\nstatus. We establish identification of a causal effect under broad assumptions,\nand use Bayesian Markov Chain Monte Carlo to quantify uncertainty. Our analysis\nindicates that historically redlined neighborhoods are exposed to notably\nhigher NO$_2$ concentration. In contrast, the disparities in PM$_{2.5}$ between\nthese neighborhoods are less pronounced. Among the cities analyzed, Los\nAngeles, CA, and Atlanta, GA, demonstrate the most significant effects for both\nNO$_2$ and PM$_{2.5}$."
                },
                "authors": [
                    {
                        "name": "Xiaodan Zhou"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Brian J Reich"
                    }
                ],
                "author_detail": {
                    "name": "Brian J Reich"
                },
                "author": "Brian J Reich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11617v1",
                "updated": "2025-03-14T17:36:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    36,
                    8,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:36:08Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    36,
                    8,
                    4,
                    73,
                    0
                ],
                "title": "ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via\n  Structural-Semantic Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via\n  Structural-Semantic Instruction Tuning"
                },
                "summary": "Analysis and comprehension of assembly code are crucial in various\napplications, such as reverse engineering. However, the low information density\nand lack of explicit syntactic structures in assembly code pose significant\nchallenges. Pioneering approaches with masked language modeling (MLM)-based\nmethods have been limited by facilitating natural language interaction. While\nrecent methods based on decoder-focused large language models (LLMs) have\nsignificantly enhanced semantic representation, they still struggle to capture\nthe nuanced and sparse semantics in assembly code. In this paper, we propose\nAssembly Augmented Tuning (ASMA-Tune), an end-to-end structural-semantic\ninstruction-tuning framework. Our approach synergizes encoder architectures\nwith decoder-based LLMs through projector modules to enable comprehensive code\nunderstanding. Experiments show that ASMA-Tune outperforms existing benchmarks,\nsignificantly enhancing assembly code comprehension and instruction-following\nabilities. Our model and dataset are public at\nhttps://github.com/wxy3596/ASMA-Tune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and comprehension of assembly code are crucial in various\napplications, such as reverse engineering. However, the low information density\nand lack of explicit syntactic structures in assembly code pose significant\nchallenges. Pioneering approaches with masked language modeling (MLM)-based\nmethods have been limited by facilitating natural language interaction. While\nrecent methods based on decoder-focused large language models (LLMs) have\nsignificantly enhanced semantic representation, they still struggle to capture\nthe nuanced and sparse semantics in assembly code. In this paper, we propose\nAssembly Augmented Tuning (ASMA-Tune), an end-to-end structural-semantic\ninstruction-tuning framework. Our approach synergizes encoder architectures\nwith decoder-based LLMs through projector modules to enable comprehensive code\nunderstanding. Experiments show that ASMA-Tune outperforms existing benchmarks,\nsignificantly enhancing assembly code comprehension and instruction-following\nabilities. Our model and dataset are public at\nhttps://github.com/wxy3596/ASMA-Tune."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jinbo Su"
                    },
                    {
                        "name": "Yanming Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Yangdong Wang"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Kai Yun"
                    },
                    {
                        "name": "Chunfu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Chunfu Jia"
                },
                "author": "Chunfu Jia",
                "arxiv_comment": "19 pages, multiple figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11614v1",
                "updated": "2025-03-14T17:33:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    33,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:33:30Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    33,
                    30,
                    4,
                    73,
                    0
                ],
                "title": "Neutralizing Bias in LLM Reasoning using Entailment Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutralizing Bias in LLM Reasoning using Entailment Graphs"
                },
                "summary": "LLMs are often claimed to be capable of Natural Language Inference (NLI),\nwhich is widely regarded as a cornerstone of more complex forms of reasoning.\nHowever, recent works show that LLMs still suffer from hallucinations in NLI\ndue to attestation bias, where LLMs overly rely on propositional memory to\nbuild shortcuts. To solve the issue, we design an unsupervised framework to\nconstruct counterfactual reasoning data and fine-tune LLMs to reduce\nattestation bias. To measure bias reduction, we build bias-adversarial variants\nof NLI datasets with randomly replaced predicates in premises while keeping\nhypotheses unchanged. Extensive evaluations show that our framework can\nsignificantly reduce hallucinations from attestation bias. Then, we further\nevaluate LLMs fine-tuned with our framework on original NLI datasets and their\nbias-neutralized versions, where original entities are replaced with randomly\nsampled ones. Extensive results show that our framework consistently improves\ninferential performance on both original and bias-neutralized NLI datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are often claimed to be capable of Natural Language Inference (NLI),\nwhich is widely regarded as a cornerstone of more complex forms of reasoning.\nHowever, recent works show that LLMs still suffer from hallucinations in NLI\ndue to attestation bias, where LLMs overly rely on propositional memory to\nbuild shortcuts. To solve the issue, we design an unsupervised framework to\nconstruct counterfactual reasoning data and fine-tune LLMs to reduce\nattestation bias. To measure bias reduction, we build bias-adversarial variants\nof NLI datasets with randomly replaced predicates in premises while keeping\nhypotheses unchanged. Extensive evaluations show that our framework can\nsignificantly reduce hallucinations from attestation bias. Then, we further\nevaluate LLMs fine-tuned with our framework on original NLI datasets and their\nbias-neutralized versions, where original entities are replaced with randomly\nsampled ones. Extensive results show that our framework consistently improves\ninferential performance on both original and bias-neutralized NLI datasets."
                },
                "authors": [
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Mark Steedman"
                    }
                ],
                "author_detail": {
                    "name": "Mark Steedman"
                },
                "author": "Mark Steedman",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11612v1",
                "updated": "2025-03-14T17:29:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    29,
                    27,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:29:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    29,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "Enhanced Soups for Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Soups for Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNN) have demonstrated state-of-the-art performance in\nnumerous scientific and high-performance computing (HPC) applications. Recent\nwork suggests that \"souping\" (combining) individually trained GNNs into a\nsingle model can improve performance without increasing compute and memory\ncosts during inference. However, existing souping algorithms are often slow and\nmemory-intensive, which limits their scalability.\n  We introduce Learned Souping for GNNs, a gradient-descent-based souping\nstrategy that substantially reduces time and memory overhead compared to\nexisting methods. Our approach is evaluated across multiple Open Graph\nBenchmark (OGB) datasets and GNN architectures, achieving up to 1.2% accuracy\nimprovement and 2.1X speedup. Additionally, we propose Partition Learned\nSouping, a novel partition-based variant of learned souping that significantly\nreduces memory usage. On the ogbn-products dataset with GraphSAGE, partition\nlearned souping achieves a 24.5X speedup and a 76% memory reduction without\ncompromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNN) have demonstrated state-of-the-art performance in\nnumerous scientific and high-performance computing (HPC) applications. Recent\nwork suggests that \"souping\" (combining) individually trained GNNs into a\nsingle model can improve performance without increasing compute and memory\ncosts during inference. However, existing souping algorithms are often slow and\nmemory-intensive, which limits their scalability.\n  We introduce Learned Souping for GNNs, a gradient-descent-based souping\nstrategy that substantially reduces time and memory overhead compared to\nexisting methods. Our approach is evaluated across multiple Open Graph\nBenchmark (OGB) datasets and GNN architectures, achieving up to 1.2% accuracy\nimprovement and 2.1X speedup. Additionally, we propose Partition Learned\nSouping, a novel partition-based variant of learned souping that significantly\nreduces memory usage. On the ogbn-products dataset with GraphSAGE, partition\nlearned souping achieves a 24.5X speedup and a 76% memory reduction without\ncompromising accuracy."
                },
                "authors": [
                    {
                        "name": "Joseph Zuber"
                    },
                    {
                        "name": "Aishwarya Sarkar"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Ali Jannesari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Jannesari"
                },
                "author": "Ali Jannesari",
                "arxiv_comment": "10 pages, 4 figures, 3 tables, accepted to GrAPL 2025 (colocated with\n  IPDPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03461v2",
                "updated": "2025-03-14T17:27:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    27,
                    0,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-04T14:21:27Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    14,
                    21,
                    27,
                    4,
                    278,
                    0
                ],
                "title": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval-Augmented Generation"
                },
                "summary": "While retrieval-augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. A common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10% of their computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While retrieval-augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. A common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10% of their computational cost."
                },
                "authors": [
                    {
                        "name": "Tobias Leemann"
                    },
                    {
                        "name": "Periklis Petridis"
                    },
                    {
                        "name": "Giuseppe Vietri"
                    },
                    {
                        "name": "Dionysis Manousakas"
                    },
                    {
                        "name": "Aaron Roth"
                    },
                    {
                        "name": "Sergul Aydore"
                    }
                ],
                "author_detail": {
                    "name": "Sergul Aydore"
                },
                "author": "Sergul Aydore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11609v1",
                "updated": "2025-03-14T17:24:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    24,
                    1,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:24:01Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    24,
                    1,
                    4,
                    73,
                    0
                ],
                "title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages"
                },
                "summary": "An old-school recipe for training a classifier is to (i) learn a good feature\nextractor and (ii) optimize a linear layer atop. When only a handful of samples\nare available per category, as in Few-Shot Adaptation (FSA), data are\ninsufficient to fit a large number of parameters, rendering the above\nimpractical. This is especially true with large pre-trained Vision-Language\nModels (VLMs), which motivated successful research at the intersection of\nParameter-Efficient Fine-tuning (PEFT) and FSA. In this work, we start by\nanalyzing the learning dynamics of PEFT techniques when trained on few-shot\ndata from only a subset of categories, referred to as the ``base'' classes. We\nshow that such dynamics naturally splits into two distinct phases: (i)\ntask-level feature extraction and (ii) specialization to the available\nconcepts. To accommodate this dynamic, we then depart from prompt- or\nadapter-based methods and tackle FSA differently. Specifically, given a fixed\ncomputational budget, we split it to (i) learn a task-specific feature\nextractor via PEFT and (ii) train a linear classifier on top. We call this\nscheme Two-Stage Few-Shot Adaptation (2SFS). Differently from established\nmethods, our scheme enables a novel form of selective inference at a category\nlevel, i.e., at test time, only novel categories are embedded by the adapted\ntext encoder, while embeddings of base categories are available within the\nclassifier. Results with fixed hyperparameters across two settings, three\nbackbones, and eleven datasets, show that 2SFS matches or surpasses the\nstate-of-the-art, while established methods degrade significantly across\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An old-school recipe for training a classifier is to (i) learn a good feature\nextractor and (ii) optimize a linear layer atop. When only a handful of samples\nare available per category, as in Few-Shot Adaptation (FSA), data are\ninsufficient to fit a large number of parameters, rendering the above\nimpractical. This is especially true with large pre-trained Vision-Language\nModels (VLMs), which motivated successful research at the intersection of\nParameter-Efficient Fine-tuning (PEFT) and FSA. In this work, we start by\nanalyzing the learning dynamics of PEFT techniques when trained on few-shot\ndata from only a subset of categories, referred to as the ``base'' classes. We\nshow that such dynamics naturally splits into two distinct phases: (i)\ntask-level feature extraction and (ii) specialization to the available\nconcepts. To accommodate this dynamic, we then depart from prompt- or\nadapter-based methods and tackle FSA differently. Specifically, given a fixed\ncomputational budget, we split it to (i) learn a task-specific feature\nextractor via PEFT and (ii) train a linear classifier on top. We call this\nscheme Two-Stage Few-Shot Adaptation (2SFS). Differently from established\nmethods, our scheme enables a novel form of selective inference at a category\nlevel, i.e., at test time, only novel categories are embedded by the adapted\ntext encoder, while embeddings of base categories are available within the\nclassifier. Results with fixed hyperparameters across two settings, three\nbackbones, and eleven datasets, show that 2SFS matches or surpasses the\nstate-of-the-art, while established methods degrade significantly across\nsettings."
                },
                "authors": [
                    {
                        "name": "Matteo Farina"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    },
                    {
                        "name": "Giovanni Iacca"
                    },
                    {
                        "name": "Elisa Ricci"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Ricci"
                },
                "author": "Elisa Ricci",
                "arxiv_comment": "Camera-ready version for CVPR 2025 (w/ SuppMat, 23 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02603v2",
                "updated": "2025-03-14T17:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    9,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-03T15:44:42Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    42,
                    3,
                    277,
                    0
                ],
                "title": "Agents' Room: Narrative Generation through Multi-step Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents' Room: Narrative Generation through Multi-step Collaboration"
                },
                "summary": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output."
                },
                "authors": [
                    {
                        "name": "Fantine Huot"
                    },
                    {
                        "name": "Reinald Kim Amplayo"
                    },
                    {
                        "name": "Jennimaria Palomaki"
                    },
                    {
                        "name": "Alice Shoshana Jakobovits"
                    },
                    {
                        "name": "Elizabeth Clark"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11597v1",
                "updated": "2025-03-14T17:08:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    8,
                    46,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:08:46Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    8,
                    46,
                    4,
                    73,
                    0
                ],
                "title": "Microlensing Constraints on the Stellar and Planetary Mass Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microlensing Constraints on the Stellar and Planetary Mass Functions"
                },
                "summary": "The mass function (MF) of isolated objects measured by microlensing consists\nof both a stellar and a planetary component. We compare the microlensing MFs of\nGould et al (2022) and Sumi et al (2023) to other measurements of the MF. The\nabundance of brown dwarfs in the Sumi et al (2023) stellar MF is consistent\nwith measurements from the local solar neighborhood (Kirkpatrick et al 2024).\nMicrolensing free-floating planets ($\\mu$FFPs) may may be free-floating or\norbit host stars with semimajor axes $a\\gtrsim 10~\\mathrm{au}$ and therefore\ncan constrain the populations of both free-floating planetary-mass objects and\nwide-orbit planets. Comparisons to radial velocity and direct imaging planet\npopulations suggest that either most of the $\\mu$FFP population with masses\n$>1~M_{\\rm Jup}$ is bound to hosts more massive than M dwarfs or some fraction\nof the observed bound population actually comes from the low-mass tail of the\nstellar population. The $\\mu$FFP population also places strong constraints on\nplanets inferred from debris disks and gaps in protoplanetary disks observed by\nALMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mass function (MF) of isolated objects measured by microlensing consists\nof both a stellar and a planetary component. We compare the microlensing MFs of\nGould et al (2022) and Sumi et al (2023) to other measurements of the MF. The\nabundance of brown dwarfs in the Sumi et al (2023) stellar MF is consistent\nwith measurements from the local solar neighborhood (Kirkpatrick et al 2024).\nMicrolensing free-floating planets ($\\mu$FFPs) may may be free-floating or\norbit host stars with semimajor axes $a\\gtrsim 10~\\mathrm{au}$ and therefore\ncan constrain the populations of both free-floating planetary-mass objects and\nwide-orbit planets. Comparisons to radial velocity and direct imaging planet\npopulations suggest that either most of the $\\mu$FFP population with masses\n$>1~M_{\\rm Jup}$ is bound to hosts more massive than M dwarfs or some fraction\nof the observed bound population actually comes from the low-mass tail of the\nstellar population. The $\\mu$FFP population also places strong constraints on\nplanets inferred from debris disks and gaps in protoplanetary disks observed by\nALMA."
                },
                "authors": [
                    {
                        "name": "Jennifer C. Yee"
                    },
                    {
                        "name": "Scott J. Kenyon"
                    }
                ],
                "author_detail": {
                    "name": "Scott J. Kenyon"
                },
                "author": "Scott J. Kenyon",
                "arxiv_comment": "submitted to AAS Journals, 21 pages with 5 tables and 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16236v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16236v3",
                "updated": "2025-03-14T17:08:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    8,
                    22,
                    4,
                    73,
                    0
                ],
                "published": "2024-05-25T13:54:05Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    13,
                    54,
                    5,
                    5,
                    146,
                    0
                ],
                "title": "A transfer learning framework for weak-to-strong generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A transfer learning framework for weak-to-strong generalization"
                },
                "summary": "Modern large language model (LLM) alignment techniques rely on human\nfeedback, but it is unclear whether these techniques fundamentally limit the\ncapabilities of aligned LLMs. In particular, it is unknown if it is possible to\nalign (stronger) LLMs with superhuman capabilities with (weaker) human feedback\nwithout degrading their capabilities. This is an instance of the weak-to-strong\ngeneralization problem: using feedback from a weaker (less capable) model to\ntrain a stronger (more capable) model. We prove that weak-to-strong\ngeneralization is possible by eliciting latent knowledge from pre-trained LLMs.\nIn particular, we cast the weak-to-strong generalization problem as a transfer\nlearning problem in which we wish to transfer a latent concept prior from a\nweak model to a strong pre-trained model. We prove that a naive fine-tuning\napproach suffers from fundamental limitations, but an alternative\nrefinement-based approach suggested by the problem structure provably overcomes\nthe limitations of fine-tuning. Finally, we demonstrate the practical\napplicability of the refinement approach in multiple LLM alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) alignment techniques rely on human\nfeedback, but it is unclear whether these techniques fundamentally limit the\ncapabilities of aligned LLMs. In particular, it is unknown if it is possible to\nalign (stronger) LLMs with superhuman capabilities with (weaker) human feedback\nwithout degrading their capabilities. This is an instance of the weak-to-strong\ngeneralization problem: using feedback from a weaker (less capable) model to\ntrain a stronger (more capable) model. We prove that weak-to-strong\ngeneralization is possible by eliciting latent knowledge from pre-trained LLMs.\nIn particular, we cast the weak-to-strong generalization problem as a transfer\nlearning problem in which we wish to transfer a latent concept prior from a\nweak model to a strong pre-trained model. We prove that a naive fine-tuning\napproach suffers from fundamental limitations, but an alternative\nrefinement-based approach suggested by the problem structure provably overcomes\nthe limitations of fine-tuning. Finally, we demonstrate the practical\napplicability of the refinement approach in multiple LLM alignment tasks."
                },
                "authors": [
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Moulinath Banerjee"
                    },
                    {
                        "name": "Ya'acov Ritov"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Yuekai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuekai Sun"
                },
                "author": "Yuekai Sun",
                "arxiv_comment": "v2: Major changes to set up, theory, and experiments v3: Camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16236v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16236v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11586v1",
                "updated": "2025-03-14T16:55:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    55,
                    46,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:55:46Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    55,
                    46,
                    4,
                    73,
                    0
                ],
                "title": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs\n  using Semantic Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs\n  using Semantic Space"
                },
                "summary": "Large language models (LLMs) are used in chatbots or AI assistants to hold\nconversations with a human user. In such applications, the quality (e.g., user\nengagement, safety) of a conversation is important and can only be exactly\nknown at the end of the conversation. To maximize its expected quality,\nconversation planning reasons about the stochastic transitions within a\nconversation to select the optimal LLM response at each turn. Existing\nsimulation-based conversation planning algorithms typically select the optimal\nresponse by simulating future conversations with a large number of LLM queries\nat every turn. However, this process is extremely time-consuming and hence\nimpractical for real-time conversations. This paper presents a novel approach\ncalled Semantic space COnversation Planning with improved Efficiency (SCOPE)\nthat exploits the dense semantic representation of conversations to perform\nconversation planning efficiently. In particular, SCOPE models the stochastic\ntransitions in conversation semantics and their associated rewards to plan\nentirely within the semantic space. This allows us to select the optimal LLM\nresponse at every conversation turn without needing additional LLM queries for\nsimulation. As a result, SCOPE can perform conversation planning 70 times\nfaster than conventional simulation-based planning algorithms when applied to a\nwide variety of conversation starters and two reward functions seen in the real\nworld, yet achieving a higher reward within a practical planning budget. Our\ncode can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used in chatbots or AI assistants to hold\nconversations with a human user. In such applications, the quality (e.g., user\nengagement, safety) of a conversation is important and can only be exactly\nknown at the end of the conversation. To maximize its expected quality,\nconversation planning reasons about the stochastic transitions within a\nconversation to select the optimal LLM response at each turn. Existing\nsimulation-based conversation planning algorithms typically select the optimal\nresponse by simulating future conversations with a large number of LLM queries\nat every turn. However, this process is extremely time-consuming and hence\nimpractical for real-time conversations. This paper presents a novel approach\ncalled Semantic space COnversation Planning with improved Efficiency (SCOPE)\nthat exploits the dense semantic representation of conversations to perform\nconversation planning efficiently. In particular, SCOPE models the stochastic\ntransitions in conversation semantics and their associated rewards to plan\nentirely within the semantic space. This allows us to select the optimal LLM\nresponse at every conversation turn without needing additional LLM queries for\nsimulation. As a result, SCOPE can perform conversation planning 70 times\nfaster than conventional simulation-based planning algorithms when applied to a\nwide variety of conversation starters and two reward functions seen in the real\nworld, yet achieving a higher reward within a practical planning budget. Our\ncode can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE."
                },
                "authors": [
                    {
                        "name": "Zhiliang Chen"
                    },
                    {
                        "name": "Xinyuan Niu"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00096v2",
                "updated": "2025-03-14T16:52:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    52,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2023-09-29T19:09:27Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    19,
                    9,
                    27,
                    4,
                    272,
                    0
                ],
                "title": "Towards Few-Call Model Stealing via Active Self-Paced Knowledge\n  Distillation and Diffusion-Based Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Few-Call Model Stealing via Active Self-Paced Knowledge\n  Distillation and Diffusion-Based Image Generation"
                },
                "summary": "Diffusion models showcase strong capabilities in image synthesis, being used\nin many computer vision tasks with great success. To this end, we propose to\nexplore a new use case, namely to copy black-box classification models without\nhaving access to the original training data, the architecture, and the weights\nof the model, i.e. the model is only exposed through an inference API. More\nspecifically, we can only observe the (soft or hard) labels for some image\nsamples passed as input to the model. Furthermore, we consider an additional\nconstraint limiting the number of model calls, mostly focusing our research on\nfew-call model stealing. In order to solve the model extraction task given the\napplied restrictions, we propose the following framework. As training data, we\ncreate a synthetic data set (called proxy data set) by leveraging the ability\nof diffusion models to generate realistic and diverse images. Given a maximum\nnumber of allowed API calls, we pass the respective number of samples through\nthe black-box model to collect labels. Finally, we distill the knowledge of the\nblack-box teacher (attacked model) into a student model (copy of the attacked\nmodel), harnessing both labeled and unlabeled data generated by the diffusion\nmodel. We employ a novel active self-paced learning framework to make the most\nof the proxy data during distillation. Our empirical results on three data sets\nconfirm the superiority of our framework over four state-of-the-art methods in\nthe few-call model extraction scenario. We release our code for free\nnon-commercial use at https://github.com/vladhondru25/model-stealing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models showcase strong capabilities in image synthesis, being used\nin many computer vision tasks with great success. To this end, we propose to\nexplore a new use case, namely to copy black-box classification models without\nhaving access to the original training data, the architecture, and the weights\nof the model, i.e. the model is only exposed through an inference API. More\nspecifically, we can only observe the (soft or hard) labels for some image\nsamples passed as input to the model. Furthermore, we consider an additional\nconstraint limiting the number of model calls, mostly focusing our research on\nfew-call model stealing. In order to solve the model extraction task given the\napplied restrictions, we propose the following framework. As training data, we\ncreate a synthetic data set (called proxy data set) by leveraging the ability\nof diffusion models to generate realistic and diverse images. Given a maximum\nnumber of allowed API calls, we pass the respective number of samples through\nthe black-box model to collect labels. Finally, we distill the knowledge of the\nblack-box teacher (attacked model) into a student model (copy of the attacked\nmodel), harnessing both labeled and unlabeled data generated by the diffusion\nmodel. We employ a novel active self-paced learning framework to make the most\nof the proxy data during distillation. Our empirical results on three data sets\nconfirm the superiority of our framework over four state-of-the-art methods in\nthe few-call model extraction scenario. We release our code for free\nnon-commercial use at https://github.com/vladhondru25/model-stealing."
                },
                "authors": [
                    {
                        "name": "Vlad Hondru"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Tudor Ionescu"
                },
                "author": "Radu Tudor Ionescu",
                "arxiv_comment": "Accepted in Artificial Intelligence Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11579v1",
                "updated": "2025-03-14T16:45:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    45,
                    23,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:45:23Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    45,
                    23,
                    4,
                    73,
                    0
                ],
                "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers"
                },
                "summary": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640$\\times$360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640$\\times$360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks."
                },
                "authors": [
                    {
                        "name": "Weiming Ren"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Cong Wei"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "Project Page: https://tiger-ai-lab.github.io/Vamba/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11573v1",
                "updated": "2025-03-14T16:40:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    25,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:40:25Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    25,
                    4,
                    73,
                    0
                ],
                "title": "Synthesizing Access Control Policies using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Access Control Policies using Large Language Models"
                },
                "summary": "Cloud compute systems allow administrators to write access control policies\nthat govern access to private data. While policies are written in convenient\nlanguages, such as AWS Identity and Access Management Policy Language, manually\nwritten policies often become complex and error prone. In this paper, we\ninvestigate whether and how well Large Language Models (LLMs) can be used to\nsynthesize access control policies. Our investigation focuses on the task of\ntaking an access control request specification and zero-shot prompting LLMs to\nsynthesize a well-formed access control policy which correctly adheres to the\nrequest specification. We consider two scenarios, one which the request\nspecification is given as a concrete list of requests to be allowed or denied,\nand another in which a natural language description is used to specify sets of\nrequests to be allowed or denied. We then argue that for zero-shot prompting,\nmore precise and structured prompts using a syntax based approach are necessary\nand experimentally show preliminary results validating our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud compute systems allow administrators to write access control policies\nthat govern access to private data. While policies are written in convenient\nlanguages, such as AWS Identity and Access Management Policy Language, manually\nwritten policies often become complex and error prone. In this paper, we\ninvestigate whether and how well Large Language Models (LLMs) can be used to\nsynthesize access control policies. Our investigation focuses on the task of\ntaking an access control request specification and zero-shot prompting LLMs to\nsynthesize a well-formed access control policy which correctly adheres to the\nrequest specification. We consider two scenarios, one which the request\nspecification is given as a concrete list of requests to be allowed or denied,\nand another in which a natural language description is used to specify sets of\nrequests to be allowed or denied. We then argue that for zero-shot prompting,\nmore precise and structured prompts using a syntax based approach are necessary\nand experimentally show preliminary results validating our approach."
                },
                "authors": [
                    {
                        "name": "Adarsh Vatsa"
                    },
                    {
                        "name": "Pratyush Patel"
                    },
                    {
                        "name": "William Eiers"
                    }
                ],
                "author_detail": {
                    "name": "William Eiers"
                },
                "author": "William Eiers",
                "arxiv_comment": "to be published in the NLBSE Workshop at ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11572v1",
                "updated": "2025-03-14T16:40:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    2,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:40:02Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    2,
                    4,
                    73,
                    0
                ],
                "title": "Implicit Bias-Like Patterns in Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Bias-Like Patterns in Reasoning Models"
                },
                "summary": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Messi H. J. Lee"
                    },
                    {
                        "name": "Calvin K. Lai"
                    }
                ],
                "author_detail": {
                    "name": "Calvin K. Lai"
                },
                "author": "Calvin K. Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.04517v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.04517v2",
                "updated": "2025-03-14T16:34:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    34,
                    24,
                    4,
                    73,
                    0
                ],
                "published": "2022-09-09T20:39:22Z",
                "published_parsed": [
                    2022,
                    9,
                    9,
                    20,
                    39,
                    22,
                    4,
                    252,
                    0
                ],
                "title": "Affinity-VAE: incorporating prior knowledge in representation learning\n  from scientific images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affinity-VAE: incorporating prior knowledge in representation learning\n  from scientific images"
                },
                "summary": "Learning compact and interpretable representations of data is a critical\nchallenge in scientific image analysis. Here, we introduce Affinity-VAE, a\ngenerative model that enables us to impose our scientific intuition about the\nsimilarity of instances in the dataset on the learned representation during\ntraining. We demonstrate the utility of the approach in the scientific domain\nof cryo-electron tomography (cryo-ET) where a significant current challenge is\nto identify similar molecules within a noisy and low contrast tomographic image\nvolume. This task is distinct from classification in that, at inference time,\nit is unknown whether an instance is part of the training set or not. We\ntrained affinity-VAE using prior knowledge of protein structure to inform the\nlatent space. Our model is able to create rotationally-invariant,\nmorphologically homogeneous clusters in the latent representation, with\nimproved cluster separation compared to other approaches. It achieves\ncompetitive performance on protein classification with the added benefit of\ndisentangling object pose, structural similarity and an interpretable latent\nrepresentation. In the context of cryo-ET data, affinity-VAE captures the\norientation of identified proteins in 3D which can be used as a prior for\nsubsequent scientific experiments. Extracting physical principles from a\ntrained network is of significant importance in scientific imaging where a\nground truth training set is not always feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning compact and interpretable representations of data is a critical\nchallenge in scientific image analysis. Here, we introduce Affinity-VAE, a\ngenerative model that enables us to impose our scientific intuition about the\nsimilarity of instances in the dataset on the learned representation during\ntraining. We demonstrate the utility of the approach in the scientific domain\nof cryo-electron tomography (cryo-ET) where a significant current challenge is\nto identify similar molecules within a noisy and low contrast tomographic image\nvolume. This task is distinct from classification in that, at inference time,\nit is unknown whether an instance is part of the training set or not. We\ntrained affinity-VAE using prior knowledge of protein structure to inform the\nlatent space. Our model is able to create rotationally-invariant,\nmorphologically homogeneous clusters in the latent representation, with\nimproved cluster separation compared to other approaches. It achieves\ncompetitive performance on protein classification with the added benefit of\ndisentangling object pose, structural similarity and an interpretable latent\nrepresentation. In the context of cryo-ET data, affinity-VAE captures the\norientation of identified proteins in 3D which can be used as a prior for\nsubsequent scientific experiments. Extracting physical principles from a\ntrained network is of significant importance in scientific imaging where a\nground truth training set is not always feasible."
                },
                "authors": [
                    {
                        "name": "Marjan Famili"
                    },
                    {
                        "name": "Jola Mirecka"
                    },
                    {
                        "name": "Camila Rangel Smith"
                    },
                    {
                        "name": "Anna Kotaska"
                    },
                    {
                        "name": "Nikolai Juraschko"
                    },
                    {
                        "name": "Beatriz Costa-Gomes"
                    },
                    {
                        "name": "Colin M. Palmer"
                    },
                    {
                        "name": "Jeyan Thiyagalingam"
                    },
                    {
                        "name": "Tom Burnley"
                    },
                    {
                        "name": "Mark Basham"
                    },
                    {
                        "name": "Alan R. Lowe"
                    }
                ],
                "author_detail": {
                    "name": "Alan R. Lowe"
                },
                "author": "Alan R. Lowe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.04517v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.04517v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11562v1",
                "updated": "2025-03-14T16:30:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    30,
                    31,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:30:31Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    30,
                    31,
                    4,
                    73,
                    0
                ],
                "title": "Designing Neural Synthesizers for Low Latency Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Neural Synthesizers for Low Latency Interaction"
                },
                "summary": "Neural Audio Synthesis (NAS) models offer interactive musical control over\nhigh-quality, expressive audio generators. While these models can operate in\nreal-time, they often suffer from high latency, making them unsuitable for\nintimate musical interaction. The impact of architectural choices in deep\nlearning models on audio latency remains largely unexplored in the NAS\nliterature. In this work, we investigate the sources of latency and jitter\ntypically found in interactive NAS models. We then apply this analysis to the\ntask of timbre transfer using RAVE, a convolutional variational autoencoder for\naudio waveforms introduced by Caillon et al. in 2021. Finally, we present an\niterative design approach for optimizing latency. This culminates with a model\nwe call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is\nlow-latency and exhibits better pitch and loudness replication while showing\ntimbre modification capabilities similar to RAVE. We implement it in a\nspecialized inference framework for low-latency, real-time inference and\npresent a proof-of-concept audio plugin compatible with audio signals from\nmusical instruments. We expect the challenges and guidelines described in this\ndocument to support NAS researchers in designing models for low-latency\ninference from the ground up, enriching the landscape of possibilities for\nmusicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Synthesis (NAS) models offer interactive musical control over\nhigh-quality, expressive audio generators. While these models can operate in\nreal-time, they often suffer from high latency, making them unsuitable for\nintimate musical interaction. The impact of architectural choices in deep\nlearning models on audio latency remains largely unexplored in the NAS\nliterature. In this work, we investigate the sources of latency and jitter\ntypically found in interactive NAS models. We then apply this analysis to the\ntask of timbre transfer using RAVE, a convolutional variational autoencoder for\naudio waveforms introduced by Caillon et al. in 2021. Finally, we present an\niterative design approach for optimizing latency. This culminates with a model\nwe call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is\nlow-latency and exhibits better pitch and loudness replication while showing\ntimbre modification capabilities similar to RAVE. We implement it in a\nspecialized inference framework for low-latency, real-time inference and\npresent a proof-of-concept audio plugin compatible with audio signals from\nmusical instruments. We expect the challenges and guidelines described in this\ndocument to support NAS researchers in designing models for low-latency\ninference from the ground up, enriching the landscape of possibilities for\nmusicians."
                },
                "authors": [
                    {
                        "name": "Franco Caspe"
                    },
                    {
                        "name": "Jordie Shier"
                    },
                    {
                        "name": "Mark Sandler"
                    },
                    {
                        "name": "Charalampos Saitis"
                    },
                    {
                        "name": "Andrew McPherson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew McPherson"
                },
                "author": "Andrew McPherson",
                "arxiv_comment": "See website at fcaspe.github.io/brave - 13 pages, 5 figures, accepted\n  to the Journal of the Audio Engineering Society",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16560v2",
                "updated": "2025-03-14T16:18:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    18,
                    50,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-25T02:20:42Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    20,
                    42,
                    2,
                    269,
                    0
                ],
                "title": "Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have shown outstanding performance across\nnumerous real-world tasks. However, the autoregressive nature of these models\nmakes the inference process slow and costly. Speculative decoding has emerged\nas a promising solution, leveraging a smaller auxiliary model to draft future\ntokens, which are then validated simultaneously by the larger model, achieving\na speed-up of 1-2x. Although speculative decoding matches the same distribution\nas multinomial sampling, multinomial sampling itself is prone to suboptimal\noutputs, whereas beam sampling is widely recognized for producing\nhigher-quality results by maintaining multiple candidate sequences at each\nstep. This paper explores the novel integration of speculative decoding with\nbeam sampling. However, there are four key challenges: (1) how to generate\nmultiple sequences from the larger model's distribution given drafts sequences\nfrom the small model; (2) how to dynamically optimize the number of beams to\nbalance efficiency and accuracy; (3) how to efficiently verify the multiple\ndrafts in parallel; and (4) how to address the extra memory costs inherent in\nbeam sampling. To address these challenges, we propose dynamic-width\nspeculative beam decoding (DSBD). Specifically, we first introduce a novel\ndraft and verification scheme that generates multiple sequences following the\nlarge model's distribution based on beam sampling trajectories from the small\nmodel. Then, we introduce an adaptive mechanism to dynamically tune the number\nof beams based on the context, optimizing efficiency and effectiveness.\nBesides, we extend tree-based parallel verification to handle multiple trees\nsimultaneously, accelerating the verification process. Finally, we illustrate a\nsimple modification to our algorithm to mitigate the memory overhead of beam\nsampling...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown outstanding performance across\nnumerous real-world tasks. However, the autoregressive nature of these models\nmakes the inference process slow and costly. Speculative decoding has emerged\nas a promising solution, leveraging a smaller auxiliary model to draft future\ntokens, which are then validated simultaneously by the larger model, achieving\na speed-up of 1-2x. Although speculative decoding matches the same distribution\nas multinomial sampling, multinomial sampling itself is prone to suboptimal\noutputs, whereas beam sampling is widely recognized for producing\nhigher-quality results by maintaining multiple candidate sequences at each\nstep. This paper explores the novel integration of speculative decoding with\nbeam sampling. However, there are four key challenges: (1) how to generate\nmultiple sequences from the larger model's distribution given drafts sequences\nfrom the small model; (2) how to dynamically optimize the number of beams to\nbalance efficiency and accuracy; (3) how to efficiently verify the multiple\ndrafts in parallel; and (4) how to address the extra memory costs inherent in\nbeam sampling. To address these challenges, we propose dynamic-width\nspeculative beam decoding (DSBD). Specifically, we first introduce a novel\ndraft and verification scheme that generates multiple sequences following the\nlarge model's distribution based on beam sampling trajectories from the small\nmodel. Then, we introduce an adaptive mechanism to dynamically tune the number\nof beams based on the context, optimizing efficiency and effectiveness.\nBesides, we extend tree-based parallel verification to handle multiple trees\nsimultaneously, accelerating the verification process. Finally, we illustrate a\nsimple modification to our algorithm to mitigate the memory overhead of beam\nsampling..."
                },
                "authors": [
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Jason Cong"
                    },
                    {
                        "name": "Yizhou Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Sun"
                },
                "author": "Yizhou Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11552v1",
                "updated": "2025-03-14T16:15:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    15,
                    54,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:15:54Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    15,
                    54,
                    4,
                    73,
                    0
                ],
                "title": "Goal-oriented Spectrum Sharing: Trading Edge Inference Power for Data\n  Streaming Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented Spectrum Sharing: Trading Edge Inference Power for Data\n  Streaming Performance"
                },
                "summary": "We study the problem of spectrum sharing between goal-oriented (GO) and\nlegacy data-oriented (DO) systems. For the former, data quality and\nrepresentation is no longer optimized based on classical communication key\nperformance indicators, but rather configured on the fly to achieve the goal of\ncommunication with the least resource overhead. This paradigm can be followed\nto flexibly adapt wireless and in-network artificial intelligence operations\nacross different nodes (e.g., access points, users, sensors or actuators) to\ndata traffic, channel conditions, energy availability and distributed computing\ncapabilities. In this paper, we argue and demonstrate that computing and\nlearning/inference operation performance strongly affect lower layers, calling\nfor a real cross-layer optimization that encompasses physical and computation\nresource orchestration, up to the application level. Focusing on a\ncommunication channel shared among a GO and a DO user, we define a\ngoal-effective achievable rate region (GEARR), to assess the maximum data rate\nattainable by the latter, subject to goal achievement guarantees for the\nformer. Finally, we propose a cross-layer dynamic resource orchestration able\nto reach the boundaries of the GEARR, under different goaleffectiveness and\ncompute resource consumption constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of spectrum sharing between goal-oriented (GO) and\nlegacy data-oriented (DO) systems. For the former, data quality and\nrepresentation is no longer optimized based on classical communication key\nperformance indicators, but rather configured on the fly to achieve the goal of\ncommunication with the least resource overhead. This paradigm can be followed\nto flexibly adapt wireless and in-network artificial intelligence operations\nacross different nodes (e.g., access points, users, sensors or actuators) to\ndata traffic, channel conditions, energy availability and distributed computing\ncapabilities. In this paper, we argue and demonstrate that computing and\nlearning/inference operation performance strongly affect lower layers, calling\nfor a real cross-layer optimization that encompasses physical and computation\nresource orchestration, up to the application level. Focusing on a\ncommunication channel shared among a GO and a DO user, we define a\ngoal-effective achievable rate region (GEARR), to assess the maximum data rate\nattainable by the latter, subject to goal achievement guarantees for the\nformer. Finally, we propose a cross-layer dynamic resource orchestration able\nto reach the boundaries of the GEARR, under different goaleffectiveness and\ncompute resource consumption constraints."
                },
                "authors": [
                    {
                        "name": "Mattia Merluzzi"
                    },
                    {
                        "name": "Miltiadis C. Filippou"
                    }
                ],
                "author_detail": {
                    "name": "Miltiadis C. Filippou"
                },
                "author": "Miltiadis C. Filippou",
                "arxiv_comment": "Submitted to EUSIPCO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.21030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.21030v2",
                "updated": "2025-03-14T16:14:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    14,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2024-05-31T17:21:52Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    17,
                    21,
                    52,
                    4,
                    152,
                    0
                ],
                "title": "Standards for Belief Representations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standards for Belief Representations in LLMs"
                },
                "summary": "As large language models (LLMs) continue to demonstrate remarkable abilities\nacross various domains, computer scientists are developing methods to\nunderstand their cognitive processes, particularly concerning how (and if) LLMs\ninternally represent their beliefs about the world. However, this field\ncurrently lacks a unified theoretical foundation to underpin the study of\nbelief in LLMs. This article begins filling this gap by proposing adequacy\nconditions for a representation in an LLM to count as belief-like. We argue\nthat, while the project of belief measurement in LLMs shares striking features\nwith belief measurement as carried out in decision theory and formal\nepistemology, it also differs in ways that should change how we measure belief.\nThus, drawing from insights in philosophy and contemporary practices of machine\nlearning, we establish four criteria that balance theoretical considerations\nwith practical constraints. Our proposed criteria include accuracy, coherence,\nuniformity, and use, which together help lay the groundwork for a comprehensive\nunderstanding of belief representation in LLMs. We draw on empirical work\nshowing the limitations of using various criteria in isolation to identify\nbelief representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to demonstrate remarkable abilities\nacross various domains, computer scientists are developing methods to\nunderstand their cognitive processes, particularly concerning how (and if) LLMs\ninternally represent their beliefs about the world. However, this field\ncurrently lacks a unified theoretical foundation to underpin the study of\nbelief in LLMs. This article begins filling this gap by proposing adequacy\nconditions for a representation in an LLM to count as belief-like. We argue\nthat, while the project of belief measurement in LLMs shares striking features\nwith belief measurement as carried out in decision theory and formal\nepistemology, it also differs in ways that should change how we measure belief.\nThus, drawing from insights in philosophy and contemporary practices of machine\nlearning, we establish four criteria that balance theoretical considerations\nwith practical constraints. Our proposed criteria include accuracy, coherence,\nuniformity, and use, which together help lay the groundwork for a comprehensive\nunderstanding of belief representation in LLMs. We draw on empirical work\nshowing the limitations of using various criteria in isolation to identify\nbelief representations."
                },
                "authors": [
                    {
                        "name": "Daniel A. Herrmann"
                    },
                    {
                        "name": "Benjamin A. Levinstein"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin A. Levinstein"
                },
                "author": "Benjamin A. Levinstein",
                "arxiv_doi": "10.1007/s11023-024-09709-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11023-024-09709-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.21030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.21030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Minds and Machines 35:5 (2025) 1-25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11549v1",
                "updated": "2025-03-14T16:12:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    12,
                    23,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:12:23Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    12,
                    23,
                    4,
                    73,
                    0
                ],
                "title": "Similarity-Aware Token Pruning: Your VLM but Faster",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-Aware Token Pruning: Your VLM but Faster"
                },
                "summary": "The computational demands of Vision Transformers (ViTs) and Vision-Language\nModels (VLMs) remain a significant challenge due to the quadratic complexity of\nself-attention. While token pruning offers a promising solution, existing\nmethods often introduce training overhead or fail to adapt dynamically across\nlayers. We present SAINT, a training-free token pruning framework that\nleverages token similarity and a graph-based formulation to dynamically\noptimize pruning rates and redundancy thresholds. Through systematic analysis,\nwe identify a universal three-stage token evolution process\n(aligner-explorer-aggregator) in transformers, enabling aggressive pruning in\nearly stages without sacrificing critical information. For ViTs, SAINT doubles\nthe throughput of ViT-H/14 at 224px with only 0.6% accuracy loss on\nImageNet-1K, surpassing the closest competitor by 0.8%. For VLMs, we apply\nSAINT in three modes: ViT-only, LLM-only, and hybrid. SAINT reduces LLaVA-13B's\ntokens by 75%, achieving latency comparable to LLaVA-7B with less than 1%\nperformance loss across benchmarks. Our work establishes a unified, practical\nframework for efficient inference in ViTs and VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational demands of Vision Transformers (ViTs) and Vision-Language\nModels (VLMs) remain a significant challenge due to the quadratic complexity of\nself-attention. While token pruning offers a promising solution, existing\nmethods often introduce training overhead or fail to adapt dynamically across\nlayers. We present SAINT, a training-free token pruning framework that\nleverages token similarity and a graph-based formulation to dynamically\noptimize pruning rates and redundancy thresholds. Through systematic analysis,\nwe identify a universal three-stage token evolution process\n(aligner-explorer-aggregator) in transformers, enabling aggressive pruning in\nearly stages without sacrificing critical information. For ViTs, SAINT doubles\nthe throughput of ViT-H/14 at 224px with only 0.6% accuracy loss on\nImageNet-1K, surpassing the closest competitor by 0.8%. For VLMs, we apply\nSAINT in three modes: ViT-only, LLM-only, and hybrid. SAINT reduces LLaVA-13B's\ntokens by 75%, achieving latency comparable to LLaVA-7B with less than 1%\nperformance loss across benchmarks. Our work establishes a unified, practical\nframework for efficient inference in ViTs and VLMs."
                },
                "authors": [
                    {
                        "name": "Ahmadreza Jeddi"
                    },
                    {
                        "name": "Negin Baghbanzadeh"
                    },
                    {
                        "name": "Elham Dolatabadi"
                    },
                    {
                        "name": "Babak Taati"
                    }
                ],
                "author_detail": {
                    "name": "Babak Taati"
                },
                "author": "Babak Taati",
                "arxiv_comment": "15 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11531v1",
                "updated": "2025-03-14T15:58:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    58,
                    11,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:58:11Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    58,
                    11,
                    4,
                    73,
                    0
                ],
                "title": "Potential of large language model-powered nudges for promoting daily\n  water and energy conservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential of large language model-powered nudges for promoting daily\n  water and energy conservation"
                },
                "summary": "The increasing amount of pressure related to water and energy shortages has\nincreased the urgency of cultivating individual conservation behaviors. While\nthe concept of nudging, i.e., providing usage-based feedback, has shown promise\nin encouraging conservation behaviors, its efficacy is often constrained by the\nlack of targeted and actionable content. This study investigates the impact of\nthe use of large language models (LLMs) to provide tailored conservation\nsuggestions for conservation intentions and their rationale. Through a survey\nexperiment with 1,515 university participants, we compare three virtual nudging\nscenarios: no nudging, traditional nudging with usage statistics, and\nLLM-powered nudging with usage statistics and personalized conservation\nsuggestions. The results of statistical analyses and causal forest modeling\nreveal that nudging led to an increase in conservation intentions among\n86.9%-98.0% of the participants. LLM-powered nudging achieved a maximum\nincrease of 18.0% in conservation intentions, surpassing traditional nudging by\n88.6%. Furthermore, structural equation modeling results reveal that exposure\nto LLM-powered nudges enhances self-efficacy and outcome expectations while\ndiminishing dependence on social norms, thereby increasing intrinsic motivation\nto conserve. These findings highlight the transformative potential of LLMs in\npromoting individual water and energy conservation, representing a new frontier\nin the design of sustainable behavioral interventions and resource management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing amount of pressure related to water and energy shortages has\nincreased the urgency of cultivating individual conservation behaviors. While\nthe concept of nudging, i.e., providing usage-based feedback, has shown promise\nin encouraging conservation behaviors, its efficacy is often constrained by the\nlack of targeted and actionable content. This study investigates the impact of\nthe use of large language models (LLMs) to provide tailored conservation\nsuggestions for conservation intentions and their rationale. Through a survey\nexperiment with 1,515 university participants, we compare three virtual nudging\nscenarios: no nudging, traditional nudging with usage statistics, and\nLLM-powered nudging with usage statistics and personalized conservation\nsuggestions. The results of statistical analyses and causal forest modeling\nreveal that nudging led to an increase in conservation intentions among\n86.9%-98.0% of the participants. LLM-powered nudging achieved a maximum\nincrease of 18.0% in conservation intentions, surpassing traditional nudging by\n88.6%. Furthermore, structural equation modeling results reveal that exposure\nto LLM-powered nudges enhances self-efficacy and outcome expectations while\ndiminishing dependence on social norms, thereby increasing intrinsic motivation\nto conserve. These findings highlight the transformative potential of LLMs in\npromoting individual water and energy conservation, representing a new frontier\nin the design of sustainable behavioral interventions and resource management."
                },
                "authors": [
                    {
                        "name": "Zonghan Li"
                    },
                    {
                        "name": "Song Tong"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Kaiping Peng"
                    },
                    {
                        "name": "Chunyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Wang"
                },
                "author": "Chunyan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07594v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07594v4",
                "updated": "2025-03-14T15:37:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    37,
                    14,
                    4,
                    73,
                    0
                ],
                "published": "2024-02-12T11:48:54Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    11,
                    48,
                    54,
                    0,
                    43,
                    0
                ],
                "title": "Zero-shot Imputation with Foundation Inference Models for Dynamical\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Imputation with Foundation Inference Models for Dynamical\n  Systems"
                },
                "summary": "Dynamical systems governed by ordinary differential equations (ODEs) serve as\nmodels for a vast number of natural and social phenomena. In this work, we\noffer a fresh perspective on the classical problem of imputing missing time\nseries data, whose underlying dynamics are assumed to be determined by ODEs.\nSpecifically, we revisit ideas from amortized inference and neural operators,\nand propose a novel supervised learning framework for zero-shot time series\nimputation, through parametric functions satisfying some (hidden) ODEs. Our\nproposal consists of two components. First, a broad probability distribution\nover the space of ODE solutions, observation times and noise mechanisms, with\nwhich we generate a large, synthetic dataset of (hidden) ODE solutions, along\nwith their noisy and sparse observations. Second, a neural recognition model\nthat is trained offline, to map the generated time series onto the spaces of\ninitial conditions and time derivatives of the (hidden) ODE solutions, which we\nthen integrate to impute the missing data. We empirically demonstrate that one\nand the same (pretrained) recognition model can perform zero-shot imputation\nacross 63 distinct time series with missing values, each sampled from widely\ndifferent dynamical systems. Likewise, we demonstrate that it can perform\nzero-shot imputation of missing high-dimensional data in 10 vastly different\nsettings, spanning human motion, air quality, traffic and electricity studies,\nas well as Navier-Stokes simulations -- without requiring any fine-tuning. What\nis more, our proposal often outperforms state-of-the-art methods, which are\ntrained on the target datasets.\n  Our pretrained model, repository and tutorials are available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical systems governed by ordinary differential equations (ODEs) serve as\nmodels for a vast number of natural and social phenomena. In this work, we\noffer a fresh perspective on the classical problem of imputing missing time\nseries data, whose underlying dynamics are assumed to be determined by ODEs.\nSpecifically, we revisit ideas from amortized inference and neural operators,\nand propose a novel supervised learning framework for zero-shot time series\nimputation, through parametric functions satisfying some (hidden) ODEs. Our\nproposal consists of two components. First, a broad probability distribution\nover the space of ODE solutions, observation times and noise mechanisms, with\nwhich we generate a large, synthetic dataset of (hidden) ODE solutions, along\nwith their noisy and sparse observations. Second, a neural recognition model\nthat is trained offline, to map the generated time series onto the spaces of\ninitial conditions and time derivatives of the (hidden) ODE solutions, which we\nthen integrate to impute the missing data. We empirically demonstrate that one\nand the same (pretrained) recognition model can perform zero-shot imputation\nacross 63 distinct time series with missing values, each sampled from widely\ndifferent dynamical systems. Likewise, we demonstrate that it can perform\nzero-shot imputation of missing high-dimensional data in 10 vastly different\nsettings, spanning human motion, air quality, traffic and electricity studies,\nas well as Navier-Stokes simulations -- without requiring any fine-tuning. What\nis more, our proposal often outperforms state-of-the-art methods, which are\ntrained on the target datasets.\n  Our pretrained model, repository and tutorials are available online."
                },
                "authors": [
                    {
                        "name": "Patrick Seifner"
                    },
                    {
                        "name": "Kostadin Cvejoski"
                    },
                    {
                        "name": "Antonia Krner"
                    },
                    {
                        "name": "Ramss J. Snchez"
                    }
                ],
                "author_detail": {
                    "name": "Ramss J. Snchez"
                },
                "author": "Ramss J. Snchez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07594v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07594v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11513v1",
                "updated": "2025-03-14T15:36:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    36,
                    39,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:36:39Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    36,
                    39,
                    4,
                    73,
                    0
                ],
                "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation\n  with Autoregressive Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation\n  with Autoregressive Large Language Models"
                },
                "summary": "Text-to-video generation poses significant challenges due to the inherent\ncomplexity of video data, which spans both temporal and spatial dimensions. It\nintroduces additional redundancy, abrupt variations, and a domain gap between\nlanguage and vision tokens while generation. Addressing these challenges\nrequires an effective video tokenizer that can efficiently encode video data\nwhile preserving essential semantic and spatiotemporal information, serving as\na critical bridge between text and vision. Inspired by the observation in\nVQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for\ntext-to-video generation with hierarchical tokenizers. It utilizes a 3D causal\nVAE with a multi-layer discrete token framework, encoding video content into\nhierarchically structured codebooks. Higher layers capture semantic information\nwith higher compression, while lower layers focus on fine-grained\nspatiotemporal details, striking a balance between compression efficiency and\nreconstruction quality. Our approach efficiently encodes longer video sequences\n(e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately\n70\\% compared to baseline tokenizers, while maintaining competitive\nreconstruction quality. We explore the trade-offs between compression and\nreconstruction, while emphasizing the advantages of high-compressed semantic\ntokens in text-to-video tasks. HiTVideo aims to address the potential\nlimitations of existing video tokenizers in text-to-video generation tasks,\nstriving for higher compression ratios and simplify LLMs modeling under\nlanguage guidance, offering a scalable and promising framework for advancing\ntext to video generation. Demo page:\nhttps://ziqinzhou66.github.io/project/HiTVideo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video generation poses significant challenges due to the inherent\ncomplexity of video data, which spans both temporal and spatial dimensions. It\nintroduces additional redundancy, abrupt variations, and a domain gap between\nlanguage and vision tokens while generation. Addressing these challenges\nrequires an effective video tokenizer that can efficiently encode video data\nwhile preserving essential semantic and spatiotemporal information, serving as\na critical bridge between text and vision. Inspired by the observation in\nVQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for\ntext-to-video generation with hierarchical tokenizers. It utilizes a 3D causal\nVAE with a multi-layer discrete token framework, encoding video content into\nhierarchically structured codebooks. Higher layers capture semantic information\nwith higher compression, while lower layers focus on fine-grained\nspatiotemporal details, striking a balance between compression efficiency and\nreconstruction quality. Our approach efficiently encodes longer video sequences\n(e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately\n70\\% compared to baseline tokenizers, while maintaining competitive\nreconstruction quality. We explore the trade-offs between compression and\nreconstruction, while emphasizing the advantages of high-compressed semantic\ntokens in text-to-video tasks. HiTVideo aims to address the potential\nlimitations of existing video tokenizers in text-to-video generation tasks,\nstriving for higher compression ratios and simplify LLMs modeling under\nlanguage guidance, offering a scalable and promising framework for advancing\ntext to video generation. Demo page:\nhttps://ziqinzhou66.github.io/project/HiTVideo."
                },
                "authors": [
                    {
                        "name": "Ziqin Zhou"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Houwen Peng"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lingqiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingqiao Liu"
                },
                "author": "Lingqiao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11509v1",
                "updated": "2025-03-14T15:29:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    29,
                    58,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:29:58Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    29,
                    58,
                    4,
                    73,
                    0
                ],
                "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis"
                },
                "summary": "With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available."
                },
                "authors": [
                    {
                        "name": "Jonas Belouadi"
                    },
                    {
                        "name": "Eddy Ilg"
                    },
                    {
                        "name": "Margret Keuper"
                    },
                    {
                        "name": "Hideki Tanaka"
                    },
                    {
                        "name": "Masao Utiyama"
                    },
                    {
                        "name": "Raj Dabre"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Paolo Ponzetto"
                },
                "author": "Simone Paolo Ponzetto",
                "arxiv_comment": "Project page: https://github.com/potamides/DeTikZify",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06639v2",
                "updated": "2025-03-14T15:25:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    25,
                    46,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-09T14:36:45Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    14,
                    36,
                    45,
                    6,
                    68,
                    0
                ],
                "title": "Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss,\n  Dynamics, and Success Amplification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss,\n  Dynamics, and Success Amplification"
                },
                "summary": "Group Relative Policy Optimization (GRPO) was introduced and used\nsuccessfully to train DeepSeek R1 models for promoting reasoning capabilities\nof LLMs using verifiable or binary rewards. We show in this paper that GRPO\nwith verifiable rewards can be written as a Kullback Leibler ($\\mathsf{KL}$)\nregularized contrastive loss, where the contrastive samples are synthetic data\nsampled from the old policy. The optimal GRPO policy $\\pi_{n}$ can be expressed\nexplicitly in terms of the binary reward, as well as the first and second order\nstatistics of the old policy ($\\pi_{n-1}$) and the reference policy $\\pi_0$.\nIterating this scheme, we obtain a sequence of policies $\\pi_{n}$ for which we\ncan quantify the probability of success $p_n$. We show that the probability of\nsuccess of the policy satisfies a recurrence that converges to a fixed point of\na function that depends on the initial probability of success $p_0$ and the\nregularization parameter $\\beta$ of the $\\mathsf{KL}$ regularizer. We show that\nthe fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby\ndemonstrating that GRPO effectively amplifies the probability of success of the\npolicy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Relative Policy Optimization (GRPO) was introduced and used\nsuccessfully to train DeepSeek R1 models for promoting reasoning capabilities\nof LLMs using verifiable or binary rewards. We show in this paper that GRPO\nwith verifiable rewards can be written as a Kullback Leibler ($\\mathsf{KL}$)\nregularized contrastive loss, where the contrastive samples are synthetic data\nsampled from the old policy. The optimal GRPO policy $\\pi_{n}$ can be expressed\nexplicitly in terms of the binary reward, as well as the first and second order\nstatistics of the old policy ($\\pi_{n-1}$) and the reference policy $\\pi_0$.\nIterating this scheme, we obtain a sequence of policies $\\pi_{n}$ for which we\ncan quantify the probability of success $p_n$. We show that the probability of\nsuccess of the policy satisfies a recurrence that converges to a fixed point of\na function that depends on the initial probability of success $p_0$ and the\nregularization parameter $\\beta$ of the $\\mathsf{KL}$ regularizer. We show that\nthe fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby\ndemonstrating that GRPO effectively amplifies the probability of success of the\npolicy."
                },
                "authors": [
                    {
                        "name": "Youssef Mroueh"
                    }
                ],
                "author_detail": {
                    "name": "Youssef Mroueh"
                },
                "author": "Youssef Mroueh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11495v1",
                "updated": "2025-03-14T15:21:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    21,
                    44,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:21:44Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    21,
                    44,
                    4,
                    73,
                    0
                ],
                "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning"
                },
                "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning."
                },
                "authors": [
                    {
                        "name": "Zixu Cheng"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shaogang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Shaogang Gong"
                },
                "author": "Shaogang Gong",
                "arxiv_comment": "A benchmark for Video Spatio-Temporal Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11488v1",
                "updated": "2025-03-14T15:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    13,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:13:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    13,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Unicorn: A Universal and Collaborative Reinforcement Learning Approach\n  Towards Generalizable Network-Wide Traffic Signal Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unicorn: A Universal and Collaborative Reinforcement Learning Approach\n  Towards Generalizable Network-Wide Traffic Signal Control"
                },
                "summary": "Adaptive traffic signal control (ATSC) is crucial in reducing congestion,\nmaximizing throughput, and improving mobility in rapidly growing urban areas.\nRecent advancements in parameter-sharing multi-agent reinforcement learning\n(MARL) have greatly enhanced the scalable and adaptive optimization of complex,\ndynamic flows in large-scale homogeneous networks. However, the inherent\nheterogeneity of real-world traffic networks, with their varied intersection\ntopologies and interaction dynamics, poses substantial challenges to achieving\nscalable and effective ATSC across different traffic scenarios. To address\nthese challenges, we present Unicorn, a universal and collaborative MARL\nframework designed for efficient and adaptable network-wide ATSC. Specifically,\nwe first propose a unified approach to map the states and actions of\nintersections with varying topologies into a common structure based on traffic\nmovements. Next, we design a Universal Traffic Representation (UTR) module with\na decoder-only network for general feature extraction, enhancing the model's\nadaptability to diverse traffic scenarios. Additionally, we incorporate an\nIntersection Specifics Representation (ISR) module, designed to identify key\nlatent vectors that represent the unique intersection's topology and traffic\ndynamics through variational inference techniques. To further refine these\nlatent representations, we employ a contrastive learning approach in a\nself-supervised manner, which enables better differentiation of\nintersection-specific features. Moreover, we integrate the state-action\ndependencies of neighboring agents into policy optimization, which effectively\ncaptures dynamic agent interactions and facilitates efficient regional\ncollaboration. Our results show that Unicorn outperforms other methods across\nvarious evaluation metrics, highlighting its potential in complex, dynamic\ntraffic networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive traffic signal control (ATSC) is crucial in reducing congestion,\nmaximizing throughput, and improving mobility in rapidly growing urban areas.\nRecent advancements in parameter-sharing multi-agent reinforcement learning\n(MARL) have greatly enhanced the scalable and adaptive optimization of complex,\ndynamic flows in large-scale homogeneous networks. However, the inherent\nheterogeneity of real-world traffic networks, with their varied intersection\ntopologies and interaction dynamics, poses substantial challenges to achieving\nscalable and effective ATSC across different traffic scenarios. To address\nthese challenges, we present Unicorn, a universal and collaborative MARL\nframework designed for efficient and adaptable network-wide ATSC. Specifically,\nwe first propose a unified approach to map the states and actions of\nintersections with varying topologies into a common structure based on traffic\nmovements. Next, we design a Universal Traffic Representation (UTR) module with\na decoder-only network for general feature extraction, enhancing the model's\nadaptability to diverse traffic scenarios. Additionally, we incorporate an\nIntersection Specifics Representation (ISR) module, designed to identify key\nlatent vectors that represent the unique intersection's topology and traffic\ndynamics through variational inference techniques. To further refine these\nlatent representations, we employ a contrastive learning approach in a\nself-supervised manner, which enables better differentiation of\nintersection-specific features. Moreover, we integrate the state-action\ndependencies of neighboring agents into policy optimization, which effectively\ncaptures dynamic agent interactions and facilitates efficient regional\ncollaboration. Our results show that Unicorn outperforms other methods across\nvarious evaluation metrics, highlighting its potential in complex, dynamic\ntraffic networks."
                },
                "authors": [
                    {
                        "name": "Yifeng Zhang"
                    },
                    {
                        "name": "Yilin Liu"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Peizhuo Li"
                    },
                    {
                        "name": "Mingfeng Fan"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Sartoretti"
                },
                "author": "Guillaume Sartoretti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11486v1",
                "updated": "2025-03-14T15:11:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    11,
                    29,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:11:29Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    11,
                    29,
                    4,
                    73,
                    0
                ],
                "title": "A Review of DeepSeek Models' Key Innovative Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of DeepSeek Models' Key Innovative Techniques"
                },
                "summary": "DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models\n(LLMs) for general-purpose tasks and reasoning, achieving performance\ncomparable to state-of-the-art closed-source models from companies like OpenAI\nand Anthropic -- while requiring only a fraction of their training costs.\nUnderstanding the key innovative techniques behind DeepSeek's success is\ncrucial for advancing LLM research. In this paper, we review the core\ntechniques driving the remarkable effectiveness and efficiency of these models,\nincluding refinements to the transformer architecture, innovations such as\nMulti-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the\nco-design of algorithms, frameworks, and hardware, the Group Relative Policy\nOptimization algorithm, post-training with pure reinforcement learning and\niterative training alternating between supervised fine-tuning and reinforcement\nlearning. Additionally, we identify several open questions and highlight\npotential research opportunities in this rapidly advancing field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models\n(LLMs) for general-purpose tasks and reasoning, achieving performance\ncomparable to state-of-the-art closed-source models from companies like OpenAI\nand Anthropic -- while requiring only a fraction of their training costs.\nUnderstanding the key innovative techniques behind DeepSeek's success is\ncrucial for advancing LLM research. In this paper, we review the core\ntechniques driving the remarkable effectiveness and efficiency of these models,\nincluding refinements to the transformer architecture, innovations such as\nMulti-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the\nco-design of algorithms, frameworks, and hardware, the Group Relative Policy\nOptimization algorithm, post-training with pure reinforcement learning and\niterative training alternating between supervised fine-tuning and reinforcement\nlearning. Additionally, we identify several open questions and highlight\npotential research opportunities in this rapidly advancing field."
                },
                "authors": [
                    {
                        "name": "Chengen Wang"
                    },
                    {
                        "name": "Murat Kantarcioglu"
                    }
                ],
                "author_detail": {
                    "name": "Murat Kantarcioglu"
                },
                "author": "Murat Kantarcioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11470v1",
                "updated": "2025-03-14T14:56:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    56,
                    23,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:56:23Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    56,
                    23,
                    4,
                    73,
                    0
                ],
                "title": "Topological Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Dictionary Learning"
                },
                "summary": "The aim of this paper is to introduce a novel dictionary learning algorithm\nfor sparse representation of signals defined over combinatorial topological\nspaces, specifically, regular cell complexes. Leveraging Hodge theory, we embed\ntopology into the dictionary structure via concatenated sub-dictionaries, each\nas a polynomial of Hodge Laplacians, yielding localized spectral topological\nfilter frames. The learning problem is cast to jointly infer the underlying\ncell complex and optimize the dictionary coefficients and the sparse signal\nrepresentation. We efficiently solve the problem via iterative alternating\nalgorithms. Numerical results on both synthetic and real data show the\neffectiveness of the proposed procedure in jointly learning the sparse\nrepresentations and the underlying relational structure of topological signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The aim of this paper is to introduce a novel dictionary learning algorithm\nfor sparse representation of signals defined over combinatorial topological\nspaces, specifically, regular cell complexes. Leveraging Hodge theory, we embed\ntopology into the dictionary structure via concatenated sub-dictionaries, each\nas a polynomial of Hodge Laplacians, yielding localized spectral topological\nfilter frames. The learning problem is cast to jointly infer the underlying\ncell complex and optimize the dictionary coefficients and the sparse signal\nrepresentation. We efficiently solve the problem via iterative alternating\nalgorithms. Numerical results on both synthetic and real data show the\neffectiveness of the proposed procedure in jointly learning the sparse\nrepresentations and the underlying relational structure of topological signals."
                },
                "authors": [
                    {
                        "name": "Enrico Grimaldi"
                    },
                    {
                        "name": "Claudio Battiloro"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Di Lorenzo"
                },
                "author": "Paolo Di Lorenzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15100v2",
                "updated": "2025-03-14T14:47:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    52,
                    4,
                    73,
                    0
                ],
                "published": "2024-12-19T17:48:03Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    48,
                    3,
                    3,
                    354,
                    0
                ],
                "title": "Tests for model misspecification in simulation-based inference: from\n  local distortions to global model checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tests for model misspecification in simulation-based inference: from\n  local distortions to global model checks"
                },
                "summary": "Model misspecification analysis strategies, such as anomaly detection, model\nvalidation, and model comparison are a key component of scientific model\ndevelopment. Over the last few years, there has been a rapid rise in the use of\nsimulation-based inference (SBI) techniques for Bayesian parameter estimation,\napplied to increasingly complex forward models. To move towards fully\nsimulation-based analysis pipelines, however, there is an urgent need for a\ncomprehensive simulation-based framework for model misspecification analysis.\nIn this work, we provide a solid and flexible foundation for a wide range of\nmodel discrepancy analysis tasks, using distortion-driven model\nmisspecification tests. From a theoretical perspective, we introduce the\nstatistical framework built around performing many hypothesis tests for\ndistortions of the simulation model. We also make explicit analytic connections\nto classical techniques: anomaly detection, model validation, and\ngoodness-of-fit residual analysis. Furthermore, we introduce an efficient\nself-calibrating training algorithm that is useful for practitioners. We\ndemonstrate the performance of the framework in multiple scenarios, making the\nconnection to classical results where they are valid. Finally, we show how to\nconduct such a distortion-driven model misspecification test for real\ngravitational wave data, specifically on the event GW150914.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model misspecification analysis strategies, such as anomaly detection, model\nvalidation, and model comparison are a key component of scientific model\ndevelopment. Over the last few years, there has been a rapid rise in the use of\nsimulation-based inference (SBI) techniques for Bayesian parameter estimation,\napplied to increasingly complex forward models. To move towards fully\nsimulation-based analysis pipelines, however, there is an urgent need for a\ncomprehensive simulation-based framework for model misspecification analysis.\nIn this work, we provide a solid and flexible foundation for a wide range of\nmodel discrepancy analysis tasks, using distortion-driven model\nmisspecification tests. From a theoretical perspective, we introduce the\nstatistical framework built around performing many hypothesis tests for\ndistortions of the simulation model. We also make explicit analytic connections\nto classical techniques: anomaly detection, model validation, and\ngoodness-of-fit residual analysis. Furthermore, we introduce an efficient\nself-calibrating training algorithm that is useful for practitioners. We\ndemonstrate the performance of the framework in multiple scenarios, making the\nconnection to classical results where they are valid. Finally, we show how to\nconduct such a distortion-driven model misspecification test for real\ngravitational wave data, specifically on the event GW150914."
                },
                "authors": [
                    {
                        "name": "Noemi Anau Montel"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "arxiv_comment": "11 pages, 5 figures. Code available on github (NoemiAM/mist) at\n  https://github.com/NoemiAM/mist - v2: version accepted by PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11458v1",
                "updated": "2025-03-14T14:47:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    4,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:04Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    4,
                    4,
                    73,
                    0
                ],
                "title": "Integrating LLMs in Gamified Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs in Gamified Systems"
                },
                "summary": "In this work, a thorough mathematical framework for incorporating Large\nLanguage Models (LLMs) into gamified systems is presented with an emphasis on\nimproving task dynamics, user engagement, and reward systems. Personalized\nfeedback, adaptive learning, and dynamic content creation are all made possible\nby integrating LLMs and are crucial for improving user engagement and system\nperformance. A simulated environment tests the framework's adaptability and\ndemonstrates its potential for real-world applications in various industries,\nincluding business, healthcare, and education. The findings demonstrate how\nLLMs can offer customized experiences that raise system effectiveness and user\nretention. This study also examines the difficulties this framework aims to\nsolve, highlighting its importance in maximizing involvement and encouraging\nsustained behavioral change in a range of sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a thorough mathematical framework for incorporating Large\nLanguage Models (LLMs) into gamified systems is presented with an emphasis on\nimproving task dynamics, user engagement, and reward systems. Personalized\nfeedback, adaptive learning, and dynamic content creation are all made possible\nby integrating LLMs and are crucial for improving user engagement and system\nperformance. A simulated environment tests the framework's adaptability and\ndemonstrates its potential for real-world applications in various industries,\nincluding business, healthcare, and education. The findings demonstrate how\nLLMs can offer customized experiences that raise system effectiveness and user\nretention. This study also examines the difficulties this framework aims to\nsolve, highlighting its importance in maximizing involvement and encouraging\nsustained behavioral change in a range of sectors."
                },
                "authors": [
                    {
                        "name": "Carlos J. Costa"
                    }
                ],
                "author_detail": {
                    "name": "Carlos J. Costa"
                },
                "author": "Carlos J. Costa",
                "arxiv_comment": "9 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11444v1",
                "updated": "2025-03-14T14:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    29,
                    17,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:29:17Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    29,
                    17,
                    4,
                    73,
                    0
                ],
                "title": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery"
                },
                "summary": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https://app.aios.foundation,\nthe code is at https://github.com/agiresearch/Cerebrum, and video is at\nhttps://app.aios.foundation/video-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https://app.aios.foundation,\nthe code is at https://github.com/agiresearch/Cerebrum, and video is at\nhttps://app.aios.foundation/video-demo."
                },
                "authors": [
                    {
                        "name": "Balaji Rama"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "Accepted to the 2025 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL) - System\n  Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11441v1",
                "updated": "2025-03-14T14:28:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    28,
                    19,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:28:19Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    28,
                    19,
                    4,
                    73,
                    0
                ],
                "title": "D3: Diversity, Difficulty, and Dependability-Aware Data Selection for\n  Sample-Efficient LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D3: Diversity, Difficulty, and Dependability-Aware Data Selection for\n  Sample-Efficient LLM Instruction Tuning"
                },
                "summary": "Recent advancements in instruction tuning for large language models (LLMs)\nsuggest that a small, high-quality dataset can significantly equip LLMs with\ninstruction-following capabilities, outperforming large datasets often burdened\nby quality and redundancy issues. However, the challenge lies in automatically\nidentifying valuable subsets from large datasets to boost both the\neffectiveness and efficiency of instruction tuning. In this paper, we first\nestablish data selection criteria based on three distinct aspects of data\nvalue: diversity, difficulty, and dependability, and then propose the D3 method\ncomprising two key steps of scoring and selection. Specifically, in the scoring\nstep, we define the diversity function to measure sample distinctiveness and\nintroduce the uncertainty-based prediction difficulty to evaluate sample\ndifficulty by mitigating the interference of context-oriented generation\ndiversity. Additionally, we integrate an external LLM for dependability\nassessment. In the selection step, we formulate the D3 weighted coreset\nobjective, which jointly optimizes three aspects of data value to solve for the\nmost valuable subset. The two steps of D3 can iterate multiple rounds,\nincorporating feedback to refine the selection focus adaptively. Experiments on\nthree datasets demonstrate the effectiveness of D3 in endowing LLMs with\ncompetitive or even superior instruction-following capabilities using less than\n10% of the entire dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in instruction tuning for large language models (LLMs)\nsuggest that a small, high-quality dataset can significantly equip LLMs with\ninstruction-following capabilities, outperforming large datasets often burdened\nby quality and redundancy issues. However, the challenge lies in automatically\nidentifying valuable subsets from large datasets to boost both the\neffectiveness and efficiency of instruction tuning. In this paper, we first\nestablish data selection criteria based on three distinct aspects of data\nvalue: diversity, difficulty, and dependability, and then propose the D3 method\ncomprising two key steps of scoring and selection. Specifically, in the scoring\nstep, we define the diversity function to measure sample distinctiveness and\nintroduce the uncertainty-based prediction difficulty to evaluate sample\ndifficulty by mitigating the interference of context-oriented generation\ndiversity. Additionally, we integrate an external LLM for dependability\nassessment. In the selection step, we formulate the D3 weighted coreset\nobjective, which jointly optimizes three aspects of data value to solve for the\nmost valuable subset. The two steps of D3 can iterate multiple rounds,\nincorporating feedback to refine the selection focus adaptively. Experiments on\nthree datasets demonstrate the effectiveness of D3 in endowing LLMs with\ncompetitive or even superior instruction-following capabilities using less than\n10% of the entire dataset."
                },
                "authors": [
                    {
                        "name": "Jia Zhang"
                    },
                    {
                        "name": "Chen-Xi Zhang"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Yi-Xuan Jin"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19209v3",
                "updated": "2025-03-14T13:57:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    57,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2024-05-29T15:49:09Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    15,
                    49,
                    9,
                    2,
                    150,
                    0
                ],
                "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos"
                },
                "summary": "Long-form video understanding is complicated by the high redundancy of video\ndata and the abundance of query-irrelevant information. To tackle these\nchallenges, we propose VideoTree, a training-free framework which builds a\nquery-adaptive and hierarchical video representation for LLM reasoning over\nlong-form videos. First, VideoTree extracts query-relevant information from the\ninput video through an iterative process, progressively refining the selection\nof keyframes based on their relevance to the query. Furthermore, VideoTree\nleverages the inherent hierarchical structure of long video data, which is\noften overlooked by existing LLM-based methods. Specifically, we incorporate\nmulti-granularity information into a tree-based representation, allowing\nVideoTree to extract query-relevant details from long videos in a\ncoarse-to-fine manner. This enables the model to effectively handle a wide\nrange of video queries with varying levels of detail. Finally, VideoTree\naggregates the hierarchical query-relevant information within the tree\nstructure and feeds it into an LLM reasoning model to answer the query. Our\nexperiments show that our method improves both reasoning accuracy and\nefficiency. Specifically, VideoTree outperforms existing training-free\napproaches on EgoSchema and NExT-QA with less inference time, achieving 61.1%\nand 75.6% accuracy on the test set without additional video-specific training.\nMoreover, on the long split of Video-MME (average 44 minutes), VideoTree\nachieves better performance than GPT-4V and many other MLLMs that were\nextensively trained on video data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding is complicated by the high redundancy of video\ndata and the abundance of query-irrelevant information. To tackle these\nchallenges, we propose VideoTree, a training-free framework which builds a\nquery-adaptive and hierarchical video representation for LLM reasoning over\nlong-form videos. First, VideoTree extracts query-relevant information from the\ninput video through an iterative process, progressively refining the selection\nof keyframes based on their relevance to the query. Furthermore, VideoTree\nleverages the inherent hierarchical structure of long video data, which is\noften overlooked by existing LLM-based methods. Specifically, we incorporate\nmulti-granularity information into a tree-based representation, allowing\nVideoTree to extract query-relevant details from long videos in a\ncoarse-to-fine manner. This enables the model to effectively handle a wide\nrange of video queries with varying levels of detail. Finally, VideoTree\naggregates the hierarchical query-relevant information within the tree\nstructure and feeds it into an LLM reasoning model to answer the query. Our\nexperiments show that our method improves both reasoning accuracy and\nefficiency. Specifically, VideoTree outperforms existing training-free\napproaches on EgoSchema and NExT-QA with less inference time, achieving 61.1%\nand 75.6% accuracy on the test set without additional video-specific training.\nMoreover, on the long split of Video-MME (average 44 minutes), VideoTree\nachieves better performance than GPT-4V and many other MLLMs that were\nextensively trained on video data."
                },
                "authors": [
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "CVPR 2025; First three authors contributed equally; Project page:\n  https://videotree2024.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10422v2",
                "updated": "2025-03-14T13:56:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    56,
                    52,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-13T14:43:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    14,
                    43,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "Category Prompt Mamba Network for Nuclei Segmentation and Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category Prompt Mamba Network for Nuclei Segmentation and Classification"
                },
                "summary": "Nuclei segmentation and classification provide an essential basis for tumor\nimmune microenvironment analysis. The previous nuclei segmentation and\nclassification models require splitting large images into smaller patches for\ntraining, leading to two significant issues. First, nuclei at the borders of\nadjacent patches often misalign during inference. Second, this patch-based\napproach significantly increases the model's training and inference time.\nRecently, Mamba has garnered attention for its ability to model large-scale\nimages with linear time complexity and low memory consumption. It offers a\npromising solution for training nuclei segmentation and classification models\non full-sized images. However, the Mamba orientation-based scanning method\nlacks account for category-specific features, resulting in sub-optimal\nperformance in scenarios with imbalanced class distributions. To address these\nchallenges, this paper introduces a novel scanning strategy based on category\nprobability sorting, which independently ranks and scans features for each\ncategory according to confidence from high to low. This approach enhances the\nfeature representation of uncertain samples and mitigates the issues caused by\nimbalanced distributions. Extensive experiments conducted on four public\ndatasets demonstrate that our method outperforms state-of-the-art approaches,\ndelivering superior performance in nuclei segmentation and classification\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nuclei segmentation and classification provide an essential basis for tumor\nimmune microenvironment analysis. The previous nuclei segmentation and\nclassification models require splitting large images into smaller patches for\ntraining, leading to two significant issues. First, nuclei at the borders of\nadjacent patches often misalign during inference. Second, this patch-based\napproach significantly increases the model's training and inference time.\nRecently, Mamba has garnered attention for its ability to model large-scale\nimages with linear time complexity and low memory consumption. It offers a\npromising solution for training nuclei segmentation and classification models\non full-sized images. However, the Mamba orientation-based scanning method\nlacks account for category-specific features, resulting in sub-optimal\nperformance in scenarios with imbalanced class distributions. To address these\nchallenges, this paper introduces a novel scanning strategy based on category\nprobability sorting, which independently ranks and scans features for each\ncategory according to confidence from high to low. This approach enhances the\nfeature representation of uncertain samples and mitigates the issues caused by\nimbalanced distributions. Extensive experiments conducted on four public\ndatasets demonstrate that our method outperforms state-of-the-art approaches,\ndelivering superior performance in nuclei segmentation and classification\ntasks."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zijie Fang"
                    },
                    {
                        "name": "Yifeng Wang"
                    },
                    {
                        "name": "Lingbo Zhang"
                    },
                    {
                        "name": "Xianchao Guan"
                    },
                    {
                        "name": "Yongbing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongbing Zhang"
                },
                "author": "Yongbing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11409v1",
                "updated": "2025-03-14T13:51:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    51,
                    52,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:51:52Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    51,
                    52,
                    4,
                    73,
                    0
                ],
                "title": "LuSeg: Efficient Negative and Positive Obstacles Segmentation via\n  Contrast-Driven Multi-Modal Feature Fusion on the Lunar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LuSeg: Efficient Negative and Positive Obstacles Segmentation via\n  Contrast-Driven Multi-Modal Feature Fusion on the Lunar"
                },
                "summary": "As lunar exploration missions grow increasingly complex, ensuring safe and\nautonomous rover-based surface exploration has become one of the key challenges\nin lunar exploration tasks. In this work, we have developed a lunar surface\nsimulation system called the Lunar Exploration Simulator System (LESS) and the\nLunarSeg dataset, which provides RGB-D data for lunar obstacle segmentation\nthat includes both positive and negative obstacles. Additionally, we propose a\nnovel two-stage segmentation network called LuSeg. Through contrastive\nlearning, it enforces semantic consistency between the RGB encoder from Stage I\nand the depth encoder from Stage II. Experimental results on our proposed\nLunarSeg dataset and additional public real-world NPO road obstacle dataset\ndemonstrate that LuSeg achieves state-of-the-art segmentation performance for\nboth positive and negative obstacles while maintaining a high inference speed\nof approximately 57\\,Hz. We have released the implementation of our LESS\nsystem, LunarSeg dataset, and the code of LuSeg\nat:https://github.com/nubot-nudt/LuSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As lunar exploration missions grow increasingly complex, ensuring safe and\nautonomous rover-based surface exploration has become one of the key challenges\nin lunar exploration tasks. In this work, we have developed a lunar surface\nsimulation system called the Lunar Exploration Simulator System (LESS) and the\nLunarSeg dataset, which provides RGB-D data for lunar obstacle segmentation\nthat includes both positive and negative obstacles. Additionally, we propose a\nnovel two-stage segmentation network called LuSeg. Through contrastive\nlearning, it enforces semantic consistency between the RGB encoder from Stage I\nand the depth encoder from Stage II. Experimental results on our proposed\nLunarSeg dataset and additional public real-world NPO road obstacle dataset\ndemonstrate that LuSeg achieves state-of-the-art segmentation performance for\nboth positive and negative obstacles while maintaining a high inference speed\nof approximately 57\\,Hz. We have released the implementation of our LESS\nsystem, LunarSeg dataset, and the code of LuSeg\nat:https://github.com/nubot-nudt/LuSeg."
                },
                "authors": [
                    {
                        "name": "Shuaifeng Jiao"
                    },
                    {
                        "name": "Zhiwen Zeng"
                    },
                    {
                        "name": "Zhuoqun Su"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    },
                    {
                        "name": "Zongtan Zhou"
                    },
                    {
                        "name": "Huimin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Lu"
                },
                "author": "Huimin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11402v1",
                "updated": "2025-03-14T13:43:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    43,
                    43,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:43:43Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    43,
                    43,
                    4,
                    73,
                    0
                ],
                "title": "Quality In, Quality Out: Investigating Training Data's Role in AI Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality In, Quality Out: Investigating Training Data's Role in AI Code\n  Generation"
                },
                "summary": "Deep Learning-based code generators have seen significant advancements in\nrecent years. Tools such as GitHub Copilot are used by thousands of developers\nwith the main promise of a boost in productivity. However, researchers have\nrecently questioned their impact on code quality showing, for example, that\ncode generated by DL-based tools may be affected by security vulnerabilities.\nSince DL models are trained on large code corpora, one may conjecture that\nlow-quality code they output is the result of low-quality code they have seen\nduring training. However, there is very little empirical evidence documenting\nthis phenomenon. Indeed, most of previous work look at the frequency with which\ncommercial code generators recommend low-quality code without the possibility\nof relating this to their training set. We investigate the extent to which\nlow-quality code instances seen during training affect the quality of the code\ngenerated at inference time. We start by fine-tuning a pre-trained DL model on\na large-scale dataset being representative of those usually adopted in the\ntraining of code generators. We show that 4.98% of functions in this dataset\nexhibit one or more quality issues related to security, maintainability, best\npractices, etc. We use the fine-tuned model to generate 551k Python functions,\nshowing that 5.85% of them are affected by at least one quality issue. We then\nremove from the training set the low-quality functions, and use the cleaned\ndataset to fine-tune a second model which has been used to generate the same\n551k Python functions. We show that the model trained on the cleaned dataset\nexhibits similar performance in terms of functional correctness as compared to\nthe original model while, however, generating a statistically significant lower\nnumber of low-quality functions (2.16%). Our study empirically documents the\nimportance of high-quality training data for code generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based code generators have seen significant advancements in\nrecent years. Tools such as GitHub Copilot are used by thousands of developers\nwith the main promise of a boost in productivity. However, researchers have\nrecently questioned their impact on code quality showing, for example, that\ncode generated by DL-based tools may be affected by security vulnerabilities.\nSince DL models are trained on large code corpora, one may conjecture that\nlow-quality code they output is the result of low-quality code they have seen\nduring training. However, there is very little empirical evidence documenting\nthis phenomenon. Indeed, most of previous work look at the frequency with which\ncommercial code generators recommend low-quality code without the possibility\nof relating this to their training set. We investigate the extent to which\nlow-quality code instances seen during training affect the quality of the code\ngenerated at inference time. We start by fine-tuning a pre-trained DL model on\na large-scale dataset being representative of those usually adopted in the\ntraining of code generators. We show that 4.98% of functions in this dataset\nexhibit one or more quality issues related to security, maintainability, best\npractices, etc. We use the fine-tuned model to generate 551k Python functions,\nshowing that 5.85% of them are affected by at least one quality issue. We then\nremove from the training set the low-quality functions, and use the cleaned\ndataset to fine-tune a second model which has been used to generate the same\n551k Python functions. We show that the model trained on the cleaned dataset\nexhibits similar performance in terms of functional correctness as compared to\nthe original model while, however, generating a statistically significant lower\nnumber of low-quality functions (2.16%). Our study empirically documents the\nimportance of high-quality training data for code generators."
                },
                "authors": [
                    {
                        "name": "Cristina Improta"
                    },
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Pietro Liguori"
                    },
                    {
                        "name": "Domenico Cotroneo"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "arxiv_comment": "Accepted to the 33rd IEEE/ACM International Conference on Program\n  Comprehension (ICPC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09210v2",
                "updated": "2025-03-14T13:40:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    40,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2023-07-18T12:46:34Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    12,
                    46,
                    34,
                    1,
                    199,
                    0
                ],
                "title": "Nested stochastic block model for simultaneously clustering networks and\n  nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nested stochastic block model for simultaneously clustering networks and\n  nodes"
                },
                "summary": "We introduce the nested stochastic block model (NSBM) to cluster a collection\nof networks while simultaneously detecting communities within each network.\nNSBM has several appealing features including the ability to work on unlabeled\nnetworks with potentially different node sets, the flexibility to model\nheterogeneous communities, and the means to automatically select the number of\nclasses for the networks and the number of communities within each network.\nThis is accomplished via a Bayesian model, with a novel application of the\nnested Dirichlet process (NDP) as a prior to jointly model the between-network\nand within-network clusters. The dependency introduced by the network data\ncreates nontrivial challenges for the NDP, especially in the development of\nefficient samplers. For posterior inference, we propose several Markov chain\nMonte Carlo algorithms including a standard Gibbs sampler, a collapsed Gibbs\nsampler, and two blocked Gibbs samplers that ultimately return two levels of\nclustering labels from both within and across the networks. Extensive\nsimulation studies are carried out which demonstrate that the model provides\nvery accurate estimates of both levels of the clustering structure. We also\napply our model to two social network datasets that cannot be analyzed using\nany previous method in the literature due to the anonymity of the nodes and the\nvarying number of nodes in each network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the nested stochastic block model (NSBM) to cluster a collection\nof networks while simultaneously detecting communities within each network.\nNSBM has several appealing features including the ability to work on unlabeled\nnetworks with potentially different node sets, the flexibility to model\nheterogeneous communities, and the means to automatically select the number of\nclasses for the networks and the number of communities within each network.\nThis is accomplished via a Bayesian model, with a novel application of the\nnested Dirichlet process (NDP) as a prior to jointly model the between-network\nand within-network clusters. The dependency introduced by the network data\ncreates nontrivial challenges for the NDP, especially in the development of\nefficient samplers. For posterior inference, we propose several Markov chain\nMonte Carlo algorithms including a standard Gibbs sampler, a collapsed Gibbs\nsampler, and two blocked Gibbs samplers that ultimately return two levels of\nclustering labels from both within and across the networks. Extensive\nsimulation studies are carried out which demonstrate that the model provides\nvery accurate estimates of both levels of the clustering structure. We also\napply our model to two social network datasets that cannot be analyzed using\nany previous method in the literature due to the anonymity of the nodes and the\nvarying number of nodes in each network."
                },
                "authors": [
                    {
                        "name": "Nathaniel Josephs"
                    },
                    {
                        "name": "Arash A. Amini"
                    },
                    {
                        "name": "Marina Paez"
                    },
                    {
                        "name": "Lizhen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lizhen Lin"
                },
                "author": "Lizhen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11390v1",
                "updated": "2025-03-14T13:34:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    34,
                    14,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:34:14Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    34,
                    14,
                    4,
                    73,
                    0
                ],
                "title": "On continuity of Chatterjee's rank correlation and related dependence\n  measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On continuity of Chatterjee's rank correlation and related dependence\n  measures"
                },
                "summary": "While measures of concordance -- such as Spearman's rho, Kendall's tau, and\nBlomqvist's beta -- are continuous with respect to weak convergence,\nChatterjee's rank correlation xi recently introduced in Azadkia and Chatterjee\n[5] does not share this property, causing drawbacks in statistical inference as\npointed out in B\\\"ucher and Dette [7]. As we study in this paper, xi is instead\nweakly continuous with respect to conditionally independent copies -- the\nMarkov products. To establish weak continuity of Markov products, we provide\nseveral sufficient conditions, including copula-based criteria and conditions\nrelying on the concept of conditional weak convergence in Sweeting [36]. As a\nconsequence, we also obtain continuity results for xi and related dependence\nmeasures and verify their continuity in the parameters of standard models such\nas multivariate elliptical and l1-norm symmetric distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While measures of concordance -- such as Spearman's rho, Kendall's tau, and\nBlomqvist's beta -- are continuous with respect to weak convergence,\nChatterjee's rank correlation xi recently introduced in Azadkia and Chatterjee\n[5] does not share this property, causing drawbacks in statistical inference as\npointed out in B\\\"ucher and Dette [7]. As we study in this paper, xi is instead\nweakly continuous with respect to conditionally independent copies -- the\nMarkov products. To establish weak continuity of Markov products, we provide\nseveral sufficient conditions, including copula-based criteria and conditions\nrelying on the concept of conditional weak convergence in Sweeting [36]. As a\nconsequence, we also obtain continuity results for xi and related dependence\nmeasures and verify their continuity in the parameters of standard models such\nas multivariate elliptical and l1-norm symmetric distributions."
                },
                "authors": [
                    {
                        "name": "Jonathan Ansari"
                    },
                    {
                        "name": "Sebastian Fuchs"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Fuchs"
                },
                "author": "Sebastian Fuchs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05788v2",
                "updated": "2025-03-14T13:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    28,
                    4,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-28T01:20:01Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    1,
                    20,
                    1,
                    4,
                    59,
                    0
                ],
                "title": "Emergent Abilities in Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Abilities in Large Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight."
                },
                "authors": [
                    {
                        "name": "Leonardo Berti"
                    },
                    {
                        "name": "Flavio Giorgi"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11384v1",
                "updated": "2025-03-14T13:27:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    27,
                    35,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:27:35Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    27,
                    35,
                    4,
                    73,
                    0
                ],
                "title": "Optimizing Large Language Models for Detecting Symptoms of Comorbid\n  Depression or Anxiety in Chronic Diseases: Insights from Patient Messages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Models for Detecting Symptoms of Comorbid\n  Depression or Anxiety in Chronic Diseases: Insights from Patient Messages"
                },
                "summary": "Patients with diabetes are at increased risk of comorbid depression or\nanxiety, complicating their management. This study evaluated the performance of\nlarge language models (LLMs) in detecting these symptoms from secure patient\nmessages. We applied multiple approaches, including engineered prompts,\nsystemic persona, temperature adjustments, and zero-shot and few-shot learning,\nto identify the best-performing model and enhance performance. Three out of\nfive LLMs demonstrated excellent performance (over 90% of F-1 and accuracy),\nwith Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot\napproach. While LLMs showed promise in binary classification and handling\ncomplex metrics like Patient Health Questionnaire-4, inconsistencies in\nchallenging cases warrant further real-life assessment. The findings highlight\nthe potential of LLMs to assist in timely screening and referrals, providing\nvaluable empirical knowledge for real-world triage systems that could improve\nmental health care for patients with chronic diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patients with diabetes are at increased risk of comorbid depression or\nanxiety, complicating their management. This study evaluated the performance of\nlarge language models (LLMs) in detecting these symptoms from secure patient\nmessages. We applied multiple approaches, including engineered prompts,\nsystemic persona, temperature adjustments, and zero-shot and few-shot learning,\nto identify the best-performing model and enhance performance. Three out of\nfive LLMs demonstrated excellent performance (over 90% of F-1 and accuracy),\nwith Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot\napproach. While LLMs showed promise in binary classification and handling\ncomplex metrics like Patient Health Questionnaire-4, inconsistencies in\nchallenging cases warrant further real-life assessment. The findings highlight\nthe potential of LLMs to assist in timely screening and referrals, providing\nvaluable empirical knowledge for real-world triage systems that could improve\nmental health care for patients with chronic diseases."
                },
                "authors": [
                    {
                        "name": "Jiyeong Kim"
                    },
                    {
                        "name": "Stephen P. Ma"
                    },
                    {
                        "name": "Michael L. Chen"
                    },
                    {
                        "name": "Isaac R. Galatzer-Levy"
                    },
                    {
                        "name": "John Torous"
                    },
                    {
                        "name": "Peter J. van Roessel"
                    },
                    {
                        "name": "Christopher Sharp"
                    },
                    {
                        "name": "Michael A. Pfeffer"
                    },
                    {
                        "name": "Carolyn I. Rodriguez"
                    },
                    {
                        "name": "Eleni Linos"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan H. Chen"
                },
                "author": "Jonathan H. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11381v1",
                "updated": "2025-03-14T13:25:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    25,
                    41,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:25:41Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    25,
                    41,
                    4,
                    73,
                    0
                ],
                "title": "Modeling Subjectivity in Cognitive Appraisal with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Subjectivity in Cognitive Appraisal with Language Models"
                },
                "summary": "As the utilization of language models in interdisciplinary, human-centered\nstudies grow, the expectation of model capabilities continues to evolve. Beyond\nexcelling at conventional tasks, models are recently expected to perform well\non user-centric measurements involving confidence and human (dis)agreement --\nfactors that reflect subjective preferences. While modeling of subjectivity\nplays an essential role in cognitive science and has been extensively studied,\nit remains under-explored within the NLP community. In light of this gap, we\nexplore how language models can harness subjectivity by conducting\ncomprehensive experiments and analysis across various scenarios using both\nfine-tuned models and prompt-based large language models (LLMs). Our\nquantitative and qualitative experimental results indicate that existing\npost-hoc calibration approaches often fail to produce satisfactory results.\nHowever, our findings reveal that personality traits and demographical\ninformation are critical for measuring subjectivity. Furthermore, our in-depth\nanalysis offers valuable insights for future research and development in the\ninterdisciplinary studies of NLP and cognitive science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the utilization of language models in interdisciplinary, human-centered\nstudies grow, the expectation of model capabilities continues to evolve. Beyond\nexcelling at conventional tasks, models are recently expected to perform well\non user-centric measurements involving confidence and human (dis)agreement --\nfactors that reflect subjective preferences. While modeling of subjectivity\nplays an essential role in cognitive science and has been extensively studied,\nit remains under-explored within the NLP community. In light of this gap, we\nexplore how language models can harness subjectivity by conducting\ncomprehensive experiments and analysis across various scenarios using both\nfine-tuned models and prompt-based large language models (LLMs). Our\nquantitative and qualitative experimental results indicate that existing\npost-hoc calibration approaches often fail to produce satisfactory results.\nHowever, our findings reveal that personality traits and demographical\ninformation are critical for measuring subjectivity. Furthermore, our in-depth\nanalysis offers valuable insights for future research and development in the\ninterdisciplinary studies of NLP and cognitive science."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Desmond C. Ong"
                    },
                    {
                        "name": "Petr Slovak"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11376v1",
                "updated": "2025-03-14T13:21:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    21,
                    59,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:21:59Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    21,
                    59,
                    4,
                    73,
                    0
                ],
                "title": "Annotating Scientific Uncertainty: A comprehensive model using\n  linguistic patterns and comparison with existing approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating Scientific Uncertainty: A comprehensive model using\n  linguistic patterns and comparison with existing approaches"
                },
                "summary": "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages."
                },
                "authors": [
                    {
                        "name": "Panggih Kusuma Ningrum"
                    },
                    {
                        "name": "Philipp Mayr"
                    },
                    {
                        "name": "Nina Smirnova"
                    },
                    {
                        "name": "Iana Atanassova"
                    }
                ],
                "author_detail": {
                    "name": "Iana Atanassova"
                },
                "author": "Iana Atanassova",
                "arxiv_comment": "Paper Accepted for Publication in the Journal of Informetrics (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11375v1",
                "updated": "2025-03-14T13:18:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    18,
                    21,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:18:21Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    18,
                    21,
                    4,
                    73,
                    0
                ],
                "title": "Difference-in-Differences Meets Synthetic Control: Doubly Robust\n  Identification and Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difference-in-Differences Meets Synthetic Control: Doubly Robust\n  Identification and Estimation"
                },
                "summary": "Difference-in-Differences (DiD) and Synthetic Control (SC) are widely used\nmethods for causal inference in panel data, each with its own strengths and\nlimitations. In this paper, we propose a novel methodology that integrates the\nadvantages of both DiD and SC approaches. Our integrated approach provides a\ndoubly robust identification strategy for causal effects in panel data with a\ngroup structure, identifying the average treatment effect on the treated (ATT)\nunder either the parallel trends assumption or the group-level SC assumption.\nBuilding on this identification result, we develop a unified semiparametric\nframework for estimating the ATT. Notably, while the identification-robust\nmoment function satisfies Neyman orthogonality under the parallel trends\nassumption, it does not under the SC assumption, leading to different\nasymptotic variances under these two identification strategies. To address this\nchallenge, we propose a multiplier bootstrap method that consistently\napproximates the asymptotic distribution, regardless of which identification\nassumption holds. Furthermore, we extend our methodology to accommodate\nrepeated cross-sectional data and staggered treatment designs. As an empirical\napplication, we apply our method to evaluate the impact of the 2003 minimum\nwage increase in Alaska on family income.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difference-in-Differences (DiD) and Synthetic Control (SC) are widely used\nmethods for causal inference in panel data, each with its own strengths and\nlimitations. In this paper, we propose a novel methodology that integrates the\nadvantages of both DiD and SC approaches. Our integrated approach provides a\ndoubly robust identification strategy for causal effects in panel data with a\ngroup structure, identifying the average treatment effect on the treated (ATT)\nunder either the parallel trends assumption or the group-level SC assumption.\nBuilding on this identification result, we develop a unified semiparametric\nframework for estimating the ATT. Notably, while the identification-robust\nmoment function satisfies Neyman orthogonality under the parallel trends\nassumption, it does not under the SC assumption, leading to different\nasymptotic variances under these two identification strategies. To address this\nchallenge, we propose a multiplier bootstrap method that consistently\napproximates the asymptotic distribution, regardless of which identification\nassumption holds. Furthermore, we extend our methodology to accommodate\nrepeated cross-sectional data and staggered treatment designs. As an empirical\napplication, we apply our method to evaluate the impact of the 2003 minimum\nwage increase in Alaska on family income."
                },
                "authors": [
                    {
                        "name": "Yixiao Sun"
                    },
                    {
                        "name": "Haitian Xie"
                    },
                    {
                        "name": "Yuhang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Zhang"
                },
                "author": "Yuhang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11371v1",
                "updated": "2025-03-14T13:15:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    15,
                    54,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:15:54Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    15,
                    54,
                    4,
                    73,
                    0
                ],
                "title": "EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation"
                },
                "summary": "Visual 3D motion estimation aims to infer the motion of 2D pixels in 3D space\nbased on visual cues. The key challenge arises from depth variation induced\nspatio-temporal motion inconsistencies, disrupting the assumptions of local\nspatial or temporal motion smoothness in previous motion estimation frameworks.\nIn contrast, event cameras offer new possibilities for 3D motion estimation\nthrough continuous adaptive pixel-level responses to scene changes. This paper\npresents EMoTive, a novel event-based framework that models spatio-temporal\ntrajectories via event-guided non-uniform parametric curves, effectively\ncharacterizing locally heterogeneous spatio-temporal motion. Specifically, we\nfirst introduce Event Kymograph - an event projection method that leverages a\ncontinuous temporal projection kernel and decouples spatial observations to\nencode fine-grained temporal evolution explicitly. For motion representation,\nwe introduce a density-aware adaptation mechanism to fuse spatial and temporal\nfeatures under event guidance, coupled with a non-uniform rational curve\nparameterization framework to adaptively model heterogeneous trajectories. The\nfinal 3D motion estimation is achieved through multi-temporal sampling of\nparametric trajectories, yielding optical flow and depth motion fields. To\nfacilitate evaluation, we introduce CarlaEvent3D, a multi-dynamic synthetic\ndataset for comprehensive validation. Extensive experiments on both this\ndataset and a real-world benchmark demonstrate the effectiveness of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual 3D motion estimation aims to infer the motion of 2D pixels in 3D space\nbased on visual cues. The key challenge arises from depth variation induced\nspatio-temporal motion inconsistencies, disrupting the assumptions of local\nspatial or temporal motion smoothness in previous motion estimation frameworks.\nIn contrast, event cameras offer new possibilities for 3D motion estimation\nthrough continuous adaptive pixel-level responses to scene changes. This paper\npresents EMoTive, a novel event-based framework that models spatio-temporal\ntrajectories via event-guided non-uniform parametric curves, effectively\ncharacterizing locally heterogeneous spatio-temporal motion. Specifically, we\nfirst introduce Event Kymograph - an event projection method that leverages a\ncontinuous temporal projection kernel and decouples spatial observations to\nencode fine-grained temporal evolution explicitly. For motion representation,\nwe introduce a density-aware adaptation mechanism to fuse spatial and temporal\nfeatures under event guidance, coupled with a non-uniform rational curve\nparameterization framework to adaptively model heterogeneous trajectories. The\nfinal 3D motion estimation is achieved through multi-temporal sampling of\nparametric trajectories, yielding optical flow and depth motion fields. To\nfacilitate evaluation, we introduce CarlaEvent3D, a multi-dynamic synthetic\ndataset for comprehensive validation. Extensive experiments on both this\ndataset and a real-world benchmark demonstrate the effectiveness of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Zengyu Wan"
                    },
                    {
                        "name": "Wei Zhai"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Zhengjun Zha"
                    }
                ],
                "author_detail": {
                    "name": "Zhengjun Zha"
                },
                "author": "Zhengjun Zha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12486v2",
                "updated": "2025-03-14T13:13:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    13,
                    13,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-18T03:15:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    15,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20941v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20941v3",
                "updated": "2025-03-14T13:12:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    12,
                    38,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-28T11:49:58Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    49,
                    58,
                    0,
                    302,
                    0
                ],
                "title": "Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine\n  Translation"
                },
                "summary": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and the outputs from GPT4-as-a-judge are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and the outputs from GPT4-as-a-judge are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT"
                },
                "authors": [
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "Accepted at NAACL 2025 Student Research Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20941v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20941v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10637v2",
                "updated": "2025-03-14T13:11:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    11,
                    59,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-13T17:59:56Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    59,
                    56,
                    3,
                    72,
                    0
                ],
                "title": "Distilling Diversity and Control in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Diversity and Control in Diffusion Models"
                },
                "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info"
                },
                "authors": [
                    {
                        "name": "Rohit Gandikota"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Project Page: https://distillation.baulab.info",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11367v1",
                "updated": "2025-03-14T13:07:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:07:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "title": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware"
                },
                "summary": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput."
                },
                "authors": [
                    {
                        "name": "Insu Jang"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Nikhil Bansal"
                    },
                    {
                        "name": "Ang Chen"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07263v2",
                "updated": "2025-03-14T13:03:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    3,
                    24,
                    4,
                    73,
                    0
                ],
                "published": "2023-10-11T07:39:42Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    39,
                    42,
                    2,
                    284,
                    0
                ],
                "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPAL: Corrective Planning of Robot Actions with Large Language Models"
                },
                "summary": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation."
                },
                "authors": [
                    {
                        "name": "Frank Joublin"
                    },
                    {
                        "name": "Antonello Ceravola"
                    },
                    {
                        "name": "Pavel Smirnov"
                    },
                    {
                        "name": "Felix Ocker"
                    },
                    {
                        "name": "Joerg Deigmoeller"
                    },
                    {
                        "name": "Anna Belardinelli"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Stephan Hasler"
                    },
                    {
                        "name": "Daniel Tanneberg"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "arxiv_comment": "IEEE International Conference on Robotics and Automation (ICRA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17236v2",
                "updated": "2025-03-14T12:43:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    43,
                    27,
                    4,
                    73,
                    0
                ],
                "published": "2025-01-28T19:00:30Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    19,
                    0,
                    30,
                    1,
                    28,
                    0
                ],
                "title": "Joint inference of the Milky Way star formation history and IMF from\n  Gaia all-sky $G < 13$ data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint inference of the Milky Way star formation history and IMF from\n  Gaia all-sky $G < 13$ data"
                },
                "summary": "Despite the fundamental importance of the Milky Way's star formation history\n(SFH) and initial mass function (IMF), their consistent derivation remains\nelusive. We aim to simultaneously infer the IMF and the SFH of the Galactic\ndisc comparing Gaia data with the mock catalog resulting from the Besan\\c{c}on\npopulation synthesis model (BGM). Our goal is also to estimate the impact of\nthe systematics present in current stellar evolutionary models (SEMs) on this\ninference. We use a new implementation of the BGM Fast Approximate Simulations\n(BGM FASt) framework to fit the seven million star Gaia DR3 all-sky $G<13$\ncolor-magnitude diagram (CMD) to the most updated dynamically self-consistent\nBGM. Our derived SFH supports an abrupt decrease of the star formation\napproximately 1-1.5 Gyr ago followed by a significant enhancement with a wide\nplateau in the range 2-6 Gyr ago. A remarkable hiatus appears around 5-7 Gyr\nago with a $\\sim$1 Gyr shift depending on the set of stellar models. A complex\nevolution at ages older than 8 Gyr deserves further investigation. Precise but\ndiscrepant values using different SEMs are found for the power-law indices of\nthe IMF. In our fiducial execution with PARSEC SEM, the slope takes a value of\n$\\alpha_2 = 1.45^{+0.19}_{-0.12}$ for the range [0.5-1.53]$M_\\odot$, while for\nmasses larger than 1.53 $M_\\odot$ we obtain $\\alpha_3 = 1.98^{+0.13}_{-0.05}$.\nUsing STAREVOL SEM, the inferred values are $\\alpha_2 = 2.48^{+0.09}_{-0.11}$\nand $\\alpha_3 = 1.64^{+0.15}_{-0.02}$. We find the solution with PARSEC to have\na significantly higher likelihood than that obtained with STAREVOL. The BGM\nFASt framework is now ready to address executions fitting all-sky Gaia data up\nto 14-17 apparent limiting magnitude. This will naturally allow us to derive\nboth a reliable SFH for the early epochs of the Galactic disc evolution and a\nprecise slope for the IMF at low masses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the fundamental importance of the Milky Way's star formation history\n(SFH) and initial mass function (IMF), their consistent derivation remains\nelusive. We aim to simultaneously infer the IMF and the SFH of the Galactic\ndisc comparing Gaia data with the mock catalog resulting from the Besan\\c{c}on\npopulation synthesis model (BGM). Our goal is also to estimate the impact of\nthe systematics present in current stellar evolutionary models (SEMs) on this\ninference. We use a new implementation of the BGM Fast Approximate Simulations\n(BGM FASt) framework to fit the seven million star Gaia DR3 all-sky $G<13$\ncolor-magnitude diagram (CMD) to the most updated dynamically self-consistent\nBGM. Our derived SFH supports an abrupt decrease of the star formation\napproximately 1-1.5 Gyr ago followed by a significant enhancement with a wide\nplateau in the range 2-6 Gyr ago. A remarkable hiatus appears around 5-7 Gyr\nago with a $\\sim$1 Gyr shift depending on the set of stellar models. A complex\nevolution at ages older than 8 Gyr deserves further investigation. Precise but\ndiscrepant values using different SEMs are found for the power-law indices of\nthe IMF. In our fiducial execution with PARSEC SEM, the slope takes a value of\n$\\alpha_2 = 1.45^{+0.19}_{-0.12}$ for the range [0.5-1.53]$M_\\odot$, while for\nmasses larger than 1.53 $M_\\odot$ we obtain $\\alpha_3 = 1.98^{+0.13}_{-0.05}$.\nUsing STAREVOL SEM, the inferred values are $\\alpha_2 = 2.48^{+0.09}_{-0.11}$\nand $\\alpha_3 = 1.64^{+0.15}_{-0.02}$. We find the solution with PARSEC to have\na significantly higher likelihood than that obtained with STAREVOL. The BGM\nFASt framework is now ready to address executions fitting all-sky Gaia data up\nto 14-17 apparent limiting magnitude. This will naturally allow us to derive\nboth a reliable SFH for the early epochs of the Galactic disc evolution and a\nprecise slope for the IMF at low masses."
                },
                "authors": [
                    {
                        "name": "Marc del Alczar-Juli"
                    },
                    {
                        "name": "Francesca Figueras"
                    },
                    {
                        "name": "Annie C. Robin"
                    },
                    {
                        "name": "Olivier Bienaym"
                    },
                    {
                        "name": "Friedrich Anders"
                    }
                ],
                "author_detail": {
                    "name": "Friedrich Anders"
                },
                "author": "Friedrich Anders",
                "arxiv_comment": "22 pages, 14 figures; accepted for publication in Astronomy &\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11348v1",
                "updated": "2025-03-14T12:32:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    32,
                    40,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:32:40Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    32,
                    40,
                    4,
                    73,
                    0
                ],
                "title": "RESPONSE: Benchmarking the Ability of Language Models to Undertake\n  Commonsense Reasoning in Crisis Situation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RESPONSE: Benchmarking the Ability of Language Models to Undertake\n  Commonsense Reasoning in Crisis Situation"
                },
                "summary": "An interesting class of commonsense reasoning problems arises when people are\nfaced with natural disasters. To investigate this topic, we present\n\\textsf{RESPONSE}, a human-curated dataset containing 1789 annotated instances\nfeaturing 6037 sets of questions designed to assess LLMs' commonsense reasoning\nin disaster situations across different time frames. The dataset includes\nproblem descriptions, missing resources, time-sensitive solutions, and their\njustifications, with a subset validated by environmental engineers. Through\nboth automatic metrics and human evaluation, we compare LLM-generated\nrecommendations against human responses. Our findings show that even\nstate-of-the-art models like GPT-4 achieve only 37\\% human-evaluated\ncorrectness for immediate response actions, highlighting significant room for\nimprovement in LLMs' ability for commonsense reasoning in crises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An interesting class of commonsense reasoning problems arises when people are\nfaced with natural disasters. To investigate this topic, we present\n\\textsf{RESPONSE}, a human-curated dataset containing 1789 annotated instances\nfeaturing 6037 sets of questions designed to assess LLMs' commonsense reasoning\nin disaster situations across different time frames. The dataset includes\nproblem descriptions, missing resources, time-sensitive solutions, and their\njustifications, with a subset validated by environmental engineers. Through\nboth automatic metrics and human evaluation, we compare LLM-generated\nrecommendations against human responses. Our findings show that even\nstate-of-the-art models like GPT-4 achieve only 37\\% human-evaluated\ncorrectness for immediate response actions, highlighting significant room for\nimprovement in LLMs' ability for commonsense reasoning in crises."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03551v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03551v4",
                "updated": "2025-03-14T12:30:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    30,
                    28,
                    4,
                    73,
                    0
                ],
                "published": "2024-03-06T08:51:09Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    8,
                    51,
                    9,
                    2,
                    66,
                    0
                ],
                "title": "Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers"
                },
                "summary": "Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge."
                },
                "authors": [
                    {
                        "name": "Tim Selig"
                    },
                    {
                        "name": "Thomas Mrz"
                    },
                    {
                        "name": "Martin Storath"
                    },
                    {
                        "name": "Andreas Weinmann"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Weinmann"
                },
                "author": "Andreas Weinmann",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03551v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03551v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16290v2",
                "updated": "2025-03-14T12:27:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    27,
                    20,
                    4,
                    73,
                    0
                ],
                "published": "2024-12-20T19:00:12Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    19,
                    0,
                    12,
                    4,
                    355,
                    0
                ],
                "title": "A Probabilistic Model to Estimate Number Densities from Column Densities\n  in Molecular Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Model to Estimate Number Densities from Column Densities\n  in Molecular Clouds"
                },
                "summary": "Constraining the physical and chemical evolution of molecular clouds is\nessential to our understanding of star formation. These investigations often\nnecessitate knowledge of some local representative number density of the gas\nalong the line of sight. However, constraining the number density is a\ndifficult endeavor. Robust constraints of the number density often require line\nobservations of specific molecules along with radiation transfer modeling,\nwhich provides densities traced by that specific molecule. Column density maps\nof molecular clouds are more readily available, with many high-fidelity maps\ncalculated from dust emission and extinction, in particular from surveys\nconducted with the Herschel Space Observatory. We introduce a new probabilistic\nmodel which is based on the assumption that the total hydrogen nuclei column\ndensity along a line of sight can be decomposed into a turbulent component and\na gravitationally-dominated component. Therefore, for each pixel in a column\ndensity map, the line of sight is decomposed into characteristic diffuse\n(dubbed ``turbulent'') and dense (dubbed ``gravitational'') gas number\ndensities from column density maps. The method thus exploits a physical model\nof turbulence to decouple the random turbulent column from gas in dense bound\nstructures empirically using the observed column density maps. We find the\nmodel produces reasonable turbulent and gravitational densities in the Taurus\nL1495/B213 and Polaris Flare clouds. The model can also be used to infer an\neffective attenuating column density into the cloud, which is useful for\nastrochemical models of the clouds. We conclude by demonstrating an application\nof this method by predicting the emission of the [C II], [C I], and CO (J =\n1-0) lines across the Taurus L1495/B213 region at the native resolution of the\ncolumn density map utilizing a grid of photodissociation-region models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the physical and chemical evolution of molecular clouds is\nessential to our understanding of star formation. These investigations often\nnecessitate knowledge of some local representative number density of the gas\nalong the line of sight. However, constraining the number density is a\ndifficult endeavor. Robust constraints of the number density often require line\nobservations of specific molecules along with radiation transfer modeling,\nwhich provides densities traced by that specific molecule. Column density maps\nof molecular clouds are more readily available, with many high-fidelity maps\ncalculated from dust emission and extinction, in particular from surveys\nconducted with the Herschel Space Observatory. We introduce a new probabilistic\nmodel which is based on the assumption that the total hydrogen nuclei column\ndensity along a line of sight can be decomposed into a turbulent component and\na gravitationally-dominated component. Therefore, for each pixel in a column\ndensity map, the line of sight is decomposed into characteristic diffuse\n(dubbed ``turbulent'') and dense (dubbed ``gravitational'') gas number\ndensities from column density maps. The method thus exploits a physical model\nof turbulence to decouple the random turbulent column from gas in dense bound\nstructures empirically using the observed column density maps. We find the\nmodel produces reasonable turbulent and gravitational densities in the Taurus\nL1495/B213 and Polaris Flare clouds. The model can also be used to infer an\neffective attenuating column density into the cloud, which is useful for\nastrochemical models of the clouds. We conclude by demonstrating an application\nof this method by predicting the emission of the [C II], [C I], and CO (J =\n1-0) lines across the Taurus L1495/B213 region at the native resolution of the\ncolumn density map utilizing a grid of photodissociation-region models."
                },
                "authors": [
                    {
                        "name": "Brandt A. L. Gaches"
                    },
                    {
                        "name": "Michael Y. Grudi"
                    }
                ],
                "author_detail": {
                    "name": "Michael Y. Grudi"
                },
                "author": "Michael Y. Grudi",
                "arxiv_comment": "Accepted to A&A. Public GitHub with example lower resolution\n  calculation here: https://github.com/AstroBrandt/GMCDensityEstimation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11347v1",
                "updated": "2025-03-14T12:25:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    25,
                    27,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:25:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    25,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "Integrating Dynamical Systems Modeling with Spatiotemporal scRNA-seq\n  Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Dynamical Systems Modeling with Spatiotemporal scRNA-seq\n  Data Analysis"
                },
                "summary": "Understanding the dynamic nature of biological systems is fundamental to\ndeciphering cellular behavior, developmental processes, and disease\nprogression. Single-cell RNA sequencing (scRNA-seq) has provided static\nsnapshots of gene expression, offering valuable insights into cellular states\nat a single time point. Recent advancements in temporally resolved scRNA-seq,\nspatial transcriptomics (ST), and time-series spatial transcriptomics\n(temporal-ST) have further revolutionized our ability to study the\nspatiotemporal dynamics of individual cells. These technologies, when combined\nwith computational frameworks such as Markov chains, stochastic differential\nequations (SDEs), and generative models like optimal transport and\nSchr\\\"odinger bridges, enable the reconstruction of dynamic cellular\ntrajectories and cell fate decisions. This review discusses how these dynamical\nsystem approaches offer new opportunities to model and infer cellular dynamics\nfrom a systematic perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamic nature of biological systems is fundamental to\ndeciphering cellular behavior, developmental processes, and disease\nprogression. Single-cell RNA sequencing (scRNA-seq) has provided static\nsnapshots of gene expression, offering valuable insights into cellular states\nat a single time point. Recent advancements in temporally resolved scRNA-seq,\nspatial transcriptomics (ST), and time-series spatial transcriptomics\n(temporal-ST) have further revolutionized our ability to study the\nspatiotemporal dynamics of individual cells. These technologies, when combined\nwith computational frameworks such as Markov chains, stochastic differential\nequations (SDEs), and generative models like optimal transport and\nSchr\\\"odinger bridges, enable the reconstruction of dynamic cellular\ntrajectories and cell fate decisions. This review discusses how these dynamical\nsystem approaches offer new opportunities to model and infer cellular dynamics\nfrom a systematic perspective."
                },
                "authors": [
                    {
                        "name": "Zhenyi Zhang"
                    },
                    {
                        "name": "Yuhao Sun"
                    },
                    {
                        "name": "Qiangwei Peng"
                    },
                    {
                        "name": "Tiejun Li"
                    },
                    {
                        "name": "Peijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peijie Zhou"
                },
                "author": "Peijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11346v1",
                "updated": "2025-03-14T12:23:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    23,
                    45,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:23:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    23,
                    45,
                    4,
                    73,
                    0
                ],
                "title": "AIstorian lets AI be a historian: A KG-powered multi-agent system for\n  accurate biography generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIstorian lets AI be a historian: A KG-powered multi-agent system for\n  accurate biography generation"
                },
                "summary": "Huawei has always been committed to exploring the AI application in\nhistorical research. Biography generation, as a specialized form of abstractive\nsummarization, plays a crucial role in historical research but faces unique\nchallenges that existing large language models (LLMs) struggle to address.\nThese challenges include maintaining stylistic adherence to historical writing\nconventions, ensuring factual fidelity, and handling fragmented information\nacross multiple documents. We present AIstorian, a novel end-to-end agentic\nsystem featured with a knowledge graph (KG)-powered retrieval-augmented\ngeneration (RAG) and anti-hallucination multi-agents. Specifically, AIstorian\nintroduces an in-context learning based chunking strategy and a KG-based index\nfor accurate and efficient reference retrieval. Meanwhile, AIstorian\norchestrates multi-agents to conduct on-the-fly hallucination detection and\nerror-type-aware correction. Additionally, to teach LLMs a certain language\nstyle, we finetune LLMs based on a two-step training approach combining data\naugmentation-enhanced supervised fine-tuning with stylistic preference\noptimization. Extensive experiments on a real-life historical Jinshi dataset\ndemonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and\na 47.6% reduction in hallucination rate compared to existing baselines. The\ndata and code are available at: https://github.com/ZJU-DAILY/AIstorian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Huawei has always been committed to exploring the AI application in\nhistorical research. Biography generation, as a specialized form of abstractive\nsummarization, plays a crucial role in historical research but faces unique\nchallenges that existing large language models (LLMs) struggle to address.\nThese challenges include maintaining stylistic adherence to historical writing\nconventions, ensuring factual fidelity, and handling fragmented information\nacross multiple documents. We present AIstorian, a novel end-to-end agentic\nsystem featured with a knowledge graph (KG)-powered retrieval-augmented\ngeneration (RAG) and anti-hallucination multi-agents. Specifically, AIstorian\nintroduces an in-context learning based chunking strategy and a KG-based index\nfor accurate and efficient reference retrieval. Meanwhile, AIstorian\norchestrates multi-agents to conduct on-the-fly hallucination detection and\nerror-type-aware correction. Additionally, to teach LLMs a certain language\nstyle, we finetune LLMs based on a two-step training approach combining data\naugmentation-enhanced supervised fine-tuning with stylistic preference\noptimization. Extensive experiments on a real-life historical Jinshi dataset\ndemonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and\na 47.6% reduction in hallucination rate compared to existing baselines. The\ndata and code are available at: https://github.com/ZJU-DAILY/AIstorian."
                },
                "authors": [
                    {
                        "name": "Fengyu Li"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Junhao Zhu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Yanfei Zhang"
                    },
                    {
                        "name": "Jia Zhou"
                    },
                    {
                        "name": "Hui Zu"
                    },
                    {
                        "name": "Jingwen Zhao"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "arxiv_affiliation": "Zhejiang University",
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15277v2",
                "updated": "2025-03-14T12:22:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    22,
                    49,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-22T15:21:38Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    21,
                    38,
                    4,
                    327,
                    0
                ],
                "title": "Foundation Cures Personalization: Improving Personalized Models' Prompt\n  Consistency via Hidden Foundation Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Cures Personalization: Improving Personalized Models' Prompt\n  Consistency via Hidden Foundation Knowledge"
                },
                "summary": "Facial personalization faces challenges to maintain identity fidelity without\ndisrupting the foundation model's prompt consistency. The mainstream\npersonalization models employ identity embedding to integrate identity\ninformation within the cross-attention mechanisms of UNet. However, our\npreliminary experimental findings reveal that identity embeddings compromise\nthe effectiveness of other tokens in the prompt, thereby limiting high prompt\nconsistency and controllability. Moreover, by deactivating identity embedding,\npersonalization models still demonstrate the underlying foundation models'\nability to control facial attributes precisely. It suggests that such\nfoundation models' knowledge can be leveraged to \\textbf{cure} the ill-aligned\nprompt consistency of personalization models. Building upon these insights, we\npropose \\textbf{FreeCure}, a framework that improves the prompt consistency of\npersonalization models with their latent foundation models' knowledge. First,\nby setting a dual inference paradigm with/without identity embedding, we\nidentify attributes (\\textit{e.g.}, hair, accessories, etc.) for enhancements.\nSecond, we introduce a novel foundation-aware self-attention module, coupled\nwith an inversion-based process to bring well-aligned attribute information to\nthe personalization process. Our approach is \\textbf{training-free}, and can\neffectively enhance a wide array of facial attributes in a non-intrusive\nmanner; and it can be seamlessly integrated into existing popular\npersonalization models, without harming their well-trained modules. FreeCure\nhas demonstrated significant improvements in prompt consistency across a\ndiverse set of state-of-the-art facial personalization models while maintaining\nthe integrity of original identity fidelity. The project page is available\n\\href{https://github.com/YIYANGCAI/freecure-project-page}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial personalization faces challenges to maintain identity fidelity without\ndisrupting the foundation model's prompt consistency. The mainstream\npersonalization models employ identity embedding to integrate identity\ninformation within the cross-attention mechanisms of UNet. However, our\npreliminary experimental findings reveal that identity embeddings compromise\nthe effectiveness of other tokens in the prompt, thereby limiting high prompt\nconsistency and controllability. Moreover, by deactivating identity embedding,\npersonalization models still demonstrate the underlying foundation models'\nability to control facial attributes precisely. It suggests that such\nfoundation models' knowledge can be leveraged to \\textbf{cure} the ill-aligned\nprompt consistency of personalization models. Building upon these insights, we\npropose \\textbf{FreeCure}, a framework that improves the prompt consistency of\npersonalization models with their latent foundation models' knowledge. First,\nby setting a dual inference paradigm with/without identity embedding, we\nidentify attributes (\\textit{e.g.}, hair, accessories, etc.) for enhancements.\nSecond, we introduce a novel foundation-aware self-attention module, coupled\nwith an inversion-based process to bring well-aligned attribute information to\nthe personalization process. Our approach is \\textbf{training-free}, and can\neffectively enhance a wide array of facial attributes in a non-intrusive\nmanner; and it can be seamlessly integrated into existing popular\npersonalization models, without harming their well-trained modules. FreeCure\nhas demonstrated significant improvements in prompt consistency across a\ndiverse set of state-of-the-art facial personalization models while maintaining\nthe integrity of original identity fidelity. The project page is available\n\\href{https://github.com/YIYANGCAI/freecure-project-page}{here}."
                },
                "authors": [
                    {
                        "name": "Yiyang Cai"
                    },
                    {
                        "name": "Zhengkai Jiang"
                    },
                    {
                        "name": "Yulong Liu"
                    },
                    {
                        "name": "Chunyang Jiang"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Wenhan Luo"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11343v1",
                "updated": "2025-03-14T12:18:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    18,
                    33,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:18:33Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    18,
                    33,
                    4,
                    73,
                    0
                ],
                "title": "FG-DFPN: Flow Guided Deformable Frame Prediction Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FG-DFPN: Flow Guided Deformable Frame Prediction Network"
                },
                "summary": "Video frame prediction remains a fundamental challenge in computer vision\nwith direct implications for autonomous systems, video compression, and media\nsynthesis. We present FG-DFPN, a novel architecture that harnesses the synergy\nbetween optical flow estimation and deformable convolutions to model complex\nspatio-temporal dynamics. By guiding deformable sampling with motion cues, our\napproach addresses the limitations of fixed-kernel networks when handling\ndiverse motion patterns. The multi-scale design enables FG-DFPN to\nsimultaneously capture global scene transformations and local object movements\nwith remarkable precision. Our experiments demonstrate that FG-DFPN achieves\nstate-of-the-art performance on eight diverse MPEG test sequences,\noutperforming existing methods by 1dB PSNR while maintaining competitive\ninference speeds. The integration of motion cues with adaptive geometric\ntransformations makes FG-DFPN a promising solution for next-generation video\nprocessing systems that require high-fidelity temporal predictions. The model\nand instructions to reproduce our results will be released at:\nhttps://github.com/KUIS-AI-Tekalp-Research Group/frame-prediction",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video frame prediction remains a fundamental challenge in computer vision\nwith direct implications for autonomous systems, video compression, and media\nsynthesis. We present FG-DFPN, a novel architecture that harnesses the synergy\nbetween optical flow estimation and deformable convolutions to model complex\nspatio-temporal dynamics. By guiding deformable sampling with motion cues, our\napproach addresses the limitations of fixed-kernel networks when handling\ndiverse motion patterns. The multi-scale design enables FG-DFPN to\nsimultaneously capture global scene transformations and local object movements\nwith remarkable precision. Our experiments demonstrate that FG-DFPN achieves\nstate-of-the-art performance on eight diverse MPEG test sequences,\noutperforming existing methods by 1dB PSNR while maintaining competitive\ninference speeds. The integration of motion cues with adaptive geometric\ntransformations makes FG-DFPN a promising solution for next-generation video\nprocessing systems that require high-fidelity temporal predictions. The model\nand instructions to reproduce our results will be released at:\nhttps://github.com/KUIS-AI-Tekalp-Research Group/frame-prediction"
                },
                "authors": [
                    {
                        "name": "M. Akn Ylmaz"
                    },
                    {
                        "name": "Ahmet Bilican"
                    },
                    {
                        "name": "A. Murat Tekalp"
                    }
                ],
                "author_detail": {
                    "name": "A. Murat Tekalp"
                },
                "author": "A. Murat Tekalp",
                "arxiv_comment": "Submitted to 33th European Signal Processing Conference (EUSIPCO)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2204.12733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2204.12733v2",
                "updated": "2025-03-14T12:16:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    16,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2022-04-27T06:59:16Z",
                "published_parsed": [
                    2022,
                    4,
                    27,
                    6,
                    59,
                    16,
                    2,
                    117,
                    0
                ],
                "title": "Modeling complex measurement error in microbiome experiments to estimate\n  relative abundances and detection effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling complex measurement error in microbiome experiments to estimate\n  relative abundances and detection effects"
                },
                "summary": "Accurate estimates of microbial species abundances are needed to advance our\nunderstanding of the role that microbiomes play in human and environmental\nhealth. However, artificially constructed microbiomes demonstrate that\nintuitive estimators of microbial relative abundances are biased. To address\nthis, we propose a semiparametric method to estimate relative abundances,\nspecies detection effects, and/or cross-sample contamination in microbiome\nexperiments. We show that certain experimental designs result in identifiable\nmodel parameters, and we present consistent estimators and asymptotically valid\ninference procedures. Notably, our procedure can estimate relative abundances\non the boundary of the simplex. We demonstrate the utility of the method for\ncomparing experimental protocols, removing cross-sample contamination, and\nestimating species' detectability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate estimates of microbial species abundances are needed to advance our\nunderstanding of the role that microbiomes play in human and environmental\nhealth. However, artificially constructed microbiomes demonstrate that\nintuitive estimators of microbial relative abundances are biased. To address\nthis, we propose a semiparametric method to estimate relative abundances,\nspecies detection effects, and/or cross-sample contamination in microbiome\nexperiments. We show that certain experimental designs result in identifiable\nmodel parameters, and we present consistent estimators and asymptotically valid\ninference procedures. Notably, our procedure can estimate relative abundances\non the boundary of the simplex. We demonstrate the utility of the method for\ncomparing experimental protocols, removing cross-sample contamination, and\nestimating species' detectability."
                },
                "authors": [
                    {
                        "name": "David S Clausen"
                    },
                    {
                        "name": "Amy D Willis"
                    }
                ],
                "author_detail": {
                    "name": "Amy D Willis"
                },
                "author": "Amy D Willis",
                "arxiv_comment": "v2 includes detailed identifiability results, a complete proof of\n  weak convergence, additional simulation results, and clarified exposition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2204.12733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2204.12733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11339v1",
                "updated": "2025-03-14T12:09:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    9,
                    58,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:09:58Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    9,
                    58,
                    4,
                    73,
                    0
                ],
                "title": "Contextual Similarity Distillation: Ensemble Uncertainties with a Single\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Similarity Distillation: Ensemble Uncertainties with a Single\n  Model"
                },
                "summary": "Uncertainty quantification is a critical aspect of reinforcement learning and\ndeep learning, with numerous applications ranging from efficient exploration\nand stable offline reinforcement learning to outlier detection in medical\ndiagnostics. The scale of modern neural networks, however, complicates the use\nof many theoretically well-motivated approaches such as full Bayesian\ninference. Approximate methods like deep ensembles can provide reliable\nuncertainty estimates but still remain computationally expensive. In this work,\nwe propose contextual similarity distillation, a novel approach that explicitly\nestimates the variance of an ensemble of deep neural networks with a single\nmodel, without ever learning or evaluating such an ensemble in the first place.\nOur method builds on the predictable learning dynamics of wide neural networks,\ngoverned by the neural tangent kernel, to derive an efficient approximation of\nthe predictive variance of an infinite ensemble. Specifically, we reinterpret\nthe computation of ensemble variance as a supervised regression problem with\nkernel similarities as regression targets. The resulting model can estimate\npredictive variance at inference time with a single forward pass, and can make\nuse of unlabeled target-domain data or data augmentations to refine its\nuncertainty estimates. We empirically validate our method across a variety of\nout-of-distribution detection benchmarks and sparse-reward reinforcement\nlearning environments. We find that our single-model method performs\ncompetitively and sometimes superior to ensemble-based baselines and serves as\na reliable signal for efficient exploration. These results, we believe,\nposition contextual similarity distillation as a principled and scalable\nalternative for uncertainty quantification in reinforcement learning and\ngeneral deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification is a critical aspect of reinforcement learning and\ndeep learning, with numerous applications ranging from efficient exploration\nand stable offline reinforcement learning to outlier detection in medical\ndiagnostics. The scale of modern neural networks, however, complicates the use\nof many theoretically well-motivated approaches such as full Bayesian\ninference. Approximate methods like deep ensembles can provide reliable\nuncertainty estimates but still remain computationally expensive. In this work,\nwe propose contextual similarity distillation, a novel approach that explicitly\nestimates the variance of an ensemble of deep neural networks with a single\nmodel, without ever learning or evaluating such an ensemble in the first place.\nOur method builds on the predictable learning dynamics of wide neural networks,\ngoverned by the neural tangent kernel, to derive an efficient approximation of\nthe predictive variance of an infinite ensemble. Specifically, we reinterpret\nthe computation of ensemble variance as a supervised regression problem with\nkernel similarities as regression targets. The resulting model can estimate\npredictive variance at inference time with a single forward pass, and can make\nuse of unlabeled target-domain data or data augmentations to refine its\nuncertainty estimates. We empirically validate our method across a variety of\nout-of-distribution detection benchmarks and sparse-reward reinforcement\nlearning environments. We find that our single-model method performs\ncompetitively and sometimes superior to ensemble-based baselines and serves as\na reliable signal for efficient exploration. These results, we believe,\nposition contextual similarity distillation as a principled and scalable\nalternative for uncertainty quantification in reinforcement learning and\ngeneral deep learning."
                },
                "authors": [
                    {
                        "name": "Moritz A. Zanger"
                    },
                    {
                        "name": "Pascal R. Van der Vaart"
                    },
                    {
                        "name": "Wendelin Bhmer"
                    },
                    {
                        "name": "Matthijs T. J. Spaan"
                    }
                ],
                "author_detail": {
                    "name": "Matthijs T. J. Spaan"
                },
                "author": "Matthijs T. J. Spaan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10351v2",
                "updated": "2025-03-14T12:09:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    9,
                    34,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-13T13:27:53Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    27,
                    53,
                    3,
                    72,
                    0
                ],
                "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Trends for Modern Machine Translation with Large Reasoning Models"
                },
                "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it."
                },
                "authors": [
                    {
                        "name": "Sinuo Liu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "Zifu Shang"
                    }
                ],
                "author_detail": {
                    "name": "Zifu Shang"
                },
                "author": "Zifu Shang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:1701.04715 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11336v1",
                "updated": "2025-03-14T12:05:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    5,
                    6,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:05:06Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    5,
                    6,
                    4,
                    73,
                    0
                ],
                "title": "Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in\n  Large Language Models"
                },
                "summary": "In this paper, we introduce Rule-Guided Feedback (RGF), a framework designed\nto enhance Large Language Model (LLM) performance through structured rule\nadherence and strategic information seeking. RGF implements a teacher-student\nparadigm where rule-following is forced through established guidelines. Our\nframework employs a Teacher model that rigorously evaluates each student output\nagainst task-specific rules, providing constructive guidance rather than direct\nanswers when detecting deviations. This iterative feedback loop serves two\ncrucial purposes: maintaining solutions within defined constraints and\nencouraging proactive information seeking to resolve uncertainties. We evaluate\nRGF on diverse tasks including Checkmate-in-One puzzles, Sonnet Writing,\nPenguins-In-a-Table classification, GSM8k, and StrategyQA. Our findings suggest\nthat structured feedback mechanisms can significantly enhance LLMs' performance\nacross various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Rule-Guided Feedback (RGF), a framework designed\nto enhance Large Language Model (LLM) performance through structured rule\nadherence and strategic information seeking. RGF implements a teacher-student\nparadigm where rule-following is forced through established guidelines. Our\nframework employs a Teacher model that rigorously evaluates each student output\nagainst task-specific rules, providing constructive guidance rather than direct\nanswers when detecting deviations. This iterative feedback loop serves two\ncrucial purposes: maintaining solutions within defined constraints and\nencouraging proactive information seeking to resolve uncertainties. We evaluate\nRGF on diverse tasks including Checkmate-in-One puzzles, Sonnet Writing,\nPenguins-In-a-Table classification, GSM8k, and StrategyQA. Our findings suggest\nthat structured feedback mechanisms can significantly enhance LLMs' performance\nacross various domains."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11335v1",
                "updated": "2025-03-14T12:03:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    3,
                    29,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:03:29Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    3,
                    29,
                    4,
                    73,
                    0
                ],
                "title": "APLA: A Simple Adaptation Method for Vision Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APLA: A Simple Adaptation Method for Vision Transformers"
                },
                "summary": "Existing adaptation techniques typically require architectural modifications\nor added parameters, leading to high computational costs and complexity. We\nintroduce Attention Projection Layer Adaptation (APLA), a simple approach to\nadapt vision transformers (ViTs) without altering the architecture or adding\nparameters. Through a systematic analysis, we find that the layer immediately\nafter the attention mechanism is crucial for adaptation. By updating only this\nprojection layer, or even just a random subset of this layer's weights, APLA\nachieves state-of-the-art performance while reducing GPU memory usage by up to\n52.63% and training time by up to 43.0%, with no extra cost at inference.\nAcross 46 datasets covering a variety of tasks including scene classification,\nmedical imaging, satellite imaging, and fine-grained classification, APLA\nconsistently outperforms 17 other leading adaptation methods, including full\nfine-tuning, on classification, segmentation, and detection tasks. The code is\navailable at https://github.com/MoeinSorkhei/APLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing adaptation techniques typically require architectural modifications\nor added parameters, leading to high computational costs and complexity. We\nintroduce Attention Projection Layer Adaptation (APLA), a simple approach to\nadapt vision transformers (ViTs) without altering the architecture or adding\nparameters. Through a systematic analysis, we find that the layer immediately\nafter the attention mechanism is crucial for adaptation. By updating only this\nprojection layer, or even just a random subset of this layer's weights, APLA\nachieves state-of-the-art performance while reducing GPU memory usage by up to\n52.63% and training time by up to 43.0%, with no extra cost at inference.\nAcross 46 datasets covering a variety of tasks including scene classification,\nmedical imaging, satellite imaging, and fine-grained classification, APLA\nconsistently outperforms 17 other leading adaptation methods, including full\nfine-tuning, on classification, segmentation, and detection tasks. The code is\navailable at https://github.com/MoeinSorkhei/APLA."
                },
                "authors": [
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Christos Matsoukas"
                    }
                ],
                "author_detail": {
                    "name": "Christos Matsoukas"
                },
                "author": "Christos Matsoukas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04284v3",
                "updated": "2025-03-14T11:52:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    52,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2024-08-08T07:43:17Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    7,
                    43,
                    17,
                    3,
                    221,
                    0
                ],
                "title": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection"
                },
                "summary": "The ease of access to large language models (LLMs) has enabled a widespread\nof machine-generated texts, and now it is often hard to tell whether a piece of\ntext was human-written or machine-generated. This raises concerns about\npotential misuse, particularly within educational and academic domains. Thus,\nit is important to develop practical systems that can automate the process.\nHere, we present one such system, LLM-DetectAIve, designed for fine-grained\ndetection. Unlike most previous work on machine-generated text detection, which\nfocused on binary classification, LLM-DetectAIve supports four categories: (i)\nhuman-written, (ii) machine-generated, (iii) machine-written, then\nmachine-humanized, and (iv) human-written, then machine-polished. Category\n(iii) aims to detect attempts to obfuscate the fact that a text was\nmachine-generated, while category (iv) looks for cases where the LLM was used\nto polish a human-written text, which is typically acceptable in academic\nwriting, but not in education. Our experiments show that LLM-DetectAIve can\neffectively identify the above four categories, which makes it a potentially\nuseful tool in education, academia, and other domains.\n  LLM-DetectAIve is publicly accessible at\nhttps://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system\nis available at https://youtu.be/E8eT_bE7k8c.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ease of access to large language models (LLMs) has enabled a widespread\nof machine-generated texts, and now it is often hard to tell whether a piece of\ntext was human-written or machine-generated. This raises concerns about\npotential misuse, particularly within educational and academic domains. Thus,\nit is important to develop practical systems that can automate the process.\nHere, we present one such system, LLM-DetectAIve, designed for fine-grained\ndetection. Unlike most previous work on machine-generated text detection, which\nfocused on binary classification, LLM-DetectAIve supports four categories: (i)\nhuman-written, (ii) machine-generated, (iii) machine-written, then\nmachine-humanized, and (iv) human-written, then machine-polished. Category\n(iii) aims to detect attempts to obfuscate the fact that a text was\nmachine-generated, while category (iv) looks for cases where the LLM was used\nto polish a human-written text, which is typically acceptable in academic\nwriting, but not in education. Our experiments show that LLM-DetectAIve can\neffectively identify the above four categories, which makes it a potentially\nuseful tool in education, academia, and other domains.\n  LLM-DetectAIve is publicly accessible at\nhttps://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system\nis available at https://youtu.be/E8eT_bE7k8c."
                },
                "authors": [
                    {
                        "name": "Mervat Abassy"
                    },
                    {
                        "name": "Kareem Elozeiri"
                    },
                    {
                        "name": "Alexander Aziz"
                    },
                    {
                        "name": "Minh Ngoc Ta"
                    },
                    {
                        "name": "Raj Vardhan Tomar"
                    },
                    {
                        "name": "Bimarsha Adhikari"
                    },
                    {
                        "name": "Saad El Dine Ahmed"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Osama Mohammed Afzal"
                    },
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Jiahui Geng"
                    },
                    {
                        "name": "Hasan Iqbal"
                    },
                    {
                        "name": "Zain Muhammad Mujahid"
                    },
                    {
                        "name": "Tarek Mahmoud"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05231v2",
                "updated": "2025-03-14T11:46:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    46,
                    43,
                    4,
                    73,
                    0
                ],
                "published": "2024-02-07T20:11:38Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    20,
                    11,
                    38,
                    2,
                    38,
                    0
                ],
                "title": "Estimating Fold Changes from Partially Observed Outcomes with\n  Applications in Microbial Metagenomics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Fold Changes from Partially Observed Outcomes with\n  Applications in Microbial Metagenomics"
                },
                "summary": "We consider the problem of estimating fold-changes in the expected value of a\nmultivariate outcome observed with unknown sample-specific and\ncategory-specific perturbations. This challenge arises in high-throughput\nsequencing studies of the abundance of microbial taxa because microbes are\nsystematically over- and under-detected relative to their true abundances. Our\nmodel admits a partially identifiable estimand, and we establish full\nidentifiability by imposing interpretable parameter constraints. To reduce bias\nand guarantee the existence of estimators in the presence of sparse\nobservations, we apply an asymptotically negligible and constraint-invariant\npenalty to our estimating function. We develop a fast coordinate descent\nalgorithm for estimation, and an augmented Lagrangian algorithm for estimation\nunder null hypotheses. We construct a model-robust score test and demonstrate\nvalid inference even for small sample sizes and violated distributional\nassumptions. The flexibility of the approach and comparisons to related methods\nare illustrated through a meta-analysis of microbial associations with\ncolorectal cancer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of estimating fold-changes in the expected value of a\nmultivariate outcome observed with unknown sample-specific and\ncategory-specific perturbations. This challenge arises in high-throughput\nsequencing studies of the abundance of microbial taxa because microbes are\nsystematically over- and under-detected relative to their true abundances. Our\nmodel admits a partially identifiable estimand, and we establish full\nidentifiability by imposing interpretable parameter constraints. To reduce bias\nand guarantee the existence of estimators in the presence of sparse\nobservations, we apply an asymptotically negligible and constraint-invariant\npenalty to our estimating function. We develop a fast coordinate descent\nalgorithm for estimation, and an augmented Lagrangian algorithm for estimation\nunder null hypotheses. We construct a model-robust score test and demonstrate\nvalid inference even for small sample sizes and violated distributional\nassumptions. The flexibility of the approach and comparisons to related methods\nare illustrated through a meta-analysis of microbial associations with\ncolorectal cancer."
                },
                "authors": [
                    {
                        "name": "David S Clausen"
                    },
                    {
                        "name": "Sarah Teichman"
                    },
                    {
                        "name": "Amy D Willis"
                    }
                ],
                "author_detail": {
                    "name": "Amy D Willis"
                },
                "author": "Amy D Willis",
                "arxiv_comment": "v2 includes clarified exposition, additional examples, expanded\n  simulation study, and supporting theory; Dr Teichman contributed\n  substantially to v2 and is now recognised as a coauthor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11315v1",
                "updated": "2025-03-14T11:31:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    31,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:31:30Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    31,
                    30,
                    4,
                    73,
                    0
                ],
                "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with\n  Minimal Multimodal Speech Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with\n  Minimal Multimodal Speech Tokens"
                },
                "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early av-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.74% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early av-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.74% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%."
                },
                "authors": [
                    {
                        "name": "Jeong Hun Yeo"
                    },
                    {
                        "name": "Hyeongseop Rha"
                    },
                    {
                        "name": "Se Jin Park"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "The code and models are available\n  https://github.com/JeongHun0716/MMS-LLaMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11314v1",
                "updated": "2025-03-14T11:30:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    30,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:30:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    30,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large\n  Language Models via Representation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large\n  Language Models via Representation Engineering"
                },
                "summary": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios."
                },
                "authors": [
                    {
                        "name": "Xinyu Tang"
                    },
                    {
                        "name": "Xiaolei Wang"
                    },
                    {
                        "name": "Zhihao Lv"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Zhang"
                },
                "author": "Zhiqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11302v1",
                "updated": "2025-03-14T11:11:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:11:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Are formal and functional linguistic mechanisms dissociated?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are formal and functional linguistic mechanisms dissociated?"
                },
                "summary": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist."
                },
                "authors": [
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "arxiv_comment": "35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11301v1",
                "updated": "2025-03-14T11:11:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    0,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:11:00Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    0,
                    4,
                    73,
                    0
                ],
                "title": "GNNs as Predictors of Agentic Workflow Performances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNNs as Predictors of Agentic Workflow Performances"
                },
                "summary": "Agentic workflows invoked by Large Language Models (LLMs) have achieved\nremarkable success in handling complex tasks. However, optimizing such\nworkflows is costly and inefficient in real-world applications due to extensive\ninvocations of LLMs. To fill this gap, this position paper formulates agentic\nworkflows as computational graphs and advocates Graph Neural Networks (GNNs) as\nefficient predictors of agentic workflow performances, avoiding repeated LLM\ninvocations for evaluation. To empirically ground this position, we construct\nFLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic\nworkflow performances. With extensive experiments, we arrive at the following\nconclusion: GNNs are simple yet effective predictors. This conclusion supports\nnew applications of GNNs and a novel direction towards automating agentic\nworkflow optimization. All codes, models, and data are available at\nhttps://github.com/youngsoul0731/Flora-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic workflows invoked by Large Language Models (LLMs) have achieved\nremarkable success in handling complex tasks. However, optimizing such\nworkflows is costly and inefficient in real-world applications due to extensive\ninvocations of LLMs. To fill this gap, this position paper formulates agentic\nworkflows as computational graphs and advocates Graph Neural Networks (GNNs) as\nefficient predictors of agentic workflow performances, avoiding repeated LLM\ninvocations for evaluation. To empirically ground this position, we construct\nFLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic\nworkflow performances. With extensive experiments, we arrive at the following\nconclusion: GNNs are simple yet effective predictors. This conclusion supports\nnew applications of GNNs and a novel direction towards automating agentic\nworkflow optimization. All codes, models, and data are available at\nhttps://github.com/youngsoul0731/Flora-Bench."
                },
                "authors": [
                    {
                        "name": "Yuanshuo Zhang"
                    },
                    {
                        "name": "Yuchen Hou"
                    },
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Xiaowen Dong"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v1",
                "updated": "2025-03-14T10:39:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06171v2",
                "updated": "2025-03-14T10:23:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    23,
                    6,
                    4,
                    73,
                    0
                ],
                "published": "2024-12-09T03:05:34Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    5,
                    34,
                    0,
                    344,
                    0
                ],
                "title": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any\n  Granularity"
                },
                "summary": "How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU."
                },
                "authors": [
                    {
                        "name": "Huaxin Zhang"
                    },
                    {
                        "name": "Xiaohao Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Jialong Zuo"
                    },
                    {
                        "name": "Xiaonan Huang"
                    },
                    {
                        "name": "Changxin Gao"
                    },
                    {
                        "name": "Shanjun Zhang"
                    },
                    {
                        "name": "Li Yu"
                    },
                    {
                        "name": "Nong Sang"
                    }
                ],
                "author_detail": {
                    "name": "Nong Sang"
                },
                "author": "Nong Sang",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11261v1",
                "updated": "2025-03-14T10:15:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    15,
                    53,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T10:15:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    15,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Neural network emulation of reionization to constrain new physics with\n  early- and late-time probes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network emulation of reionization to constrain new physics with\n  early- and late-time probes"
                },
                "summary": "The optical depth to reionization, a key parameter of the $\\Lambda$CDM model,\ncan be computed within astrophysical frameworks for star formation by modeling\nthe evolution of the intergalactic medium. Accurate evaluation of this\nparameter is thus crucial for joint statistical analyses of CMB data and\nlate-time probes such as the 21 cm power spectrum, requiring consistent\nintegration into cosmological solvers. However, modeling the optical depth with\nsufficient precision in a computationally feasible manner for MCMC analyses is\nchallenging due to the complexities of the nonlinear astrophysics. We introduce\nNNERO (Neural Network Emulator for Reionization and Optical depth), a framework\nthat leverages neural networks to emulate the evolution of the free-electron\nfraction during cosmic dawn and reionization. We demonstrate its effectiveness\nby simultaneously constraining cosmological and astrophysical parameters in\nboth standard cold dark matter and non-cold dark matter scenarios, including\nmodels with massive neutrinos and warm dark matter, showcasing its potential\nfor efficient and accurate parameter inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The optical depth to reionization, a key parameter of the $\\Lambda$CDM model,\ncan be computed within astrophysical frameworks for star formation by modeling\nthe evolution of the intergalactic medium. Accurate evaluation of this\nparameter is thus crucial for joint statistical analyses of CMB data and\nlate-time probes such as the 21 cm power spectrum, requiring consistent\nintegration into cosmological solvers. However, modeling the optical depth with\nsufficient precision in a computationally feasible manner for MCMC analyses is\nchallenging due to the complexities of the nonlinear astrophysics. We introduce\nNNERO (Neural Network Emulator for Reionization and Optical depth), a framework\nthat leverages neural networks to emulate the evolution of the free-electron\nfraction during cosmic dawn and reionization. We demonstrate its effectiveness\nby simultaneously constraining cosmological and astrophysical parameters in\nboth standard cold dark matter and non-cold dark matter scenarios, including\nmodels with massive neutrinos and warm dark matter, showcasing its potential\nfor efficient and accurate parameter inference."
                },
                "authors": [
                    {
                        "name": "Gatan Facchinetti"
                    }
                ],
                "author_detail": {
                    "name": "Gatan Facchinetti"
                },
                "author": "Gatan Facchinetti",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10686v3",
                "updated": "2025-03-14T10:13:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    13,
                    46,
                    4,
                    73,
                    0
                ],
                "published": "2024-02-16T13:41:18Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    13,
                    41,
                    18,
                    4,
                    47,
                    0
                ],
                "title": "On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks"
                },
                "summary": "In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the likelihood ratio attack (LiRA) within an\ninformation-theoretical framework that allows the investigation of the impact\nof the aleatoric uncertainty in the true data generation process, of the\nepistemic uncertainty caused by a limited training data set, and of the\ncalibration level of the target model. We compare three different settings, in\nwhich the attacker receives decreasingly informative feedback from the target\nmodel: confidence vector (CV) disclosure, in which the output probability\nvector is released; true label confidence (TLC) disclosure, in which only the\nprobability assigned to the true label is made available by the model; and\ndecision set (DS) disclosure, in which an adaptive prediction set is produced\nas in conformal prediction. We derive bounds on the advantage of an MIA\nadversary with the aim of offering insights into the impact of uncertainty and\ncalibration on the effectiveness of MIAs. Simulation results demonstrate that\nthe derived analytical bounds predict well the effectiveness of MIAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the likelihood ratio attack (LiRA) within an\ninformation-theoretical framework that allows the investigation of the impact\nof the aleatoric uncertainty in the true data generation process, of the\nepistemic uncertainty caused by a limited training data set, and of the\ncalibration level of the target model. We compare three different settings, in\nwhich the attacker receives decreasingly informative feedback from the target\nmodel: confidence vector (CV) disclosure, in which the output probability\nvector is released; true label confidence (TLC) disclosure, in which only the\nprobability assigned to the true label is made available by the model; and\ndecision set (DS) disclosure, in which an adaptive prediction set is produced\nas in conformal prediction. We derive bounds on the advantage of an MIA\nadversary with the aim of offering insights into the impact of uncertainty and\ncalibration on the effectiveness of MIAs. Simulation results demonstrate that\nthe derived analytical bounds predict well the effectiveness of MIAs."
                },
                "authors": [
                    {
                        "name": "Meiyi Zhu"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Chunyan Feng"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "16 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08161v3",
                "updated": "2025-03-14T10:09:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    9,
                    13,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-11T08:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    26,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "OASIS: Order-Augmented Strategy for Improved Code Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Order-Augmented Strategy for Improved Code Search"
                },
                "summary": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training."
                },
                "authors": [
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11256v1",
                "updated": "2025-03-14T10:07:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    7,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T10:07:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    7,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "Line of Duty: Evaluating LLM Self-Knowledge via Consistency in\n  Feasibility Boundaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Line of Duty: Evaluating LLM Self-Knowledge via Consistency in\n  Feasibility Boundaries"
                },
                "summary": "As LLMs grow more powerful, their most profound achievement may be\nrecognising when to say \"I don't know\". Existing studies on LLM self-knowledge\nhave been largely constrained by human-defined notions of feasibility, often\nneglecting the reasons behind unanswerability by LLMs and failing to study\ndeficient types of self-knowledge. This study aims to obtain intrinsic insights\ninto different types of LLM self-knowledge with a novel methodology: allowing\nthem the flexibility to set their own feasibility boundaries and then analysing\nthe consistency of these limits. We find that even frontier models like GPT-4o\nand Mistral Large are not sure of their own capabilities more than 80% of the\ntime, highlighting a significant lack of trustworthiness in responses. Our\nanalysis of confidence balance in LLMs indicates that models swing between\noverconfidence and conservatism in feasibility boundaries depending on task\ncategories and that the most significant self-knowledge weaknesses lie in\ntemporal awareness and contextual understanding. These difficulties in\ncontextual comprehension additionally lead models to question their operational\nboundaries, resulting in considerable confusion within the self-knowledge of\nLLMs. We make our code and results available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Self_Knowledge_Eval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs grow more powerful, their most profound achievement may be\nrecognising when to say \"I don't know\". Existing studies on LLM self-knowledge\nhave been largely constrained by human-defined notions of feasibility, often\nneglecting the reasons behind unanswerability by LLMs and failing to study\ndeficient types of self-knowledge. This study aims to obtain intrinsic insights\ninto different types of LLM self-knowledge with a novel methodology: allowing\nthem the flexibility to set their own feasibility boundaries and then analysing\nthe consistency of these limits. We find that even frontier models like GPT-4o\nand Mistral Large are not sure of their own capabilities more than 80% of the\ntime, highlighting a significant lack of trustworthiness in responses. Our\nanalysis of confidence balance in LLMs indicates that models swing between\noverconfidence and conservatism in feasibility boundaries depending on task\ncategories and that the most significant self-knowledge weaknesses lie in\ntemporal awareness and contextual understanding. These difficulties in\ncontextual comprehension additionally lead models to question their operational\nboundaries, resulting in considerable confusion within the self-knowledge of\nLLMs. We make our code and results available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Self_Knowledge_Eval"
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    },
                    {
                        "name": "Vijaykant Nadadur"
                    }
                ],
                "author_detail": {
                    "name": "Vijaykant Nadadur"
                },
                "author": "Vijaykant Nadadur",
                "arxiv_comment": "14 pages, 8 figures, Accepted to the 5th TrustNLP Workshop at NAACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11248v1",
                "updated": "2025-03-14T10:00:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    0,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T10:00:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    0,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Reasoning-Grounded Natural Language Explanations for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Grounded Natural Language Explanations for Language Models"
                },
                "summary": "We propose a large language model explainability technique for obtaining\nfaithful natural language explanations by grounding the explanations in a\nreasoning process. When converted to a sequence of tokens, the outputs of the\nreasoning process can become part of the model context and later be decoded to\nnatural language as the model produces either the final answer or the\nexplanation. To improve the faithfulness of the explanations, we propose to use\na joint predict-explain approach, in which the answers and explanations are\ninferred directly from the reasoning sequence, without the explanations being\ndependent on the answers and vice versa. We demonstrate the plausibility of the\nproposed technique by achieving a high alignment between answers and\nexplanations in several problem domains, observing that language models often\nsimply copy the partial decisions from the reasoning sequence into the final\nanswers or explanations. Furthermore, we show that the proposed use of\nreasoning can also improve the quality of the answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a large language model explainability technique for obtaining\nfaithful natural language explanations by grounding the explanations in a\nreasoning process. When converted to a sequence of tokens, the outputs of the\nreasoning process can become part of the model context and later be decoded to\nnatural language as the model produces either the final answer or the\nexplanation. To improve the faithfulness of the explanations, we propose to use\na joint predict-explain approach, in which the answers and explanations are\ninferred directly from the reasoning sequence, without the explanations being\ndependent on the answers and vice versa. We demonstrate the plausibility of the\nproposed technique by achieving a high alignment between answers and\nexplanations in several problem domains, observing that language models often\nsimply copy the partial decisions from the reasoning sequence into the final\nanswers or explanations. Furthermore, we show that the proposed use of\nreasoning can also improve the quality of the answers."
                },
                "authors": [
                    {
                        "name": "Vojtech Cahlik"
                    },
                    {
                        "name": "Rodrigo Alves"
                    },
                    {
                        "name": "Pavel Kordik"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Kordik"
                },
                "author": "Pavel Kordik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06664v2",
                "updated": "2025-03-14T09:54:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    54,
                    17,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-09T08:19:25Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    8,
                    19,
                    25,
                    2,
                    283,
                    0
                ],
                "title": "Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning"
                },
                "summary": "Diffusion models are trained by learning a sequence of models that reverse\neach step of noise corruption. Typically, the model parameters are fully shared\nacross multiple timesteps to enhance training efficiency. However, since the\ndenoising tasks differ at each timestep, the gradients computed at different\ntimesteps may conflict, potentially degrading the overall performance of image\ngeneration. To solve this issue, this work proposes a\n\\textbf{De}couple-then-\\textbf{Me}rge (\\textbf{DeMe}) framework, which begins\nwith a pretrained model and finetunes separate models tailored to specific\ntimesteps. We introduce several improved techniques during the finetuning stage\nto promote effective knowledge sharing while minimizing training interference\nacross timesteps. Finally, after finetuning, these separate models can be\nmerged into a single model in the parameter space, ensuring efficient and\npractical inference. Experimental results show significant generation quality\nimprovements upon 6 benchmarks including Stable Diffusion on COCO30K,\nImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10.\nCode is available at \\href{https://github.com/MqLeet/DeMe}{GitHub}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are trained by learning a sequence of models that reverse\neach step of noise corruption. Typically, the model parameters are fully shared\nacross multiple timesteps to enhance training efficiency. However, since the\ndenoising tasks differ at each timestep, the gradients computed at different\ntimesteps may conflict, potentially degrading the overall performance of image\ngeneration. To solve this issue, this work proposes a\n\\textbf{De}couple-then-\\textbf{Me}rge (\\textbf{DeMe}) framework, which begins\nwith a pretrained model and finetunes separate models tailored to specific\ntimesteps. We introduce several improved techniques during the finetuning stage\nto promote effective knowledge sharing while minimizing training interference\nacross timesteps. Finally, after finetuning, these separate models can be\nmerged into a single model in the parameter space, ensuring efficient and\npractical inference. Experimental results show significant generation quality\nimprovements upon 6 benchmarks including Stable Diffusion on COCO30K,\nImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10.\nCode is available at \\href{https://github.com/MqLeet/DeMe}{GitHub}."
                },
                "authors": [
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Li Niu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11244v1",
                "updated": "2025-03-14T09:52:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    52,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:52:30Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    52,
                    30,
                    4,
                    73,
                    0
                ],
                "title": "LLMPerf: GPU Performance Modeling meets Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPerf: GPU Performance Modeling meets Large Language Models"
                },
                "summary": "Performance modeling, a pivotal domain in program cost analysis, currently\nrelies on manually crafted models constrained by various program and hardware\nlimitations, especially in the intricate landscape of GPGPU. Meanwhile, Large\nLanguage Models (LLMs) have demonstrated their effectiveness in addressing\ndiverse programming challenges. Our work establishes a connection between LLMs\nand performance modeling, employing the LLM as a performance estimator. Through\nexperimental exploration with carefully designed large-scale OpenCL datasets,\nwe highlight the potential capability as well as the main difficulties of using\nLLMs in handling performance modeling tasks for OpenCL device source programs.\nAs the first study for this line of work, our LLM-based performance model\nachieves a mean absolute percentage error of $24.25\\%$ for a large-scale\ngenerated validation set. On a set of publicly available OpenCL programs, our\nmodel achieves a mean absolute percentage error of $46.1\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance modeling, a pivotal domain in program cost analysis, currently\nrelies on manually crafted models constrained by various program and hardware\nlimitations, especially in the intricate landscape of GPGPU. Meanwhile, Large\nLanguage Models (LLMs) have demonstrated their effectiveness in addressing\ndiverse programming challenges. Our work establishes a connection between LLMs\nand performance modeling, employing the LLM as a performance estimator. Through\nexperimental exploration with carefully designed large-scale OpenCL datasets,\nwe highlight the potential capability as well as the main difficulties of using\nLLMs in handling performance modeling tasks for OpenCL device source programs.\nAs the first study for this line of work, our LLM-based performance model\nachieves a mean absolute percentage error of $24.25\\%$ for a large-scale\ngenerated validation set. On a set of publicly available OpenCL programs, our\nmodel achieves a mean absolute percentage error of $46.1\\%$."
                },
                "authors": [
                    {
                        "name": "Khoi N. M. Nguyen"
                    },
                    {
                        "name": "Hoang Duy Nguyen Do"
                    },
                    {
                        "name": "Huyen Thao Le"
                    },
                    {
                        "name": "Thanh Tuan Dao"
                    }
                ],
                "author_detail": {
                    "name": "Thanh Tuan Dao"
                },
                "author": "Thanh Tuan Dao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05609v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05609v2",
                "updated": "2025-03-14T09:51:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    51,
                    44,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-08T14:52:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    52,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis"
                },
                "summary": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and an off-the-shelf\nLarge Language Model (LLM) to generate disease diagnoses based on the predicted\nconcepts. Furthermore, our approach supports test-time human intervention,\nenabling corrections to predicted concepts, which improves final diagnoses and\nenhances transparency in decision-making. We validate our approach on three\nskin lesion datasets, demonstrating that it outperforms traditional CBMs and\nstate-of-the-art explainable methods, all without requiring any training and\nutilizing only a few annotated examples. The code is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and an off-the-shelf\nLarge Language Model (LLM) to generate disease diagnoses based on the predicted\nconcepts. Furthermore, our approach supports test-time human intervention,\nenabling corrections to predicted concepts, which improves final diagnoses and\nenhances transparency in decision-making. We validate our approach on three\nskin lesion datasets, demonstrating that it outperforms traditional CBMs and\nstate-of-the-art explainable methods, all without requiring any training and\nutilizing only a few annotated examples. The code is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis."
                },
                "authors": [
                    {
                        "name": "Cristiano Patrcio"
                    },
                    {
                        "name": "Lus F. Teixeira"
                    },
                    {
                        "name": "Joo C. Neves"
                    }
                ],
                "author_detail": {
                    "name": "Joo C. Neves"
                },
                "author": "Joo C. Neves",
                "arxiv_doi": "10.1016/j.csbj.2025.02.013",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.csbj.2025.02.013",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.05609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05609v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Computational and Structural Biotechnology Journal",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11237v1",
                "updated": "2025-03-14T09:42:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    42,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:42:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    42,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "Collaboration is all you need: LLM Assisted Safe Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration is all you need: LLM Assisted Safe Code Translation"
                },
                "summary": "This paper introduces UniTranslator, a visionary framework that re-imagines\ncode translation as a collaborative endeavor among multiple, compact LLMs. By\norchestrating the interaction of specialized agents, each focused on different\naspects of the translation process and grounded in a deep understanding of\nprogramming concepts, UniTranslator achieves a level of accuracy and efficiency\nthat rivals larger, monolithic models. Our preliminary evaluation demonstrates\nthe potential of UniTranslator to overcome the limitations of existing\napproaches and unlock the power of smaller LLMs for complex code translation\ntasks. We explore the effectiveness of this dynamic multi-agent paradigm in\nhandling diverse language pairs, including low-resource languages, and in\nmitigating common issues such as code artifacts and hallucinations through the\nuse of Natural Language Inference (NLI) grounding and iterative feedback\nmechanisms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces UniTranslator, a visionary framework that re-imagines\ncode translation as a collaborative endeavor among multiple, compact LLMs. By\norchestrating the interaction of specialized agents, each focused on different\naspects of the translation process and grounded in a deep understanding of\nprogramming concepts, UniTranslator achieves a level of accuracy and efficiency\nthat rivals larger, monolithic models. Our preliminary evaluation demonstrates\nthe potential of UniTranslator to overcome the limitations of existing\napproaches and unlock the power of smaller LLMs for complex code translation\ntasks. We explore the effectiveness of this dynamic multi-agent paradigm in\nhandling diverse language pairs, including low-resource languages, and in\nmitigating common issues such as code artifacts and hallucinations through the\nuse of Natural Language Inference (NLI) grounding and iterative feedback\nmechanisms"
                },
                "authors": [
                    {
                        "name": "Rabimba Karanjai"
                    },
                    {
                        "name": "Sam Blackshear"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Weidong Shi"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Shi"
                },
                "author": "Weidong Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.03309v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.03309v5",
                "updated": "2025-03-14T09:33:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    33,
                    2,
                    4,
                    73,
                    0
                ],
                "published": "2023-10-05T04:47:49Z",
                "published_parsed": [
                    2023,
                    10,
                    5,
                    4,
                    47,
                    49,
                    3,
                    278,
                    0
                ],
                "title": "Concise and Organized Perception Facilitates Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise and Organized Perception Facilitates Reasoning in Large Language\n  Models"
                },
                "summary": "Exploiting large language models (LLMs) to tackle reasoning has garnered\ngrowing attention. It still remains highly challenging to achieve satisfactory\nresults in complex logical problems, characterized by plenty of premises within\nthe context and requiring multi-hop reasoning. In particular, the reasoning\ncapabilities of LLMs are brittle to disorder and distractibility. In this work,\nwe first examine the mechanism from the perspective of information flow and\nreveal that LLMs confront difficulties akin to human-like cognitive biases when\ndealing with disordered and irrelevant content in reasoning tasks. However, in\ncontrast to LLMs, disordered and irrelevant content does not significantly\ndecrease human performance, as humans have a propensity to distill the most\nrelevant information and systematically organize their thoughts, aiding them in\nresponding to questions.Stem from that, we further propose a novel reasoning\napproach named Concise and Organized Perception (COP). COP carefully analyzes\nthe given statements to identify the most pertinent information while\neliminating redundancy efficiently. It then prompts the LLMs in a more\norganized form that adapts to the model's inference process. By perceiving\nconcise and organized context, the reasoning abilities of LLMs can be better\nelicited. Extensive experimental results on several popular logical benchmarks\n(ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark\n(DI-GSM) show that COP significantly outperforms previous state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting large language models (LLMs) to tackle reasoning has garnered\ngrowing attention. It still remains highly challenging to achieve satisfactory\nresults in complex logical problems, characterized by plenty of premises within\nthe context and requiring multi-hop reasoning. In particular, the reasoning\ncapabilities of LLMs are brittle to disorder and distractibility. In this work,\nwe first examine the mechanism from the perspective of information flow and\nreveal that LLMs confront difficulties akin to human-like cognitive biases when\ndealing with disordered and irrelevant content in reasoning tasks. However, in\ncontrast to LLMs, disordered and irrelevant content does not significantly\ndecrease human performance, as humans have a propensity to distill the most\nrelevant information and systematically organize their thoughts, aiding them in\nresponding to questions.Stem from that, we further propose a novel reasoning\napproach named Concise and Organized Perception (COP). COP carefully analyzes\nthe given statements to identify the most pertinent information while\neliminating redundancy efficiently. It then prompts the LLMs in a more\norganized form that adapts to the model's inference process. By perceiving\nconcise and organized context, the reasoning abilities of LLMs can be better\nelicited. Extensive experimental results on several popular logical benchmarks\n(ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark\n(DI-GSM) show that COP significantly outperforms previous state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Junjie Liu"
                    },
                    {
                        "name": "Shaotian Yan"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Zhengdong Xiao"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by NAACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.03309v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.03309v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11232v1",
                "updated": "2025-03-14T09:31:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    31,
                    1,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:31:01Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    31,
                    1,
                    4,
                    73,
                    0
                ],
                "title": "PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment."
                },
                "authors": [
                    {
                        "name": "Ahmed Frikha"
                    },
                    {
                        "name": "Muhammad Reza Ar Razi"
                    },
                    {
                        "name": "Krishna Kanth Nakka"
                    },
                    {
                        "name": "Ricardo Mendes"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Xuebing Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Zhou"
                },
                "author": "Xuebing Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11229v1",
                "updated": "2025-03-14T09:26:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    26,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:26:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    26,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "Exploring the Potential of Large Multimodal Models as Effective\n  Alternatives for Pronunciation Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Multimodal Models as Effective\n  Alternatives for Pronunciation Assessment"
                },
                "summary": "Large Multimodal Models (LMMs) have demonstrated exceptional performance\nacross a wide range of domains. This paper explores their potential in\npronunciation assessment tasks, with a particular focus on evaluating the\ncapabilities of the Generative Pre-trained Transformer (GPT) model,\nspecifically GPT-4o. Our study investigates its ability to process speech and\naudio for pronunciation assessment across multiple levels of granularity and\ndimensions, with an emphasis on feedback generation and scoring. For our\nexperiments, we use the publicly available Speechocean762 dataset. The\nevaluation focuses on two key aspects: multi-level scoring and the practicality\nof the generated feedback. Scoring results are compared against the manual\nscores provided in the Speechocean762 dataset, while feedback quality is\nassessed using Large Language Models (LLMs). The findings highlight the\neffectiveness of integrating LMMs with traditional methods for pronunciation\nassessment, offering insights into the model's strengths and identifying areas\nfor further improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have demonstrated exceptional performance\nacross a wide range of domains. This paper explores their potential in\npronunciation assessment tasks, with a particular focus on evaluating the\ncapabilities of the Generative Pre-trained Transformer (GPT) model,\nspecifically GPT-4o. Our study investigates its ability to process speech and\naudio for pronunciation assessment across multiple levels of granularity and\ndimensions, with an emphasis on feedback generation and scoring. For our\nexperiments, we use the publicly available Speechocean762 dataset. The\nevaluation focuses on two key aspects: multi-level scoring and the practicality\nof the generated feedback. Scoring results are compared against the manual\nscores provided in the Speechocean762 dataset, while feedback quality is\nassessed using Large Language Models (LLMs). The findings highlight the\neffectiveness of integrating LMMs with traditional methods for pronunciation\nassessment, offering insights into the model's strengths and identifying areas\nfor further improvement."
                },
                "authors": [
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Kun Liu"
                    },
                    {
                        "name": "Yan Deng"
                    },
                    {
                        "name": "Wenning Wei"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18266v3",
                "updated": "2025-03-14T09:14:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    14,
                    26,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-27T12:03:52Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    52,
                    2,
                    332,
                    0
                ],
                "title": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria"
                },
                "summary": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems."
                },
                "authors": [
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Shuo Gao"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Wentian Yi"
                    },
                    {
                        "name": "Yuxuan Jin"
                    },
                    {
                        "name": "Xiaoxue Zhai"
                    },
                    {
                        "name": "Sixuan Lei"
                    },
                    {
                        "name": "Hongbei Meng"
                    },
                    {
                        "name": "Zibo Zhang"
                    },
                    {
                        "name": "Muzi Xu"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Xuhang Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Hongyun Yang"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Wenyu Wang"
                    },
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Xiaodong Feng"
                    },
                    {
                        "name": "Peter Smielewski"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Wenhui Song"
                    },
                    {
                        "name": "Martin Birchall"
                    },
                    {
                        "name": "Luigi G. Occhipinti"
                    }
                ],
                "author_detail": {
                    "name": "Luigi G. Occhipinti"
                },
                "author": "Luigi G. Occhipinti",
                "arxiv_comment": "5 figures, 45 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17174v2",
                "updated": "2025-03-14T08:56:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    56,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-20T08:28:23Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    8,
                    28,
                    23,
                    4,
                    264,
                    0
                ],
                "title": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency"
                },
                "summary": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal illusions between \\textit{a step of reasoning} and \\textit{corresponding\nstate transitions} are becoming a significant obstacle to advancing LLMs'\nreasoning capabilities, especially in long-range reasoning tasks. This paper\nproposes a non-chain-based reasoning framework for simultaneous consideration\nof causal significance and consistency, i.e., the Causal Significance and\nConsistency Enhancer (CSCE). We customize LLM's loss function utilizing\ntreatment effect assessments to enhance its reasoning ability from two aspects:\ncausal significance and consistency. This ensures that the model captures\nessential causal relationships and maintains robust and consistent performance\nacross various scenarios. Additionally, we transform the reasoning process from\nthe cascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal illusions between \\textit{a step of reasoning} and \\textit{corresponding\nstate transitions} are becoming a significant obstacle to advancing LLMs'\nreasoning capabilities, especially in long-range reasoning tasks. This paper\nproposes a non-chain-based reasoning framework for simultaneous consideration\nof causal significance and consistency, i.e., the Causal Significance and\nConsistency Enhancer (CSCE). We customize LLM's loss function utilizing\ntreatment effect assessments to enhance its reasoning ability from two aspects:\ncausal significance and consistency. This ensures that the model captures\nessential causal relationships and maintains robust and consistent performance\nacross various scenarios. Additionally, we transform the reasoning process from\nthe cascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Kangsheng Wang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Zizheng Guo"
                    },
                    {
                        "name": "Tianyu Hu"
                    },
                    {
                        "name": "Huimin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Ma"
                },
                "author": "Huimin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11205v1",
                "updated": "2025-03-14T08:49:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    49,
                    52,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    49,
                    52,
                    4,
                    73,
                    0
                ],
                "title": "LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free\n  Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free\n  Video LLMs"
                },
                "summary": "Training-free video large language models (LLMs) leverage pretrained Image\nLLMs to process video content without the need for further training. A key\nchallenge in such approaches is the difficulty of retaining essential visual\nand temporal information, constrained by the token limits in Image LLMs. To\naddress this, we propose a two-stage method for selecting query-relevant tokens\nbased on the LLM attention scores: compressing the video sequence and then\nexpanding the sequence. However, during the compression stage, Image LLMs often\nexhibit a positional attention bias in video sequences, where attention is\noverly concentrated on later frames, causing early-frame information to be\nunderutilized. To alleviate this attention bias during sequence compression, we\npropose Gridded Attention Pooling for preserving spatiotemporal structure.\nAdditionally, we introduce Visual Summarization Tail to effectively utilize\nthis bias, facilitating overall video understanding during sequence expansion.\nIn this way, our method effectively Mitigates and Leverages attention Bias\n(LLaVA-MLB), enabling the frozen Image LLM for detailed video understanding.\nExperiments on several benchmarks demonstrate that our approach outperforms\nstate-of-the-art methods, achieving superior performance in both efficiency and\naccuracy. Our code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free video large language models (LLMs) leverage pretrained Image\nLLMs to process video content without the need for further training. A key\nchallenge in such approaches is the difficulty of retaining essential visual\nand temporal information, constrained by the token limits in Image LLMs. To\naddress this, we propose a two-stage method for selecting query-relevant tokens\nbased on the LLM attention scores: compressing the video sequence and then\nexpanding the sequence. However, during the compression stage, Image LLMs often\nexhibit a positional attention bias in video sequences, where attention is\noverly concentrated on later frames, causing early-frame information to be\nunderutilized. To alleviate this attention bias during sequence compression, we\npropose Gridded Attention Pooling for preserving spatiotemporal structure.\nAdditionally, we introduce Visual Summarization Tail to effectively utilize\nthis bias, facilitating overall video understanding during sequence expansion.\nIn this way, our method effectively Mitigates and Leverages attention Bias\n(LLaVA-MLB), enabling the frozen Image LLM for detailed video understanding.\nExperiments on several benchmarks demonstrate that our approach outperforms\nstate-of-the-art methods, achieving superior performance in both efficiency and\naccuracy. Our code will be released."
                },
                "authors": [
                    {
                        "name": "Leqi Shen"
                    },
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Guoqiang Gong"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yifeng Zhang"
                    },
                    {
                        "name": "Pengzhang Liu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12928v3",
                "updated": "2025-03-14T08:48:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    48,
                    51,
                    4,
                    73,
                    0
                ],
                "published": "2024-08-23T09:14:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "ParGo: Bridging Vision-Language with Partial and Global Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParGo: Bridging Vision-Language with Partial and Global Views"
                },
                "summary": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability."
                },
                "authors": [
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02465v2",
                "updated": "2025-03-14T08:44:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    44,
                    6,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-04T06:28:22Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    28,
                    22,
                    2,
                    248,
                    0
                ],
                "title": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels"
                },
                "summary": "Recently, significant efforts have been devoted to enhancing the long-context\ncapabilities of Large Language Models (LLMs), particularly in long-context\nreasoning. To facilitate this research, we propose \\textbf{DetectiveQA}, a\ndataset specifically designed for narrative reasoning within long contexts. We\nleverage detective novels, averaging over 100k tokens, to create a dataset\ncontaining 1200 human-annotated questions in both Chinese and English, each\npaired with corresponding reference reasoning steps. Furthermore, we introduce\na step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning\nprocesses. We validate our approach and evaluate the mainstream LLMs, including\nGPT-4, Claude, and LLaMA, revealing persistent long-context reasoning\nchallenges and demonstrating their evidence-retrieval challenges. Our findings\noffer valuable insights into the study of long-context reasoning and lay the\nbase for more rigorous evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant efforts have been devoted to enhancing the long-context\ncapabilities of Large Language Models (LLMs), particularly in long-context\nreasoning. To facilitate this research, we propose \\textbf{DetectiveQA}, a\ndataset specifically designed for narrative reasoning within long contexts. We\nleverage detective novels, averaging over 100k tokens, to create a dataset\ncontaining 1200 human-annotated questions in both Chinese and English, each\npaired with corresponding reference reasoning steps. Furthermore, we introduce\na step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning\nprocesses. We validate our approach and evaluate the mainstream LLMs, including\nGPT-4, Claude, and LLaMA, revealing persistent long-context reasoning\nchallenges and demonstrating their evidence-retrieval challenges. Our findings\noffer valuable insights into the study of long-context reasoning and lay the\nbase for more rigorous evaluations."
                },
                "authors": [
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Xiangyang Liu"
                    },
                    {
                        "name": "Tianxiang Sun"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11197v1",
                "updated": "2025-03-14T08:43:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:43:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering"
                },
                "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi/r1-aqa and https://huggingface.co/mispeech/r1-aqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi/r1-aqa and https://huggingface.co/mispeech/r1-aqa."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jizhong Liu"
                    },
                    {
                        "name": "Heinrich Dinkel"
                    },
                    {
                        "name": "Yadong Niu"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11187v1",
                "updated": "2025-03-14T08:33:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    33,
                    8,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:33:08Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    33,
                    8,
                    4,
                    73,
                    0
                ],
                "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models"
                },
                "summary": "Video Large Language Models have shown impressive capabilities in video\ncomprehension, yet their practical deployment is hindered by substantial\ninference costs caused by redundant video tokens. Existing pruning techniques\nfail to fully exploit the spatiotemporal redundancy inherent in video data. To\nbridge this gap, we perform a systematic analysis of video redundancy from two\nperspectives: temporal context and visual context. Leveraging this insight, we\npropose Dynamic Density Pruning for Fast Video LLMs termed FastVID.\nSpecifically, FastVID dynamically partitions videos into temporally ordered\nsegments to preserve temporal structure and applies a density-based token\npruning strategy to maintain essential visual information. Our method\nsignificantly reduces computational overhead while maintaining temporal and\nvisual integrity. Extensive evaluations show that FastVID achieves\nstate-of-the-art performance across various short- and long-video benchmarks on\nleading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID\neffectively prunes 90% of video tokens while retaining 98.0% of\nLLaVA-OneVision's original performance. The code is available at\nhttps://github.com/LunarShen/FastVID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models have shown impressive capabilities in video\ncomprehension, yet their practical deployment is hindered by substantial\ninference costs caused by redundant video tokens. Existing pruning techniques\nfail to fully exploit the spatiotemporal redundancy inherent in video data. To\nbridge this gap, we perform a systematic analysis of video redundancy from two\nperspectives: temporal context and visual context. Leveraging this insight, we\npropose Dynamic Density Pruning for Fast Video LLMs termed FastVID.\nSpecifically, FastVID dynamically partitions videos into temporally ordered\nsegments to preserve temporal structure and applies a density-based token\npruning strategy to maintain essential visual information. Our method\nsignificantly reduces computational overhead while maintaining temporal and\nvisual integrity. Extensive evaluations show that FastVID achieves\nstate-of-the-art performance across various short- and long-video benchmarks on\nleading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID\neffectively prunes 90% of video tokens while retaining 98.0% of\nLLaVA-OneVision's original performance. The code is available at\nhttps://github.com/LunarShen/FastVID."
                },
                "authors": [
                    {
                        "name": "Leqi Shen"
                    },
                    {
                        "name": "Guoqiang Gong"
                    },
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Yifeng Zhang"
                    },
                    {
                        "name": "Pengzhang Liu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11650v1",
                "updated": "2025-03-14T17:59:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    41,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:59:41Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    41,
                    4,
                    73,
                    0
                ],
                "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training"
                },
                "summary": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels."
                },
                "authors": [
                    {
                        "name": "Chonghao Sima"
                    },
                    {
                        "name": "Kashyap Chitta"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Andreas Geiger"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvarez"
                },
                "author": "Jose M. Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11646v1",
                "updated": "2025-03-14T17:59:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:59:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    59,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning"
                },
                "summary": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of data efficiency, where quality outweighs quantity, has emerged\nas a cornerstone in robotic manipulation, especially given the high costs\nassociated with real-world data collection. We propose that maximizing the\ninformational density of individual demonstrations can dramatically reduce\nreliance on large-scale datasets while improving task performance. To this end,\nwe introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework\nthat redefines robotic data acquisition through real-time, bidirectional\nhuman-environment interactions. Unlike conventional pipelines that passively\nrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:\nduring a single episode, an adversarial operator dynamically alters object\nstates, environmental conditions, and linguistic commands, while the\ntele-operator adaptively adjusts actions to overcome these evolving challenges.\nThis process compresses diverse failure-recovery behaviors, compositional task\nvariations, and environmental perturbations into minimal demonstrations. Our\nexperiments demonstrate that ADC-trained models achieve superior compositional\ngeneralization to unseen task instructions, enhanced robustness to perceptual\nperturbations, and emergent error recovery capabilities. Strikingly, models\ntrained with merely 20% of the demonstration volume collected through ADC\nsignificantly outperform traditional approaches using full datasets. These\nadvances bridge the gap between data-centric learning paradigms and practical\nrobotic deployment, demonstrating that strategic data acquisition, not merely\npost-hoc processing, is critical for scalable, real-world robot learning.\nAdditionally, we are curating a large-scale ADC-Robotics dataset comprising\nreal-world manipulation tasks with adversarial perturbations. This benchmark\nwill be open-sourced to facilitate advancements in robotic imitation learning."
                },
                "authors": [
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Shu Jiang"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Guanghui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui Ren"
                },
                "author": "Guanghui Ren",
                "arxiv_comment": "More information can be found on our project\n  page:https://sites.google.com/view/adc-robot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17686v3",
                "updated": "2025-03-14T17:56:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    56,
                    9,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-26T18:53:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    53,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "Filter, Correlate, Compress: Training-Free Token Reduction for MLLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filter, Correlate, Compress: Training-Free Token Reduction for MLLM\n  Acceleration"
                },
                "summary": "The quadratic complexity of Multimodal Large Language Models (MLLMs) with\nrespect to sequence length poses significant computational and memory\nchallenges, hindering their real-world deployment. While existing training-free\ntoken reduction methods aim to address these inefficiencies, how to precisely\nidentify redundant visual tokens and recover the essential information from the\ndiscarded tokens remain unclear. In this paper, we propose a\n''filter-correlate-compress'' framework that decomposes the token reduction\ninto three stages: filtering redundant tokens, correlating discarded\ninformation to preserved tokens, and compressing tokens to minimize redundancy.\nFollowing the framework, we propose a solution FiCoCo to identify limitations\nin single redundancy assessment, propose adaptive strategies to retain critical\ninformation from discarded tokens, and mitigate semantic dilution during token\nfusion. Two specialized variants, FiCoCo-V (for vision encoders) and FiCoCo-L\n(for LLM decoders), further optimize efficiency across MLLM architectures.\nExtensive experiments demonstrate that FiCoCo achieves up to 5.7x/14.7x FLOPs\nreduction with 92.8%/93.6% performance retention on LLaVA-1.5-7B/LLaVA-NeXT-7B.\nOur methods consistently outperform state-of-the-art training-free approaches,\nshowcasing effectiveness and generalizability across model architectures,\nsizes, and tasks without requiring retraining. Our project page is at\nhttps://ficoco-accelerate.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of Multimodal Large Language Models (MLLMs) with\nrespect to sequence length poses significant computational and memory\nchallenges, hindering their real-world deployment. While existing training-free\ntoken reduction methods aim to address these inefficiencies, how to precisely\nidentify redundant visual tokens and recover the essential information from the\ndiscarded tokens remain unclear. In this paper, we propose a\n''filter-correlate-compress'' framework that decomposes the token reduction\ninto three stages: filtering redundant tokens, correlating discarded\ninformation to preserved tokens, and compressing tokens to minimize redundancy.\nFollowing the framework, we propose a solution FiCoCo to identify limitations\nin single redundancy assessment, propose adaptive strategies to retain critical\ninformation from discarded tokens, and mitigate semantic dilution during token\nfusion. Two specialized variants, FiCoCo-V (for vision encoders) and FiCoCo-L\n(for LLM decoders), further optimize efficiency across MLLM architectures.\nExtensive experiments demonstrate that FiCoCo achieves up to 5.7x/14.7x FLOPs\nreduction with 92.8%/93.6% performance retention on LLaVA-1.5-7B/LLaVA-NeXT-7B.\nOur methods consistently outperform state-of-the-art training-free approaches,\nshowcasing effectiveness and generalizability across model architectures,\nsizes, and tasks without requiring retraining. Our project page is at\nhttps://ficoco-accelerate.github.io/."
                },
                "authors": [
                    {
                        "name": "Yuhang Han"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Honggang Chen"
                    },
                    {
                        "name": "Qingsen Yan"
                    },
                    {
                        "name": "Siteng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siteng Huang"
                },
                "author": "Siteng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11617v1",
                "updated": "2025-03-14T17:36:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    36,
                    8,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:36:08Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    36,
                    8,
                    4,
                    73,
                    0
                ],
                "title": "ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via\n  Structural-Semantic Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via\n  Structural-Semantic Instruction Tuning"
                },
                "summary": "Analysis and comprehension of assembly code are crucial in various\napplications, such as reverse engineering. However, the low information density\nand lack of explicit syntactic structures in assembly code pose significant\nchallenges. Pioneering approaches with masked language modeling (MLM)-based\nmethods have been limited by facilitating natural language interaction. While\nrecent methods based on decoder-focused large language models (LLMs) have\nsignificantly enhanced semantic representation, they still struggle to capture\nthe nuanced and sparse semantics in assembly code. In this paper, we propose\nAssembly Augmented Tuning (ASMA-Tune), an end-to-end structural-semantic\ninstruction-tuning framework. Our approach synergizes encoder architectures\nwith decoder-based LLMs through projector modules to enable comprehensive code\nunderstanding. Experiments show that ASMA-Tune outperforms existing benchmarks,\nsignificantly enhancing assembly code comprehension and instruction-following\nabilities. Our model and dataset are public at\nhttps://github.com/wxy3596/ASMA-Tune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and comprehension of assembly code are crucial in various\napplications, such as reverse engineering. However, the low information density\nand lack of explicit syntactic structures in assembly code pose significant\nchallenges. Pioneering approaches with masked language modeling (MLM)-based\nmethods have been limited by facilitating natural language interaction. While\nrecent methods based on decoder-focused large language models (LLMs) have\nsignificantly enhanced semantic representation, they still struggle to capture\nthe nuanced and sparse semantics in assembly code. In this paper, we propose\nAssembly Augmented Tuning (ASMA-Tune), an end-to-end structural-semantic\ninstruction-tuning framework. Our approach synergizes encoder architectures\nwith decoder-based LLMs through projector modules to enable comprehensive code\nunderstanding. Experiments show that ASMA-Tune outperforms existing benchmarks,\nsignificantly enhancing assembly code comprehension and instruction-following\nabilities. Our model and dataset are public at\nhttps://github.com/wxy3596/ASMA-Tune."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jinbo Su"
                    },
                    {
                        "name": "Yanming Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Yangdong Wang"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Kai Yun"
                    },
                    {
                        "name": "Chunfu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Chunfu Jia"
                },
                "author": "Chunfu Jia",
                "arxiv_comment": "19 pages, multiple figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11614v1",
                "updated": "2025-03-14T17:33:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    33,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:33:30Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    33,
                    30,
                    4,
                    73,
                    0
                ],
                "title": "Neutralizing Bias in LLM Reasoning using Entailment Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutralizing Bias in LLM Reasoning using Entailment Graphs"
                },
                "summary": "LLMs are often claimed to be capable of Natural Language Inference (NLI),\nwhich is widely regarded as a cornerstone of more complex forms of reasoning.\nHowever, recent works show that LLMs still suffer from hallucinations in NLI\ndue to attestation bias, where LLMs overly rely on propositional memory to\nbuild shortcuts. To solve the issue, we design an unsupervised framework to\nconstruct counterfactual reasoning data and fine-tune LLMs to reduce\nattestation bias. To measure bias reduction, we build bias-adversarial variants\nof NLI datasets with randomly replaced predicates in premises while keeping\nhypotheses unchanged. Extensive evaluations show that our framework can\nsignificantly reduce hallucinations from attestation bias. Then, we further\nevaluate LLMs fine-tuned with our framework on original NLI datasets and their\nbias-neutralized versions, where original entities are replaced with randomly\nsampled ones. Extensive results show that our framework consistently improves\ninferential performance on both original and bias-neutralized NLI datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are often claimed to be capable of Natural Language Inference (NLI),\nwhich is widely regarded as a cornerstone of more complex forms of reasoning.\nHowever, recent works show that LLMs still suffer from hallucinations in NLI\ndue to attestation bias, where LLMs overly rely on propositional memory to\nbuild shortcuts. To solve the issue, we design an unsupervised framework to\nconstruct counterfactual reasoning data and fine-tune LLMs to reduce\nattestation bias. To measure bias reduction, we build bias-adversarial variants\nof NLI datasets with randomly replaced predicates in premises while keeping\nhypotheses unchanged. Extensive evaluations show that our framework can\nsignificantly reduce hallucinations from attestation bias. Then, we further\nevaluate LLMs fine-tuned with our framework on original NLI datasets and their\nbias-neutralized versions, where original entities are replaced with randomly\nsampled ones. Extensive results show that our framework consistently improves\ninferential performance on both original and bias-neutralized NLI datasets."
                },
                "authors": [
                    {
                        "name": "Liang Cheng"
                    },
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Mark Steedman"
                    }
                ],
                "author_detail": {
                    "name": "Mark Steedman"
                },
                "author": "Mark Steedman",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03461v2",
                "updated": "2025-03-14T17:27:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    27,
                    0,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-04T14:21:27Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    14,
                    21,
                    27,
                    4,
                    278,
                    0
                ],
                "title": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval-Augmented Generation"
                },
                "summary": "While retrieval-augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. A common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10% of their computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While retrieval-augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. A common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10% of their computational cost."
                },
                "authors": [
                    {
                        "name": "Tobias Leemann"
                    },
                    {
                        "name": "Periklis Petridis"
                    },
                    {
                        "name": "Giuseppe Vietri"
                    },
                    {
                        "name": "Dionysis Manousakas"
                    },
                    {
                        "name": "Aaron Roth"
                    },
                    {
                        "name": "Sergul Aydore"
                    }
                ],
                "author_detail": {
                    "name": "Sergul Aydore"
                },
                "author": "Sergul Aydore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02603v2",
                "updated": "2025-03-14T17:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    9,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-03T15:44:42Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    42,
                    3,
                    277,
                    0
                ],
                "title": "Agents' Room: Narrative Generation through Multi-step Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents' Room: Narrative Generation through Multi-step Collaboration"
                },
                "summary": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output."
                },
                "authors": [
                    {
                        "name": "Fantine Huot"
                    },
                    {
                        "name": "Reinald Kim Amplayo"
                    },
                    {
                        "name": "Jennimaria Palomaki"
                    },
                    {
                        "name": "Alice Shoshana Jakobovits"
                    },
                    {
                        "name": "Elizabeth Clark"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16236v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16236v3",
                "updated": "2025-03-14T17:08:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    8,
                    22,
                    4,
                    73,
                    0
                ],
                "published": "2024-05-25T13:54:05Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    13,
                    54,
                    5,
                    5,
                    146,
                    0
                ],
                "title": "A transfer learning framework for weak-to-strong generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A transfer learning framework for weak-to-strong generalization"
                },
                "summary": "Modern large language model (LLM) alignment techniques rely on human\nfeedback, but it is unclear whether these techniques fundamentally limit the\ncapabilities of aligned LLMs. In particular, it is unknown if it is possible to\nalign (stronger) LLMs with superhuman capabilities with (weaker) human feedback\nwithout degrading their capabilities. This is an instance of the weak-to-strong\ngeneralization problem: using feedback from a weaker (less capable) model to\ntrain a stronger (more capable) model. We prove that weak-to-strong\ngeneralization is possible by eliciting latent knowledge from pre-trained LLMs.\nIn particular, we cast the weak-to-strong generalization problem as a transfer\nlearning problem in which we wish to transfer a latent concept prior from a\nweak model to a strong pre-trained model. We prove that a naive fine-tuning\napproach suffers from fundamental limitations, but an alternative\nrefinement-based approach suggested by the problem structure provably overcomes\nthe limitations of fine-tuning. Finally, we demonstrate the practical\napplicability of the refinement approach in multiple LLM alignment tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) alignment techniques rely on human\nfeedback, but it is unclear whether these techniques fundamentally limit the\ncapabilities of aligned LLMs. In particular, it is unknown if it is possible to\nalign (stronger) LLMs with superhuman capabilities with (weaker) human feedback\nwithout degrading their capabilities. This is an instance of the weak-to-strong\ngeneralization problem: using feedback from a weaker (less capable) model to\ntrain a stronger (more capable) model. We prove that weak-to-strong\ngeneralization is possible by eliciting latent knowledge from pre-trained LLMs.\nIn particular, we cast the weak-to-strong generalization problem as a transfer\nlearning problem in which we wish to transfer a latent concept prior from a\nweak model to a strong pre-trained model. We prove that a naive fine-tuning\napproach suffers from fundamental limitations, but an alternative\nrefinement-based approach suggested by the problem structure provably overcomes\nthe limitations of fine-tuning. Finally, we demonstrate the practical\napplicability of the refinement approach in multiple LLM alignment tasks."
                },
                "authors": [
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Moulinath Banerjee"
                    },
                    {
                        "name": "Ya'acov Ritov"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Yuekai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yuekai Sun"
                },
                "author": "Yuekai Sun",
                "arxiv_comment": "v2: Major changes to set up, theory, and experiments v3: Camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16236v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16236v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11588v1",
                "updated": "2025-03-14T16:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Generalization performance of neural mapping schemes for the space-time\n  interpolation of satellite-derived ocean colour datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalization performance of neural mapping schemes for the space-time\n  interpolation of satellite-derived ocean colour datasets"
                },
                "summary": "Neural mapping schemes have become appealing approaches to deliver gap-free\nsatellite-derived products for sea surface tracers. The generalization\nperformance of these learning-based approaches naturally arises as a key\nchallenge. This is particularly true for satellite-derived ocean colour\nproducts given the variety of bio-optical variables of interest, as well as the\ndiversity of processes and scales involved. Considering region-specific and\nparameter-specific neural mapping schemes will result in substantial training\ncosts. This study addresses generalization performance of neural mapping\nschemes to deliver gap-free satellite-derived ocean colour products. We develop\na comprehensive experimental framework using real multi-sensor ocean colour\ndatasets for two regions (the Mediterranean Sea and the North Sea) and a\nrepresentative set of bio-optical parameters (Chlorophyll-a concentration,\nsuspended particulate matter concentration, particulate backscattering\ncoefficient). We consider several neural mapping schemes, and we report\nexcellent generalization performance across regions and bio-optical parameters\nwithout any fine-tuning using appropriate dataset-specific normalization\nprocedures. We discuss further how these results provide new insights towards\nthe large-scale deployment of neural schemes for the processing of\nsatellite-derived ocean colour datasets beyond case-study-specific\ndemonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural mapping schemes have become appealing approaches to deliver gap-free\nsatellite-derived products for sea surface tracers. The generalization\nperformance of these learning-based approaches naturally arises as a key\nchallenge. This is particularly true for satellite-derived ocean colour\nproducts given the variety of bio-optical variables of interest, as well as the\ndiversity of processes and scales involved. Considering region-specific and\nparameter-specific neural mapping schemes will result in substantial training\ncosts. This study addresses generalization performance of neural mapping\nschemes to deliver gap-free satellite-derived ocean colour products. We develop\na comprehensive experimental framework using real multi-sensor ocean colour\ndatasets for two regions (the Mediterranean Sea and the North Sea) and a\nrepresentative set of bio-optical parameters (Chlorophyll-a concentration,\nsuspended particulate matter concentration, particulate backscattering\ncoefficient). We consider several neural mapping schemes, and we report\nexcellent generalization performance across regions and bio-optical parameters\nwithout any fine-tuning using appropriate dataset-specific normalization\nprocedures. We discuss further how these results provide new insights towards\nthe large-scale deployment of neural schemes for the processing of\nsatellite-derived ocean colour datasets beyond case-study-specific\ndemonstrations."
                },
                "authors": [
                    {
                        "name": "Thi Thuy Nga Nguyen"
                    },
                    {
                        "name": "Clment Dorffer"
                    },
                    {
                        "name": "Frdric Jourdin"
                    },
                    {
                        "name": "Ronan Fablet"
                    }
                ],
                "author_detail": {
                    "name": "Ronan Fablet"
                },
                "author": "Ronan Fablet",
                "arxiv_comment": "9 pages, 6 figures. Submitted to IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11586v1",
                "updated": "2025-03-14T16:55:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    55,
                    46,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:55:46Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    55,
                    46,
                    4,
                    73,
                    0
                ],
                "title": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs\n  using Semantic Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs\n  using Semantic Space"
                },
                "summary": "Large language models (LLMs) are used in chatbots or AI assistants to hold\nconversations with a human user. In such applications, the quality (e.g., user\nengagement, safety) of a conversation is important and can only be exactly\nknown at the end of the conversation. To maximize its expected quality,\nconversation planning reasons about the stochastic transitions within a\nconversation to select the optimal LLM response at each turn. Existing\nsimulation-based conversation planning algorithms typically select the optimal\nresponse by simulating future conversations with a large number of LLM queries\nat every turn. However, this process is extremely time-consuming and hence\nimpractical for real-time conversations. This paper presents a novel approach\ncalled Semantic space COnversation Planning with improved Efficiency (SCOPE)\nthat exploits the dense semantic representation of conversations to perform\nconversation planning efficiently. In particular, SCOPE models the stochastic\ntransitions in conversation semantics and their associated rewards to plan\nentirely within the semantic space. This allows us to select the optimal LLM\nresponse at every conversation turn without needing additional LLM queries for\nsimulation. As a result, SCOPE can perform conversation planning 70 times\nfaster than conventional simulation-based planning algorithms when applied to a\nwide variety of conversation starters and two reward functions seen in the real\nworld, yet achieving a higher reward within a practical planning budget. Our\ncode can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are used in chatbots or AI assistants to hold\nconversations with a human user. In such applications, the quality (e.g., user\nengagement, safety) of a conversation is important and can only be exactly\nknown at the end of the conversation. To maximize its expected quality,\nconversation planning reasons about the stochastic transitions within a\nconversation to select the optimal LLM response at each turn. Existing\nsimulation-based conversation planning algorithms typically select the optimal\nresponse by simulating future conversations with a large number of LLM queries\nat every turn. However, this process is extremely time-consuming and hence\nimpractical for real-time conversations. This paper presents a novel approach\ncalled Semantic space COnversation Planning with improved Efficiency (SCOPE)\nthat exploits the dense semantic representation of conversations to perform\nconversation planning efficiently. In particular, SCOPE models the stochastic\ntransitions in conversation semantics and their associated rewards to plan\nentirely within the semantic space. This allows us to select the optimal LLM\nresponse at every conversation turn without needing additional LLM queries for\nsimulation. As a result, SCOPE can perform conversation planning 70 times\nfaster than conventional simulation-based planning algorithms when applied to a\nwide variety of conversation starters and two reward functions seen in the real\nworld, yet achieving a higher reward within a practical planning budget. Our\ncode can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE."
                },
                "authors": [
                    {
                        "name": "Zhiliang Chen"
                    },
                    {
                        "name": "Xinyuan Niu"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05068v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05068v5",
                "updated": "2025-03-14T16:45:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    45,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-08T12:10:13Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    12,
                    10,
                    13,
                    6,
                    252,
                    0
                ],
                "title": "Improving early detection of gravitational waves from binary neutron\n  stars using CNNs and FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving early detection of gravitational waves from binary neutron\n  stars using CNNs and FPGAs"
                },
                "summary": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel \\texttt{WaveNet}-based method, leveraging the state-of-the-art\nML model, to produce early-warning alerts for BNS systems. Using simulated GW\nsignals embedded in Gaussian noise from the Advanced LIGO and Advanced Virgo\ndetectors' third observing run (O3) as a proof-of-concept dataset, we\ndemonstrate significant performance improvements. Compared to the current\nleading ML-based early-warning system, our approach enhances detection accuracy\nfrom 66.81\\% to 76.22\\% at a 1\\% false alarm probability. Furthermore, we\nevaluate the time, energy, and economical cost of our model across CPU, GPU,\nand FPGA platforms, showcasing its potential for deployment in real-time\ngravitational wave detection pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel \\texttt{WaveNet}-based method, leveraging the state-of-the-art\nML model, to produce early-warning alerts for BNS systems. Using simulated GW\nsignals embedded in Gaussian noise from the Advanced LIGO and Advanced Virgo\ndetectors' third observing run (O3) as a proof-of-concept dataset, we\ndemonstrate significant performance improvements. Compared to the current\nleading ML-based early-warning system, our approach enhances detection accuracy\nfrom 66.81\\% to 76.22\\% at a 1\\% false alarm probability. Furthermore, we\nevaluate the time, energy, and economical cost of our model across CPU, GPU,\nand FPGA platforms, showcasing its potential for deployment in real-time\ngravitational wave detection pipelines."
                },
                "authors": [
                    {
                        "name": "Ana Martins"
                    },
                    {
                        "name": "Melissa Lopez"
                    },
                    {
                        "name": "Quirijn Meijer"
                    },
                    {
                        "name": "Gregory Baltus"
                    },
                    {
                        "name": "Marc van der Sluys"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    },
                    {
                        "name": "Sarah Caudill"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Caudill"
                },
                "author": "Sarah Caudill",
                "arxiv_comment": "21 pages, 7 figures, 3 tables, submitted to Machine Learning Science\n  and Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05068v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05068v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11573v1",
                "updated": "2025-03-14T16:40:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    25,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:40:25Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    25,
                    4,
                    73,
                    0
                ],
                "title": "Synthesizing Access Control Policies using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Access Control Policies using Large Language Models"
                },
                "summary": "Cloud compute systems allow administrators to write access control policies\nthat govern access to private data. While policies are written in convenient\nlanguages, such as AWS Identity and Access Management Policy Language, manually\nwritten policies often become complex and error prone. In this paper, we\ninvestigate whether and how well Large Language Models (LLMs) can be used to\nsynthesize access control policies. Our investigation focuses on the task of\ntaking an access control request specification and zero-shot prompting LLMs to\nsynthesize a well-formed access control policy which correctly adheres to the\nrequest specification. We consider two scenarios, one which the request\nspecification is given as a concrete list of requests to be allowed or denied,\nand another in which a natural language description is used to specify sets of\nrequests to be allowed or denied. We then argue that for zero-shot prompting,\nmore precise and structured prompts using a syntax based approach are necessary\nand experimentally show preliminary results validating our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud compute systems allow administrators to write access control policies\nthat govern access to private data. While policies are written in convenient\nlanguages, such as AWS Identity and Access Management Policy Language, manually\nwritten policies often become complex and error prone. In this paper, we\ninvestigate whether and how well Large Language Models (LLMs) can be used to\nsynthesize access control policies. Our investigation focuses on the task of\ntaking an access control request specification and zero-shot prompting LLMs to\nsynthesize a well-formed access control policy which correctly adheres to the\nrequest specification. We consider two scenarios, one which the request\nspecification is given as a concrete list of requests to be allowed or denied,\nand another in which a natural language description is used to specify sets of\nrequests to be allowed or denied. We then argue that for zero-shot prompting,\nmore precise and structured prompts using a syntax based approach are necessary\nand experimentally show preliminary results validating our approach."
                },
                "authors": [
                    {
                        "name": "Adarsh Vatsa"
                    },
                    {
                        "name": "Pratyush Patel"
                    },
                    {
                        "name": "William Eiers"
                    }
                ],
                "author_detail": {
                    "name": "William Eiers"
                },
                "author": "William Eiers",
                "arxiv_comment": "to be published in the NLBSE Workshop at ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11572v1",
                "updated": "2025-03-14T16:40:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    2,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:40:02Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    40,
                    2,
                    4,
                    73,
                    0
                ],
                "title": "Implicit Bias-Like Patterns in Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Bias-Like Patterns in Reasoning Models"
                },
                "summary": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Messi H. J. Lee"
                    },
                    {
                        "name": "Calvin K. Lai"
                    }
                ],
                "author_detail": {
                    "name": "Calvin K. Lai"
                },
                "author": "Calvin K. Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11570v1",
                "updated": "2025-03-14T16:38:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    38,
                    32,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:38:32Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    38,
                    32,
                    4,
                    73,
                    0
                ],
                "title": "Additive Manufacturing for Advanced Quantum Technologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Additive Manufacturing for Advanced Quantum Technologies"
                },
                "summary": "The development of quantum technology has opened up exciting opportunities to\nrevolutionize computing and communication, timing and navigation systems,\nenable non-invasive imaging of the human body, and probe fundamental physics\nwith unprecedented precision. Alongside these advancements has come an increase\nin experimental complexity and a correspondingly greater dependence on compact,\nefficient and reliable hardware. The drive to move quantum technologies from\nlaboratory prototypes to portable, real-world instruments has incentivized\nminiaturization of experimental systems relating to a strong demand for\nsmaller, more robust and less power-hungry quantum hardware and for\nincreasingly specialized and intricate components. Additive manufacturing,\nalready heralded as game-changing for many manufacturing sectors, is especially\nwell-suited to this task owing to the comparatively large amount of design\nfreedom it enables and its ability to produce intricate three-dimensional forms\nand specialized components. Herein we review work conducted to date on the\napplication of additive manufacturing to quantum technologies, discuss the\ncurrent state of the art in additive manufacturing in optics, optomechanics,\nmagnetic components and vacuum equipment, and consider pathways for future\nadvancement. We also give an overview of the research and application areas\nmost likely to be impacted by the deployment of additive manufacturing\ntechniques within the quantum technology sector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of quantum technology has opened up exciting opportunities to\nrevolutionize computing and communication, timing and navigation systems,\nenable non-invasive imaging of the human body, and probe fundamental physics\nwith unprecedented precision. Alongside these advancements has come an increase\nin experimental complexity and a correspondingly greater dependence on compact,\nefficient and reliable hardware. The drive to move quantum technologies from\nlaboratory prototypes to portable, real-world instruments has incentivized\nminiaturization of experimental systems relating to a strong demand for\nsmaller, more robust and less power-hungry quantum hardware and for\nincreasingly specialized and intricate components. Additive manufacturing,\nalready heralded as game-changing for many manufacturing sectors, is especially\nwell-suited to this task owing to the comparatively large amount of design\nfreedom it enables and its ability to produce intricate three-dimensional forms\nand specialized components. Herein we review work conducted to date on the\napplication of additive manufacturing to quantum technologies, discuss the\ncurrent state of the art in additive manufacturing in optics, optomechanics,\nmagnetic components and vacuum equipment, and consider pathways for future\nadvancement. We also give an overview of the research and application areas\nmost likely to be impacted by the deployment of additive manufacturing\ntechniques within the quantum technology sector."
                },
                "authors": [
                    {
                        "name": "F. Wang"
                    },
                    {
                        "name": "N. Cooper"
                    },
                    {
                        "name": "D. Johnson"
                    },
                    {
                        "name": "B. Hopton"
                    },
                    {
                        "name": "T. M. Fromhold"
                    },
                    {
                        "name": "R. Hague"
                    },
                    {
                        "name": "A. Murray"
                    },
                    {
                        "name": "R. McMullen"
                    },
                    {
                        "name": "L. Turyanska"
                    },
                    {
                        "name": "L. Hackermller"
                    }
                ],
                "author_detail": {
                    "name": "L. Hackermller"
                },
                "author": "L. Hackermller",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11566v1",
                "updated": "2025-03-14T16:35:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    35,
                    22,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:35:22Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    35,
                    22,
                    4,
                    73,
                    0
                ],
                "title": "Experimental evaluation of xApp Conflict Mitigation Framework in O-RAN:\n  Insights from Testbed deployment in OTIC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental evaluation of xApp Conflict Mitigation Framework in O-RAN:\n  Insights from Testbed deployment in OTIC"
                },
                "summary": "Conflict Mitigation (CM) in Open Radio Access Network (O-RAN) is a topic that\nis gaining importance as commercial O-RAN deployments become more complex.\nAlthough research on CM is already covered in terms of simulated network\nscenarios, it lacks validation using real-world deployment and Over The Air\n(OTA) Radio Frequency (RF) transmission. Our objective is to conduct the first\nassessment of the Conflict Mitigation Framework (CMF) for O-RAN using a\nreal-world testbed and OTA RF transmission. This paper presents results of an\nexperiment using a dedicated testbed built in an O-RAN Open Test and\nIntegration Center (OTIC) to confirm the validity of one of the Conflict\nResolution (CR) schemes proposed by existing research. The results show that\nthe implemented conflict detection and resolution mechanisms allow a\nsignificant improvement in network operation stability by reducing the\nvariability of the measured Downlink (DL) throughput by 78%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conflict Mitigation (CM) in Open Radio Access Network (O-RAN) is a topic that\nis gaining importance as commercial O-RAN deployments become more complex.\nAlthough research on CM is already covered in terms of simulated network\nscenarios, it lacks validation using real-world deployment and Over The Air\n(OTA) Radio Frequency (RF) transmission. Our objective is to conduct the first\nassessment of the Conflict Mitigation Framework (CMF) for O-RAN using a\nreal-world testbed and OTA RF transmission. This paper presents results of an\nexperiment using a dedicated testbed built in an O-RAN Open Test and\nIntegration Center (OTIC) to confirm the validity of one of the Conflict\nResolution (CR) schemes proposed by existing research. The results show that\nthe implemented conflict detection and resolution mechanisms allow a\nsignificant improvement in network operation stability by reducing the\nvariability of the measured Downlink (DL) throughput by 78%."
                },
                "authors": [
                    {
                        "name": "Abida Sultana"
                    },
                    {
                        "name": "Cezary Adamczyk"
                    },
                    {
                        "name": "Mayukh Roy Chowdhury"
                    },
                    {
                        "name": "Adrian Kliks"
                    },
                    {
                        "name": "Aloizio Da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Aloizio Da Silva"
                },
                "author": "Aloizio Da Silva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16560v2",
                "updated": "2025-03-14T16:18:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    18,
                    50,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-25T02:20:42Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    20,
                    42,
                    2,
                    269,
                    0
                ],
                "title": "Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have shown outstanding performance across\nnumerous real-world tasks. However, the autoregressive nature of these models\nmakes the inference process slow and costly. Speculative decoding has emerged\nas a promising solution, leveraging a smaller auxiliary model to draft future\ntokens, which are then validated simultaneously by the larger model, achieving\na speed-up of 1-2x. Although speculative decoding matches the same distribution\nas multinomial sampling, multinomial sampling itself is prone to suboptimal\noutputs, whereas beam sampling is widely recognized for producing\nhigher-quality results by maintaining multiple candidate sequences at each\nstep. This paper explores the novel integration of speculative decoding with\nbeam sampling. However, there are four key challenges: (1) how to generate\nmultiple sequences from the larger model's distribution given drafts sequences\nfrom the small model; (2) how to dynamically optimize the number of beams to\nbalance efficiency and accuracy; (3) how to efficiently verify the multiple\ndrafts in parallel; and (4) how to address the extra memory costs inherent in\nbeam sampling. To address these challenges, we propose dynamic-width\nspeculative beam decoding (DSBD). Specifically, we first introduce a novel\ndraft and verification scheme that generates multiple sequences following the\nlarge model's distribution based on beam sampling trajectories from the small\nmodel. Then, we introduce an adaptive mechanism to dynamically tune the number\nof beams based on the context, optimizing efficiency and effectiveness.\nBesides, we extend tree-based parallel verification to handle multiple trees\nsimultaneously, accelerating the verification process. Finally, we illustrate a\nsimple modification to our algorithm to mitigate the memory overhead of beam\nsampling...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown outstanding performance across\nnumerous real-world tasks. However, the autoregressive nature of these models\nmakes the inference process slow and costly. Speculative decoding has emerged\nas a promising solution, leveraging a smaller auxiliary model to draft future\ntokens, which are then validated simultaneously by the larger model, achieving\na speed-up of 1-2x. Although speculative decoding matches the same distribution\nas multinomial sampling, multinomial sampling itself is prone to suboptimal\noutputs, whereas beam sampling is widely recognized for producing\nhigher-quality results by maintaining multiple candidate sequences at each\nstep. This paper explores the novel integration of speculative decoding with\nbeam sampling. However, there are four key challenges: (1) how to generate\nmultiple sequences from the larger model's distribution given drafts sequences\nfrom the small model; (2) how to dynamically optimize the number of beams to\nbalance efficiency and accuracy; (3) how to efficiently verify the multiple\ndrafts in parallel; and (4) how to address the extra memory costs inherent in\nbeam sampling. To address these challenges, we propose dynamic-width\nspeculative beam decoding (DSBD). Specifically, we first introduce a novel\ndraft and verification scheme that generates multiple sequences following the\nlarge model's distribution based on beam sampling trajectories from the small\nmodel. Then, we introduce an adaptive mechanism to dynamically tune the number\nof beams based on the context, optimizing efficiency and effectiveness.\nBesides, we extend tree-based parallel verification to handle multiple trees\nsimultaneously, accelerating the verification process. Finally, we illustrate a\nsimple modification to our algorithm to mitigate the memory overhead of beam\nsampling..."
                },
                "authors": [
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Jason Cong"
                    },
                    {
                        "name": "Yizhou Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Sun"
                },
                "author": "Yizhou Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.21030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.21030v2",
                "updated": "2025-03-14T16:14:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    14,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2024-05-31T17:21:52Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    17,
                    21,
                    52,
                    4,
                    152,
                    0
                ],
                "title": "Standards for Belief Representations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standards for Belief Representations in LLMs"
                },
                "summary": "As large language models (LLMs) continue to demonstrate remarkable abilities\nacross various domains, computer scientists are developing methods to\nunderstand their cognitive processes, particularly concerning how (and if) LLMs\ninternally represent their beliefs about the world. However, this field\ncurrently lacks a unified theoretical foundation to underpin the study of\nbelief in LLMs. This article begins filling this gap by proposing adequacy\nconditions for a representation in an LLM to count as belief-like. We argue\nthat, while the project of belief measurement in LLMs shares striking features\nwith belief measurement as carried out in decision theory and formal\nepistemology, it also differs in ways that should change how we measure belief.\nThus, drawing from insights in philosophy and contemporary practices of machine\nlearning, we establish four criteria that balance theoretical considerations\nwith practical constraints. Our proposed criteria include accuracy, coherence,\nuniformity, and use, which together help lay the groundwork for a comprehensive\nunderstanding of belief representation in LLMs. We draw on empirical work\nshowing the limitations of using various criteria in isolation to identify\nbelief representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to demonstrate remarkable abilities\nacross various domains, computer scientists are developing methods to\nunderstand their cognitive processes, particularly concerning how (and if) LLMs\ninternally represent their beliefs about the world. However, this field\ncurrently lacks a unified theoretical foundation to underpin the study of\nbelief in LLMs. This article begins filling this gap by proposing adequacy\nconditions for a representation in an LLM to count as belief-like. We argue\nthat, while the project of belief measurement in LLMs shares striking features\nwith belief measurement as carried out in decision theory and formal\nepistemology, it also differs in ways that should change how we measure belief.\nThus, drawing from insights in philosophy and contemporary practices of machine\nlearning, we establish four criteria that balance theoretical considerations\nwith practical constraints. Our proposed criteria include accuracy, coherence,\nuniformity, and use, which together help lay the groundwork for a comprehensive\nunderstanding of belief representation in LLMs. We draw on empirical work\nshowing the limitations of using various criteria in isolation to identify\nbelief representations."
                },
                "authors": [
                    {
                        "name": "Daniel A. Herrmann"
                    },
                    {
                        "name": "Benjamin A. Levinstein"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin A. Levinstein"
                },
                "author": "Benjamin A. Levinstein",
                "arxiv_doi": "10.1007/s11023-024-09709-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11023-024-09709-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.21030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.21030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Minds and Machines 35:5 (2025) 1-25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11549v1",
                "updated": "2025-03-14T16:12:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    12,
                    23,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T16:12:23Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    12,
                    23,
                    4,
                    73,
                    0
                ],
                "title": "Similarity-Aware Token Pruning: Your VLM but Faster",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-Aware Token Pruning: Your VLM but Faster"
                },
                "summary": "The computational demands of Vision Transformers (ViTs) and Vision-Language\nModels (VLMs) remain a significant challenge due to the quadratic complexity of\nself-attention. While token pruning offers a promising solution, existing\nmethods often introduce training overhead or fail to adapt dynamically across\nlayers. We present SAINT, a training-free token pruning framework that\nleverages token similarity and a graph-based formulation to dynamically\noptimize pruning rates and redundancy thresholds. Through systematic analysis,\nwe identify a universal three-stage token evolution process\n(aligner-explorer-aggregator) in transformers, enabling aggressive pruning in\nearly stages without sacrificing critical information. For ViTs, SAINT doubles\nthe throughput of ViT-H/14 at 224px with only 0.6% accuracy loss on\nImageNet-1K, surpassing the closest competitor by 0.8%. For VLMs, we apply\nSAINT in three modes: ViT-only, LLM-only, and hybrid. SAINT reduces LLaVA-13B's\ntokens by 75%, achieving latency comparable to LLaVA-7B with less than 1%\nperformance loss across benchmarks. Our work establishes a unified, practical\nframework for efficient inference in ViTs and VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational demands of Vision Transformers (ViTs) and Vision-Language\nModels (VLMs) remain a significant challenge due to the quadratic complexity of\nself-attention. While token pruning offers a promising solution, existing\nmethods often introduce training overhead or fail to adapt dynamically across\nlayers. We present SAINT, a training-free token pruning framework that\nleverages token similarity and a graph-based formulation to dynamically\noptimize pruning rates and redundancy thresholds. Through systematic analysis,\nwe identify a universal three-stage token evolution process\n(aligner-explorer-aggregator) in transformers, enabling aggressive pruning in\nearly stages without sacrificing critical information. For ViTs, SAINT doubles\nthe throughput of ViT-H/14 at 224px with only 0.6% accuracy loss on\nImageNet-1K, surpassing the closest competitor by 0.8%. For VLMs, we apply\nSAINT in three modes: ViT-only, LLM-only, and hybrid. SAINT reduces LLaVA-13B's\ntokens by 75%, achieving latency comparable to LLaVA-7B with less than 1%\nperformance loss across benchmarks. Our work establishes a unified, practical\nframework for efficient inference in ViTs and VLMs."
                },
                "authors": [
                    {
                        "name": "Ahmadreza Jeddi"
                    },
                    {
                        "name": "Negin Baghbanzadeh"
                    },
                    {
                        "name": "Elham Dolatabadi"
                    },
                    {
                        "name": "Babak Taati"
                    }
                ],
                "author_detail": {
                    "name": "Babak Taati"
                },
                "author": "Babak Taati",
                "arxiv_comment": "15 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18092v2",
                "updated": "2025-03-14T16:12:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    12,
                    10,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-27T07:04:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    4,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Training Noise Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Noise Token Pruning"
                },
                "summary": "In the present work we present Training Noise Token (TNT) Pruning for vision\ntransformers. Our method relaxes the discrete token dropping condition to\ncontinuous additive noise, providing smooth optimization in training, while\nretaining discrete dropping computational gains in deployment settings. We\nprovide theoretical connections to Rate-Distortion literature, and empirical\nevaluations on the ImageNet dataset using ViT and DeiT architectures\ndemonstrating TNT's advantages over previous pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the present work we present Training Noise Token (TNT) Pruning for vision\ntransformers. Our method relaxes the discrete token dropping condition to\ncontinuous additive noise, providing smooth optimization in training, while\nretaining discrete dropping computational gains in deployment settings. We\nprovide theoretical connections to Rate-Distortion literature, and empirical\nevaluations on the ImageNet dataset using ViT and DeiT architectures\ndemonstrating TNT's advantages over previous pruning methods."
                },
                "authors": [
                    {
                        "name": "Mingxing Rao"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Daniel Moyer"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Moyer"
                },
                "author": "Daniel Moyer",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11531v1",
                "updated": "2025-03-14T15:58:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    58,
                    11,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:58:11Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    58,
                    11,
                    4,
                    73,
                    0
                ],
                "title": "Potential of large language model-powered nudges for promoting daily\n  water and energy conservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential of large language model-powered nudges for promoting daily\n  water and energy conservation"
                },
                "summary": "The increasing amount of pressure related to water and energy shortages has\nincreased the urgency of cultivating individual conservation behaviors. While\nthe concept of nudging, i.e., providing usage-based feedback, has shown promise\nin encouraging conservation behaviors, its efficacy is often constrained by the\nlack of targeted and actionable content. This study investigates the impact of\nthe use of large language models (LLMs) to provide tailored conservation\nsuggestions for conservation intentions and their rationale. Through a survey\nexperiment with 1,515 university participants, we compare three virtual nudging\nscenarios: no nudging, traditional nudging with usage statistics, and\nLLM-powered nudging with usage statistics and personalized conservation\nsuggestions. The results of statistical analyses and causal forest modeling\nreveal that nudging led to an increase in conservation intentions among\n86.9%-98.0% of the participants. LLM-powered nudging achieved a maximum\nincrease of 18.0% in conservation intentions, surpassing traditional nudging by\n88.6%. Furthermore, structural equation modeling results reveal that exposure\nto LLM-powered nudges enhances self-efficacy and outcome expectations while\ndiminishing dependence on social norms, thereby increasing intrinsic motivation\nto conserve. These findings highlight the transformative potential of LLMs in\npromoting individual water and energy conservation, representing a new frontier\nin the design of sustainable behavioral interventions and resource management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing amount of pressure related to water and energy shortages has\nincreased the urgency of cultivating individual conservation behaviors. While\nthe concept of nudging, i.e., providing usage-based feedback, has shown promise\nin encouraging conservation behaviors, its efficacy is often constrained by the\nlack of targeted and actionable content. This study investigates the impact of\nthe use of large language models (LLMs) to provide tailored conservation\nsuggestions for conservation intentions and their rationale. Through a survey\nexperiment with 1,515 university participants, we compare three virtual nudging\nscenarios: no nudging, traditional nudging with usage statistics, and\nLLM-powered nudging with usage statistics and personalized conservation\nsuggestions. The results of statistical analyses and causal forest modeling\nreveal that nudging led to an increase in conservation intentions among\n86.9%-98.0% of the participants. LLM-powered nudging achieved a maximum\nincrease of 18.0% in conservation intentions, surpassing traditional nudging by\n88.6%. Furthermore, structural equation modeling results reveal that exposure\nto LLM-powered nudges enhances self-efficacy and outcome expectations while\ndiminishing dependence on social norms, thereby increasing intrinsic motivation\nto conserve. These findings highlight the transformative potential of LLMs in\npromoting individual water and energy conservation, representing a new frontier\nin the design of sustainable behavioral interventions and resource management."
                },
                "authors": [
                    {
                        "name": "Zonghan Li"
                    },
                    {
                        "name": "Song Tong"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Kaiping Peng"
                    },
                    {
                        "name": "Chunyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chunyan Wang"
                },
                "author": "Chunyan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11513v1",
                "updated": "2025-03-14T15:36:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    36,
                    39,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:36:39Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    36,
                    39,
                    4,
                    73,
                    0
                ],
                "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation\n  with Autoregressive Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation\n  with Autoregressive Large Language Models"
                },
                "summary": "Text-to-video generation poses significant challenges due to the inherent\ncomplexity of video data, which spans both temporal and spatial dimensions. It\nintroduces additional redundancy, abrupt variations, and a domain gap between\nlanguage and vision tokens while generation. Addressing these challenges\nrequires an effective video tokenizer that can efficiently encode video data\nwhile preserving essential semantic and spatiotemporal information, serving as\na critical bridge between text and vision. Inspired by the observation in\nVQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for\ntext-to-video generation with hierarchical tokenizers. It utilizes a 3D causal\nVAE with a multi-layer discrete token framework, encoding video content into\nhierarchically structured codebooks. Higher layers capture semantic information\nwith higher compression, while lower layers focus on fine-grained\nspatiotemporal details, striking a balance between compression efficiency and\nreconstruction quality. Our approach efficiently encodes longer video sequences\n(e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately\n70\\% compared to baseline tokenizers, while maintaining competitive\nreconstruction quality. We explore the trade-offs between compression and\nreconstruction, while emphasizing the advantages of high-compressed semantic\ntokens in text-to-video tasks. HiTVideo aims to address the potential\nlimitations of existing video tokenizers in text-to-video generation tasks,\nstriving for higher compression ratios and simplify LLMs modeling under\nlanguage guidance, offering a scalable and promising framework for advancing\ntext to video generation. Demo page:\nhttps://ziqinzhou66.github.io/project/HiTVideo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-video generation poses significant challenges due to the inherent\ncomplexity of video data, which spans both temporal and spatial dimensions. It\nintroduces additional redundancy, abrupt variations, and a domain gap between\nlanguage and vision tokens while generation. Addressing these challenges\nrequires an effective video tokenizer that can efficiently encode video data\nwhile preserving essential semantic and spatiotemporal information, serving as\na critical bridge between text and vision. Inspired by the observation in\nVQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for\ntext-to-video generation with hierarchical tokenizers. It utilizes a 3D causal\nVAE with a multi-layer discrete token framework, encoding video content into\nhierarchically structured codebooks. Higher layers capture semantic information\nwith higher compression, while lower layers focus on fine-grained\nspatiotemporal details, striking a balance between compression efficiency and\nreconstruction quality. Our approach efficiently encodes longer video sequences\n(e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately\n70\\% compared to baseline tokenizers, while maintaining competitive\nreconstruction quality. We explore the trade-offs between compression and\nreconstruction, while emphasizing the advantages of high-compressed semantic\ntokens in text-to-video tasks. HiTVideo aims to address the potential\nlimitations of existing video tokenizers in text-to-video generation tasks,\nstriving for higher compression ratios and simplify LLMs modeling under\nlanguage guidance, offering a scalable and promising framework for advancing\ntext to video generation. Demo page:\nhttps://ziqinzhou66.github.io/project/HiTVideo."
                },
                "authors": [
                    {
                        "name": "Ziqin Zhou"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Houwen Peng"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lingqiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingqiao Liu"
                },
                "author": "Lingqiao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11504v1",
                "updated": "2025-03-14T15:28:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    28,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:28:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    28,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Multi-agent coordination for on-demand data gathering with periodic\n  information upload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent coordination for on-demand data gathering with periodic\n  information upload"
                },
                "summary": "In this paper we develop a method for planning and coordinating a multi-agent\nteam deployment to periodically gather information on demand. A static\noperation center (OC) periodically requests information from changing goal\nlocations. The objective is to gather data in the goals and to deliver it to\nthe OC, balancing the refreshing time and the total number of information\npackages. The system automatically splits the team in two roles: workers to\ngather data, or collectors to retransmit the data to the OC. The proposed three\nstep method: 1) finds out the best area partition for the workers; 2) obtains\nthe best balance between workers and collectors, and with whom the workers must\nto communicate, a collector or the OC; 3) computes the best tour for the\nworkers to visit the goals and deliver them to the OC or to a collector in\nmovement. The method is tested in simulations in different scenarios, providing\nthe best area partition algorithm and the best balance between collectors and\nworkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we develop a method for planning and coordinating a multi-agent\nteam deployment to periodically gather information on demand. A static\noperation center (OC) periodically requests information from changing goal\nlocations. The objective is to gather data in the goals and to deliver it to\nthe OC, balancing the refreshing time and the total number of information\npackages. The system automatically splits the team in two roles: workers to\ngather data, or collectors to retransmit the data to the OC. The proposed three\nstep method: 1) finds out the best area partition for the workers; 2) obtains\nthe best balance between workers and collectors, and with whom the workers must\nto communicate, a collector or the OC; 3) computes the best tour for the\nworkers to visit the goals and deliver them to the OC or to a collector in\nmovement. The method is tested in simulations in different scenarios, providing\nthe best area partition algorithm and the best balance between collectors and\nworkers."
                },
                "authors": [
                    {
                        "name": "Yaroslav Marchukov"
                    },
                    {
                        "name": "Luis Montano"
                    }
                ],
                "author_detail": {
                    "name": "Luis Montano"
                },
                "author": "Luis Montano",
                "arxiv_doi": "10.1007/978-3-030-24209-1_13",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-24209-1_13",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.11504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06639v2",
                "updated": "2025-03-14T15:25:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    25,
                    46,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-09T14:36:45Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    14,
                    36,
                    45,
                    6,
                    68,
                    0
                ],
                "title": "Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss,\n  Dynamics, and Success Amplification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss,\n  Dynamics, and Success Amplification"
                },
                "summary": "Group Relative Policy Optimization (GRPO) was introduced and used\nsuccessfully to train DeepSeek R1 models for promoting reasoning capabilities\nof LLMs using verifiable or binary rewards. We show in this paper that GRPO\nwith verifiable rewards can be written as a Kullback Leibler ($\\mathsf{KL}$)\nregularized contrastive loss, where the contrastive samples are synthetic data\nsampled from the old policy. The optimal GRPO policy $\\pi_{n}$ can be expressed\nexplicitly in terms of the binary reward, as well as the first and second order\nstatistics of the old policy ($\\pi_{n-1}$) and the reference policy $\\pi_0$.\nIterating this scheme, we obtain a sequence of policies $\\pi_{n}$ for which we\ncan quantify the probability of success $p_n$. We show that the probability of\nsuccess of the policy satisfies a recurrence that converges to a fixed point of\na function that depends on the initial probability of success $p_0$ and the\nregularization parameter $\\beta$ of the $\\mathsf{KL}$ regularizer. We show that\nthe fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby\ndemonstrating that GRPO effectively amplifies the probability of success of the\npolicy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Relative Policy Optimization (GRPO) was introduced and used\nsuccessfully to train DeepSeek R1 models for promoting reasoning capabilities\nof LLMs using verifiable or binary rewards. We show in this paper that GRPO\nwith verifiable rewards can be written as a Kullback Leibler ($\\mathsf{KL}$)\nregularized contrastive loss, where the contrastive samples are synthetic data\nsampled from the old policy. The optimal GRPO policy $\\pi_{n}$ can be expressed\nexplicitly in terms of the binary reward, as well as the first and second order\nstatistics of the old policy ($\\pi_{n-1}$) and the reference policy $\\pi_0$.\nIterating this scheme, we obtain a sequence of policies $\\pi_{n}$ for which we\ncan quantify the probability of success $p_n$. We show that the probability of\nsuccess of the policy satisfies a recurrence that converges to a fixed point of\na function that depends on the initial probability of success $p_0$ and the\nregularization parameter $\\beta$ of the $\\mathsf{KL}$ regularizer. We show that\nthe fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby\ndemonstrating that GRPO effectively amplifies the probability of success of the\npolicy."
                },
                "authors": [
                    {
                        "name": "Youssef Mroueh"
                    }
                ],
                "author_detail": {
                    "name": "Youssef Mroueh"
                },
                "author": "Youssef Mroueh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11495v1",
                "updated": "2025-03-14T15:21:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    21,
                    44,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:21:44Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    21,
                    44,
                    4,
                    73,
                    0
                ],
                "title": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning"
                },
                "summary": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning."
                },
                "authors": [
                    {
                        "name": "Zixu Cheng"
                    },
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shaogang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Shaogang Gong"
                },
                "author": "Shaogang Gong",
                "arxiv_comment": "A benchmark for Video Spatio-Temporal Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11486v1",
                "updated": "2025-03-14T15:11:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    11,
                    29,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T15:11:29Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    15,
                    11,
                    29,
                    4,
                    73,
                    0
                ],
                "title": "A Review of DeepSeek Models' Key Innovative Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of DeepSeek Models' Key Innovative Techniques"
                },
                "summary": "DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models\n(LLMs) for general-purpose tasks and reasoning, achieving performance\ncomparable to state-of-the-art closed-source models from companies like OpenAI\nand Anthropic -- while requiring only a fraction of their training costs.\nUnderstanding the key innovative techniques behind DeepSeek's success is\ncrucial for advancing LLM research. In this paper, we review the core\ntechniques driving the remarkable effectiveness and efficiency of these models,\nincluding refinements to the transformer architecture, innovations such as\nMulti-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the\nco-design of algorithms, frameworks, and hardware, the Group Relative Policy\nOptimization algorithm, post-training with pure reinforcement learning and\niterative training alternating between supervised fine-tuning and reinforcement\nlearning. Additionally, we identify several open questions and highlight\npotential research opportunities in this rapidly advancing field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models\n(LLMs) for general-purpose tasks and reasoning, achieving performance\ncomparable to state-of-the-art closed-source models from companies like OpenAI\nand Anthropic -- while requiring only a fraction of their training costs.\nUnderstanding the key innovative techniques behind DeepSeek's success is\ncrucial for advancing LLM research. In this paper, we review the core\ntechniques driving the remarkable effectiveness and efficiency of these models,\nincluding refinements to the transformer architecture, innovations such as\nMulti-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the\nco-design of algorithms, frameworks, and hardware, the Group Relative Policy\nOptimization algorithm, post-training with pure reinforcement learning and\niterative training alternating between supervised fine-tuning and reinforcement\nlearning. Additionally, we identify several open questions and highlight\npotential research opportunities in this rapidly advancing field."
                },
                "authors": [
                    {
                        "name": "Chengen Wang"
                    },
                    {
                        "name": "Murat Kantarcioglu"
                    }
                ],
                "author_detail": {
                    "name": "Murat Kantarcioglu"
                },
                "author": "Murat Kantarcioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11458v1",
                "updated": "2025-03-14T14:47:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    4,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:04Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    4,
                    4,
                    73,
                    0
                ],
                "title": "Integrating LLMs in Gamified Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs in Gamified Systems"
                },
                "summary": "In this work, a thorough mathematical framework for incorporating Large\nLanguage Models (LLMs) into gamified systems is presented with an emphasis on\nimproving task dynamics, user engagement, and reward systems. Personalized\nfeedback, adaptive learning, and dynamic content creation are all made possible\nby integrating LLMs and are crucial for improving user engagement and system\nperformance. A simulated environment tests the framework's adaptability and\ndemonstrates its potential for real-world applications in various industries,\nincluding business, healthcare, and education. The findings demonstrate how\nLLMs can offer customized experiences that raise system effectiveness and user\nretention. This study also examines the difficulties this framework aims to\nsolve, highlighting its importance in maximizing involvement and encouraging\nsustained behavioral change in a range of sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a thorough mathematical framework for incorporating Large\nLanguage Models (LLMs) into gamified systems is presented with an emphasis on\nimproving task dynamics, user engagement, and reward systems. Personalized\nfeedback, adaptive learning, and dynamic content creation are all made possible\nby integrating LLMs and are crucial for improving user engagement and system\nperformance. A simulated environment tests the framework's adaptability and\ndemonstrates its potential for real-world applications in various industries,\nincluding business, healthcare, and education. The findings demonstrate how\nLLMs can offer customized experiences that raise system effectiveness and user\nretention. This study also examines the difficulties this framework aims to\nsolve, highlighting its importance in maximizing involvement and encouraging\nsustained behavioral change in a range of sectors."
                },
                "authors": [
                    {
                        "name": "Carlos J. Costa"
                    }
                ],
                "author_detail": {
                    "name": "Carlos J. Costa"
                },
                "author": "Carlos J. Costa",
                "arxiv_comment": "9 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11449v1",
                "updated": "2025-03-14T14:34:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    34,
                    36,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:34:36Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    34,
                    36,
                    4,
                    73,
                    0
                ],
                "title": "Optimizing 6G Dense Network Deployment for the Metaverse Using Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing 6G Dense Network Deployment for the Metaverse Using Deep\n  Reinforcement Learning"
                },
                "summary": "As the Metaverse envisions deeply immersive and pervasive connectivity in 6G\nnetworks, Integrated Access and Backhaul (IAB) emerges as a critical enabler to\nmeet the demanding requirements of massive and immersive communications. IAB\nnetworks offer a scalable solution for expanding broadband coverage in urban\nenvironments. However, optimizing IAB node deployment to ensure reliable\ncoverage while minimizing costs remains challenging due to location constraints\nand the dynamic nature of cities. Existing heuristic methods, such as Greedy\nAlgorithms, have been employed to address these optimization problems. This\nwork presents a novel Deep Reinforcement Learning ( DRL) approach for IAB\nnetwork planning, tailored to future 6G scenarios that seek to support\nultra-high data rates and dense device connectivity required by immersive\nMetaverse applications. We utilize Deep Q-Network (DQN) with action elimination\nand integrate DQN, Double Deep Q-Network ( DDQN), and Dueling DQN architectures\nto effectively manage large state and action spaces. Simulations with various\ninitial donor configurations demonstrate the effectiveness of our DRL approach,\nwith Dueling DQN reducing node count by an average of 12.3% compared to\ntraditional heuristics. The study underscores how advanced DRL techniques can\naddress complex network planning challenges in 6G-enabled Metaverse contexts,\nproviding an efficient and adaptive solution for IAB deployment in diverse\nurban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the Metaverse envisions deeply immersive and pervasive connectivity in 6G\nnetworks, Integrated Access and Backhaul (IAB) emerges as a critical enabler to\nmeet the demanding requirements of massive and immersive communications. IAB\nnetworks offer a scalable solution for expanding broadband coverage in urban\nenvironments. However, optimizing IAB node deployment to ensure reliable\ncoverage while minimizing costs remains challenging due to location constraints\nand the dynamic nature of cities. Existing heuristic methods, such as Greedy\nAlgorithms, have been employed to address these optimization problems. This\nwork presents a novel Deep Reinforcement Learning ( DRL) approach for IAB\nnetwork planning, tailored to future 6G scenarios that seek to support\nultra-high data rates and dense device connectivity required by immersive\nMetaverse applications. We utilize Deep Q-Network (DQN) with action elimination\nand integrate DQN, Double Deep Q-Network ( DDQN), and Dueling DQN architectures\nto effectively manage large state and action spaces. Simulations with various\ninitial donor configurations demonstrate the effectiveness of our DRL approach,\nwith Dueling DQN reducing node count by an average of 12.3% compared to\ntraditional heuristics. The study underscores how advanced DRL techniques can\naddress complex network planning challenges in 6G-enabled Metaverse contexts,\nproviding an efficient and adaptive solution for IAB deployment in diverse\nurban environments."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Swarna Chetty"
                    },
                    {
                        "name": "Qiao Wang"
                    },
                    {
                        "name": "Chenrui Sun"
                    },
                    {
                        "name": "Paul Daniel Mitchell"
                    },
                    {
                        "name": "David Grace"
                    },
                    {
                        "name": "Hamed Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Ahmadi"
                },
                "author": "Hamed Ahmadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11444v1",
                "updated": "2025-03-14T14:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    29,
                    17,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:29:17Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    29,
                    17,
                    4,
                    73,
                    0
                ],
                "title": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery"
                },
                "summary": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https://app.aios.foundation,\nthe code is at https://github.com/agiresearch/Cerebrum, and video is at\nhttps://app.aios.foundation/video-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https://app.aios.foundation,\nthe code is at https://github.com/agiresearch/Cerebrum, and video is at\nhttps://app.aios.foundation/video-demo."
                },
                "authors": [
                    {
                        "name": "Balaji Rama"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "Accepted to the 2025 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL) - System\n  Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11441v1",
                "updated": "2025-03-14T14:28:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    28,
                    19,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:28:19Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    28,
                    19,
                    4,
                    73,
                    0
                ],
                "title": "D3: Diversity, Difficulty, and Dependability-Aware Data Selection for\n  Sample-Efficient LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D3: Diversity, Difficulty, and Dependability-Aware Data Selection for\n  Sample-Efficient LLM Instruction Tuning"
                },
                "summary": "Recent advancements in instruction tuning for large language models (LLMs)\nsuggest that a small, high-quality dataset can significantly equip LLMs with\ninstruction-following capabilities, outperforming large datasets often burdened\nby quality and redundancy issues. However, the challenge lies in automatically\nidentifying valuable subsets from large datasets to boost both the\neffectiveness and efficiency of instruction tuning. In this paper, we first\nestablish data selection criteria based on three distinct aspects of data\nvalue: diversity, difficulty, and dependability, and then propose the D3 method\ncomprising two key steps of scoring and selection. Specifically, in the scoring\nstep, we define the diversity function to measure sample distinctiveness and\nintroduce the uncertainty-based prediction difficulty to evaluate sample\ndifficulty by mitigating the interference of context-oriented generation\ndiversity. Additionally, we integrate an external LLM for dependability\nassessment. In the selection step, we formulate the D3 weighted coreset\nobjective, which jointly optimizes three aspects of data value to solve for the\nmost valuable subset. The two steps of D3 can iterate multiple rounds,\nincorporating feedback to refine the selection focus adaptively. Experiments on\nthree datasets demonstrate the effectiveness of D3 in endowing LLMs with\ncompetitive or even superior instruction-following capabilities using less than\n10% of the entire dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in instruction tuning for large language models (LLMs)\nsuggest that a small, high-quality dataset can significantly equip LLMs with\ninstruction-following capabilities, outperforming large datasets often burdened\nby quality and redundancy issues. However, the challenge lies in automatically\nidentifying valuable subsets from large datasets to boost both the\neffectiveness and efficiency of instruction tuning. In this paper, we first\nestablish data selection criteria based on three distinct aspects of data\nvalue: diversity, difficulty, and dependability, and then propose the D3 method\ncomprising two key steps of scoring and selection. Specifically, in the scoring\nstep, we define the diversity function to measure sample distinctiveness and\nintroduce the uncertainty-based prediction difficulty to evaluate sample\ndifficulty by mitigating the interference of context-oriented generation\ndiversity. Additionally, we integrate an external LLM for dependability\nassessment. In the selection step, we formulate the D3 weighted coreset\nobjective, which jointly optimizes three aspects of data value to solve for the\nmost valuable subset. The two steps of D3 can iterate multiple rounds,\nincorporating feedback to refine the selection focus adaptively. Experiments on\nthree datasets demonstrate the effectiveness of D3 in endowing LLMs with\ncompetitive or even superior instruction-following capabilities using less than\n10% of the entire dataset."
                },
                "authors": [
                    {
                        "name": "Jia Zhang"
                    },
                    {
                        "name": "Chen-Xi Zhang"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Yi-Xuan Jin"
                    },
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11431v1",
                "updated": "2025-03-14T14:21:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    21,
                    8,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:21:08Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    21,
                    8,
                    4,
                    73,
                    0
                ],
                "title": "High-rate discrete-modulated continuous-variable quantum key\n  distribution with composable security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-rate discrete-modulated continuous-variable quantum key\n  distribution with composable security"
                },
                "summary": "Continuous-variable quantum key distribution holds the potential to generate\nhigh secret key rates, making it a prime candidate for high-rate metropolitan\nquantum network applications. However, despite these promising opportunities,\nthe realization of high-rate continuous-variable quantum key distribution\nsystems with composable security remains an elusive goal. Here, we report a\ndiscrete-modulated continuous-variable quantum key distribution system with a\ncomposable secret key rate of 18.93 Mbps against collective attacks over a 25\nkm fiber channel. This record-breaking rate is achieved through the probability\nshaped 16QAM-modulated protocol, which employs semidefinite programming to\nensure its composable security. Furthermore, we have employed a fully digital\nand precise quantum signal processing technique to reduce excess noise to\nextremely low levels, thereby facilitating efficient broadband system\noperation. While ensuring low complexity and cost, our system achieves a\nperformance advantage of over an order of magnitude compared to previous\ncontinuous-variable quantum key distribution systems, providing a promising\nsolution for future deployment of quantum key distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-variable quantum key distribution holds the potential to generate\nhigh secret key rates, making it a prime candidate for high-rate metropolitan\nquantum network applications. However, despite these promising opportunities,\nthe realization of high-rate continuous-variable quantum key distribution\nsystems with composable security remains an elusive goal. Here, we report a\ndiscrete-modulated continuous-variable quantum key distribution system with a\ncomposable secret key rate of 18.93 Mbps against collective attacks over a 25\nkm fiber channel. This record-breaking rate is achieved through the probability\nshaped 16QAM-modulated protocol, which employs semidefinite programming to\nensure its composable security. Furthermore, we have employed a fully digital\nand precise quantum signal processing technique to reduce excess noise to\nextremely low levels, thereby facilitating efficient broadband system\noperation. While ensuring low complexity and cost, our system achieves a\nperformance advantage of over an order of magnitude compared to previous\ncontinuous-variable quantum key distribution systems, providing a promising\nsolution for future deployment of quantum key distribution."
                },
                "authors": [
                    {
                        "name": "Mingze Wu"
                    },
                    {
                        "name": "Yan Pan"
                    },
                    {
                        "name": "Junhui Li"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Yun Shao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Bingjie Xu"
                    },
                    {
                        "name": "Yichen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Zhang"
                },
                "author": "Yichen Zhang",
                "arxiv_comment": "12 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19209v3",
                "updated": "2025-03-14T13:57:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    57,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2024-05-29T15:49:09Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    15,
                    49,
                    9,
                    2,
                    150,
                    0
                ],
                "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos"
                },
                "summary": "Long-form video understanding is complicated by the high redundancy of video\ndata and the abundance of query-irrelevant information. To tackle these\nchallenges, we propose VideoTree, a training-free framework which builds a\nquery-adaptive and hierarchical video representation for LLM reasoning over\nlong-form videos. First, VideoTree extracts query-relevant information from the\ninput video through an iterative process, progressively refining the selection\nof keyframes based on their relevance to the query. Furthermore, VideoTree\nleverages the inherent hierarchical structure of long video data, which is\noften overlooked by existing LLM-based methods. Specifically, we incorporate\nmulti-granularity information into a tree-based representation, allowing\nVideoTree to extract query-relevant details from long videos in a\ncoarse-to-fine manner. This enables the model to effectively handle a wide\nrange of video queries with varying levels of detail. Finally, VideoTree\naggregates the hierarchical query-relevant information within the tree\nstructure and feeds it into an LLM reasoning model to answer the query. Our\nexperiments show that our method improves both reasoning accuracy and\nefficiency. Specifically, VideoTree outperforms existing training-free\napproaches on EgoSchema and NExT-QA with less inference time, achieving 61.1%\nand 75.6% accuracy on the test set without additional video-specific training.\nMoreover, on the long split of Video-MME (average 44 minutes), VideoTree\nachieves better performance than GPT-4V and many other MLLMs that were\nextensively trained on video data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding is complicated by the high redundancy of video\ndata and the abundance of query-irrelevant information. To tackle these\nchallenges, we propose VideoTree, a training-free framework which builds a\nquery-adaptive and hierarchical video representation for LLM reasoning over\nlong-form videos. First, VideoTree extracts query-relevant information from the\ninput video through an iterative process, progressively refining the selection\nof keyframes based on their relevance to the query. Furthermore, VideoTree\nleverages the inherent hierarchical structure of long video data, which is\noften overlooked by existing LLM-based methods. Specifically, we incorporate\nmulti-granularity information into a tree-based representation, allowing\nVideoTree to extract query-relevant details from long videos in a\ncoarse-to-fine manner. This enables the model to effectively handle a wide\nrange of video queries with varying levels of detail. Finally, VideoTree\naggregates the hierarchical query-relevant information within the tree\nstructure and feeds it into an LLM reasoning model to answer the query. Our\nexperiments show that our method improves both reasoning accuracy and\nefficiency. Specifically, VideoTree outperforms existing training-free\napproaches on EgoSchema and NExT-QA with less inference time, achieving 61.1%\nand 75.6% accuracy on the test set without additional video-specific training.\nMoreover, on the long split of Video-MME (average 44 minutes), VideoTree\nachieves better performance than GPT-4V and many other MLLMs that were\nextensively trained on video data."
                },
                "authors": [
                    {
                        "name": "Ziyang Wang"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "CVPR 2025; First three authors contributed equally; Project page:\n  https://videotree2024.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05788v2",
                "updated": "2025-03-14T13:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    28,
                    4,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-28T01:20:01Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    1,
                    20,
                    1,
                    4,
                    59,
                    0
                ],
                "title": "Emergent Abilities in Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Abilities in Large Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight."
                },
                "authors": [
                    {
                        "name": "Leonardo Berti"
                    },
                    {
                        "name": "Flavio Giorgi"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11384v1",
                "updated": "2025-03-14T13:27:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    27,
                    35,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:27:35Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    27,
                    35,
                    4,
                    73,
                    0
                ],
                "title": "Optimizing Large Language Models for Detecting Symptoms of Comorbid\n  Depression or Anxiety in Chronic Diseases: Insights from Patient Messages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Models for Detecting Symptoms of Comorbid\n  Depression or Anxiety in Chronic Diseases: Insights from Patient Messages"
                },
                "summary": "Patients with diabetes are at increased risk of comorbid depression or\nanxiety, complicating their management. This study evaluated the performance of\nlarge language models (LLMs) in detecting these symptoms from secure patient\nmessages. We applied multiple approaches, including engineered prompts,\nsystemic persona, temperature adjustments, and zero-shot and few-shot learning,\nto identify the best-performing model and enhance performance. Three out of\nfive LLMs demonstrated excellent performance (over 90% of F-1 and accuracy),\nwith Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot\napproach. While LLMs showed promise in binary classification and handling\ncomplex metrics like Patient Health Questionnaire-4, inconsistencies in\nchallenging cases warrant further real-life assessment. The findings highlight\nthe potential of LLMs to assist in timely screening and referrals, providing\nvaluable empirical knowledge for real-world triage systems that could improve\nmental health care for patients with chronic diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patients with diabetes are at increased risk of comorbid depression or\nanxiety, complicating their management. This study evaluated the performance of\nlarge language models (LLMs) in detecting these symptoms from secure patient\nmessages. We applied multiple approaches, including engineered prompts,\nsystemic persona, temperature adjustments, and zero-shot and few-shot learning,\nto identify the best-performing model and enhance performance. Three out of\nfive LLMs demonstrated excellent performance (over 90% of F-1 and accuracy),\nwith Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot\napproach. While LLMs showed promise in binary classification and handling\ncomplex metrics like Patient Health Questionnaire-4, inconsistencies in\nchallenging cases warrant further real-life assessment. The findings highlight\nthe potential of LLMs to assist in timely screening and referrals, providing\nvaluable empirical knowledge for real-world triage systems that could improve\nmental health care for patients with chronic diseases."
                },
                "authors": [
                    {
                        "name": "Jiyeong Kim"
                    },
                    {
                        "name": "Stephen P. Ma"
                    },
                    {
                        "name": "Michael L. Chen"
                    },
                    {
                        "name": "Isaac R. Galatzer-Levy"
                    },
                    {
                        "name": "John Torous"
                    },
                    {
                        "name": "Peter J. van Roessel"
                    },
                    {
                        "name": "Christopher Sharp"
                    },
                    {
                        "name": "Michael A. Pfeffer"
                    },
                    {
                        "name": "Carolyn I. Rodriguez"
                    },
                    {
                        "name": "Eleni Linos"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan H. Chen"
                },
                "author": "Jonathan H. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11381v1",
                "updated": "2025-03-14T13:25:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    25,
                    41,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:25:41Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    25,
                    41,
                    4,
                    73,
                    0
                ],
                "title": "Modeling Subjectivity in Cognitive Appraisal with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Subjectivity in Cognitive Appraisal with Language Models"
                },
                "summary": "As the utilization of language models in interdisciplinary, human-centered\nstudies grow, the expectation of model capabilities continues to evolve. Beyond\nexcelling at conventional tasks, models are recently expected to perform well\non user-centric measurements involving confidence and human (dis)agreement --\nfactors that reflect subjective preferences. While modeling of subjectivity\nplays an essential role in cognitive science and has been extensively studied,\nit remains under-explored within the NLP community. In light of this gap, we\nexplore how language models can harness subjectivity by conducting\ncomprehensive experiments and analysis across various scenarios using both\nfine-tuned models and prompt-based large language models (LLMs). Our\nquantitative and qualitative experimental results indicate that existing\npost-hoc calibration approaches often fail to produce satisfactory results.\nHowever, our findings reveal that personality traits and demographical\ninformation are critical for measuring subjectivity. Furthermore, our in-depth\nanalysis offers valuable insights for future research and development in the\ninterdisciplinary studies of NLP and cognitive science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the utilization of language models in interdisciplinary, human-centered\nstudies grow, the expectation of model capabilities continues to evolve. Beyond\nexcelling at conventional tasks, models are recently expected to perform well\non user-centric measurements involving confidence and human (dis)agreement --\nfactors that reflect subjective preferences. While modeling of subjectivity\nplays an essential role in cognitive science and has been extensively studied,\nit remains under-explored within the NLP community. In light of this gap, we\nexplore how language models can harness subjectivity by conducting\ncomprehensive experiments and analysis across various scenarios using both\nfine-tuned models and prompt-based large language models (LLMs). Our\nquantitative and qualitative experimental results indicate that existing\npost-hoc calibration approaches often fail to produce satisfactory results.\nHowever, our findings reveal that personality traits and demographical\ninformation are critical for measuring subjectivity. Furthermore, our in-depth\nanalysis offers valuable insights for future research and development in the\ninterdisciplinary studies of NLP and cognitive science."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Hainiu Xu"
                    },
                    {
                        "name": "Desmond C. Ong"
                    },
                    {
                        "name": "Petr Slovak"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11376v1",
                "updated": "2025-03-14T13:21:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    21,
                    59,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:21:59Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    21,
                    59,
                    4,
                    73,
                    0
                ],
                "title": "Annotating Scientific Uncertainty: A comprehensive model using\n  linguistic patterns and comparison with existing approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating Scientific Uncertainty: A comprehensive model using\n  linguistic patterns and comparison with existing approaches"
                },
                "summary": "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages."
                },
                "authors": [
                    {
                        "name": "Panggih Kusuma Ningrum"
                    },
                    {
                        "name": "Philipp Mayr"
                    },
                    {
                        "name": "Nina Smirnova"
                    },
                    {
                        "name": "Iana Atanassova"
                    }
                ],
                "author_detail": {
                    "name": "Iana Atanassova"
                },
                "author": "Iana Atanassova",
                "arxiv_comment": "Paper Accepted for Publication in the Journal of Informetrics (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12486v2",
                "updated": "2025-03-14T13:13:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    13,
                    13,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-18T03:15:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    15,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20941v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20941v3",
                "updated": "2025-03-14T13:12:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    12,
                    38,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-28T11:49:58Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    49,
                    58,
                    0,
                    302,
                    0
                ],
                "title": "Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine\n  Translation"
                },
                "summary": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and the outputs from GPT4-as-a-judge are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and the outputs from GPT4-as-a-judge are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT"
                },
                "authors": [
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "Accepted at NAACL 2025 Student Research Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20941v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20941v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11367v1",
                "updated": "2025-03-14T13:07:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T13:07:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "title": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware"
                },
                "summary": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput."
                },
                "authors": [
                    {
                        "name": "Insu Jang"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Nikhil Bansal"
                    },
                    {
                        "name": "Ang Chen"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07263v2",
                "updated": "2025-03-14T13:03:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    3,
                    24,
                    4,
                    73,
                    0
                ],
                "published": "2023-10-11T07:39:42Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    39,
                    42,
                    2,
                    284,
                    0
                ],
                "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPAL: Corrective Planning of Robot Actions with Large Language Models"
                },
                "summary": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation."
                },
                "authors": [
                    {
                        "name": "Frank Joublin"
                    },
                    {
                        "name": "Antonello Ceravola"
                    },
                    {
                        "name": "Pavel Smirnov"
                    },
                    {
                        "name": "Felix Ocker"
                    },
                    {
                        "name": "Joerg Deigmoeller"
                    },
                    {
                        "name": "Anna Belardinelli"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Stephan Hasler"
                    },
                    {
                        "name": "Daniel Tanneberg"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "arxiv_comment": "IEEE International Conference on Robotics and Automation (ICRA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11348v1",
                "updated": "2025-03-14T12:32:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    32,
                    40,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:32:40Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    32,
                    40,
                    4,
                    73,
                    0
                ],
                "title": "RESPONSE: Benchmarking the Ability of Language Models to Undertake\n  Commonsense Reasoning in Crisis Situation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RESPONSE: Benchmarking the Ability of Language Models to Undertake\n  Commonsense Reasoning in Crisis Situation"
                },
                "summary": "An interesting class of commonsense reasoning problems arises when people are\nfaced with natural disasters. To investigate this topic, we present\n\\textsf{RESPONSE}, a human-curated dataset containing 1789 annotated instances\nfeaturing 6037 sets of questions designed to assess LLMs' commonsense reasoning\nin disaster situations across different time frames. The dataset includes\nproblem descriptions, missing resources, time-sensitive solutions, and their\njustifications, with a subset validated by environmental engineers. Through\nboth automatic metrics and human evaluation, we compare LLM-generated\nrecommendations against human responses. Our findings show that even\nstate-of-the-art models like GPT-4 achieve only 37\\% human-evaluated\ncorrectness for immediate response actions, highlighting significant room for\nimprovement in LLMs' ability for commonsense reasoning in crises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An interesting class of commonsense reasoning problems arises when people are\nfaced with natural disasters. To investigate this topic, we present\n\\textsf{RESPONSE}, a human-curated dataset containing 1789 annotated instances\nfeaturing 6037 sets of questions designed to assess LLMs' commonsense reasoning\nin disaster situations across different time frames. The dataset includes\nproblem descriptions, missing resources, time-sensitive solutions, and their\njustifications, with a subset validated by environmental engineers. Through\nboth automatic metrics and human evaluation, we compare LLM-generated\nrecommendations against human responses. Our findings show that even\nstate-of-the-art models like GPT-4 achieve only 37\\% human-evaluated\ncorrectness for immediate response actions, highlighting significant room for\nimprovement in LLMs' ability for commonsense reasoning in crises."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11346v1",
                "updated": "2025-03-14T12:23:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    23,
                    45,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:23:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    23,
                    45,
                    4,
                    73,
                    0
                ],
                "title": "AIstorian lets AI be a historian: A KG-powered multi-agent system for\n  accurate biography generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIstorian lets AI be a historian: A KG-powered multi-agent system for\n  accurate biography generation"
                },
                "summary": "Huawei has always been committed to exploring the AI application in\nhistorical research. Biography generation, as a specialized form of abstractive\nsummarization, plays a crucial role in historical research but faces unique\nchallenges that existing large language models (LLMs) struggle to address.\nThese challenges include maintaining stylistic adherence to historical writing\nconventions, ensuring factual fidelity, and handling fragmented information\nacross multiple documents. We present AIstorian, a novel end-to-end agentic\nsystem featured with a knowledge graph (KG)-powered retrieval-augmented\ngeneration (RAG) and anti-hallucination multi-agents. Specifically, AIstorian\nintroduces an in-context learning based chunking strategy and a KG-based index\nfor accurate and efficient reference retrieval. Meanwhile, AIstorian\norchestrates multi-agents to conduct on-the-fly hallucination detection and\nerror-type-aware correction. Additionally, to teach LLMs a certain language\nstyle, we finetune LLMs based on a two-step training approach combining data\naugmentation-enhanced supervised fine-tuning with stylistic preference\noptimization. Extensive experiments on a real-life historical Jinshi dataset\ndemonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and\na 47.6% reduction in hallucination rate compared to existing baselines. The\ndata and code are available at: https://github.com/ZJU-DAILY/AIstorian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Huawei has always been committed to exploring the AI application in\nhistorical research. Biography generation, as a specialized form of abstractive\nsummarization, plays a crucial role in historical research but faces unique\nchallenges that existing large language models (LLMs) struggle to address.\nThese challenges include maintaining stylistic adherence to historical writing\nconventions, ensuring factual fidelity, and handling fragmented information\nacross multiple documents. We present AIstorian, a novel end-to-end agentic\nsystem featured with a knowledge graph (KG)-powered retrieval-augmented\ngeneration (RAG) and anti-hallucination multi-agents. Specifically, AIstorian\nintroduces an in-context learning based chunking strategy and a KG-based index\nfor accurate and efficient reference retrieval. Meanwhile, AIstorian\norchestrates multi-agents to conduct on-the-fly hallucination detection and\nerror-type-aware correction. Additionally, to teach LLMs a certain language\nstyle, we finetune LLMs based on a two-step training approach combining data\naugmentation-enhanced supervised fine-tuning with stylistic preference\noptimization. Extensive experiments on a real-life historical Jinshi dataset\ndemonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and\na 47.6% reduction in hallucination rate compared to existing baselines. The\ndata and code are available at: https://github.com/ZJU-DAILY/AIstorian."
                },
                "authors": [
                    {
                        "name": "Fengyu Li"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Junhao Zhu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Yanfei Zhang"
                    },
                    {
                        "name": "Jia Zhou"
                    },
                    {
                        "name": "Hui Zu"
                    },
                    {
                        "name": "Jingwen Zhao"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "arxiv_affiliation": "Zhejiang University",
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10351v2",
                "updated": "2025-03-14T12:09:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    9,
                    34,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-13T13:27:53Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    27,
                    53,
                    3,
                    72,
                    0
                ],
                "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Trends for Modern Machine Translation with Large Reasoning Models"
                },
                "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it."
                },
                "authors": [
                    {
                        "name": "Sinuo Liu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "Zifu Shang"
                    }
                ],
                "author_detail": {
                    "name": "Zifu Shang"
                },
                "author": "Zifu Shang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:1701.04715 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11336v1",
                "updated": "2025-03-14T12:05:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    5,
                    6,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T12:05:06Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    12,
                    5,
                    6,
                    4,
                    73,
                    0
                ],
                "title": "Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in\n  Large Language Models"
                },
                "summary": "In this paper, we introduce Rule-Guided Feedback (RGF), a framework designed\nto enhance Large Language Model (LLM) performance through structured rule\nadherence and strategic information seeking. RGF implements a teacher-student\nparadigm where rule-following is forced through established guidelines. Our\nframework employs a Teacher model that rigorously evaluates each student output\nagainst task-specific rules, providing constructive guidance rather than direct\nanswers when detecting deviations. This iterative feedback loop serves two\ncrucial purposes: maintaining solutions within defined constraints and\nencouraging proactive information seeking to resolve uncertainties. We evaluate\nRGF on diverse tasks including Checkmate-in-One puzzles, Sonnet Writing,\nPenguins-In-a-Table classification, GSM8k, and StrategyQA. Our findings suggest\nthat structured feedback mechanisms can significantly enhance LLMs' performance\nacross various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Rule-Guided Feedback (RGF), a framework designed\nto enhance Large Language Model (LLM) performance through structured rule\nadherence and strategic information seeking. RGF implements a teacher-student\nparadigm where rule-following is forced through established guidelines. Our\nframework employs a Teacher model that rigorously evaluates each student output\nagainst task-specific rules, providing constructive guidance rather than direct\nanswers when detecting deviations. This iterative feedback loop serves two\ncrucial purposes: maintaining solutions within defined constraints and\nencouraging proactive information seeking to resolve uncertainties. We evaluate\nRGF on diverse tasks including Checkmate-in-One puzzles, Sonnet Writing,\nPenguins-In-a-Table classification, GSM8k, and StrategyQA. Our findings suggest\nthat structured feedback mechanisms can significantly enhance LLMs' performance\nacross various domains."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04284v3",
                "updated": "2025-03-14T11:52:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    52,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2024-08-08T07:43:17Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    7,
                    43,
                    17,
                    3,
                    221,
                    0
                ],
                "title": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection"
                },
                "summary": "The ease of access to large language models (LLMs) has enabled a widespread\nof machine-generated texts, and now it is often hard to tell whether a piece of\ntext was human-written or machine-generated. This raises concerns about\npotential misuse, particularly within educational and academic domains. Thus,\nit is important to develop practical systems that can automate the process.\nHere, we present one such system, LLM-DetectAIve, designed for fine-grained\ndetection. Unlike most previous work on machine-generated text detection, which\nfocused on binary classification, LLM-DetectAIve supports four categories: (i)\nhuman-written, (ii) machine-generated, (iii) machine-written, then\nmachine-humanized, and (iv) human-written, then machine-polished. Category\n(iii) aims to detect attempts to obfuscate the fact that a text was\nmachine-generated, while category (iv) looks for cases where the LLM was used\nto polish a human-written text, which is typically acceptable in academic\nwriting, but not in education. Our experiments show that LLM-DetectAIve can\neffectively identify the above four categories, which makes it a potentially\nuseful tool in education, academia, and other domains.\n  LLM-DetectAIve is publicly accessible at\nhttps://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system\nis available at https://youtu.be/E8eT_bE7k8c.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ease of access to large language models (LLMs) has enabled a widespread\nof machine-generated texts, and now it is often hard to tell whether a piece of\ntext was human-written or machine-generated. This raises concerns about\npotential misuse, particularly within educational and academic domains. Thus,\nit is important to develop practical systems that can automate the process.\nHere, we present one such system, LLM-DetectAIve, designed for fine-grained\ndetection. Unlike most previous work on machine-generated text detection, which\nfocused on binary classification, LLM-DetectAIve supports four categories: (i)\nhuman-written, (ii) machine-generated, (iii) machine-written, then\nmachine-humanized, and (iv) human-written, then machine-polished. Category\n(iii) aims to detect attempts to obfuscate the fact that a text was\nmachine-generated, while category (iv) looks for cases where the LLM was used\nto polish a human-written text, which is typically acceptable in academic\nwriting, but not in education. Our experiments show that LLM-DetectAIve can\neffectively identify the above four categories, which makes it a potentially\nuseful tool in education, academia, and other domains.\n  LLM-DetectAIve is publicly accessible at\nhttps://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system\nis available at https://youtu.be/E8eT_bE7k8c."
                },
                "authors": [
                    {
                        "name": "Mervat Abassy"
                    },
                    {
                        "name": "Kareem Elozeiri"
                    },
                    {
                        "name": "Alexander Aziz"
                    },
                    {
                        "name": "Minh Ngoc Ta"
                    },
                    {
                        "name": "Raj Vardhan Tomar"
                    },
                    {
                        "name": "Bimarsha Adhikari"
                    },
                    {
                        "name": "Saad El Dine Ahmed"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Osama Mohammed Afzal"
                    },
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Jiahui Geng"
                    },
                    {
                        "name": "Hasan Iqbal"
                    },
                    {
                        "name": "Zain Muhammad Mujahid"
                    },
                    {
                        "name": "Tarek Mahmoud"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Artem Shelmanov"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11315v1",
                "updated": "2025-03-14T11:31:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    31,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:31:30Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    31,
                    30,
                    4,
                    73,
                    0
                ],
                "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with\n  Minimal Multimodal Speech Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with\n  Minimal Multimodal Speech Tokens"
                },
                "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early av-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.74% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early av-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.74% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%."
                },
                "authors": [
                    {
                        "name": "Jeong Hun Yeo"
                    },
                    {
                        "name": "Hyeongseop Rha"
                    },
                    {
                        "name": "Se Jin Park"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "The code and models are available\n  https://github.com/JeongHun0716/MMS-LLaMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11314v1",
                "updated": "2025-03-14T11:30:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    30,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:30:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    30,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large\n  Language Models via Representation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large\n  Language Models via Representation Engineering"
                },
                "summary": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios."
                },
                "authors": [
                    {
                        "name": "Xinyu Tang"
                    },
                    {
                        "name": "Xiaolei Wang"
                    },
                    {
                        "name": "Zhihao Lv"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Zhang"
                },
                "author": "Zhiqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11305v1",
                "updated": "2025-03-14T11:18:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    18,
                    47,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:18:47Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    18,
                    47,
                    4,
                    73,
                    0
                ],
                "title": "Lightweight Learning for Grant-Free Activity Detection in Cell-Free\n  Massive MIMO Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Learning for Grant-Free Activity Detection in Cell-Free\n  Massive MIMO Networks"
                },
                "summary": "Grant-free random access (GF-RA) is a promising access technique for massive\nmachine-type communications (mMTC) in future wireless networks, particularly in\nthe context of 5G and beyond (6G) systems. Within the context of GF-RA, this\nstudy investigates the efficiency of employing supervised machine learning\ntechniques to tackle the challenges on the device activity detection (AD).\nGF-RA addresses scalability by employing non-orthogonal pilot sequences, which\nprovides an efficient alternative comparing to conventional grant-based random\naccess (GB-RA) technique that are constrained by the scarcity of orthogonal\npreamble resources. In this paper, we propose a novel lightweight data-driven\nalgorithmic framework specifically designed for activity detection in GF-RA for\nmMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks.\nWe propose two distinct framework deployment strategies, centralized and\ndecentralized, both tailored to streamline the proposed approach implementation\nacross network infrastructures. Moreover, we introduce optimized post-detection\nmethodologies complemented by a clustering stage to enhance overall detection\nperformances. Our 3GPP-compliant simulations have validated that the proposed\nalgorithm achieves state-of-the-art model-based activity detection accuracy\nwhile significantly reducing complexity. Achieving 99% accuracy, it\ndemonstrates real-world viability and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grant-free random access (GF-RA) is a promising access technique for massive\nmachine-type communications (mMTC) in future wireless networks, particularly in\nthe context of 5G and beyond (6G) systems. Within the context of GF-RA, this\nstudy investigates the efficiency of employing supervised machine learning\ntechniques to tackle the challenges on the device activity detection (AD).\nGF-RA addresses scalability by employing non-orthogonal pilot sequences, which\nprovides an efficient alternative comparing to conventional grant-based random\naccess (GB-RA) technique that are constrained by the scarcity of orthogonal\npreamble resources. In this paper, we propose a novel lightweight data-driven\nalgorithmic framework specifically designed for activity detection in GF-RA for\nmMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks.\nWe propose two distinct framework deployment strategies, centralized and\ndecentralized, both tailored to streamline the proposed approach implementation\nacross network infrastructures. Moreover, we introduce optimized post-detection\nmethodologies complemented by a clustering stage to enhance overall detection\nperformances. Our 3GPP-compliant simulations have validated that the proposed\nalgorithm achieves state-of-the-art model-based activity detection accuracy\nwhile significantly reducing complexity. Achieving 99% accuracy, it\ndemonstrates real-world viability and effectiveness."
                },
                "authors": [
                    {
                        "name": "Ali Elkeshawy"
                    },
                    {
                        "name": "Haifa Fares"
                    },
                    {
                        "name": "Amor Nafkha"
                    }
                ],
                "author_detail": {
                    "name": "Amor Nafkha"
                },
                "author": "Amor Nafkha",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.07160",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11302v1",
                "updated": "2025-03-14T11:11:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:11:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Are formal and functional linguistic mechanisms dissociated?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are formal and functional linguistic mechanisms dissociated?"
                },
                "summary": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist."
                },
                "authors": [
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "arxiv_comment": "35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11301v1",
                "updated": "2025-03-14T11:11:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    0,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T11:11:00Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    0,
                    4,
                    73,
                    0
                ],
                "title": "GNNs as Predictors of Agentic Workflow Performances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNNs as Predictors of Agentic Workflow Performances"
                },
                "summary": "Agentic workflows invoked by Large Language Models (LLMs) have achieved\nremarkable success in handling complex tasks. However, optimizing such\nworkflows is costly and inefficient in real-world applications due to extensive\ninvocations of LLMs. To fill this gap, this position paper formulates agentic\nworkflows as computational graphs and advocates Graph Neural Networks (GNNs) as\nefficient predictors of agentic workflow performances, avoiding repeated LLM\ninvocations for evaluation. To empirically ground this position, we construct\nFLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic\nworkflow performances. With extensive experiments, we arrive at the following\nconclusion: GNNs are simple yet effective predictors. This conclusion supports\nnew applications of GNNs and a novel direction towards automating agentic\nworkflow optimization. All codes, models, and data are available at\nhttps://github.com/youngsoul0731/Flora-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic workflows invoked by Large Language Models (LLMs) have achieved\nremarkable success in handling complex tasks. However, optimizing such\nworkflows is costly and inefficient in real-world applications due to extensive\ninvocations of LLMs. To fill this gap, this position paper formulates agentic\nworkflows as computational graphs and advocates Graph Neural Networks (GNNs) as\nefficient predictors of agentic workflow performances, avoiding repeated LLM\ninvocations for evaluation. To empirically ground this position, we construct\nFLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic\nworkflow performances. With extensive experiments, we arrive at the following\nconclusion: GNNs are simple yet effective predictors. This conclusion supports\nnew applications of GNNs and a novel direction towards automating agentic\nworkflow optimization. All codes, models, and data are available at\nhttps://github.com/youngsoul0731/Flora-Bench."
                },
                "authors": [
                    {
                        "name": "Yuanshuo Zhang"
                    },
                    {
                        "name": "Yuchen Hou"
                    },
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Xiaowen Dong"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11495v2",
                "updated": "2025-03-14T10:41:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    41,
                    6,
                    4,
                    73,
                    0
                ],
                "published": "2024-06-17T12:59:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    12,
                    59,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Online Context Learning for Socially Compliant Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Context Learning for Socially Compliant Navigation"
                },
                "summary": "Robot social navigation needs to adapt to different human factors and\nenvironmental contexts. However, since these factors and contexts are difficult\nto predict and cannot be exhaustively enumerated, traditional learning-based\nmethods have difficulty in ensuring the social attributes of robots in\nlong-term and cross-environment deployments. This letter introduces an online\ncontext learning method that aims to empower robots to adapt to new social\nenvironments online. The proposed method adopts a two-layer structure. The\nbottom layer is built using a deep reinforcement learning-based method to\nensure the output of basic robot navigation commands. The upper layer is\nimplemented using an online robot learning-based method to socialize the\ncontrol commands suggested by the bottom layer. Experiments using a\ncommunity-wide simulator show that our method outperforms the state-of-the-art\nones. Experimental results in the most challenging scenarios show that our\nmethod improves the performance of the state-of-the-art by 8%. The source code\nof the proposed method, the data used, and the tools for the per-training step\nare publicly available at https://github.com/Nedzhaken/SOCSARL-OL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot social navigation needs to adapt to different human factors and\nenvironmental contexts. However, since these factors and contexts are difficult\nto predict and cannot be exhaustively enumerated, traditional learning-based\nmethods have difficulty in ensuring the social attributes of robots in\nlong-term and cross-environment deployments. This letter introduces an online\ncontext learning method that aims to empower robots to adapt to new social\nenvironments online. The proposed method adopts a two-layer structure. The\nbottom layer is built using a deep reinforcement learning-based method to\nensure the output of basic robot navigation commands. The upper layer is\nimplemented using an online robot learning-based method to socialize the\ncontrol commands suggested by the bottom layer. Experiments using a\ncommunity-wide simulator show that our method outperforms the state-of-the-art\nones. Experimental results in the most challenging scenarios show that our\nmethod improves the performance of the state-of-the-art by 8%. The source code\nof the proposed method, the data used, and the tools for the per-training step\nare publicly available at https://github.com/Nedzhaken/SOCSARL-OL."
                },
                "authors": [
                    {
                        "name": "Iaroslav Okunevich"
                    },
                    {
                        "name": "Alexandre Lombard"
                    },
                    {
                        "name": "Tomas Krajnik"
                    },
                    {
                        "name": "Yassine Ruichek"
                    },
                    {
                        "name": "Zhi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yan"
                },
                "author": "Zhi Yan",
                "arxiv_comment": "8 pages, 4 figures, 1 table, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v1",
                "updated": "2025-03-14T10:39:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06171v2",
                "updated": "2025-03-14T10:23:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    23,
                    6,
                    4,
                    73,
                    0
                ],
                "published": "2024-12-09T03:05:34Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    3,
                    5,
                    34,
                    0,
                    344,
                    0
                ],
                "title": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any\n  Granularity"
                },
                "summary": "How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU."
                },
                "authors": [
                    {
                        "name": "Huaxin Zhang"
                    },
                    {
                        "name": "Xiaohao Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Jialong Zuo"
                    },
                    {
                        "name": "Xiaonan Huang"
                    },
                    {
                        "name": "Changxin Gao"
                    },
                    {
                        "name": "Shanjun Zhang"
                    },
                    {
                        "name": "Li Yu"
                    },
                    {
                        "name": "Nong Sang"
                    }
                ],
                "author_detail": {
                    "name": "Nong Sang"
                },
                "author": "Nong Sang",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08161v3",
                "updated": "2025-03-14T10:09:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    9,
                    13,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-11T08:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    26,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "OASIS: Order-Augmented Strategy for Improved Code Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Order-Augmented Strategy for Improved Code Search"
                },
                "summary": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training."
                },
                "authors": [
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11256v1",
                "updated": "2025-03-14T10:07:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    7,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T10:07:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    7,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "Line of Duty: Evaluating LLM Self-Knowledge via Consistency in\n  Feasibility Boundaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Line of Duty: Evaluating LLM Self-Knowledge via Consistency in\n  Feasibility Boundaries"
                },
                "summary": "As LLMs grow more powerful, their most profound achievement may be\nrecognising when to say \"I don't know\". Existing studies on LLM self-knowledge\nhave been largely constrained by human-defined notions of feasibility, often\nneglecting the reasons behind unanswerability by LLMs and failing to study\ndeficient types of self-knowledge. This study aims to obtain intrinsic insights\ninto different types of LLM self-knowledge with a novel methodology: allowing\nthem the flexibility to set their own feasibility boundaries and then analysing\nthe consistency of these limits. We find that even frontier models like GPT-4o\nand Mistral Large are not sure of their own capabilities more than 80% of the\ntime, highlighting a significant lack of trustworthiness in responses. Our\nanalysis of confidence balance in LLMs indicates that models swing between\noverconfidence and conservatism in feasibility boundaries depending on task\ncategories and that the most significant self-knowledge weaknesses lie in\ntemporal awareness and contextual understanding. These difficulties in\ncontextual comprehension additionally lead models to question their operational\nboundaries, resulting in considerable confusion within the self-knowledge of\nLLMs. We make our code and results available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Self_Knowledge_Eval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs grow more powerful, their most profound achievement may be\nrecognising when to say \"I don't know\". Existing studies on LLM self-knowledge\nhave been largely constrained by human-defined notions of feasibility, often\nneglecting the reasons behind unanswerability by LLMs and failing to study\ndeficient types of self-knowledge. This study aims to obtain intrinsic insights\ninto different types of LLM self-knowledge with a novel methodology: allowing\nthem the flexibility to set their own feasibility boundaries and then analysing\nthe consistency of these limits. We find that even frontier models like GPT-4o\nand Mistral Large are not sure of their own capabilities more than 80% of the\ntime, highlighting a significant lack of trustworthiness in responses. Our\nanalysis of confidence balance in LLMs indicates that models swing between\noverconfidence and conservatism in feasibility boundaries depending on task\ncategories and that the most significant self-knowledge weaknesses lie in\ntemporal awareness and contextual understanding. These difficulties in\ncontextual comprehension additionally lead models to question their operational\nboundaries, resulting in considerable confusion within the self-knowledge of\nLLMs. We make our code and results available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Self_Knowledge_Eval"
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    },
                    {
                        "name": "Vijaykant Nadadur"
                    }
                ],
                "author_detail": {
                    "name": "Vijaykant Nadadur"
                },
                "author": "Vijaykant Nadadur",
                "arxiv_comment": "14 pages, 8 figures, Accepted to the 5th TrustNLP Workshop at NAACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11255v1",
                "updated": "2025-03-14T10:06:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    6,
                    52,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T10:06:52Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    6,
                    52,
                    4,
                    73,
                    0
                ],
                "title": "Federated Koopman-Reservoir Learning for Large-Scale Multivariate\n  Time-Series Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Koopman-Reservoir Learning for Large-Scale Multivariate\n  Time-Series Anomaly Detection"
                },
                "summary": "The proliferation of edge devices has dramatically increased the generation\nof multivariate time-series (MVTS) data, essential for applications from\nhealthcare to smart cities. Such data streams, however, are vulnerable to\nanomalies that signal crucial problems like system failures or security\nincidents. Traditional MVTS anomaly detection methods, encompassing statistical\nand centralized machine learning approaches, struggle with the heterogeneity,\nvariability, and privacy concerns of large-scale, distributed environments. In\nresponse, we introduce FedKO, a novel unsupervised Federated Learning framework\nthat leverages the linear predictive capabilities of Koopman operator theory\nalong with the dynamic adaptability of Reservoir Computing. This enables\neffective spatiotemporal processing and privacy preservation for MVTS data.\nFedKO is formulated as a bi-level optimization problem, utilizing a specific\nfederated algorithm to explore a shared Reservoir-Koopman model across diverse\ndatasets. Such a model is then deployable on edge devices for efficient\ndetection of anomalies in local MVTS streams. Experimental results across\nvarious datasets showcase FedKO's superior performance against state-of-the-art\nmethods in MVTS anomaly detection. Moreover, FedKO reduces up to 8x\ncommunication size and 2x memory usage, making it highly suitable for\nlarge-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of edge devices has dramatically increased the generation\nof multivariate time-series (MVTS) data, essential for applications from\nhealthcare to smart cities. Such data streams, however, are vulnerable to\nanomalies that signal crucial problems like system failures or security\nincidents. Traditional MVTS anomaly detection methods, encompassing statistical\nand centralized machine learning approaches, struggle with the heterogeneity,\nvariability, and privacy concerns of large-scale, distributed environments. In\nresponse, we introduce FedKO, a novel unsupervised Federated Learning framework\nthat leverages the linear predictive capabilities of Koopman operator theory\nalong with the dynamic adaptability of Reservoir Computing. This enables\neffective spatiotemporal processing and privacy preservation for MVTS data.\nFedKO is formulated as a bi-level optimization problem, utilizing a specific\nfederated algorithm to explore a shared Reservoir-Koopman model across diverse\ndatasets. Such a model is then deployable on edge devices for efficient\ndetection of anomalies in local MVTS streams. Experimental results across\nvarious datasets showcase FedKO's superior performance against state-of-the-art\nmethods in MVTS anomaly detection. Moreover, FedKO reduces up to 8x\ncommunication size and 2x memory usage, making it highly suitable for\nlarge-scale systems."
                },
                "authors": [
                    {
                        "name": "Long Tan Le"
                    },
                    {
                        "name": "Tung-Anh Nguyen"
                    },
                    {
                        "name": "Han Shu"
                    },
                    {
                        "name": "Suranga Seneviratne"
                    },
                    {
                        "name": "Choong Seon Hong"
                    },
                    {
                        "name": "Nguyen H. Tran"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen H. Tran"
                },
                "author": "Nguyen H. Tran",
                "arxiv_comment": "Accepted at SDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11246v1",
                "updated": "2025-03-14T09:54:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    54,
                    36,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:54:36Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    54,
                    36,
                    4,
                    73,
                    0
                ],
                "title": "Cost-effective Deep Learning Infrastructure with NVIDIA GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-effective Deep Learning Infrastructure with NVIDIA GPU"
                },
                "summary": "The growing demand for computational power is driven by advancements in deep\nlearning, the increasing need for big data processing, and the requirements of\nscientific simulations for academic and research purposes. Developing countries\nlike Nepal often struggle with the resources needed to invest in new and better\nhardware for these purposes. However, optimizing and building on existing\ntechnology can still meet these computing demands effectively. To address these\nneeds, we built a cluster using four NVIDIA GeForce GTX 1650 GPUs. The cluster\nconsists of four nodes: one master node that controls and manages the entire\ncluster, and three compute nodes dedicated to processing tasks. The master node\nis equipped with all necessary software for package management, resource\nscheduling, and deployment, such as Anaconda and Slurm. In addition, a Network\nFile Storage (NFS) system was integrated to provide the additional storage\nrequired by the cluster. Given that the cluster is accessible via ssh by a\npublic domain address, which poses significant cybersecurity risks, we\nimplemented fail2ban to mitigate brute force attacks and enhance security.\nDespite the continuous challenges encountered during the design and\nimplementation process, this project demonstrates how powerful computational\nclusters can be built to handle resource-intensive tasks in various demanding\nfields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for computational power is driven by advancements in deep\nlearning, the increasing need for big data processing, and the requirements of\nscientific simulations for academic and research purposes. Developing countries\nlike Nepal often struggle with the resources needed to invest in new and better\nhardware for these purposes. However, optimizing and building on existing\ntechnology can still meet these computing demands effectively. To address these\nneeds, we built a cluster using four NVIDIA GeForce GTX 1650 GPUs. The cluster\nconsists of four nodes: one master node that controls and manages the entire\ncluster, and three compute nodes dedicated to processing tasks. The master node\nis equipped with all necessary software for package management, resource\nscheduling, and deployment, such as Anaconda and Slurm. In addition, a Network\nFile Storage (NFS) system was integrated to provide the additional storage\nrequired by the cluster. Given that the cluster is accessible via ssh by a\npublic domain address, which poses significant cybersecurity risks, we\nimplemented fail2ban to mitigate brute force attacks and enhance security.\nDespite the continuous challenges encountered during the design and\nimplementation process, this project demonstrates how powerful computational\nclusters can be built to handle resource-intensive tasks in various demanding\nfields."
                },
                "authors": [
                    {
                        "name": "Aatiz Ghimire"
                    },
                    {
                        "name": "Shahnawaz Alam"
                    },
                    {
                        "name": "Siman Giri"
                    },
                    {
                        "name": "Madhav Prasad Ghimire"
                    }
                ],
                "author_detail": {
                    "name": "Madhav Prasad Ghimire"
                },
                "author": "Madhav Prasad Ghimire",
                "arxiv_comment": "10 Pages,6 Figures, this paper was presented in National Data and\n  Computing Conference 2024 and will be published into KUSET Journal by\n  Kathmandu University",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11244v1",
                "updated": "2025-03-14T09:52:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    52,
                    30,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:52:30Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    52,
                    30,
                    4,
                    73,
                    0
                ],
                "title": "LLMPerf: GPU Performance Modeling meets Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMPerf: GPU Performance Modeling meets Large Language Models"
                },
                "summary": "Performance modeling, a pivotal domain in program cost analysis, currently\nrelies on manually crafted models constrained by various program and hardware\nlimitations, especially in the intricate landscape of GPGPU. Meanwhile, Large\nLanguage Models (LLMs) have demonstrated their effectiveness in addressing\ndiverse programming challenges. Our work establishes a connection between LLMs\nand performance modeling, employing the LLM as a performance estimator. Through\nexperimental exploration with carefully designed large-scale OpenCL datasets,\nwe highlight the potential capability as well as the main difficulties of using\nLLMs in handling performance modeling tasks for OpenCL device source programs.\nAs the first study for this line of work, our LLM-based performance model\nachieves a mean absolute percentage error of $24.25\\%$ for a large-scale\ngenerated validation set. On a set of publicly available OpenCL programs, our\nmodel achieves a mean absolute percentage error of $46.1\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance modeling, a pivotal domain in program cost analysis, currently\nrelies on manually crafted models constrained by various program and hardware\nlimitations, especially in the intricate landscape of GPGPU. Meanwhile, Large\nLanguage Models (LLMs) have demonstrated their effectiveness in addressing\ndiverse programming challenges. Our work establishes a connection between LLMs\nand performance modeling, employing the LLM as a performance estimator. Through\nexperimental exploration with carefully designed large-scale OpenCL datasets,\nwe highlight the potential capability as well as the main difficulties of using\nLLMs in handling performance modeling tasks for OpenCL device source programs.\nAs the first study for this line of work, our LLM-based performance model\nachieves a mean absolute percentage error of $24.25\\%$ for a large-scale\ngenerated validation set. On a set of publicly available OpenCL programs, our\nmodel achieves a mean absolute percentage error of $46.1\\%$."
                },
                "authors": [
                    {
                        "name": "Khoi N. M. Nguyen"
                    },
                    {
                        "name": "Hoang Duy Nguyen Do"
                    },
                    {
                        "name": "Huyen Thao Le"
                    },
                    {
                        "name": "Thanh Tuan Dao"
                    }
                ],
                "author_detail": {
                    "name": "Thanh Tuan Dao"
                },
                "author": "Thanh Tuan Dao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05609v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05609v2",
                "updated": "2025-03-14T09:51:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    51,
                    44,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-08T14:52:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    14,
                    52,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis"
                },
                "summary": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and an off-the-shelf\nLarge Language Model (LLM) to generate disease diagnoses based on the predicted\nconcepts. Furthermore, our approach supports test-time human intervention,\nenabling corrections to predicted concepts, which improves final diagnoses and\nenhances transparency in decision-making. We validate our approach on three\nskin lesion datasets, demonstrating that it outperforms traditional CBMs and\nstate-of-the-art explainable methods, all without requiring any training and\nutilizing only a few annotated examples. The code is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and an off-the-shelf\nLarge Language Model (LLM) to generate disease diagnoses based on the predicted\nconcepts. Furthermore, our approach supports test-time human intervention,\nenabling corrections to predicted concepts, which improves final diagnoses and\nenhances transparency in decision-making. We validate our approach on three\nskin lesion datasets, demonstrating that it outperforms traditional CBMs and\nstate-of-the-art explainable methods, all without requiring any training and\nutilizing only a few annotated examples. The code is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis."
                },
                "authors": [
                    {
                        "name": "Cristiano Patrcio"
                    },
                    {
                        "name": "Lus F. Teixeira"
                    },
                    {
                        "name": "Joo C. Neves"
                    }
                ],
                "author_detail": {
                    "name": "Joo C. Neves"
                },
                "author": "Joo C. Neves",
                "arxiv_doi": "10.1016/j.csbj.2025.02.013",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.csbj.2025.02.013",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.05609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05609v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Computational and Structural Biotechnology Journal",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11237v1",
                "updated": "2025-03-14T09:42:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    42,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:42:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    42,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "Collaboration is all you need: LLM Assisted Safe Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration is all you need: LLM Assisted Safe Code Translation"
                },
                "summary": "This paper introduces UniTranslator, a visionary framework that re-imagines\ncode translation as a collaborative endeavor among multiple, compact LLMs. By\norchestrating the interaction of specialized agents, each focused on different\naspects of the translation process and grounded in a deep understanding of\nprogramming concepts, UniTranslator achieves a level of accuracy and efficiency\nthat rivals larger, monolithic models. Our preliminary evaluation demonstrates\nthe potential of UniTranslator to overcome the limitations of existing\napproaches and unlock the power of smaller LLMs for complex code translation\ntasks. We explore the effectiveness of this dynamic multi-agent paradigm in\nhandling diverse language pairs, including low-resource languages, and in\nmitigating common issues such as code artifacts and hallucinations through the\nuse of Natural Language Inference (NLI) grounding and iterative feedback\nmechanisms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces UniTranslator, a visionary framework that re-imagines\ncode translation as a collaborative endeavor among multiple, compact LLMs. By\norchestrating the interaction of specialized agents, each focused on different\naspects of the translation process and grounded in a deep understanding of\nprogramming concepts, UniTranslator achieves a level of accuracy and efficiency\nthat rivals larger, monolithic models. Our preliminary evaluation demonstrates\nthe potential of UniTranslator to overcome the limitations of existing\napproaches and unlock the power of smaller LLMs for complex code translation\ntasks. We explore the effectiveness of this dynamic multi-agent paradigm in\nhandling diverse language pairs, including low-resource languages, and in\nmitigating common issues such as code artifacts and hallucinations through the\nuse of Natural Language Inference (NLI) grounding and iterative feedback\nmechanisms"
                },
                "authors": [
                    {
                        "name": "Rabimba Karanjai"
                    },
                    {
                        "name": "Sam Blackshear"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Weidong Shi"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Shi"
                },
                "author": "Weidong Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.03309v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.03309v5",
                "updated": "2025-03-14T09:33:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    33,
                    2,
                    4,
                    73,
                    0
                ],
                "published": "2023-10-05T04:47:49Z",
                "published_parsed": [
                    2023,
                    10,
                    5,
                    4,
                    47,
                    49,
                    3,
                    278,
                    0
                ],
                "title": "Concise and Organized Perception Facilitates Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise and Organized Perception Facilitates Reasoning in Large Language\n  Models"
                },
                "summary": "Exploiting large language models (LLMs) to tackle reasoning has garnered\ngrowing attention. It still remains highly challenging to achieve satisfactory\nresults in complex logical problems, characterized by plenty of premises within\nthe context and requiring multi-hop reasoning. In particular, the reasoning\ncapabilities of LLMs are brittle to disorder and distractibility. In this work,\nwe first examine the mechanism from the perspective of information flow and\nreveal that LLMs confront difficulties akin to human-like cognitive biases when\ndealing with disordered and irrelevant content in reasoning tasks. However, in\ncontrast to LLMs, disordered and irrelevant content does not significantly\ndecrease human performance, as humans have a propensity to distill the most\nrelevant information and systematically organize their thoughts, aiding them in\nresponding to questions.Stem from that, we further propose a novel reasoning\napproach named Concise and Organized Perception (COP). COP carefully analyzes\nthe given statements to identify the most pertinent information while\neliminating redundancy efficiently. It then prompts the LLMs in a more\norganized form that adapts to the model's inference process. By perceiving\nconcise and organized context, the reasoning abilities of LLMs can be better\nelicited. Extensive experimental results on several popular logical benchmarks\n(ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark\n(DI-GSM) show that COP significantly outperforms previous state-of-the-art\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting large language models (LLMs) to tackle reasoning has garnered\ngrowing attention. It still remains highly challenging to achieve satisfactory\nresults in complex logical problems, characterized by plenty of premises within\nthe context and requiring multi-hop reasoning. In particular, the reasoning\ncapabilities of LLMs are brittle to disorder and distractibility. In this work,\nwe first examine the mechanism from the perspective of information flow and\nreveal that LLMs confront difficulties akin to human-like cognitive biases when\ndealing with disordered and irrelevant content in reasoning tasks. However, in\ncontrast to LLMs, disordered and irrelevant content does not significantly\ndecrease human performance, as humans have a propensity to distill the most\nrelevant information and systematically organize their thoughts, aiding them in\nresponding to questions.Stem from that, we further propose a novel reasoning\napproach named Concise and Organized Perception (COP). COP carefully analyzes\nthe given statements to identify the most pertinent information while\neliminating redundancy efficiently. It then prompts the LLMs in a more\norganized form that adapts to the model's inference process. By perceiving\nconcise and organized context, the reasoning abilities of LLMs can be better\nelicited. Extensive experimental results on several popular logical benchmarks\n(ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark\n(DI-GSM) show that COP significantly outperforms previous state-of-the-art\nmethods."
                },
                "authors": [
                    {
                        "name": "Junjie Liu"
                    },
                    {
                        "name": "Shaotian Yan"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Zhengdong Xiao"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by NAACL2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.03309v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.03309v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11232v1",
                "updated": "2025-03-14T09:31:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    31,
                    1,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:31:01Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    31,
                    1,
                    4,
                    73,
                    0
                ],
                "title": "PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment."
                },
                "authors": [
                    {
                        "name": "Ahmed Frikha"
                    },
                    {
                        "name": "Muhammad Reza Ar Razi"
                    },
                    {
                        "name": "Krishna Kanth Nakka"
                    },
                    {
                        "name": "Ricardo Mendes"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Xuebing Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Zhou"
                },
                "author": "Xuebing Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11229v1",
                "updated": "2025-03-14T09:26:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    26,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T09:26:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    26,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "Exploring the Potential of Large Multimodal Models as Effective\n  Alternatives for Pronunciation Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Multimodal Models as Effective\n  Alternatives for Pronunciation Assessment"
                },
                "summary": "Large Multimodal Models (LMMs) have demonstrated exceptional performance\nacross a wide range of domains. This paper explores their potential in\npronunciation assessment tasks, with a particular focus on evaluating the\ncapabilities of the Generative Pre-trained Transformer (GPT) model,\nspecifically GPT-4o. Our study investigates its ability to process speech and\naudio for pronunciation assessment across multiple levels of granularity and\ndimensions, with an emphasis on feedback generation and scoring. For our\nexperiments, we use the publicly available Speechocean762 dataset. The\nevaluation focuses on two key aspects: multi-level scoring and the practicality\nof the generated feedback. Scoring results are compared against the manual\nscores provided in the Speechocean762 dataset, while feedback quality is\nassessed using Large Language Models (LLMs). The findings highlight the\neffectiveness of integrating LMMs with traditional methods for pronunciation\nassessment, offering insights into the model's strengths and identifying areas\nfor further improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have demonstrated exceptional performance\nacross a wide range of domains. This paper explores their potential in\npronunciation assessment tasks, with a particular focus on evaluating the\ncapabilities of the Generative Pre-trained Transformer (GPT) model,\nspecifically GPT-4o. Our study investigates its ability to process speech and\naudio for pronunciation assessment across multiple levels of granularity and\ndimensions, with an emphasis on feedback generation and scoring. For our\nexperiments, we use the publicly available Speechocean762 dataset. The\nevaluation focuses on two key aspects: multi-level scoring and the practicality\nof the generated feedback. Scoring results are compared against the manual\nscores provided in the Speechocean762 dataset, while feedback quality is\nassessed using Large Language Models (LLMs). The findings highlight the\neffectiveness of integrating LMMs with traditional methods for pronunciation\nassessment, offering insights into the model's strengths and identifying areas\nfor further improvement."
                },
                "authors": [
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Kun Liu"
                    },
                    {
                        "name": "Yan Deng"
                    },
                    {
                        "name": "Wenning Wei"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18266v3",
                "updated": "2025-03-14T09:14:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    9,
                    14,
                    26,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-27T12:03:52Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    52,
                    2,
                    332,
                    0
                ],
                "title": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria"
                },
                "summary": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems."
                },
                "authors": [
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Shuo Gao"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Wentian Yi"
                    },
                    {
                        "name": "Yuxuan Jin"
                    },
                    {
                        "name": "Xiaoxue Zhai"
                    },
                    {
                        "name": "Sixuan Lei"
                    },
                    {
                        "name": "Hongbei Meng"
                    },
                    {
                        "name": "Zibo Zhang"
                    },
                    {
                        "name": "Muzi Xu"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Xuhang Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Hongyun Yang"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Wenyu Wang"
                    },
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Xiaodong Feng"
                    },
                    {
                        "name": "Peter Smielewski"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Wenhui Song"
                    },
                    {
                        "name": "Martin Birchall"
                    },
                    {
                        "name": "Luigi G. Occhipinti"
                    }
                ],
                "author_detail": {
                    "name": "Luigi G. Occhipinti"
                },
                "author": "Luigi G. Occhipinti",
                "arxiv_comment": "5 figures, 45 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17174v2",
                "updated": "2025-03-14T08:56:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    56,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-20T08:28:23Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    8,
                    28,
                    23,
                    4,
                    264,
                    0
                ],
                "title": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency"
                },
                "summary": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal illusions between \\textit{a step of reasoning} and \\textit{corresponding\nstate transitions} are becoming a significant obstacle to advancing LLMs'\nreasoning capabilities, especially in long-range reasoning tasks. This paper\nproposes a non-chain-based reasoning framework for simultaneous consideration\nof causal significance and consistency, i.e., the Causal Significance and\nConsistency Enhancer (CSCE). We customize LLM's loss function utilizing\ntreatment effect assessments to enhance its reasoning ability from two aspects:\ncausal significance and consistency. This ensures that the model captures\nessential causal relationships and maintains robust and consistent performance\nacross various scenarios. Additionally, we transform the reasoning process from\nthe cascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal illusions between \\textit{a step of reasoning} and \\textit{corresponding\nstate transitions} are becoming a significant obstacle to advancing LLMs'\nreasoning capabilities, especially in long-range reasoning tasks. This paper\nproposes a non-chain-based reasoning framework for simultaneous consideration\nof causal significance and consistency, i.e., the Causal Significance and\nConsistency Enhancer (CSCE). We customize LLM's loss function utilizing\ntreatment effect assessments to enhance its reasoning ability from two aspects:\ncausal significance and consistency. This ensures that the model captures\nessential causal relationships and maintains robust and consistent performance\nacross various scenarios. Additionally, we transform the reasoning process from\nthe cascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Kangsheng Wang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Zizheng Guo"
                    },
                    {
                        "name": "Tianyu Hu"
                    },
                    {
                        "name": "Huimin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Ma"
                },
                "author": "Huimin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11205v1",
                "updated": "2025-03-14T08:49:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    49,
                    52,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    49,
                    52,
                    4,
                    73,
                    0
                ],
                "title": "LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free\n  Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free\n  Video LLMs"
                },
                "summary": "Training-free video large language models (LLMs) leverage pretrained Image\nLLMs to process video content without the need for further training. A key\nchallenge in such approaches is the difficulty of retaining essential visual\nand temporal information, constrained by the token limits in Image LLMs. To\naddress this, we propose a two-stage method for selecting query-relevant tokens\nbased on the LLM attention scores: compressing the video sequence and then\nexpanding the sequence. However, during the compression stage, Image LLMs often\nexhibit a positional attention bias in video sequences, where attention is\noverly concentrated on later frames, causing early-frame information to be\nunderutilized. To alleviate this attention bias during sequence compression, we\npropose Gridded Attention Pooling for preserving spatiotemporal structure.\nAdditionally, we introduce Visual Summarization Tail to effectively utilize\nthis bias, facilitating overall video understanding during sequence expansion.\nIn this way, our method effectively Mitigates and Leverages attention Bias\n(LLaVA-MLB), enabling the frozen Image LLM for detailed video understanding.\nExperiments on several benchmarks demonstrate that our approach outperforms\nstate-of-the-art methods, achieving superior performance in both efficiency and\naccuracy. Our code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free video large language models (LLMs) leverage pretrained Image\nLLMs to process video content without the need for further training. A key\nchallenge in such approaches is the difficulty of retaining essential visual\nand temporal information, constrained by the token limits in Image LLMs. To\naddress this, we propose a two-stage method for selecting query-relevant tokens\nbased on the LLM attention scores: compressing the video sequence and then\nexpanding the sequence. However, during the compression stage, Image LLMs often\nexhibit a positional attention bias in video sequences, where attention is\noverly concentrated on later frames, causing early-frame information to be\nunderutilized. To alleviate this attention bias during sequence compression, we\npropose Gridded Attention Pooling for preserving spatiotemporal structure.\nAdditionally, we introduce Visual Summarization Tail to effectively utilize\nthis bias, facilitating overall video understanding during sequence expansion.\nIn this way, our method effectively Mitigates and Leverages attention Bias\n(LLaVA-MLB), enabling the frozen Image LLM for detailed video understanding.\nExperiments on several benchmarks demonstrate that our approach outperforms\nstate-of-the-art methods, achieving superior performance in both efficiency and\naccuracy. Our code will be released."
                },
                "authors": [
                    {
                        "name": "Leqi Shen"
                    },
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Guoqiang Gong"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yifeng Zhang"
                    },
                    {
                        "name": "Pengzhang Liu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12928v3",
                "updated": "2025-03-14T08:48:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    48,
                    51,
                    4,
                    73,
                    0
                ],
                "published": "2024-08-23T09:14:58Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    14,
                    58,
                    4,
                    236,
                    0
                ],
                "title": "ParGo: Bridging Vision-Language with Partial and Global Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParGo: Bridging Vision-Language with Partial and Global Views"
                },
                "summary": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability."
                },
                "authors": [
                    {
                        "name": "An-Lan Wang"
                    },
                    {
                        "name": "Bin Shan"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Kun-Yu Lin"
                    },
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Guozhi Tang"
                    },
                    {
                        "name": "Lei Liao"
                    },
                    {
                        "name": "Can Huang"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Wei-Shi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Shi Zheng"
                },
                "author": "Wei-Shi Zheng",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02465v2",
                "updated": "2025-03-14T08:44:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    44,
                    6,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-04T06:28:22Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    6,
                    28,
                    22,
                    2,
                    248,
                    0
                ],
                "title": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels"
                },
                "summary": "Recently, significant efforts have been devoted to enhancing the long-context\ncapabilities of Large Language Models (LLMs), particularly in long-context\nreasoning. To facilitate this research, we propose \\textbf{DetectiveQA}, a\ndataset specifically designed for narrative reasoning within long contexts. We\nleverage detective novels, averaging over 100k tokens, to create a dataset\ncontaining 1200 human-annotated questions in both Chinese and English, each\npaired with corresponding reference reasoning steps. Furthermore, we introduce\na step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning\nprocesses. We validate our approach and evaluate the mainstream LLMs, including\nGPT-4, Claude, and LLaMA, revealing persistent long-context reasoning\nchallenges and demonstrating their evidence-retrieval challenges. Our findings\noffer valuable insights into the study of long-context reasoning and lay the\nbase for more rigorous evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, significant efforts have been devoted to enhancing the long-context\ncapabilities of Large Language Models (LLMs), particularly in long-context\nreasoning. To facilitate this research, we propose \\textbf{DetectiveQA}, a\ndataset specifically designed for narrative reasoning within long contexts. We\nleverage detective novels, averaging over 100k tokens, to create a dataset\ncontaining 1200 human-annotated questions in both Chinese and English, each\npaired with corresponding reference reasoning steps. Furthermore, we introduce\na step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning\nprocesses. We validate our approach and evaluate the mainstream LLMs, including\nGPT-4, Claude, and LLaMA, revealing persistent long-context reasoning\nchallenges and demonstrating their evidence-retrieval challenges. Our findings\noffer valuable insights into the study of long-context reasoning and lay the\nbase for more rigorous evaluations."
                },
                "authors": [
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Xiangyang Liu"
                    },
                    {
                        "name": "Tianxiang Sun"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11197v1",
                "updated": "2025-03-14T08:43:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:43:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering"
                },
                "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi/r1-aqa and https://huggingface.co/mispeech/r1-aqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi/r1-aqa and https://huggingface.co/mispeech/r1-aqa."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jizhong Liu"
                    },
                    {
                        "name": "Heinrich Dinkel"
                    },
                    {
                        "name": "Yadong Niu"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15767v2",
                "updated": "2025-03-14T08:36:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    36,
                    9,
                    4,
                    73,
                    0
                ],
                "published": "2024-01-28T21:08:45Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    21,
                    8,
                    45,
                    6,
                    28,
                    0
                ],
                "title": "LEACH-RLC: Enhancing IoT Data Transmission with Optimized Clustering and\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEACH-RLC: Enhancing IoT Data Transmission with Optimized Clustering and\n  Reinforcement Learning"
                },
                "summary": "Wireless Sensor Networks (WSNs) play a pivotal role in enabling Internet of\nThings (IoT) devices with sensing and actuation capabilities. Operating in\nremote and resource-constrained environments, these IoT devices face challenges\nrelated to energy consumption, crucial for network longevity. Existing\nclustering protocols often suffer from high control overhead, inefficient\ncluster formation, and poor adaptability to dynamic network conditions, leading\nto suboptimal data transmission and reduced network lifetime. This paper\nintroduces Low-Energy Adaptive Clustering Hierarchy with Reinforcement\nLearning-based Controller (LEACH-RLC), a novel clustering protocol designed to\naddress these limitations by employing a Mixed Integer Linear Programming\n(MILP) approach for strategic selection of Cluster Heads (CHs) and\nnode-to-cluster assignments. Additionally, it integrates a Reinforcement\nLearning (RL) agent to minimize control overhead by learning optimal timings\nfor generating new clusters. LEACH-RLC aims to balance control overhead\nreduction without compromising overall network performance. Through extensive\nsimulations, this paper investigates the frequency and opportune moments for\ngenerating new clustering solutions. Results demonstrate the superior\nperformance of LEACH-RLC over state-of-the-art protocols, showcasing enhanced\nnetwork lifetime, reduced average energy consumption, and minimized control\noverhead. The proposed protocol contributes to advancing the efficiency and\nadaptability of WSNs, addressing critical challenges in IoT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Sensor Networks (WSNs) play a pivotal role in enabling Internet of\nThings (IoT) devices with sensing and actuation capabilities. Operating in\nremote and resource-constrained environments, these IoT devices face challenges\nrelated to energy consumption, crucial for network longevity. Existing\nclustering protocols often suffer from high control overhead, inefficient\ncluster formation, and poor adaptability to dynamic network conditions, leading\nto suboptimal data transmission and reduced network lifetime. This paper\nintroduces Low-Energy Adaptive Clustering Hierarchy with Reinforcement\nLearning-based Controller (LEACH-RLC), a novel clustering protocol designed to\naddress these limitations by employing a Mixed Integer Linear Programming\n(MILP) approach for strategic selection of Cluster Heads (CHs) and\nnode-to-cluster assignments. Additionally, it integrates a Reinforcement\nLearning (RL) agent to minimize control overhead by learning optimal timings\nfor generating new clusters. LEACH-RLC aims to balance control overhead\nreduction without compromising overall network performance. Through extensive\nsimulations, this paper investigates the frequency and opportune moments for\ngenerating new clustering solutions. Results demonstrate the superior\nperformance of LEACH-RLC over state-of-the-art protocols, showcasing enhanced\nnetwork lifetime, reduced average energy consumption, and minimized control\noverhead. The proposed protocol contributes to advancing the efficiency and\nadaptability of WSNs, addressing critical challenges in IoT deployments."
                },
                "authors": [
                    {
                        "name": "F. Fernando Jurado-Lasso"
                    },
                    {
                        "name": "J. F. Jurado"
                    },
                    {
                        "name": "Xenofon Fafoutis"
                    }
                ],
                "author_detail": {
                    "name": "Xenofon Fafoutis"
                },
                "author": "Xenofon Fafoutis",
                "arxiv_comment": "17 pages, 15 figures, 4 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11187v1",
                "updated": "2025-03-14T08:33:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    33,
                    8,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:33:08Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    33,
                    8,
                    4,
                    73,
                    0
                ],
                "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models"
                },
                "summary": "Video Large Language Models have shown impressive capabilities in video\ncomprehension, yet their practical deployment is hindered by substantial\ninference costs caused by redundant video tokens. Existing pruning techniques\nfail to fully exploit the spatiotemporal redundancy inherent in video data. To\nbridge this gap, we perform a systematic analysis of video redundancy from two\nperspectives: temporal context and visual context. Leveraging this insight, we\npropose Dynamic Density Pruning for Fast Video LLMs termed FastVID.\nSpecifically, FastVID dynamically partitions videos into temporally ordered\nsegments to preserve temporal structure and applies a density-based token\npruning strategy to maintain essential visual information. Our method\nsignificantly reduces computational overhead while maintaining temporal and\nvisual integrity. Extensive evaluations show that FastVID achieves\nstate-of-the-art performance across various short- and long-video benchmarks on\nleading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID\neffectively prunes 90% of video tokens while retaining 98.0% of\nLLaVA-OneVision's original performance. The code is available at\nhttps://github.com/LunarShen/FastVID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models have shown impressive capabilities in video\ncomprehension, yet their practical deployment is hindered by substantial\ninference costs caused by redundant video tokens. Existing pruning techniques\nfail to fully exploit the spatiotemporal redundancy inherent in video data. To\nbridge this gap, we perform a systematic analysis of video redundancy from two\nperspectives: temporal context and visual context. Leveraging this insight, we\npropose Dynamic Density Pruning for Fast Video LLMs termed FastVID.\nSpecifically, FastVID dynamically partitions videos into temporally ordered\nsegments to preserve temporal structure and applies a density-based token\npruning strategy to maintain essential visual information. Our method\nsignificantly reduces computational overhead while maintaining temporal and\nvisual integrity. Extensive evaluations show that FastVID achieves\nstate-of-the-art performance across various short- and long-video benchmarks on\nleading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID\neffectively prunes 90% of video tokens while retaining 98.0% of\nLLaVA-OneVision's original performance. The code is available at\nhttps://github.com/LunarShen/FastVID."
                },
                "authors": [
                    {
                        "name": "Leqi Shen"
                    },
                    {
                        "name": "Guoqiang Gong"
                    },
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Yifeng Zhang"
                    },
                    {
                        "name": "Pengzhang Liu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11185v1",
                "updated": "2025-03-14T08:32:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    32,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:32:12Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    32,
                    12,
                    4,
                    73,
                    0
                ],
                "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer\n  Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer\n  Detoxification"
                },
                "summary": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use\ncrafted prompts to elicit toxic responses. These attacks exploit LLMs'\ndifficulty in dynamically detecting harmful intents during the generation\nprocess. Traditional safety alignment methods, often relying on the initial few\ngeneration steps, are ineffective due to limited computational budget. This\npaper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to\nprogressively detoxify generated content, significantly improving both the\ncomputational budget and effectiveness of mitigating harmful generation. Our\napproach uses a hybrid loss function operating on hidden states to directly\nimprove LLMs' inherent awareness of toxity during generation. Furthermore, we\nredefine safe responses by generating semantically relevant answers to harmful\nqueries, thereby increasing robustness against representation-mutation attacks.\nEvaluations across multiple LLMs demonstrate state-of-the-art defense\nperformance against six different attack types, reducing Attack Success Rates\nby up to two orders of magnitude compared to previous state-of-the-art defense\nwhile preserving utility. This work advances LLM safety by addressing\nlimitations of conventional alignment through dynamic, context-aware\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use\ncrafted prompts to elicit toxic responses. These attacks exploit LLMs'\ndifficulty in dynamically detecting harmful intents during the generation\nprocess. Traditional safety alignment methods, often relying on the initial few\ngeneration steps, are ineffective due to limited computational budget. This\npaper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to\nprogressively detoxify generated content, significantly improving both the\ncomputational budget and effectiveness of mitigating harmful generation. Our\napproach uses a hybrid loss function operating on hidden states to directly\nimprove LLMs' inherent awareness of toxity during generation. Furthermore, we\nredefine safe responses by generating semantically relevant answers to harmful\nqueries, thereby increasing robustness against representation-mutation attacks.\nEvaluations across multiple LLMs demonstrate state-of-the-art defense\nperformance against six different attack types, reducing Attack Success Rates\nby up to two orders of magnitude compared to previous state-of-the-art defense\nwhile preserving utility. This work advances LLM safety by addressing\nlimitations of conventional alignment through dynamic, context-aware\nmitigation."
                },
                "authors": [
                    {
                        "name": "Yingjie Zhang"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Guozhu Meng"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11164v1",
                "updated": "2025-03-14T08:05:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    5,
                    49,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T08:05:49Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    5,
                    49,
                    4,
                    73,
                    0
                ],
                "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity"
                },
                "summary": "N:M structured pruning is essential for large language models (LLMs) because\nit can remove less important network weights and reduce the memory and\ncomputation requirements. Existing pruning methods mainly focus on designing\nmetrics to measure the importance of network components to guide pruning. Apart\nfrom the impact of these metrics, we observe that different layers have\ndifferent sensitivities over the network performance. Thus, we propose an\nefficient method based on the trace of Fisher Information Matrix (FIM) to\nquantitatively measure and verify the different sensitivities across layers.\nBased on this, we propose Mixed Sparsity Pruning (MSP) which uses a\npruning-oriented evolutionary algorithm (EA) to determine the optimal sparsity\nlevels for different layers. To guarantee fast convergence and achieve\npromising performance, we utilize efficient FIM-inspired layer-wise sensitivity\nto initialize the population of EA. In addition, our MSP can work as a\nplug-and-play module, ready to be integrated into existing pruning methods.\nExtensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot\ntasks demonstrate our superior performance. In particular, in extreme pruning\nratio (e.g. 75%), our method significantly outperforms existing methods in\nterms of perplexity (PPL) by orders of magnitude (Figure 1).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N:M structured pruning is essential for large language models (LLMs) because\nit can remove less important network weights and reduce the memory and\ncomputation requirements. Existing pruning methods mainly focus on designing\nmetrics to measure the importance of network components to guide pruning. Apart\nfrom the impact of these metrics, we observe that different layers have\ndifferent sensitivities over the network performance. Thus, we propose an\nefficient method based on the trace of Fisher Information Matrix (FIM) to\nquantitatively measure and verify the different sensitivities across layers.\nBased on this, we propose Mixed Sparsity Pruning (MSP) which uses a\npruning-oriented evolutionary algorithm (EA) to determine the optimal sparsity\nlevels for different layers. To guarantee fast convergence and achieve\npromising performance, we utilize efficient FIM-inspired layer-wise sensitivity\nto initialize the population of EA. In addition, our MSP can work as a\nplug-and-play module, ready to be integrated into existing pruning methods.\nExtensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot\ntasks demonstrate our superior performance. In particular, in extreme pruning\nratio (e.g. 75%), our method significantly outperforms existing methods in\nterms of perplexity (PPL) by orders of magnitude (Figure 1)."
                },
                "authors": [
                    {
                        "name": "Chi Xu"
                    },
                    {
                        "name": "Gefei Zhang"
                    },
                    {
                        "name": "Yantong Zhu"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Guosheng Hu"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Zhihong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhihong Zhang"
                },
                "author": "Zhihong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11154v1",
                "updated": "2025-03-14T07:46:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    7,
                    46,
                    33,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T07:46:33Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    7,
                    46,
                    33,
                    4,
                    73,
                    0
                ],
                "title": "Don't Take Things Out of Context: Attention Intervention for Enhancing\n  Chain-of-Thought Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Take Things Out of Context: Attention Intervention for Enhancing\n  Chain-of-Thought Reasoning in Large Language Models"
                },
                "summary": "Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning\ncapabilities of large language models (LLMs), functioning as a whole to guide\nthese models in generating reasoning steps toward final answers. However, we\nobserve that isolated segments, words, or tokens within CoT demonstrations can\nunexpectedly disrupt the generation process of LLMs. The model may overly\nconcentrate on certain local information present in the demonstration,\nintroducing irrelevant noise into the reasoning process and potentially leading\nto incorrect answers. In this paper, we investigate the underlying mechanism of\nCoT through dynamically tracing and manipulating the inner workings of LLMs at\neach output step, which demonstrates that tokens exhibiting specific attention\ncharacteristics are more likely to induce the model to take things out of\ncontext; these tokens directly attend to the hidden states tied with\nprediction, without substantial integration of non-local information. Building\nupon these insights, we propose a Few-shot Attention Intervention method (FAI)\nthat dynamically analyzes the attention patterns of demonstrations to\naccurately identify these tokens and subsequently make targeted adjustments to\nthe attention weights to effectively suppress their distracting effect on LLMs.\nComprehensive experiments across multiple benchmarks demonstrate consistent\nimprovements over baseline methods, with a remarkable 5.91% improvement on the\nAQuA dataset, further highlighting the effectiveness of FAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning\ncapabilities of large language models (LLMs), functioning as a whole to guide\nthese models in generating reasoning steps toward final answers. However, we\nobserve that isolated segments, words, or tokens within CoT demonstrations can\nunexpectedly disrupt the generation process of LLMs. The model may overly\nconcentrate on certain local information present in the demonstration,\nintroducing irrelevant noise into the reasoning process and potentially leading\nto incorrect answers. In this paper, we investigate the underlying mechanism of\nCoT through dynamically tracing and manipulating the inner workings of LLMs at\neach output step, which demonstrates that tokens exhibiting specific attention\ncharacteristics are more likely to induce the model to take things out of\ncontext; these tokens directly attend to the hidden states tied with\nprediction, without substantial integration of non-local information. Building\nupon these insights, we propose a Few-shot Attention Intervention method (FAI)\nthat dynamically analyzes the attention patterns of demonstrations to\naccurately identify these tokens and subsequently make targeted adjustments to\nthe attention weights to effectively suppress their distracting effect on LLMs.\nComprehensive experiments across multiple benchmarks demonstrate consistent\nimprovements over baseline methods, with a remarkable 5.91% improvement on the\nAQuA dataset, further highlighting the effectiveness of FAI."
                },
                "authors": [
                    {
                        "name": "Shaotian Yan"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Junjie Liu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11127v1",
                "updated": "2025-03-14T06:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    43,
                    19,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:43:19Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    43,
                    19,
                    4,
                    73,
                    0
                ],
                "title": "Don't Forget It! Conditional Sparse Autoencoder Clamping Works for\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Forget It! Conditional Sparse Autoencoder Clamping Works for\n  Unlearning"
                },
                "summary": "Recent developments in Large Language Model (LLM) capabilities have brought\ngreat potential but also posed new risks. For example, LLMs with knowledge of\nbioweapons, advanced chemistry, or cyberattacks could cause violence if placed\nin the wrong hands or during malfunctions. Because of their nature as\nnear-black boxes, intuitive interpretation of LLM internals remains an open\nresearch question, preventing developers from easily controlling model behavior\nand capabilities. The use of Sparse Autoencoders (SAEs) has recently emerged as\na potential method of unraveling representations of concepts in LLMs internals,\nand has allowed developers to steer model outputs by directly modifying the\nhidden activations. In this paper, we use SAEs to identify unwanted concepts\nfrom the Weapons of Mass Destruction Proxy (WMDP) dataset within gemma-2-2b\ninternals and use feature steering to reduce the model's ability to answer\nharmful questions while retaining its performance on harmless queries. Our\nresults bring back optimism to the viability of SAE-based explicit knowledge\nunlearning techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Model (LLM) capabilities have brought\ngreat potential but also posed new risks. For example, LLMs with knowledge of\nbioweapons, advanced chemistry, or cyberattacks could cause violence if placed\nin the wrong hands or during malfunctions. Because of their nature as\nnear-black boxes, intuitive interpretation of LLM internals remains an open\nresearch question, preventing developers from easily controlling model behavior\nand capabilities. The use of Sparse Autoencoders (SAEs) has recently emerged as\na potential method of unraveling representations of concepts in LLMs internals,\nand has allowed developers to steer model outputs by directly modifying the\nhidden activations. In this paper, we use SAEs to identify unwanted concepts\nfrom the Weapons of Mass Destruction Proxy (WMDP) dataset within gemma-2-2b\ninternals and use feature steering to reduce the model's ability to answer\nharmful questions while retaining its performance on harmless queries. Our\nresults bring back optimism to the viability of SAE-based explicit knowledge\nunlearning techniques."
                },
                "authors": [
                    {
                        "name": "Matthew Khoriaty"
                    },
                    {
                        "name": "Andrii Shportko"
                    },
                    {
                        "name": "Gustavo Mercier"
                    },
                    {
                        "name": "Zach Wood-Doughty"
                    }
                ],
                "author_detail": {
                    "name": "Zach Wood-Doughty"
                },
                "arxiv_affiliation": "Northwestern University",
                "author": "Zach Wood-Doughty",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11113v1",
                "updated": "2025-03-14T06:24:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    24,
                    9,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:24:09Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    24,
                    9,
                    4,
                    73,
                    0
                ],
                "title": "Vipera: Towards systematic auditing of generative text-to-image models\n  at scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vipera: Towards systematic auditing of generative text-to-image models\n  at scale"
                },
                "summary": "Generative text-to-image (T2I) models are known for their risks related such\nas bias, offense, and misinformation. Current AI auditing methods face\nchallenges in scalability and thoroughness, and it is even more challenging to\nenable auditors to explore the auditing space in a structural and effective\nway. Vipera employs multiple visual cues including a scene graph to facilitate\nimage collection sensemaking and inspire auditors to explore and hierarchically\norganize the auditing criteria. Additionally, it leverages LLM-powered\nsuggestions to facilitate exploration of unexplored auditing directions. An\nobservational user study demonstrates Vipera's effectiveness in helping\nauditors organize their analyses while engaging with diverse criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative text-to-image (T2I) models are known for their risks related such\nas bias, offense, and misinformation. Current AI auditing methods face\nchallenges in scalability and thoroughness, and it is even more challenging to\nenable auditors to explore the auditing space in a structural and effective\nway. Vipera employs multiple visual cues including a scene graph to facilitate\nimage collection sensemaking and inspire auditors to explore and hierarchically\norganize the auditing criteria. Additionally, it leverages LLM-powered\nsuggestions to facilitate exploration of unexplored auditing directions. An\nobservational user study demonstrates Vipera's effectiveness in helping\nauditors organize their analyses while engaging with diverse criteria."
                },
                "authors": [
                    {
                        "name": "Yanwei Huang"
                    },
                    {
                        "name": "Wesley Hanwen Deng"
                    },
                    {
                        "name": "Sijia Xiao"
                    },
                    {
                        "name": "Motahhare Eslami"
                    },
                    {
                        "name": "Jason I. Hong"
                    },
                    {
                        "name": "Adam Perer"
                    }
                ],
                "author_detail": {
                    "name": "Adam Perer"
                },
                "author": "Adam Perer",
                "arxiv_doi": "10.1145/3706599.3719757",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3719757",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.11113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to CHI Late-Breaking Work (LBW) 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03114v2",
                "updated": "2025-03-14T05:57:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    5,
                    57,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-05T02:22:01Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    2,
                    22,
                    1,
                    2,
                    64,
                    0
                ],
                "title": "PromAssistant: Leveraging Large Language Models for Text-to-PromQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromAssistant: Leveraging Large Language Models for Text-to-PromQL"
                },
                "summary": "With the increasing complexity of modern online service systems,\nunderstanding the state and behavior of the systems is essential for ensuring\ntheir reliability and stability. Therefore, metric monitoring systems are\nwidely used and become an important infrastructure in online service systems.\nEngineers usually interact with metrics data by manually writing\ndomain-specific language (DSL) queries to achieve various analysis objectives.\nHowever, writing these queries can be challenging and time-consuming, as it\nrequires engineers to have high programming skills and understand the context\nof the system. In this paper, we focus on PromQL, which is the metric query DSL\nprovided by the widely used metric monitoring system Prometheus. We aim to\nsimplify metrics querying by enabling engineers to interact with metrics data\nin Prometheus through natural language, and we call this task text-to-PromQL.\nBuilding upon the insight, this paper proposes PromAssistant, a Large Language\nModel-based text-to-PromQL framework. PromAssistant first uses a knowledge\ngraph to describe the complex context of an online service system. Then,\nthrough the synergistic reasoning of LLMs and the knowledge graph,\nPromAssistant transforms engineers' natural language questions into PromQL\nqueries. To evaluate PromAssistant, we manually construct the first\ntext-to-PromQL benchmark dataset which contains 280 metric query questions. The\nexperiment results show that PromAssistant is effective in text-to-PromQL and\noutperforms baseline approaches. To the best of our knowledge, this paper is\nthe first study of text-to-PromQL, and PromAssistant pioneered the DSL\ngeneration framework for metric querying and analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing complexity of modern online service systems,\nunderstanding the state and behavior of the systems is essential for ensuring\ntheir reliability and stability. Therefore, metric monitoring systems are\nwidely used and become an important infrastructure in online service systems.\nEngineers usually interact with metrics data by manually writing\ndomain-specific language (DSL) queries to achieve various analysis objectives.\nHowever, writing these queries can be challenging and time-consuming, as it\nrequires engineers to have high programming skills and understand the context\nof the system. In this paper, we focus on PromQL, which is the metric query DSL\nprovided by the widely used metric monitoring system Prometheus. We aim to\nsimplify metrics querying by enabling engineers to interact with metrics data\nin Prometheus through natural language, and we call this task text-to-PromQL.\nBuilding upon the insight, this paper proposes PromAssistant, a Large Language\nModel-based text-to-PromQL framework. PromAssistant first uses a knowledge\ngraph to describe the complex context of an online service system. Then,\nthrough the synergistic reasoning of LLMs and the knowledge graph,\nPromAssistant transforms engineers' natural language questions into PromQL\nqueries. To evaluate PromAssistant, we manually construct the first\ntext-to-PromQL benchmark dataset which contains 280 metric query questions. The\nexperiment results show that PromAssistant is effective in text-to-PromQL and\noutperforms baseline approaches. To the best of our knowledge, this paper is\nthe first study of text-to-PromQL, and PromAssistant pioneered the DSL\ngeneration framework for metric querying and analysis."
                },
                "authors": [
                    {
                        "name": "Chenxi Zhang"
                    },
                    {
                        "name": "Bicheng Zhang"
                    },
                    {
                        "name": "Dingyu Yang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Miao Chen"
                    },
                    {
                        "name": "Senyu Xie"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Wei Bi"
                    },
                    {
                        "name": "Wei Li"
                    }
                ],
                "author_detail": {
                    "name": "Wei Li"
                },
                "author": "Wei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11103v1",
                "updated": "2025-03-14T05:47:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    5,
                    47,
                    17,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T05:47:17Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    5,
                    47,
                    17,
                    4,
                    73,
                    0
                ],
                "title": "Quantifying Interpretability in CLIP Models with Concept Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Interpretability in CLIP Models with Concept Consistency"
                },
                "summary": "CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. While recent work has proposed decomposition-based interpretability\nmethods for identifying textual descriptions of attention heads in CLIP, the\nimplications of conceptual consistency in these text labels on interpretability\nand model performance has not been explored. To bridge this gap, we study the\nconceptual consistency of text descriptions for attention heads in CLIP-like\nmodels. We conduct extensive experiments on six different models from OpenAI\nand OpenCLIP which vary by size, type of pre-training data and patch size. We\npropose Concept Consistency Score (CCS), a novel interpretability metric that\nmeasures how consistently individual attention heads in CLIP models align with\nspecific concepts. To assign concept labels to heads, we use in-context\nlearning with ChatGPT, guided by a few manually-curated examples, and validate\nthese labels using an LLM-as-a-judge approach. Our soft-pruning experiments\nreveal that high CCS heads are critical for preserving model performance, as\npruning them leads to a significantly larger performance drop than pruning\nrandom or low CCS heads. Notably, we find that high CCS heads capture essential\nconcepts and play a key role in out-of-domain detection, concept-specific\nreasoning, and video-language understanding. These results position CCS as a\npowerful interpretability metric for analyzing CLIP-like models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. While recent work has proposed decomposition-based interpretability\nmethods for identifying textual descriptions of attention heads in CLIP, the\nimplications of conceptual consistency in these text labels on interpretability\nand model performance has not been explored. To bridge this gap, we study the\nconceptual consistency of text descriptions for attention heads in CLIP-like\nmodels. We conduct extensive experiments on six different models from OpenAI\nand OpenCLIP which vary by size, type of pre-training data and patch size. We\npropose Concept Consistency Score (CCS), a novel interpretability metric that\nmeasures how consistently individual attention heads in CLIP models align with\nspecific concepts. To assign concept labels to heads, we use in-context\nlearning with ChatGPT, guided by a few manually-curated examples, and validate\nthese labels using an LLM-as-a-judge approach. Our soft-pruning experiments\nreveal that high CCS heads are critical for preserving model performance, as\npruning them leads to a significantly larger performance drop than pruning\nrandom or low CCS heads. Notably, we find that high CCS heads capture essential\nconcepts and play a key role in out-of-domain detection, concept-specific\nreasoning, and video-language understanding. These results position CCS as a\npowerful interpretability metric for analyzing CLIP-like models."
                },
                "authors": [
                    {
                        "name": "Avinash Madasu"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09501v2",
                "updated": "2025-03-14T05:33:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    5,
                    33,
                    47,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-12T16:05:31Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    5,
                    31,
                    2,
                    71,
                    0
                ],
                "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning"
                },
                "summary": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Ziyu Wan"
                    },
                    {
                        "name": "Yunxiang Li"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Hanjing Wang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Mark Schmidt"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11089v1",
                "updated": "2025-03-14T05:06:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    5,
                    6,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T05:06:07Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    5,
                    6,
                    7,
                    4,
                    73,
                    0
                ],
                "title": "EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for\n  Visual Spatial Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for\n  Visual Spatial Tasks"
                },
                "summary": "While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Xiaozhu Ju"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Jilei Mao"
                    },
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Jintao Wu"
                    },
                    {
                        "name": "Shixiong Gao"
                    },
                    {
                        "name": "Shihan Cai"
                    },
                    {
                        "name": "Zhiyuan Qin"
                    },
                    {
                        "name": "Linkai Liang"
                    },
                    {
                        "name": "Jiaxu Wang"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Renjing Xu"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11085v1",
                "updated": "2025-03-14T04:53:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    53,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T04:53:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    53,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code\n  Generation"
                },
                "summary": "Code generation has emerged as a key task to automate software development by\nconverting high-level descriptions into executable code. Large language models\n(LLMs) excel at this but depend heavily on input prompt quality.Manual prompt\nengineering can be time-consuming and inconsistent, limiting LLM effectiveness.\nThis paper introduces Prochemy, an innovative method for automatically refining\nprompts to boost code generation. Prochemy overcomes manual prompt limitations\nby automating optimization, ensuring consistency during inference, and\nsupporting multi-agent systems.It iteratively refines prompts based on model\nperformance, using an optimized final prompt for improved consistency across\ntasks. We tested Prochemy on natural language-based code generation and\ntranslation tasks using three LLM series. Results indicate Prochemy enhances\nexisting methods, improving performance by 5.0% for GPT-3.5-Turbo and 1.9% for\nGPT-4o over zero-shot baselines on HumanEval. In state-of-the-art LDB, Prochemy\n+ LDB surpasses standalone methods by 1.2-1.8%. For code translation, Prochemy\nboosts GPT-4o's Java-to-Python (AVATAR) performance from 74.5 to 84.1 (+12.9%)\nand Python-to-Java from 66.8 to 78.2 (+17.1%). Moreover, Prochemy maintains\nstrong performance when integrated with the o1-mini model, validating its\nefficacy in code tasks. Designed as plug-and-play, Prochemy optimizes prompts\nwith minimal human input, bridging the gap between simple prompts and complex\nframeworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation has emerged as a key task to automate software development by\nconverting high-level descriptions into executable code. Large language models\n(LLMs) excel at this but depend heavily on input prompt quality.Manual prompt\nengineering can be time-consuming and inconsistent, limiting LLM effectiveness.\nThis paper introduces Prochemy, an innovative method for automatically refining\nprompts to boost code generation. Prochemy overcomes manual prompt limitations\nby automating optimization, ensuring consistency during inference, and\nsupporting multi-agent systems.It iteratively refines prompts based on model\nperformance, using an optimized final prompt for improved consistency across\ntasks. We tested Prochemy on natural language-based code generation and\ntranslation tasks using three LLM series. Results indicate Prochemy enhances\nexisting methods, improving performance by 5.0% for GPT-3.5-Turbo and 1.9% for\nGPT-4o over zero-shot baselines on HumanEval. In state-of-the-art LDB, Prochemy\n+ LDB surpasses standalone methods by 1.2-1.8%. For code translation, Prochemy\nboosts GPT-4o's Java-to-Python (AVATAR) performance from 74.5 to 84.1 (+12.9%)\nand Python-to-Java from 66.8 to 78.2 (+17.1%). Moreover, Prochemy maintains\nstrong performance when integrated with the o1-mini model, validating its\nefficacy in code tasks. Designed as plug-and-play, Prochemy optimizes prompts\nwith minimal human input, bridging the gap between simple prompts and complex\nframeworks."
                },
                "authors": [
                    {
                        "name": "Sixiang Ye"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Liwei Guo"
                    },
                    {
                        "name": "Qingyuan Liang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11082v1",
                "updated": "2025-03-14T04:48:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    48,
                    38,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T04:48:38Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    48,
                    38,
                    4,
                    73,
                    0
                ],
                "title": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in\n  Completing Bug-prone Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in\n  Completing Bug-prone Code"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in code\ncompletion. However, the training data used to develop these models often\ncontain a significant amount of buggy code. Yet, it remains unclear to what\nextent these buggy instances influence LLMs' performance when tackling\nbug-prone code completion tasks. To fill this gap, this paper presents the\nfirst empirical study evaluating the performance of LLMs in completing\nbug-prone code. Through extensive experiments on 7 LLMs and the Defects4J\ndataset, we analyze LLMs' accuracy, robustness, and limitations in this\nchallenging context. Our experimental results show that completing bug-prone\ncode is significantly more challenging for LLMs than completing normal code.\nNotably, in bug-prone tasks, the likelihood of LLMs generating correct code is\nnearly the same as generating buggy code, and it is substantially lower than in\nnormal code completion tasks (e.g., 12.27% vs. 29.85% for GPT-4). To our\nsurprise, 44.44% of the bugs LLMs make are completely identical to the pre-fix\nversion, indicating that LLMs have been seriously biased by historical bugs\nwhen completing code. Additionally, we investigate the effectiveness of\nexisting post-processing techniques and find that while they can improve\nconsistency, they do not significantly reduce error rates in bug-prone code\nscenarios. Our research highlights the limitations of current LLMs in handling\nbug-prone code and underscores the need for improved models and post-processing\nstrategies to enhance code completion accuracy in real-world development\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in code\ncompletion. However, the training data used to develop these models often\ncontain a significant amount of buggy code. Yet, it remains unclear to what\nextent these buggy instances influence LLMs' performance when tackling\nbug-prone code completion tasks. To fill this gap, this paper presents the\nfirst empirical study evaluating the performance of LLMs in completing\nbug-prone code. Through extensive experiments on 7 LLMs and the Defects4J\ndataset, we analyze LLMs' accuracy, robustness, and limitations in this\nchallenging context. Our experimental results show that completing bug-prone\ncode is significantly more challenging for LLMs than completing normal code.\nNotably, in bug-prone tasks, the likelihood of LLMs generating correct code is\nnearly the same as generating buggy code, and it is substantially lower than in\nnormal code completion tasks (e.g., 12.27% vs. 29.85% for GPT-4). To our\nsurprise, 44.44% of the bugs LLMs make are completely identical to the pre-fix\nversion, indicating that LLMs have been seriously biased by historical bugs\nwhen completing code. Additionally, we investigate the effectiveness of\nexisting post-processing techniques and find that while they can improve\nconsistency, they do not significantly reduce error rates in bug-prone code\nscenarios. Our research highlights the limitations of current LLMs in handling\nbug-prone code and underscores the need for improved models and post-processing\nstrategies to enhance code completion accuracy in real-world development\nenvironments."
                },
                "authors": [
                    {
                        "name": "Liwei Guo"
                    },
                    {
                        "name": "Sixiang Ye"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yuxia Zhang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11074v1",
                "updated": "2025-03-14T04:34:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    34,
                    31,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T04:34:31Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    34,
                    31,
                    4,
                    73,
                    0
                ],
                "title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of\n  Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of\n  Reasoning Capabilities"
                },
                "summary": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward\nadvanced computational reasoning. Yet, this progress disrupts traditional agent\nframeworks, traditionally anchored by execution-oriented Large Language Models\n(LLMs). To explore this transformation, we propose the LaRMA framework,\nencompassing nine tasks across Tool Usage, Plan Design, and Problem Solving,\nassessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs\n(e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass\nLLMs in reasoning-intensive tasks like Plan Design, leveraging iterative\nreflection for superior outcomes; LLMs excel in execution-driven tasks such as\nTool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing\nLLMs as actors with LRMs as reflectors, optimize agent performance by blending\nexecution speed with reasoning depth; and LRMs' enhanced reasoning incurs\nhigher computational costs, prolonged processing, and behavioral challenges,\nincluding overthinking and fact-ignoring tendencies. This study fosters deeper\ninquiry into LRMs' balance of deep thinking and overthinking, laying a critical\nfoundation for future agent design advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward\nadvanced computational reasoning. Yet, this progress disrupts traditional agent\nframeworks, traditionally anchored by execution-oriented Large Language Models\n(LLMs). To explore this transformation, we propose the LaRMA framework,\nencompassing nine tasks across Tool Usage, Plan Design, and Problem Solving,\nassessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs\n(e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass\nLLMs in reasoning-intensive tasks like Plan Design, leveraging iterative\nreflection for superior outcomes; LLMs excel in execution-driven tasks such as\nTool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing\nLLMs as actors with LRMs as reflectors, optimize agent performance by blending\nexecution speed with reasoning depth; and LRMs' enhanced reasoning incurs\nhigher computational costs, prolonged processing, and behavioral challenges,\nincluding overthinking and fact-ignoring tendencies. This study fosters deeper\ninquiry into LRMs' balance of deep thinking and overthinking, laying a critical\nfoundation for future agent design advancements."
                },
                "authors": [
                    {
                        "name": "Xueyang Zhou"
                    },
                    {
                        "name": "Guiyao Tie"
                    },
                    {
                        "name": "Guowen Zhang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhigang Zuo"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Duanfeng Chu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "arxiv_comment": "71 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11069v1",
                "updated": "2025-03-14T04:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    26,
                    21,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T04:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    26,
                    21,
                    4,
                    73,
                    0
                ],
                "title": "API Agents vs. GUI Agents: Divergence and Convergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API Agents vs. GUI Agents: Divergence and Convergence"
                },
                "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13895v3",
                "updated": "2025-03-14T04:26:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    26,
                    9,
                    4,
                    73,
                    0
                ],
                "published": "2024-07-18T20:48:33Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    20,
                    48,
                    33,
                    3,
                    200,
                    0
                ],
                "title": "Improving the Robustness and Clinical Applicability of Automatic\n  Respiratory Sound Classification Using Deep Learning-Based Audio Enhancement:\n  Algorithm Development and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Robustness and Clinical Applicability of Automatic\n  Respiratory Sound Classification Using Deep Learning-Based Audio Enhancement:\n  Algorithm Development and Validation"
                },
                "summary": "Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions remains challenging for clinical\ndeployment. In addition, predicting signals with only background noise may\nreduce user trust in the system. This study explores the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement step\ninto automatic respiratory sound classification systems to improve robustness\nand clinical applicability. We conducted extensive experiments using various\naudio enhancement model architectures, including time-domain and\ntime-frequency-domain approaches, combined with multiple classification models\nto evaluate the module's effectiveness. The classification performance was\ncompared against the noise injection data augmentation method. These\nexperiments were carried out on two datasets: the ICBHI respiratory sound\ndataset and the FABS dataset. Furthermore, a physician validation study\nassessed the system's clinical utility. Integrating the audio enhancement\nmodule resulted in a 21.9% increase in the ICBHI classification score and a\n4.1% improvement on the FABS dataset in multi-class noisy scenarios.\nQuantitative analysis revealed efficiency gains, higher diagnostic confidence,\nand increased trust, with workflows using enhanced audio improving diagnostic\nsensitivity by 11.6% and enabling high-confidence diagnoses. Incorporating an\naudio enhancement algorithm boosts the robustness and clinical utility of\nautomatic respiratory sound classification systems, enhancing performance in\nnoisy environments and fostering greater trust among medical professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning techniques have shown promising results in the automatic\nclassification of respiratory sounds. However, accurately distinguishing these\nsounds in real-world noisy conditions remains challenging for clinical\ndeployment. In addition, predicting signals with only background noise may\nreduce user trust in the system. This study explores the feasibility and\neffectiveness of incorporating a deep learning-based audio enhancement step\ninto automatic respiratory sound classification systems to improve robustness\nand clinical applicability. We conducted extensive experiments using various\naudio enhancement model architectures, including time-domain and\ntime-frequency-domain approaches, combined with multiple classification models\nto evaluate the module's effectiveness. The classification performance was\ncompared against the noise injection data augmentation method. These\nexperiments were carried out on two datasets: the ICBHI respiratory sound\ndataset and the FABS dataset. Furthermore, a physician validation study\nassessed the system's clinical utility. Integrating the audio enhancement\nmodule resulted in a 21.9% increase in the ICBHI classification score and a\n4.1% improvement on the FABS dataset in multi-class noisy scenarios.\nQuantitative analysis revealed efficiency gains, higher diagnostic confidence,\nand increased trust, with workflows using enhanced audio improving diagnostic\nsensitivity by 11.6% and enabling high-confidence diagnoses. Incorporating an\naudio enhancement algorithm boosts the robustness and clinical utility of\nautomatic respiratory sound classification systems, enhancing performance in\nnoisy environments and fostering greater trust among medical professionals."
                },
                "authors": [
                    {
                        "name": "Jing-Tong Tzeng"
                    },
                    {
                        "name": "Jeng-Lin Li"
                    },
                    {
                        "name": "Huan-Yu Chen"
                    },
                    {
                        "name": "Chun-Hsiang Huang"
                    },
                    {
                        "name": "Chi-Hsin Chen"
                    },
                    {
                        "name": "Cheng-Yi Fan"
                    },
                    {
                        "name": "Edward Pei-Chuan Huang"
                    },
                    {
                        "name": "Chi-Chun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Chun Lee"
                },
                "author": "Chi-Chun Lee",
                "arxiv_doi": "10.2196/67239",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2196/67239",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on JMIR AI https://ai.jmir.org/2025/1/e67239. Demo website:\n  https://rogertzeng.github.io/ReSC-AE/",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11068v1",
                "updated": "2025-03-14T04:23:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    23,
                    59,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T04:23:59Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    23,
                    59,
                    4,
                    73,
                    0
                ],
                "title": "DeepSeek Powered Solid Dosage Formulation Design and Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek Powered Solid Dosage Formulation Design and Development"
                },
                "summary": "Pharmaceutical process design and development for generic, innovative, or\npersonalized drugs have always been a time consuming, costly, rigorous process,\nthat involves multistage evaluation for better quality control and assurance.\nLarge language models (LLMs), a type of generative artificial intelligence\nsystem, can augment laboratory research in the pharmaceutical engineering\nprocess by helping scientists to extract knowledge from literature, design\nparameters, and collect and interpret experimental data, ultimately\naccelerating scientific discovery. LLMs with prompt engineering technologies\nchange the researcher's thinking protocol from traditional empirical knowledg\nto streamlined thinking that connects the performance and structured parameters\ntogether. In this work, we investigate and evaluate how prompt engineering\ntechnologies can enhance the drug design process from different strategies such\nas zero shot, few shot, chain of thought, etc. The dissolution profile for\nspecific drugs is predicted and suggested from the LLMs model. Furthermore, the\nfundamental physical properties such as PSD, aspect ratio, and specific surface\narea could be inversely designed from the LLMs model. Finally, all the results\nare evaluated and validated by real-world cases to prove the reliability of\nprompt engineering techniques. This work breaks down any barriers in developing\na systematic framework where LLMs assist in formulation design, process\ncontrol, and decision making. Finally, we conclude the work by discussing open\nchallenges and future research directions in pharmaceutical processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pharmaceutical process design and development for generic, innovative, or\npersonalized drugs have always been a time consuming, costly, rigorous process,\nthat involves multistage evaluation for better quality control and assurance.\nLarge language models (LLMs), a type of generative artificial intelligence\nsystem, can augment laboratory research in the pharmaceutical engineering\nprocess by helping scientists to extract knowledge from literature, design\nparameters, and collect and interpret experimental data, ultimately\naccelerating scientific discovery. LLMs with prompt engineering technologies\nchange the researcher's thinking protocol from traditional empirical knowledg\nto streamlined thinking that connects the performance and structured parameters\ntogether. In this work, we investigate and evaluate how prompt engineering\ntechnologies can enhance the drug design process from different strategies such\nas zero shot, few shot, chain of thought, etc. The dissolution profile for\nspecific drugs is predicted and suggested from the LLMs model. Furthermore, the\nfundamental physical properties such as PSD, aspect ratio, and specific surface\narea could be inversely designed from the LLMs model. Finally, all the results\nare evaluated and validated by real-world cases to prove the reliability of\nprompt engineering techniques. This work breaks down any barriers in developing\na systematic framework where LLMs assist in formulation design, process\ncontrol, and decision making. Finally, we conclude the work by discussing open\nchallenges and future research directions in pharmaceutical processes."
                },
                "authors": [
                    {
                        "name": "Leqi Lin"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Kaiyuan Yang"
                    },
                    {
                        "name": "Xizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xizhong Chen"
                },
                "author": "Xizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06190v2",
                "updated": "2025-03-14T03:59:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    59,
                    35,
                    4,
                    73,
                    0
                ],
                "published": "2024-02-09T05:06:58Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    5,
                    6,
                    58,
                    4,
                    40,
                    0
                ],
                "title": "Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain"
                },
                "summary": "Standard modern machine-learning-based imaging methods have faced challenges\nin medical applications due to the high cost of dataset construction and,\nthereby, the limited labeled training data available. Additionally, upon\ndeployment, these methods are usually used to process a large volume of data on\na daily basis, imposing a high maintenance cost on medical facilities. In this\npaper, we introduce a new neural network architecture, termed LoGoNet, with a\ntailored self-supervised learning (SSL) method to mitigate such challenges.\nLoGoNet integrates a novel feature extractor within a U-shaped architecture,\nleveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture\nboth long-range and short-range feature dependencies adeptly. This is in\ncontrast to existing methods that rely on increasing network capacity to\nenhance feature extraction. This combination of novel techniques in our model\nis especially beneficial in medical image segmentation, given the difficulty of\nlearning intricate and often irregular body organ shapes, such as the spleen.\nComplementary, we propose a novel SSL method tailored for 3D images to\ncompensate for the lack of large labeled datasets. The method combines masking\nand contrastive learning techniques within a multi-task learning framework and\nis compatible with both Vision Transformer (ViT) and CNN-based models. We\ndemonstrate the efficacy of our methods in numerous tasks across two standard\ndatasets (i.e., BTCV and MSD). Benchmark comparisons with eight\nstate-of-the-art models highlight LoGoNet's superior performance in both\ninference time and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard modern machine-learning-based imaging methods have faced challenges\nin medical applications due to the high cost of dataset construction and,\nthereby, the limited labeled training data available. Additionally, upon\ndeployment, these methods are usually used to process a large volume of data on\na daily basis, imposing a high maintenance cost on medical facilities. In this\npaper, we introduce a new neural network architecture, termed LoGoNet, with a\ntailored self-supervised learning (SSL) method to mitigate such challenges.\nLoGoNet integrates a novel feature extractor within a U-shaped architecture,\nleveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture\nboth long-range and short-range feature dependencies adeptly. This is in\ncontrast to existing methods that rely on increasing network capacity to\nenhance feature extraction. This combination of novel techniques in our model\nis especially beneficial in medical image segmentation, given the difficulty of\nlearning intricate and often irregular body organ shapes, such as the spleen.\nComplementary, we propose a novel SSL method tailored for 3D images to\ncompensate for the lack of large labeled datasets. The method combines masking\nand contrastive learning techniques within a multi-task learning framework and\nis compatible with both Vision Transformer (ViT) and CNN-based models. We\ndemonstrate the efficacy of our methods in numerous tasks across two standard\ndatasets (i.e., BTCV and MSD). Benchmark comparisons with eight\nstate-of-the-art models highlight LoGoNet's superior performance in both\ninference time and accuracy."
                },
                "authors": [
                    {
                        "name": "Amin Karimi Monsefi"
                    },
                    {
                        "name": "Payam Karisani"
                    },
                    {
                        "name": "Mengxi Zhou"
                    },
                    {
                        "name": "Stacey Choi"
                    },
                    {
                        "name": "Nathan Doble"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Srinivasan Parthasarathy"
                    },
                    {
                        "name": "Rajiv Ramnath"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ramnath"
                },
                "author": "Rajiv Ramnath",
                "arxiv_doi": "10.1145/3637528.3672069",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3672069",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.06190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03279v4",
                "updated": "2025-03-14T03:56:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    56,
                    58,
                    4,
                    73,
                    0
                ],
                "published": "2024-05-06T08:52:11Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    8,
                    52,
                    11,
                    0,
                    127,
                    0
                ],
                "title": "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous\n  Prompt Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous\n  Prompt Learning"
                },
                "summary": "Model editing aims to correct outdated or erroneous knowledge in large\nlanguage models (LLMs) without the need for costly retraining. Lifelong model\nediting is the most challenging task that caters to the continuous editing\nrequirements of LLMs. Prior works primarily focus on single or batch editing;\nnevertheless, these methods fall short in lifelong editing scenarios due to\ncatastrophic knowledge forgetting and the degradation of model performance.\nAlthough retrieval-based methods alleviate these issues, they are impeded by\nslow and cumbersome processes of integrating the retrieved knowledge into the\nmodel. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous\nPrompt lEarning method, to boost editing efficacy and inference efficiency in\nlifelong learning. RECIPE first converts knowledge statements into short and\ninformative continuous prompts, prefixed to the LLM's input query embedding, to\nefficiently refine the response grounded on the knowledge. It further\nintegrates the Knowledge Sentinel (KS) that acts as an intermediary to\ncalculate a dynamic threshold, determining whether the retrieval repository\ncontains relevant knowledge. Our retriever and prompt encoder are jointly\ntrained to achieve editing properties, i.e., reliability, generality, and\nlocality. In our experiments, RECIPE is assessed extensively across multiple\nLLMs and editing datasets, where it achieves superior editing performance.\nRECIPE also demonstrates its capability to maintain the overall performance of\nLLMs alongside showcasing fast editing and inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct outdated or erroneous knowledge in large\nlanguage models (LLMs) without the need for costly retraining. Lifelong model\nediting is the most challenging task that caters to the continuous editing\nrequirements of LLMs. Prior works primarily focus on single or batch editing;\nnevertheless, these methods fall short in lifelong editing scenarios due to\ncatastrophic knowledge forgetting and the degradation of model performance.\nAlthough retrieval-based methods alleviate these issues, they are impeded by\nslow and cumbersome processes of integrating the retrieved knowledge into the\nmodel. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous\nPrompt lEarning method, to boost editing efficacy and inference efficiency in\nlifelong learning. RECIPE first converts knowledge statements into short and\ninformative continuous prompts, prefixed to the LLM's input query embedding, to\nefficiently refine the response grounded on the knowledge. It further\nintegrates the Knowledge Sentinel (KS) that acts as an intermediary to\ncalculate a dynamic threshold, determining whether the retrieval repository\ncontains relevant knowledge. Our retriever and prompt encoder are jointly\ntrained to achieve editing properties, i.e., reliability, generality, and\nlocality. In our experiments, RECIPE is assessed extensively across multiple\nLLMs and editing datasets, where it achieves superior editing performance.\nRECIPE also demonstrates its capability to maintain the overall performance of\nLLMs alongside showcasing fast editing and inference speed."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Xiaofeng He"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "arxiv_comment": "EMNLP 2024 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11061v1",
                "updated": "2025-03-14T03:54:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    54,
                    43,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T03:54:43Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    54,
                    43,
                    4,
                    73,
                    0
                ],
                "title": "Generative Modelling for Mathematical Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Modelling for Mathematical Discovery"
                },
                "summary": "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on."
                },
                "authors": [
                    {
                        "name": "Jordan S. Ellenberg"
                    },
                    {
                        "name": "Cristofero S. Fraser-Taliente"
                    },
                    {
                        "name": "Thomas R. Harvey"
                    },
                    {
                        "name": "Karan Srivastava"
                    },
                    {
                        "name": "Andrew V. Sutherland"
                    }
                ],
                "author_detail": {
                    "name": "Andrew V. Sutherland"
                },
                "author": "Andrew V. Sutherland",
                "arxiv_comment": "22 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07351v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07351v3",
                "updated": "2025-03-14T03:54:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    54,
                    26,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-11T08:22:21Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    22,
                    21,
                    1,
                    42,
                    0
                ],
                "title": "Multi-Knowledge-oriented Nighttime Haze Imaging Enhancer for\n  Vision-driven Intelligent Transportation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Knowledge-oriented Nighttime Haze Imaging Enhancer for\n  Vision-driven Intelligent Transportation Systems"
                },
                "summary": "Salient object detection (SOD) plays a critical role in intelligent\ntransportation systems (ITS), facilitating the detection and segmentation of\nkey visual elements in an image. However, adverse imaging conditions such as\nhaze during the day, low light, and haze at night severely degrade image\nquality and hinder reliable object detection in real-world scenarios. To\naddress these challenges, we propose a multi-knowledge-oriented nighttime haze\nimaging enhancer (MKoIE), which integrates three tasks: daytime dehazing,\nlow-light enhancement, and nighttime dehazing. The MKoIE incorporates two key\ninnovative components: First, the network employs a task-oriented node learning\nmechanism to handle three specific degradation types: day-time haze, low light,\nand night-time haze conditions, with an embedded self-attention module\nenhancing its performance in nighttime imaging. In addition, multi-receptive\nfield enhancement module that efficiently extracts multi-scale features through\nthree parallel depthwise separable convolution branches with different dilation\nrates, capturing comprehensive spatial information with minimal computational\noverhead to meet the requirements of real-time ITS deployment. To ensure\noptimal image reconstruction quality and visual characteristics, we suggest a\nhybrid loss function. Extensive experiments on different types of\nweather/imaging conditions illustrate that MKoIE surpasses existing methods,\nenhancing the reliability, accuracy, and operational efficiency of ITS. The\ncode is available at https://github.com/Ai-Chen-Lab/MKoIE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Salient object detection (SOD) plays a critical role in intelligent\ntransportation systems (ITS), facilitating the detection and segmentation of\nkey visual elements in an image. However, adverse imaging conditions such as\nhaze during the day, low light, and haze at night severely degrade image\nquality and hinder reliable object detection in real-world scenarios. To\naddress these challenges, we propose a multi-knowledge-oriented nighttime haze\nimaging enhancer (MKoIE), which integrates three tasks: daytime dehazing,\nlow-light enhancement, and nighttime dehazing. The MKoIE incorporates two key\ninnovative components: First, the network employs a task-oriented node learning\nmechanism to handle three specific degradation types: day-time haze, low light,\nand night-time haze conditions, with an embedded self-attention module\nenhancing its performance in nighttime imaging. In addition, multi-receptive\nfield enhancement module that efficiently extracts multi-scale features through\nthree parallel depthwise separable convolution branches with different dilation\nrates, capturing comprehensive spatial information with minimal computational\noverhead to meet the requirements of real-time ITS deployment. To ensure\noptimal image reconstruction quality and visual characteristics, we suggest a\nhybrid loss function. Extensive experiments on different types of\nweather/imaging conditions illustrate that MKoIE surpasses existing methods,\nenhancing the reliability, accuracy, and operational efficiency of ITS. The\ncode is available at https://github.com/Ai-Chen-Lab/MKoIE."
                },
                "authors": [
                    {
                        "name": "Ai Chen"
                    },
                    {
                        "name": "Yuxu Lu"
                    },
                    {
                        "name": "Dong Yang"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Yan Fu"
                    },
                    {
                        "name": "Duanbing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Duanbing Chen"
                },
                "author": "Duanbing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07351v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07351v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11060v1",
                "updated": "2025-03-14T03:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    54,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T03:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    54,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "BannerAgency: Advertising Banner Design with Multimodal LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BannerAgency: Advertising Banner Design with Multimodal LLM Agents"
                },
                "summary": "Advertising banners are critical for capturing user attention and enhancing\nadvertising campaign effectiveness. Creating aesthetically pleasing banner\ndesigns while conveying the campaign messages is challenging due to the large\nsearch space involving multiple design elements. Additionally, advertisers need\nmultiple sizes for different displays and various versions to target different\nsectors of audiences. Since design is intrinsically an iterative and subjective\nprocess, flexible editability is also in high demand for practical usage. While\ncurrent models have served as assistants to human designers in various design\ntasks, they typically handle only segments of the creative design process or\nproduce pixel-based outputs that limit editability. This paper introduces a\ntraining-free framework for fully automated banner ad design creation, enabling\nfrontier multimodal large language models (MLLMs) to streamline the production\nof effective banners with minimal manual effort across diverse marketing\ncontexts. We present BannerAgency, an MLLM agent system that collaborates with\nadvertisers to understand their brand identity and banner objectives, generates\nmatching background images, creates blueprints for foreground design elements,\nand renders the final creatives as editable components in Figma or SVG formats\nrather than static pixels. To facilitate evaluation and future research, we\nintroduce BannerRequest400, a benchmark featuring 100 unique logos paired with\n400 diverse banner requests. Through quantitative and qualitative evaluations,\nwe demonstrate the framework's effectiveness, emphasizing the quality of the\ngenerated banner designs, their adaptability to various banner requests, and\ntheir strong editability enabled by this component-based approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advertising banners are critical for capturing user attention and enhancing\nadvertising campaign effectiveness. Creating aesthetically pleasing banner\ndesigns while conveying the campaign messages is challenging due to the large\nsearch space involving multiple design elements. Additionally, advertisers need\nmultiple sizes for different displays and various versions to target different\nsectors of audiences. Since design is intrinsically an iterative and subjective\nprocess, flexible editability is also in high demand for practical usage. While\ncurrent models have served as assistants to human designers in various design\ntasks, they typically handle only segments of the creative design process or\nproduce pixel-based outputs that limit editability. This paper introduces a\ntraining-free framework for fully automated banner ad design creation, enabling\nfrontier multimodal large language models (MLLMs) to streamline the production\nof effective banners with minimal manual effort across diverse marketing\ncontexts. We present BannerAgency, an MLLM agent system that collaborates with\nadvertisers to understand their brand identity and banner objectives, generates\nmatching background images, creates blueprints for foreground design elements,\nand renders the final creatives as editable components in Figma or SVG formats\nrather than static pixels. To facilitate evaluation and future research, we\nintroduce BannerRequest400, a benchmark featuring 100 unique logos paired with\n400 diverse banner requests. Through quantitative and qualitative evaluations,\nwe demonstrate the framework's effectiveness, emphasizing the quality of the\ngenerated banner designs, their adaptability to various banner requests, and\ntheir strong editability enabled by this component-based approach."
                },
                "authors": [
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Yotaro Shimose"
                    },
                    {
                        "name": "Shingo Takamatsu"
                    }
                ],
                "author_detail": {
                    "name": "Shingo Takamatsu"
                },
                "author": "Shingo Takamatsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15432v2",
                "updated": "2025-03-14T03:47:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    47,
                    2,
                    4,
                    73,
                    0
                ],
                "published": "2024-11-23T03:19:40Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    3,
                    19,
                    40,
                    5,
                    328,
                    0
                ],
                "title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank\n  Mixture-of-Experts"
                },
                "summary": "Model editing aims to correct inaccurate knowledge, update outdated\ninformation, and incorporate new data into Large Language Models (LLMs) without\nthe need for retraining. This task poses challenges in lifelong scenarios where\nedits must be continuously applied for real-world applications. While some\neditors demonstrate strong robustness for lifelong editing in pure LLMs, Vision\nLLMs (VLLMs), which incorporate an additional vision modality, are not directly\nadaptable to existing LLM editors. In this paper, we propose LiveEdit, a\nLIfelong Vision language modEl Edit to bridge the gap between lifelong LLM\nediting and VLLMs. We begin by training an editing expert generator to\nindependently produce low-rank experts for each editing instance, with the goal\nof correcting the relevant responses of the VLLM. A hard filtering mechanism is\ndeveloped to utilize visual semantic knowledge, thereby coarsely eliminating\nvisually irrelevant experts for input queries during the inference stage of the\npost-edited model. Finally, to integrate visually relevant experts, we\nintroduce a soft routing mechanism based on textual semantic relevance to\nachieve multi-expert fusion. For evaluation, we establish a benchmark for\nlifelong VLLM editing. Extensive experiments demonstrate that LiveEdit offers\nsignificant advantages in lifelong VLLM editing scenarios. Further experiments\nvalidate the rationality and effectiveness of each module design in LiveEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct inaccurate knowledge, update outdated\ninformation, and incorporate new data into Large Language Models (LLMs) without\nthe need for retraining. This task poses challenges in lifelong scenarios where\nedits must be continuously applied for real-world applications. While some\neditors demonstrate strong robustness for lifelong editing in pure LLMs, Vision\nLLMs (VLLMs), which incorporate an additional vision modality, are not directly\nadaptable to existing LLM editors. In this paper, we propose LiveEdit, a\nLIfelong Vision language modEl Edit to bridge the gap between lifelong LLM\nediting and VLLMs. We begin by training an editing expert generator to\nindependently produce low-rank experts for each editing instance, with the goal\nof correcting the relevant responses of the VLLM. A hard filtering mechanism is\ndeveloped to utilize visual semantic knowledge, thereby coarsely eliminating\nvisually irrelevant experts for input queries during the inference stage of the\npost-edited model. Finally, to integrate visually relevant experts, we\nintroduce a soft routing mechanism based on textual semantic relevance to\nachieve multi-expert fusion. For evaluation, we establish a benchmark for\nlifelong VLLM editing. Extensive experiments demonstrate that LiveEdit offers\nsignificant advantages in lifelong VLLM editing scenarios. Further experiments\nvalidate the rationality and effectiveness of each module design in LiveEdit."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Dakan Wang"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Wangyue Li"
                    },
                    {
                        "name": "Xiaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng He"
                },
                "author": "Xiaofeng He",
                "arxiv_comment": "CVPR 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18373v2",
                "updated": "2025-03-14T03:39:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    39,
                    7,
                    4,
                    73,
                    0
                ],
                "published": "2024-10-24T02:32:56Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    2,
                    32,
                    56,
                    3,
                    298,
                    0
                ],
                "title": "UGotMe: An Embodied System for Affective Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UGotMe: An Embodied System for Affective Human-Robot Interaction"
                },
                "summary": "Equipping humanoid robots with the capability to understand emotional states\nof human interactants and express emotions appropriately according to\nsituations is essential for affective human-robot interaction. However,\nenabling current vision-aware multimodal emotion recognition models for\naffective human-robot interaction in the real-world raises embodiment\nchallenges: addressing the environmental noise issue and meeting real-time\nrequirements. First, in multiparty conversation scenarios, the noises inherited\nin the visual observation of the robot, which may come from either 1)\ndistracting objects in the scene or 2) inactive speakers appearing in the field\nof view of the robot, hinder the models from extracting emotional cues from\nvision inputs. Secondly, realtime response, a desired feature for an\ninteractive system, is also challenging to achieve. To tackle both challenges,\nwe introduce an affective human-robot interaction system called UGotMe designed\nspecifically for multiparty conversations. Two denoising strategies are\nproposed and incorporated into the system to solve the first issue.\nSpecifically, to filter out distracting objects in the scene, we propose\nextracting face images of the speakers from the raw images and introduce a\ncustomized active face extraction strategy to rule out inactive speakers. As\nfor the second issue, we employ efficient data transmission from the robot to\nthe local server to improve realtime response capability. We deploy UGotMe on a\nhuman robot named Ameca to validate its real-time inference capabilities in\npractical scenarios. Videos demonstrating real-world deployment are available\nat https://pi3-141592653.github.io/UGotMe/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equipping humanoid robots with the capability to understand emotional states\nof human interactants and express emotions appropriately according to\nsituations is essential for affective human-robot interaction. However,\nenabling current vision-aware multimodal emotion recognition models for\naffective human-robot interaction in the real-world raises embodiment\nchallenges: addressing the environmental noise issue and meeting real-time\nrequirements. First, in multiparty conversation scenarios, the noises inherited\nin the visual observation of the robot, which may come from either 1)\ndistracting objects in the scene or 2) inactive speakers appearing in the field\nof view of the robot, hinder the models from extracting emotional cues from\nvision inputs. Secondly, realtime response, a desired feature for an\ninteractive system, is also challenging to achieve. To tackle both challenges,\nwe introduce an affective human-robot interaction system called UGotMe designed\nspecifically for multiparty conversations. Two denoising strategies are\nproposed and incorporated into the system to solve the first issue.\nSpecifically, to filter out distracting objects in the scene, we propose\nextracting face images of the speakers from the raw images and introduce a\ncustomized active face extraction strategy to rule out inactive speakers. As\nfor the second issue, we employ efficient data transmission from the robot to\nthe local server to improve realtime response capability. We deploy UGotMe on a\nhuman robot named Ameca to validate its real-time inference capabilities in\npractical scenarios. Videos demonstrating real-world deployment are available\nat https://pi3-141592653.github.io/UGotMe/."
                },
                "authors": [
                    {
                        "name": "Peizhen Li"
                    },
                    {
                        "name": "Longbing Cao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Xiaohan Yu"
                    },
                    {
                        "name": "Runze Yang"
                    }
                ],
                "author_detail": {
                    "name": "Runze Yang"
                },
                "author": "Runze Yang",
                "arxiv_comment": "Accepted at ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03277v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03277v3",
                "updated": "2025-03-14T03:19:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    19,
                    0,
                    4,
                    73,
                    0
                ],
                "published": "2024-09-05T06:41:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    41,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart\n  Understanding"
                },
                "summary": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, current MLLMs still struggle to provide\nfaithful data and reliable analysis only based on charts. To address it, we\npropose ChartMoE, which employs the Mixture of Expert (MoE) architecture to\nreplace the traditional linear projector to bridge the modality gap.\nSpecifically, we train several linear connectors through distinct alignment\ntasks, which are utilized as the foundational initialization parameters for\ndifferent experts. Additionally, we introduce ChartMoE-Align, a dataset with\nnearly 1 million chart-table-JSON-code quadruples to conduct three alignment\ntasks (chart-table/JSON/code). Combined with the vanilla connector, we\ninitialize different experts diversely and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48\\% to 84.64\\% on the ChartQA benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, current MLLMs still struggle to provide\nfaithful data and reliable analysis only based on charts. To address it, we\npropose ChartMoE, which employs the Mixture of Expert (MoE) architecture to\nreplace the traditional linear projector to bridge the modality gap.\nSpecifically, we train several linear connectors through distinct alignment\ntasks, which are utilized as the foundational initialization parameters for\ndifferent experts. Additionally, we introduce ChartMoE-Align, a dataset with\nnearly 1 million chart-table-JSON-code quadruples to conduct three alignment\ntasks (chart-table/JSON/code). Combined with the vanilla connector, we\ninitialize different experts diversely and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48\\% to 84.64\\% on the ChartQA benchmark."
                },
                "authors": [
                    {
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Sinan Du"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03277v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03277v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11038v1",
                "updated": "2025-03-14T03:07:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    7,
                    2,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T03:07:02Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    3,
                    7,
                    2,
                    4,
                    73,
                    0
                ],
                "title": "ACMo: Attribute Controllable Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACMo: Attribute Controllable Motion Generation"
                },
                "summary": "Attributes such as style, fine-grained text, and trajectory are specific\nconditions for describing motion. However, existing methods often lack precise\nuser control over motion attributes and suffer from limited generalizability to\nunseen motions. This work introduces an Attribute Controllable Motion\ngeneration architecture, to address these challenges via decouple any\nconditions and control them separately. Firstly, we explored the Attribute\nDiffusion Model to imporve text-to-motion performance via decouple text and\nmotion learning, as the controllable model relies heavily on the pre-trained\nmodel. Then, we introduce Motion Adpater to quickly finetune previously unseen\nmotion patterns. Its motion prompts inputs achieve multimodal text-to-motion\ngeneration that captures user-specified styles. Finally, we propose a LLM\nPlanner to bridge the gap between unseen attributes and dataset-specific texts\nvia local knowledage for user-friendly interaction. Our approach introduces the\ncapability for motion prompts for stylize generation, enabling fine-grained and\nuser-friendly attribute control while providing performance comparable to\nstate-of-the-art methods. Project page: https://mjwei3d.github.io/ACMo/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attributes such as style, fine-grained text, and trajectory are specific\nconditions for describing motion. However, existing methods often lack precise\nuser control over motion attributes and suffer from limited generalizability to\nunseen motions. This work introduces an Attribute Controllable Motion\ngeneration architecture, to address these challenges via decouple any\nconditions and control them separately. Firstly, we explored the Attribute\nDiffusion Model to imporve text-to-motion performance via decouple text and\nmotion learning, as the controllable model relies heavily on the pre-trained\nmodel. Then, we introduce Motion Adpater to quickly finetune previously unseen\nmotion patterns. Its motion prompts inputs achieve multimodal text-to-motion\ngeneration that captures user-specified styles. Finally, we propose a LLM\nPlanner to bridge the gap between unseen attributes and dataset-specific texts\nvia local knowledage for user-friendly interaction. Our approach introduces the\ncapability for motion prompts for stylize generation, enabling fine-grained and\nuser-friendly attribute control while providing performance comparable to\nstate-of-the-art methods. Project page: https://mjwei3d.github.io/ACMo/"
                },
                "authors": [
                    {
                        "name": "Mingjie Wei"
                    },
                    {
                        "name": "Xuemei Xie"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18470v3",
                "updated": "2025-03-14T02:48:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    2,
                    48,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-04T01:30:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    1,
                    30,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World\n  Spatial Reasoning Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World\n  Spatial Reasoning Questions"
                },
                "summary": "Spatial reasoning remains a challenge for Large Language Models (LLMs), which\nstruggle with spatial data retrieval and reasoning. We propose Spatial\nRetrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to\nspatial tasks by integrating sparse spatial retrieval (spatial databases) and\ndense semantic retrieval (LLM-based similarity). A multi-objective ranking\nstrategy balances spatial constraints and semantic relevance, while an\nLLM-guided generator ensures coherent responses. Experiments on a real-world\ntourism dataset show that Spatial-RAG significantly improves spatial question\nanswering, bridging the gap between LLMs and spatial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning remains a challenge for Large Language Models (LLMs), which\nstruggle with spatial data retrieval and reasoning. We propose Spatial\nRetrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to\nspatial tasks by integrating sparse spatial retrieval (spatial databases) and\ndense semantic retrieval (LLM-based similarity). A multi-objective ranking\nstrategy balances spatial constraints and semantic relevance, while an\nLLM-guided generator ensures coherent responses. Experiments on a real-world\ntourism dataset show that Spatial-RAG significantly improves spatial question\nanswering, bridging the gap between LLMs and spatial intelligence."
                },
                "authors": [
                    {
                        "name": "Dazhou Yu"
                    },
                    {
                        "name": "Riyang Bao"
                    },
                    {
                        "name": "Gengchen Mai"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20592v2",
                "updated": "2025-03-14T02:48:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    2,
                    48,
                    3,
                    4,
                    73,
                    0
                ],
                "published": "2025-02-27T23:34:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    34,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document\n  Processing"
                },
                "summary": "Recent advances in test-time scaling have shown promising results in\nimproving Large Language Models (LLMs) performance through strategic\ncomputation allocation during inference. While this approach has demonstrated\nstrong performance improvements in logical and mathematical reasoning tasks,\nits application to natural language generation (NLG), especially summarization,\nhas yet to be explored. Multi-Document Summarization (MDS) is a challenging\ntask that focuses on extracting and synthesizing useful information from\nmultiple lengthy documents. Unlike reasoning tasks, MDS requires a more nuanced\napproach to prompt design and ensemble, as there is no \"best\" prompt to satisfy\ndiverse summarization requirements. To address this, we propose a novel\nframework that leverages inference-time scaling for this task. Precisely, we\ntake prompt ensemble approach by leveraging various prompt to first generate\ncandidate summaries and then ensemble them with an aggregator to produce a\nrefined summary. We also introduce two new evaluation metrics:\nConsistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score,\nto enhance LLM's contextual understanding while mitigating its positional bias.\nExtensive experiments demonstrate the effectiveness of our approach in\nimproving summary quality while identifying and analyzing the scaling\nboundaries in summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in test-time scaling have shown promising results in\nimproving Large Language Models (LLMs) performance through strategic\ncomputation allocation during inference. While this approach has demonstrated\nstrong performance improvements in logical and mathematical reasoning tasks,\nits application to natural language generation (NLG), especially summarization,\nhas yet to be explored. Multi-Document Summarization (MDS) is a challenging\ntask that focuses on extracting and synthesizing useful information from\nmultiple lengthy documents. Unlike reasoning tasks, MDS requires a more nuanced\napproach to prompt design and ensemble, as there is no \"best\" prompt to satisfy\ndiverse summarization requirements. To address this, we propose a novel\nframework that leverages inference-time scaling for this task. Precisely, we\ntake prompt ensemble approach by leveraging various prompt to first generate\ncandidate summaries and then ensemble them with an aggregator to produce a\nrefined summary. We also introduce two new evaluation metrics:\nConsistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score,\nto enhance LLM's contextual understanding while mitigating its positional bias.\nExtensive experiments demonstrate the effectiveness of our approach in\nimproving summary quality while identifying and analyzing the scaling\nboundaries in summarization tasks."
                },
                "authors": [
                    {
                        "name": "Juntai Cao"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Chuyuan Li"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11023v1",
                "updated": "2025-03-14T02:43:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    2,
                    43,
                    29,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T02:43:29Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    2,
                    43,
                    29,
                    4,
                    73,
                    0
                ],
                "title": "Beyond A Single AI Cluster: A Survey of Decentralized LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond A Single AI Cluster: A Survey of Decentralized LLM Training"
                },
                "summary": "The emergence of large language models (LLMs) has revolutionized AI\ndevelopment, yet their training demands computational resources beyond a single\ncluster or even datacenter, limiting accessibility to large organizations.\nDecentralized training has emerged as a promising paradigm to leverage\ndispersed resources across clusters, datacenters, and global regions,\ndemocratizing LLM development for broader communities. As the first\ncomprehensive exploration of this emerging field, we present decentralized LLM\ntraining as a resource-driven paradigm and categorize it into community-driven\nand organizational approaches. Furthermore, our in-depth analysis clarifies\ndecentralized LLM training, including: (1) position with related domain\nconcepts comparison, (2) decentralized resource development trends, and (3)\nrecent advances with discussion under a novel taxonomy. We also provide\nup-to-date case studies and explore future directions, contributing to the\nevolution of decentralized LLM training research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has revolutionized AI\ndevelopment, yet their training demands computational resources beyond a single\ncluster or even datacenter, limiting accessibility to large organizations.\nDecentralized training has emerged as a promising paradigm to leverage\ndispersed resources across clusters, datacenters, and global regions,\ndemocratizing LLM development for broader communities. As the first\ncomprehensive exploration of this emerging field, we present decentralized LLM\ntraining as a resource-driven paradigm and categorize it into community-driven\nand organizational approaches. Furthermore, our in-depth analysis clarifies\ndecentralized LLM training, including: (1) position with related domain\nconcepts comparison, (2) decentralized resource development trends, and (3)\nrecent advances with discussion under a novel taxonomy. We also provide\nup-to-date case studies and explore future directions, contributing to the\nevolution of decentralized LLM training research."
                },
                "authors": [
                    {
                        "name": "Haotian Dong"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Rongwei Lu"
                    },
                    {
                        "name": "Jiajun Luo"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ying Shen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11019v1",
                "updated": "2025-03-14T02:30:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    2,
                    30,
                    13,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T02:30:13Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    2,
                    30,
                    13,
                    4,
                    73,
                    0
                ],
                "title": "Residual Policy Gradient: A Reward View of KL-regularized Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual Policy Gradient: A Reward View of KL-regularized Objective"
                },
                "summary": "Reinforcement Learning and Imitation Learning have achieved widespread\nsuccess in many domains but remain constrained during real-world deployment.\nOne of the main issues is the additional requirements that were not considered\nduring training. To address this challenge, policy customization has been\nintroduced, aiming to adapt a prior policy while preserving its inherent\nproperties and meeting new task-specific requirements. A principled approach to\npolicy customization is Residual Q-Learning (RQL), which formulates the problem\nas a Markov Decision Process (MDP) and derives a family of value-based learning\nalgorithms. However, RQL has not yet been applied to policy gradient methods,\nwhich restricts its applicability, especially in tasks where policy gradient\nhas already proven more effective. In this work, we first derive a concise form\nof Soft Policy Gradient as a preliminary. Building on this, we introduce\nResidual Policy Gradient (RPG), which extends RQL to policy gradient methods,\nallowing policy customization in gradient-based RL settings. With the view of\nRPG, we rethink the KL-regularized objective widely used in RL fine-tuning. We\nshow that under certain assumptions, KL-regularized objective leads to a\nmaximum-entropy policy that balances the inherent properties and task-specific\nrequirements on a reward-level. Our experiments in MuJoCo demonstrate the\neffectiveness of Soft Policy Gradient and Residual Policy Gradient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning and Imitation Learning have achieved widespread\nsuccess in many domains but remain constrained during real-world deployment.\nOne of the main issues is the additional requirements that were not considered\nduring training. To address this challenge, policy customization has been\nintroduced, aiming to adapt a prior policy while preserving its inherent\nproperties and meeting new task-specific requirements. A principled approach to\npolicy customization is Residual Q-Learning (RQL), which formulates the problem\nas a Markov Decision Process (MDP) and derives a family of value-based learning\nalgorithms. However, RQL has not yet been applied to policy gradient methods,\nwhich restricts its applicability, especially in tasks where policy gradient\nhas already proven more effective. In this work, we first derive a concise form\nof Soft Policy Gradient as a preliminary. Building on this, we introduce\nResidual Policy Gradient (RPG), which extends RQL to policy gradient methods,\nallowing policy customization in gradient-based RL settings. With the view of\nRPG, we rethink the KL-regularized objective widely used in RL fine-tuning. We\nshow that under certain assumptions, KL-regularized objective leads to a\nmaximum-entropy policy that balances the inherent properties and task-specific\nrequirements on a reward-level. Our experiments in MuJoCo demonstrate the\neffectiveness of Soft Policy Gradient and Residual Policy Gradient."
                },
                "authors": [
                    {
                        "name": "Pengcheng Wang"
                    },
                    {
                        "name": "Xinghao Zhu"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Masayoshi Tomizuka"
                    },
                    {
                        "name": "Chenran Li"
                    }
                ],
                "author_detail": {
                    "name": "Chenran Li"
                },
                "author": "Chenran Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]