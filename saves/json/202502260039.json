[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v1",
                "updated": "2025-02-24T17:40:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v2",
                "updated": "2025-02-24T16:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    36,
                    32,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v1",
                "updated": "2025-02-22T14:13:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v3",
                "updated": "2025-02-20T16:01:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    1,
                    34,
                    3,
                    51,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v1",
                "updated": "2025-02-19T06:14:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v1",
                "updated": "2025-02-17T08:12:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v1",
                "updated": "2025-02-15T05:08:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_doi": "10.1103/PhysRevD.111.032007",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.032007",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 032007 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.17435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17435v1",
                "updated": "2025-02-24T18:59:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    59,
                    54,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:59:54Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    59,
                    54,
                    0,
                    55,
                    0
                ],
                "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GCC: Generative Color Constancy via Diffusing a Color Checker"
                },
                "summary": "Color constancy methods often struggle to generalize across different camera\nsensors due to varying spectral sensitivities. We present GCC, which leverages\ndiffusion models to inpaint color checkers into images for illumination\nestimation. Our key innovations include (1) a single-step deterministic\ninference approach that inpaints color checkers reflecting scene illumination,\n(2) a Laplacian decomposition technique that preserves checker structure while\nallowing illumination-dependent color adaptation, and (3) a mask-based data\naugmentation strategy for handling imprecise color checker annotations. GCC\ndemonstrates superior robustness in cross-camera scenarios, achieving\nstate-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in\nbi-directional evaluations. These results highlight our method's stability and\ngeneralization capability across different camera characteristics without\nrequiring sensor-specific training, making it a versatile solution for\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color constancy methods often struggle to generalize across different camera\nsensors due to varying spectral sensitivities. We present GCC, which leverages\ndiffusion models to inpaint color checkers into images for illumination\nestimation. Our key innovations include (1) a single-step deterministic\ninference approach that inpaints color checkers reflecting scene illumination,\n(2) a Laplacian decomposition technique that preserves checker structure while\nallowing illumination-dependent color adaptation, and (3) a mask-based data\naugmentation strategy for handling imprecise color checker annotations. GCC\ndemonstrates superior robustness in cross-camera scenarios, achieving\nstate-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in\nbi-directional evaluations. These results highlight our method's stability and\ngeneralization capability across different camera characteristics without\nrequiring sensor-specific training, making it a versatile solution for\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chen-Wei Chang"
                    },
                    {
                        "name": "Cheng-De Fan"
                    },
                    {
                        "name": "Chia-Che Chang"
                    },
                    {
                        "name": "Yi-Chen Lo"
                    },
                    {
                        "name": "Yu-Chee Tseng"
                    },
                    {
                        "name": "Jiun-Long Huang"
                    },
                    {
                        "name": "Yu-Lun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Lun Liu"
                },
                "author": "Yu-Lun Liu",
                "arxiv_comment": "Project page: https://chenwei891213.github.io/GCC/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09662v2",
                "updated": "2025-02-24T18:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-12T22:31:01Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    22,
                    31,
                    1,
                    5,
                    286,
                    0
                ],
                "title": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers\n  in RAG for Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers\n  in RAG for Coding Tasks"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge bases, achieving state-of-the-art results in\nvarious coding tasks. The core of RAG is retrieving demonstration examples,\nwhich is essential to balance effectiveness (generation quality) and efficiency\n(retrieval time) for optimal performance. However, the high-dimensional nature\nof code representations and large knowledge bases often create efficiency\nbottlenecks, which are overlooked in previous research. This paper\nsystematically evaluates the efficiency-effectiveness trade-off of retrievers\nacross three coding tasks: Program Synthesis, Commit Message Generation, and\nAssertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)\nand four dense retrievers, including one exhaustive dense retriever (SBERT's\nSemantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).\nOur findings show that while BM25 excels in effectiveness, it suffers in\nefficiency as the knowledge base grows beyond 1000 entries. In large-scale\nretrieval, efficiency differences become more pronounced, with approximate\ndense retrievers offering the greatest gains. For instance, in Commit\nGeneration task, HNSW achieves a 44x speed up, while only with a 1.74% drop in\nRougeL compared with BM25. Our results also show that increasing the number of\ndemonstrations in the prompt doesn't always improve the effectiveness and can\nincrease latency and lead to incorrect outputs. Our findings provide valuable\ninsights for practitioners aiming to build efficient and effective RAG systems\nfor coding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge bases, achieving state-of-the-art results in\nvarious coding tasks. The core of RAG is retrieving demonstration examples,\nwhich is essential to balance effectiveness (generation quality) and efficiency\n(retrieval time) for optimal performance. However, the high-dimensional nature\nof code representations and large knowledge bases often create efficiency\nbottlenecks, which are overlooked in previous research. This paper\nsystematically evaluates the efficiency-effectiveness trade-off of retrievers\nacross three coding tasks: Program Synthesis, Commit Message Generation, and\nAssertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)\nand four dense retrievers, including one exhaustive dense retriever (SBERT's\nSemantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).\nOur findings show that while BM25 excels in effectiveness, it suffers in\nefficiency as the knowledge base grows beyond 1000 entries. In large-scale\nretrieval, efficiency differences become more pronounced, with approximate\ndense retrievers offering the greatest gains. For instance, in Commit\nGeneration task, HNSW achieves a 44x speed up, while only with a 1.74% drop in\nRougeL compared with BM25. Our results also show that increasing the number of\ndemonstrations in the prompt doesn't always improve the effectiveness and can\nincrease latency and lead to incorrect outputs. Our findings provide valuable\ninsights for practitioners aiming to build efficient and effective RAG systems\nfor coding tasks."
                },
                "authors": [
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Shaiful Chowdhury"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "arxiv_comment": "11 pages, 6 figures, 6 tables, accepted by SANER 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17424v1",
                "updated": "2025-02-24T18:56:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    56,
                    3,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:56:03Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    56,
                    3,
                    0,
                    55,
                    0
                ],
                "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs"
                },
                "summary": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork."
                },
                "authors": [
                    {
                        "name": "Jan Betley"
                    },
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "Niels Warncke"
                    },
                    {
                        "name": "Anna Sztyber-Betley"
                    },
                    {
                        "name": "Xuchan Bao"
                    },
                    {
                        "name": "Martn Soto"
                    },
                    {
                        "name": "Nathan Labenz"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17422v1",
                "updated": "2025-02-24T18:54:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    54,
                    40,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:54:40Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    54,
                    40,
                    0,
                    55,
                    0
                ],
                "title": "MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk."
                },
                "authors": [
                    {
                        "name": "Jiarui Zhang"
                    },
                    {
                        "name": "Mahyar Khayatkhoei"
                    },
                    {
                        "name": "Prateek Chhikara"
                    },
                    {
                        "name": "Filip Ilievski"
                    }
                ],
                "author_detail": {
                    "name": "Filip Ilievski"
                },
                "author": "Filip Ilievski",
                "arxiv_comment": "Published as a conference paper at ICLR 2025. Code at:\n  https://github.com/saccharomycetes/mllms_know",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00201v2",
                "updated": "2025-02-24T18:54:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    54,
                    9,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-31T20:45:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    20,
                    45,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "YOLO Evolution: A Comprehensive Benchmark and Architectural Review of\n  YOLOv12, YOLO11, and Their Previous Versions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLO Evolution: A Comprehensive Benchmark and Architectural Review of\n  YOLOv12, YOLO11, and Their Previous Versions"
                },
                "summary": "This study presents a comprehensive benchmark analysis of various YOLO (You\nOnly Look Once) algorithms, from YOLOv3 to the newest addition. It represents\nthe first research to comprehensively evaluate the performance of YOLO11, the\nlatest addition to the YOLO family. It evaluates their performance on three\ndiverse datasets: Traffic Signs (with varying object sizes), African Wildlife\n(with diverse aspect ratios and at least one instance of the object per image),\nand Ships and Vessels (with small-sized objects of a single class), ensuring a\ncomprehensive assessment across datasets with distinct challenges. To ensure a\nrobust evaluation, we employ a comprehensive set of metrics, including\nPrecision, Recall, Mean Average Precision (mAP), Processing Time, GFLOPs count,\nand Model Size. Our analysis highlights the distinctive strengths and\nlimitations of each YOLO version. For example: YOLOv9 demonstrates substantial\naccuracy but struggles with detecting small objects and efficiency whereas\nYOLOv10 exhibits relatively lower accuracy due to architectural choices that\naffect its performance in overlapping object detection but excels in speed and\nefficiency. Additionally, the YOLO11 family consistently shows superior\nperformance in terms of accuracy, speed, computational efficiency, and model\nsize. YOLO11m achieved a remarkable balance of accuracy and efficiency, scoring\nmAP50-95 scores of 0.795, 0.81, and 0.325 on the Traffic Signs, African\nWildlife, and Ships datasets, respectively, while maintaining an average\ninference time of 2.4ms, a model size of 38.8Mb, and around 67.6 GFLOPs on\naverage. These results provide critical insights for both industry and\nacademia, facilitating the selection of the most suitable YOLO algorithm for\ndiverse applications and guiding future enhancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive benchmark analysis of various YOLO (You\nOnly Look Once) algorithms, from YOLOv3 to the newest addition. It represents\nthe first research to comprehensively evaluate the performance of YOLO11, the\nlatest addition to the YOLO family. It evaluates their performance on three\ndiverse datasets: Traffic Signs (with varying object sizes), African Wildlife\n(with diverse aspect ratios and at least one instance of the object per image),\nand Ships and Vessels (with small-sized objects of a single class), ensuring a\ncomprehensive assessment across datasets with distinct challenges. To ensure a\nrobust evaluation, we employ a comprehensive set of metrics, including\nPrecision, Recall, Mean Average Precision (mAP), Processing Time, GFLOPs count,\nand Model Size. Our analysis highlights the distinctive strengths and\nlimitations of each YOLO version. For example: YOLOv9 demonstrates substantial\naccuracy but struggles with detecting small objects and efficiency whereas\nYOLOv10 exhibits relatively lower accuracy due to architectural choices that\naffect its performance in overlapping object detection but excels in speed and\nefficiency. Additionally, the YOLO11 family consistently shows superior\nperformance in terms of accuracy, speed, computational efficiency, and model\nsize. YOLO11m achieved a remarkable balance of accuracy and efficiency, scoring\nmAP50-95 scores of 0.795, 0.81, and 0.325 on the Traffic Signs, African\nWildlife, and Ships datasets, respectively, while maintaining an average\ninference time of 2.4ms, a model size of 38.8Mb, and around 67.6 GFLOPs on\naverage. These results provide critical insights for both industry and\nacademia, facilitating the selection of the most suitable YOLO algorithm for\ndiverse applications and guiding future enhancements."
                },
                "authors": [
                    {
                        "name": "Nidhal Jegham"
                    },
                    {
                        "name": "Chan Young Koh"
                    },
                    {
                        "name": "Marwan Abdelatti"
                    },
                    {
                        "name": "Abdeltawab Hendawi"
                    }
                ],
                "author_detail": {
                    "name": "Abdeltawab Hendawi"
                },
                "author": "Abdeltawab Hendawi",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17420v1",
                "updated": "2025-02-24T18:52:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    52,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:52:59Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    52,
                    59,
                    0,
                    55,
                    0
                ],
                "title": "The Geometry of Refusal in Large Language Models: Concept Cones and\n  Representational Independence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Refusal in Large Language Models: Concept Cones and\n  Representational Independence"
                },
                "summary": "The safety alignment of large language models (LLMs) can be circumvented\nthrough adversarially crafted inputs, yet the mechanisms by which these attacks\nbypass safety barriers remain poorly understood. Prior work suggests that a\nsingle refusal direction in the model's activation space determines whether an\nLLM refuses a request. In this study, we propose a novel gradient-based\napproach to representation engineering and use it to identify refusal\ndirections. Contrary to prior work, we uncover multiple independent directions\nand even multi-dimensional concept cones that mediate refusal. Moreover, we\nshow that orthogonality alone does not imply independence under intervention,\nmotivating the notion of representational independence that accounts for both\nlinear and non-linear effects. Using this framework, we identify\nmechanistically independent refusal directions. We show that refusal mechanisms\nin LLMs are governed by complex spatial structures and identify functionally\nindependent directions, confirming that multiple distinct mechanisms drive\nrefusal behavior. Our gradient-based approach uncovers these mechanisms and can\nfurther serve as a foundation for future work on understanding LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety alignment of large language models (LLMs) can be circumvented\nthrough adversarially crafted inputs, yet the mechanisms by which these attacks\nbypass safety barriers remain poorly understood. Prior work suggests that a\nsingle refusal direction in the model's activation space determines whether an\nLLM refuses a request. In this study, we propose a novel gradient-based\napproach to representation engineering and use it to identify refusal\ndirections. Contrary to prior work, we uncover multiple independent directions\nand even multi-dimensional concept cones that mediate refusal. Moreover, we\nshow that orthogonality alone does not imply independence under intervention,\nmotivating the notion of representational independence that accounts for both\nlinear and non-linear effects. Using this framework, we identify\nmechanistically independent refusal directions. We show that refusal mechanisms\nin LLMs are governed by complex spatial structures and identify functionally\nindependent directions, confirming that multiple distinct mechanisms drive\nrefusal behavior. Our gradient-based approach uncovers these mechanisms and can\nfurther serve as a foundation for future work on understanding LLMs."
                },
                "authors": [
                    {
                        "name": "Tom Wollschlger"
                    },
                    {
                        "name": "Jannes Elstner"
                    },
                    {
                        "name": "Simon Geisler"
                    },
                    {
                        "name": "Vincent Cohen-Addad"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    },
                    {
                        "name": "Johannes Gasteiger"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Gasteiger"
                },
                "author": "Johannes Gasteiger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17418v1",
                "updated": "2025-02-24T18:50:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:50:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "A JWST Panchromatic Thermal Emission Spectrum of the Warm Neptune\n  Archetype GJ 436b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A JWST Panchromatic Thermal Emission Spectrum of the Warm Neptune\n  Archetype GJ 436b"
                },
                "summary": "GJ 436b is the archetype warm Neptune exoplanet. The planet's thermal\nemission spectrum was previously observed via intensive secondary eclipse\ncampaigns with Spitzer. The atmosphere has long been interpreted to be\nextremely metal-rich, out of chemical equilibrium, and potentially tidally\nheated. We present the first panchromatic emission spectrum of GJ 436b observed\nwith JWST's NIRCAM (F322W2 and F444W) and MIRI (LRS) instruments between 2.4\nand 11.9 $\\mu$m. Surprisingly, the JWST spectrum appears significantly fainter\naround 3.6 $\\mu$m than that implied by Spitzer photometry. The molecular\nabsorption features in the spectrum are relatively weak, and we only find\ntentative evidence of CO$_2$ absorption at 2$\\sigma$ significance. Under the\nassumption of a day-side blackbody, we find $T_{\\rm day}$=662.8$\\pm$5.0 K,\nwhich is similar to the zero Bond albedo equilibrium temperature. We use it to\nobtain a 3$\\sigma$ upper limit on the Bond albedo of $A_B{\\le}$0.66. To\nunderstand the spectrum we employ 1D radiative-convective models but find that\natmospheric constraints depend strongly on model assumptions. If thermochemical\nequilibrium is assumed, we find a cloudy metal-enriched atmosphere (metallicity\n$\\ge$ 300$\\times$solar). We employ 1D photochemical modeling to show that the\nobserved spectrum is also consistent with a cloud-free, relatively\nlower-metallicity atmosphere (metallicity $\\ge$ 80$\\times$solar) with a cold\ninternal temperature ($T_{\\rm int}$$\\sim$60 K). These are much lower\nmetallicities and internal temperatures than inferences from Spitzer\nphotometry. The low $T_{\\rm day}$ and non-detection of transmission features at\nhigh spectral resolution does suggest a role for cloud opacity, but this is not\ndefinitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GJ 436b is the archetype warm Neptune exoplanet. The planet's thermal\nemission spectrum was previously observed via intensive secondary eclipse\ncampaigns with Spitzer. The atmosphere has long been interpreted to be\nextremely metal-rich, out of chemical equilibrium, and potentially tidally\nheated. We present the first panchromatic emission spectrum of GJ 436b observed\nwith JWST's NIRCAM (F322W2 and F444W) and MIRI (LRS) instruments between 2.4\nand 11.9 $\\mu$m. Surprisingly, the JWST spectrum appears significantly fainter\naround 3.6 $\\mu$m than that implied by Spitzer photometry. The molecular\nabsorption features in the spectrum are relatively weak, and we only find\ntentative evidence of CO$_2$ absorption at 2$\\sigma$ significance. Under the\nassumption of a day-side blackbody, we find $T_{\\rm day}$=662.8$\\pm$5.0 K,\nwhich is similar to the zero Bond albedo equilibrium temperature. We use it to\nobtain a 3$\\sigma$ upper limit on the Bond albedo of $A_B{\\le}$0.66. To\nunderstand the spectrum we employ 1D radiative-convective models but find that\natmospheric constraints depend strongly on model assumptions. If thermochemical\nequilibrium is assumed, we find a cloudy metal-enriched atmosphere (metallicity\n$\\ge$ 300$\\times$solar). We employ 1D photochemical modeling to show that the\nobserved spectrum is also consistent with a cloud-free, relatively\nlower-metallicity atmosphere (metallicity $\\ge$ 80$\\times$solar) with a cold\ninternal temperature ($T_{\\rm int}$$\\sim$60 K). These are much lower\nmetallicities and internal temperatures than inferences from Spitzer\nphotometry. The low $T_{\\rm day}$ and non-detection of transmission features at\nhigh spectral resolution does suggest a role for cloud opacity, but this is not\ndefinitive."
                },
                "authors": [
                    {
                        "name": "Sagnick Mukherjee"
                    },
                    {
                        "name": "Everett Schlawin"
                    },
                    {
                        "name": "Taylor J. Bell"
                    },
                    {
                        "name": "Jonathan J. Fortney"
                    },
                    {
                        "name": "Thomas G. Beatty"
                    },
                    {
                        "name": "Thomas P. Greene"
                    },
                    {
                        "name": "Kazumasa Ohno"
                    },
                    {
                        "name": "Matthew M. Murphy"
                    },
                    {
                        "name": "Vivien Parmentier"
                    },
                    {
                        "name": "Michael R Line"
                    },
                    {
                        "name": "Luis Welbanks"
                    },
                    {
                        "name": "Lindsey S. Wiser"
                    },
                    {
                        "name": "Marcia J. Rieke"
                    }
                ],
                "author_detail": {
                    "name": "Marcia J. Rieke"
                },
                "author": "Marcia J. Rieke",
                "arxiv_comment": "Accepted for publication in ApJL, 27 Pages, 17 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17419v1",
                "updated": "2025-02-24T18:50:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:50:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
                },
                "summary": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field."
                },
                "authors": [
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ming-Liang Zhang"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Zengyan Liu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Pei-Jie Wang"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Fei Yin"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Cheng-Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Lin Liu"
                },
                "author": "Cheng-Lin Liu",
                "arxiv_comment": "Slow-thinking, Large Language Models, Human-like Reasoning, Decision\n  Making in AI, AGI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17416v1",
                "updated": "2025-02-24T18:49:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    49,
                    5,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:49:05Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    49,
                    5,
                    0,
                    55,
                    0
                ],
                "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning with Latent Thoughts: On the Power of Looped Transformers"
                },
                "summary": "Large language models have shown remarkable reasoning abilities and scaling\nlaws suggest that large parameter count, especially along the depth axis, is\nthe primary driver. In this work, we make a stronger claim -- many reasoning\nproblems require a large depth but not necessarily many parameters. This\nunlocks a novel application of looped models for reasoning. Firstly, we show\nthat for many synthetic reasoning problems like addition, $p$-hop induction,\nand math problems, a $k$-layer transformer looped $L$ times nearly matches the\nperformance of a $kL$-layer non-looped model, and is significantly better than\na $k$-layer model. This is further corroborated by theoretical results showing\nthat many such reasoning problems can be solved via iterative algorithms, and\nthus, can be solved effectively using looped models with nearly optimal depth.\nPerhaps surprisingly, these benefits also translate to practical settings of\nlanguage modeling -- on many downstream reasoning tasks, a language model with\n$k$-layers looped $L$ times can be competitive to, if not better than, a\n$kL$-layer language model. In fact, our empirical analysis reveals an\nintriguing phenomenon: looped and non-looped models exhibit scaling behavior\nthat depends on their effective depth, akin to the inference-time scaling of\nchain-of-thought (CoT) reasoning. We further elucidate the connection to CoT\nreasoning by proving that looped models implicitly generate latent thoughts and\ncan simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we\nalso present an interesting dichotomy between reasoning and memorization, and\ndesign a looping-based regularization that is effective on both fronts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown remarkable reasoning abilities and scaling\nlaws suggest that large parameter count, especially along the depth axis, is\nthe primary driver. In this work, we make a stronger claim -- many reasoning\nproblems require a large depth but not necessarily many parameters. This\nunlocks a novel application of looped models for reasoning. Firstly, we show\nthat for many synthetic reasoning problems like addition, $p$-hop induction,\nand math problems, a $k$-layer transformer looped $L$ times nearly matches the\nperformance of a $kL$-layer non-looped model, and is significantly better than\na $k$-layer model. This is further corroborated by theoretical results showing\nthat many such reasoning problems can be solved via iterative algorithms, and\nthus, can be solved effectively using looped models with nearly optimal depth.\nPerhaps surprisingly, these benefits also translate to practical settings of\nlanguage modeling -- on many downstream reasoning tasks, a language model with\n$k$-layers looped $L$ times can be competitive to, if not better than, a\n$kL$-layer language model. In fact, our empirical analysis reveals an\nintriguing phenomenon: looped and non-looped models exhibit scaling behavior\nthat depends on their effective depth, akin to the inference-time scaling of\nchain-of-thought (CoT) reasoning. We further elucidate the connection to CoT\nreasoning by proving that looped models implicitly generate latent thoughts and\ncan simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we\nalso present an interesting dichotomy between reasoning and memorization, and\ndesign a looping-based regularization that is effective on both fronts."
                },
                "authors": [
                    {
                        "name": "Nikunj Saunshi"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Sanjiv Kumar"
                    },
                    {
                        "name": "Sashank J. Reddi"
                    }
                ],
                "author_detail": {
                    "name": "Sashank J. Reddi"
                },
                "author": "Sashank J. Reddi",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05869v2",
                "updated": "2025-02-24T18:43:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    43,
                    5,
                    0,
                    55,
                    0
                ],
                "published": "2024-11-07T20:07:21Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    20,
                    7,
                    21,
                    3,
                    312,
                    0
                ],
                "title": "Compactly-supported nonstationary kernels for computing exact Gaussian\n  processes on big data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactly-supported nonstationary kernels for computing exact Gaussian\n  processes on big data"
                },
                "summary": "The Gaussian process (GP) is a widely used probabilistic machine learning\nmethod with implicit uncertainty characterization for stochastic function\napproximation, stochastic modeling, and analyzing real-world measurements of\nnonlinear processes. Traditional implementations of GPs involve stationary\nkernels (also termed covariance functions) that limit their flexibility, and\nexact methods for inference that prevent application to data sets with more\nthan about ten thousand points. Modern approaches to address stationarity\nassumptions generally fail to accommodate large data sets, while all attempts\nto address scalability focus on approximating the Gaussian likelihood, which\ncan involve subjectivity and lead to inaccuracies. In this work, we explicitly\nderive an alternative kernel that can discover and encode both sparsity and\nnonstationarity. We embed the kernel within a fully Bayesian GP model and\nleverage high-performance computing resources to enable the analysis of massive\ndata sets. We demonstrate the favorable performance of our novel kernel\nrelative to existing exact and approximate GP methods across a variety of\nsynthetic data examples. Furthermore, we conduct space-time prediction based on\nmore than one million measurements of daily maximum temperature and verify that\nour results outperform state-of-the-art methods in the Earth sciences. More\nbroadly, having access to exact GPs that use ultra-scalable,\nsparsity-discovering, nonstationary kernels allows GP methods to truly compete\nwith a wide variety of machine learning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gaussian process (GP) is a widely used probabilistic machine learning\nmethod with implicit uncertainty characterization for stochastic function\napproximation, stochastic modeling, and analyzing real-world measurements of\nnonlinear processes. Traditional implementations of GPs involve stationary\nkernels (also termed covariance functions) that limit their flexibility, and\nexact methods for inference that prevent application to data sets with more\nthan about ten thousand points. Modern approaches to address stationarity\nassumptions generally fail to accommodate large data sets, while all attempts\nto address scalability focus on approximating the Gaussian likelihood, which\ncan involve subjectivity and lead to inaccuracies. In this work, we explicitly\nderive an alternative kernel that can discover and encode both sparsity and\nnonstationarity. We embed the kernel within a fully Bayesian GP model and\nleverage high-performance computing resources to enable the analysis of massive\ndata sets. We demonstrate the favorable performance of our novel kernel\nrelative to existing exact and approximate GP methods across a variety of\nsynthetic data examples. Furthermore, we conduct space-time prediction based on\nmore than one million measurements of daily maximum temperature and verify that\nour results outperform state-of-the-art methods in the Earth sciences. More\nbroadly, having access to exact GPs that use ultra-scalable,\nsparsity-discovering, nonstationary kernels allows GP methods to truly compete\nwith a wide variety of machine learning methods."
                },
                "authors": [
                    {
                        "name": "Mark D. Risser"
                    },
                    {
                        "name": "Marcus M. Noack"
                    },
                    {
                        "name": "Hengrui Luo"
                    },
                    {
                        "name": "Ronald Pandolfi"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Pandolfi"
                },
                "author": "Ronald Pandolfi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17410v1",
                "updated": "2025-02-24T18:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    42,
                    19,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:42:19Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    42,
                    19,
                    0,
                    55,
                    0
                ],
                "title": "COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains, yet their optimization remains a significant challenge due to\nthe complex and high-dimensional loss landscapes they inhabit. While adaptive\noptimizers such as AdamW are widely used, they suffer from critical\nlimitations, including an inability to capture interdependencies between\ncoordinates and high memory consumption. Subsequent research, exemplified by\nSOAP, attempts to better capture coordinate interdependence but incurs greater\nmemory overhead, limiting scalability for massive LLMs. An alternative approach\naims to reduce memory consumption through low-dimensional projection, but this\nleads to substantial approximation errors, resulting in less effective\noptimization (e.g., in terms of per-token efficiency). In this paper, we\npropose COSMOS, a novel hybrid optimizer that leverages the varying importance\nof eigensubspaces in the gradient matrix to achieve memory efficiency without\ncompromising optimization performance. The design of COSMOS is motivated by our\nempirical insights and practical considerations. Specifically, COSMOS applies\nSOAP to the leading eigensubspace, which captures the primary optimization\ndynamics, and MUON to the remaining eigensubspace, which is less critical but\ncomputationally expensive to handle with SOAP. This hybrid strategy\nsignificantly reduces memory consumption while maintaining robust optimization\nperformance, making it particularly suitable for massive LLMs. Numerical\nexperiments on various datasets and transformer architectures are provided to\ndemonstrate the effectiveness of COSMOS. Our code is available at\nhttps://github.com/lliu606/COSMOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains, yet their optimization remains a significant challenge due to\nthe complex and high-dimensional loss landscapes they inhabit. While adaptive\noptimizers such as AdamW are widely used, they suffer from critical\nlimitations, including an inability to capture interdependencies between\ncoordinates and high memory consumption. Subsequent research, exemplified by\nSOAP, attempts to better capture coordinate interdependence but incurs greater\nmemory overhead, limiting scalability for massive LLMs. An alternative approach\naims to reduce memory consumption through low-dimensional projection, but this\nleads to substantial approximation errors, resulting in less effective\noptimization (e.g., in terms of per-token efficiency). In this paper, we\npropose COSMOS, a novel hybrid optimizer that leverages the varying importance\nof eigensubspaces in the gradient matrix to achieve memory efficiency without\ncompromising optimization performance. The design of COSMOS is motivated by our\nempirical insights and practical considerations. Specifically, COSMOS applies\nSOAP to the leading eigensubspace, which captures the primary optimization\ndynamics, and MUON to the remaining eigensubspace, which is less critical but\ncomputationally expensive to handle with SOAP. This hybrid strategy\nsignificantly reduces memory consumption while maintaining robust optimization\nperformance, making it particularly suitable for massive LLMs. Numerical\nexperiments on various datasets and transformer architectures are provided to\ndemonstrate the effectiveness of COSMOS. Our code is available at\nhttps://github.com/lliu606/COSMOS."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Zhenghao Xu"
                    },
                    {
                        "name": "Zixuan Zhang"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Zichong Li"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "arxiv_comment": "23 pages, 9 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.07891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.07891v2",
                "updated": "2025-02-24T18:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    42,
                    11,
                    0,
                    55,
                    0
                ],
                "published": "2023-02-15T19:00:01Z",
                "published_parsed": [
                    2023,
                    2,
                    15,
                    19,
                    0,
                    1,
                    2,
                    46,
                    0
                ],
                "title": "The brightest GRB ever detected: GRB 221009A as a highly luminous event\n  at z = 0.151",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The brightest GRB ever detected: GRB 221009A as a highly luminous event\n  at z = 0.151"
                },
                "summary": "Context: The extreme luminosity of gamma-ray bursts (GRBs) makes them\npowerful beacons for studies of the distant Universe. The most luminous bursts\nare typically detected at moderate/high redshift, where the volume for seeing\nsuch rare events is maximized and the star-formation activity is greater than\nat z = 0. For distant events, not all observations are feasible, such as at TeV\nenergies.\n  Aims: Here we present a spectroscopic redshift measurement for the\nexceptional GRB 221009A, the brightest GRB observed to date with emission\nextending well into the TeV regime.\n  Methods: We used the X-shooter spectrograph at the ESO Very Large Telescope\n(VLT) to obtain simultaneous optical to near-IR spectroscopy of the burst\nafterglow 0.5 days after the explosion.\n  Results: The spectra exhibit both absorption and emission lines from material\nin a host galaxy at z = 0.151. Thus GRB 221009A was a relatively nearby burst\nwith a luminosity distance of 745 Mpc. Its host galaxy properties\n(star-formation rate and metallicity) are consistent with those of LGRB hosts\nat low redshift. This redshift measurement yields information on the energy of\nthe burst. The inferred isotropic energy release, $E_{\\rm iso} > 5 \\times\n10^{54}$ erg, lies at the high end of the distribution, making GRB 221009A one\nof the nearest and also most energetic GRBs observed to date. We estimate that\nsuch a combination (nearby as well as intrinsically bright) occurs between once\nevery few decades to once per millennium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: The extreme luminosity of gamma-ray bursts (GRBs) makes them\npowerful beacons for studies of the distant Universe. The most luminous bursts\nare typically detected at moderate/high redshift, where the volume for seeing\nsuch rare events is maximized and the star-formation activity is greater than\nat z = 0. For distant events, not all observations are feasible, such as at TeV\nenergies.\n  Aims: Here we present a spectroscopic redshift measurement for the\nexceptional GRB 221009A, the brightest GRB observed to date with emission\nextending well into the TeV regime.\n  Methods: We used the X-shooter spectrograph at the ESO Very Large Telescope\n(VLT) to obtain simultaneous optical to near-IR spectroscopy of the burst\nafterglow 0.5 days after the explosion.\n  Results: The spectra exhibit both absorption and emission lines from material\nin a host galaxy at z = 0.151. Thus GRB 221009A was a relatively nearby burst\nwith a luminosity distance of 745 Mpc. Its host galaxy properties\n(star-formation rate and metallicity) are consistent with those of LGRB hosts\nat low redshift. This redshift measurement yields information on the energy of\nthe burst. The inferred isotropic energy release, $E_{\\rm iso} > 5 \\times\n10^{54}$ erg, lies at the high end of the distribution, making GRB 221009A one\nof the nearest and also most energetic GRBs observed to date. We estimate that\nsuch a combination (nearby as well as intrinsically bright) occurs between once\nevery few decades to once per millennium."
                },
                "authors": [
                    {
                        "name": "D. B. Malesani"
                    },
                    {
                        "name": "A. J. Levan"
                    },
                    {
                        "name": "L. Izzo"
                    },
                    {
                        "name": "A. de Ugarte Postigo"
                    },
                    {
                        "name": "G. Ghirlanda"
                    },
                    {
                        "name": "K. E. Heintz"
                    },
                    {
                        "name": "D. A. Kann"
                    },
                    {
                        "name": "G. P. Lamb"
                    },
                    {
                        "name": "J. Palmerio"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "R. Salvaterra"
                    },
                    {
                        "name": "N. R. Tanvir"
                    },
                    {
                        "name": "J. F. Ag Fernndez"
                    },
                    {
                        "name": "S. Campana"
                    },
                    {
                        "name": "A. A. Chrimes"
                    },
                    {
                        "name": "P. D'Avanzo"
                    },
                    {
                        "name": "V. D'Elia"
                    },
                    {
                        "name": "M. Della Valle"
                    },
                    {
                        "name": "M. De Pasquale"
                    },
                    {
                        "name": "J. P. U. Fynbo"
                    },
                    {
                        "name": "N. Gaspari"
                    },
                    {
                        "name": "B. P. Gompertz"
                    },
                    {
                        "name": "D. H. Hartmann"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "P. Jakobsson"
                    },
                    {
                        "name": "E. Palazzi"
                    },
                    {
                        "name": "E. Pian"
                    },
                    {
                        "name": "G. Pugliese"
                    },
                    {
                        "name": "M. E. Ravasio"
                    },
                    {
                        "name": "A. Rossi"
                    },
                    {
                        "name": "A. Saccardi"
                    },
                    {
                        "name": "P. Schady"
                    },
                    {
                        "name": "B. Schneider"
                    },
                    {
                        "name": "J. Sollerman"
                    },
                    {
                        "name": "R. L. C. Starling"
                    },
                    {
                        "name": "C. C. Thne"
                    },
                    {
                        "name": "A. J. van der Horst"
                    },
                    {
                        "name": "S. D. Vergani"
                    },
                    {
                        "name": "D. Watson"
                    },
                    {
                        "name": "K. Wiersema"
                    },
                    {
                        "name": "D. Xu"
                    },
                    {
                        "name": "T. Zafar"
                    }
                ],
                "author_detail": {
                    "name": "T. Zafar"
                },
                "author": "T. Zafar",
                "arxiv_comment": "9 pages, 4 figures, Astronomy & Astrophysics, accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.07891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.07891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04059v2",
                "updated": "2025-02-24T18:38:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    38,
                    2,
                    0,
                    55,
                    0
                ],
                "published": "2023-12-07T05:45:24Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    5,
                    45,
                    24,
                    3,
                    341,
                    0
                ],
                "title": "Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss"
                },
                "summary": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement."
                },
                "authors": [
                    {
                        "name": "Zhuoran Huang"
                    },
                    {
                        "name": "Michael P. Berry"
                    },
                    {
                        "name": "Christina Chwyl"
                    },
                    {
                        "name": "Gary Hsieh"
                    },
                    {
                        "name": "Jing Wei"
                    },
                    {
                        "name": "Evan M. Forman"
                    }
                ],
                "author_detail": {
                    "name": "Evan M. Forman"
                },
                "author": "Evan M. Forman",
                "arxiv_doi": "10.1007/s41347-025-00491-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s41347-025-00491-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 5 figures",
                "arxiv_journal_ref": "Journal of Technology in Behavioral Science (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17407v1",
                "updated": "2025-02-24T18:36:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    36,
                    15,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:36:15Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    36,
                    15,
                    0,
                    55,
                    0
                ],
                "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning"
                },
                "summary": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17403v1",
                "updated": "2025-02-24T18:30:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    30,
                    36,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:30:36Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    30,
                    36,
                    0,
                    55,
                    0
                ],
                "title": "Large Language Models are Powerful EHR Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Powerful EHR Encoders"
                },
                "summary": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications."
                },
                "authors": [
                    {
                        "name": "Stefan Hegselmann"
                    },
                    {
                        "name": "Georg von Arnim"
                    },
                    {
                        "name": "Tillmann Rheude"
                    },
                    {
                        "name": "Noel Kronenberg"
                    },
                    {
                        "name": "David Sontag"
                    },
                    {
                        "name": "Gerhard Hindricks"
                    },
                    {
                        "name": "Roland Eils"
                    },
                    {
                        "name": "Benjamin Wild"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Wild"
                },
                "author": "Benjamin Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19381v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19381v4",
                "updated": "2025-02-24T18:28:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    28,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-28T15:12:55Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    12,
                    55,
                    5,
                    272,
                    0
                ],
                "title": "HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for\n  Enhanced LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for\n  Enhanced LLM Reasoning"
                },
                "summary": "LLMs approach logical and mathematical reasoning through natural or symbolic\nlanguages. While natural language offers human-accessible flexibility but\nsuffers from ambiguity, symbolic reasoning provides precise, machine-executable\ninferences at the cost of strict domain constraints. We introduce HYBRIDMIND,\nan adaptive strategy that selects the optimal reasoning approach for each\nreasoning problem. Through extensive experiments, we evaluate both\nprompting-based approaches with state-of-the-art LLMs and fine-tuned\nopen-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a\nmeta-selector outperforms GPT-4o's natural language reasoning by 4.4\\% on FOLIO\nand 1.3\\% on MATH. More notably, using GPT-3.5-turbo as a prompted\nmeta-selector yields a 10\\% improvement on FOLIO's challenging subset compared\nto GPT-4o. We will release our code and data to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs approach logical and mathematical reasoning through natural or symbolic\nlanguages. While natural language offers human-accessible flexibility but\nsuffers from ambiguity, symbolic reasoning provides precise, machine-executable\ninferences at the cost of strict domain constraints. We introduce HYBRIDMIND,\nan adaptive strategy that selects the optimal reasoning approach for each\nreasoning problem. Through extensive experiments, we evaluate both\nprompting-based approaches with state-of-the-art LLMs and fine-tuned\nopen-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a\nmeta-selector outperforms GPT-4o's natural language reasoning by 4.4\\% on FOLIO\nand 1.3\\% on MATH. More notably, using GPT-3.5-turbo as a prompted\nmeta-selector yields a 10\\% improvement on FOLIO's challenging subset compared\nto GPT-4o. We will release our code and data to support future research."
                },
                "authors": [
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Chuhan Li"
                    },
                    {
                        "name": "Xuyuan Xiong"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19381v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19381v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17390v1",
                "updated": "2025-02-24T18:16:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    16,
                    10,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:16:10Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    16,
                    10,
                    0,
                    55,
                    0
                ],
                "title": "Mitigating Bias in RAG: Controlling the Embedder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Bias in RAG: Controlling the Embedder"
                },
                "summary": "In retrieval augmented generation (RAG) systems, each individual component --\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\ntowards outputting certain perspectives or identities. In this work, we study\nthe conflict between biases of each component and their relationship to the\noverall bias of the RAG system, which we call bias conflict. Examining both\ngender and political biases as case studies, we show that bias conflict can be\ncharacterized through a linear relationship among components despite its\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\ncreating 120 differently biased embedders, we demonstrate how to control bias\nwhile maintaining utility and reveal the importance of reverse-biasing the\nembedder to mitigate bias in the overall system. Additionally, we find that\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\nfactor to consider for debiasing. Our results underscore that a fair RAG system\ncan be better achieved by carefully controlling the bias of the embedder rather\nthan increasing its fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In retrieval augmented generation (RAG) systems, each individual component --\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\ntowards outputting certain perspectives or identities. In this work, we study\nthe conflict between biases of each component and their relationship to the\noverall bias of the RAG system, which we call bias conflict. Examining both\ngender and political biases as case studies, we show that bias conflict can be\ncharacterized through a linear relationship among components despite its\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\ncreating 120 differently biased embedders, we demonstrate how to control bias\nwhile maintaining utility and reveal the importance of reverse-biasing the\nembedder to mitigate bias in the overall system. Additionally, we find that\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\nfactor to consider for debiasing. Our results underscore that a fair RAG system\ncan be better achieved by carefully controlling the bias of the embedder rather\nthan increasing its fairness."
                },
                "authors": [
                    {
                        "name": "Taeyoun Kim"
                    },
                    {
                        "name": "Jacob Springer"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "26 pages (8 main), 12 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02597v2",
                "updated": "2025-02-24T18:15:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    15,
                    9,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-03T15:38:20Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    38,
                    20,
                    3,
                    277,
                    0
                ],
                "title": "Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR"
                },
                "summary": "We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel\narchitecture for speech recognition that extends the Token-and-Duration\nTransducer (TDT) model. Trained with randomly masked predictor network outputs,\nHAINAN supports both autoregressive inference with all network components and\nnon-autoregressive inference without the predictor. Additionally, we propose a\nnovel semi-autoregressive inference paradigm that first generates an initial\nhypothesis using non-autoregressive inference, followed by refinement steps\nwhere each token prediction is regenerated using parallelized autoregression on\nthe initial hypothesis. Experiments on multiple datasets across different\nlanguages demonstrate that HAINAN achieves efficiency parity with CTC in\nnon-autoregressive mode and with TDT in autoregressive mode. In terms of\naccuracy, autoregressive HAINAN outperforms TDT and RNN-T, while\nnon-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive\ninference further enhances the model's accuracy with minimal computational\noverhead, and even outperforms TDT results in some cases. These results\nhighlight HAINAN's flexibility in balancing accuracy and speed, positioning it\nas a strong candidate for real-world speech recognition applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel\narchitecture for speech recognition that extends the Token-and-Duration\nTransducer (TDT) model. Trained with randomly masked predictor network outputs,\nHAINAN supports both autoregressive inference with all network components and\nnon-autoregressive inference without the predictor. Additionally, we propose a\nnovel semi-autoregressive inference paradigm that first generates an initial\nhypothesis using non-autoregressive inference, followed by refinement steps\nwhere each token prediction is regenerated using parallelized autoregression on\nthe initial hypothesis. Experiments on multiple datasets across different\nlanguages demonstrate that HAINAN achieves efficiency parity with CTC in\nnon-autoregressive mode and with TDT in autoregressive mode. In terms of\naccuracy, autoregressive HAINAN outperforms TDT and RNN-T, while\nnon-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive\ninference further enhances the model's accuracy with minimal computational\noverhead, and even outperforms TDT results in some cases. These results\nhighlight HAINAN's flexibility in balancing accuracy and speed, positioning it\nas a strong candidate for real-world speech recognition applications."
                },
                "authors": [
                    {
                        "name": "Hainan Xu"
                    },
                    {
                        "name": "Travis M. Bartley"
                    },
                    {
                        "name": "Vladimir Bataev"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12272v3",
                "updated": "2025-02-24T18:15:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    15,
                    2,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-17T19:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    16,
                    37,
                    0,
                    48,
                    0
                ],
                "title": "Learning to Reason at the Frontier of Learnability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason at the Frontier of Learnability"
                },
                "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15657v2",
                "updated": "2025-02-24T18:14:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    15,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T18:28:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    28,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\n  a Safer Path?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer\n  a Safer Path?"
                },
                "summary": "The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path."
                },
                "authors": [
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Michael Cohen"
                    },
                    {
                        "name": "Damiano Fornasiere"
                    },
                    {
                        "name": "Joumana Ghosn"
                    },
                    {
                        "name": "Pietro Greiner"
                    },
                    {
                        "name": "Matt MacDermott"
                    },
                    {
                        "name": "Sren Mindermann"
                    },
                    {
                        "name": "Adam Oberman"
                    },
                    {
                        "name": "Jesse Richardson"
                    },
                    {
                        "name": "Oliver Richardson"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Pierre-Luc St-Charles"
                    },
                    {
                        "name": "David Williams-King"
                    }
                ],
                "author_detail": {
                    "name": "David Williams-King"
                },
                "author": "David Williams-King",
                "arxiv_comment": "v2 with fixed formatting for URLs and hyperlinks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17387v1",
                "updated": "2025-02-24T18:14:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    1,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:14:01Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    1,
                    0,
                    55,
                    0
                ],
                "title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models"
                },
                "summary": "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Nathan Lile"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Louis Castricato"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Chase Blagden"
                    },
                    {
                        "name": "Violet Xiang"
                    },
                    {
                        "name": "Dakota Mahan"
                    },
                    {
                        "name": "Nick Haber"
                    }
                ],
                "author_detail": {
                    "name": "Nick Haber"
                },
                "author": "Nick Haber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17383v1",
                "updated": "2025-02-24T18:08:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    8,
                    41,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:08:41Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    8,
                    41,
                    0,
                    55,
                    0
                ],
                "title": "What is a Good Question? Utility Estimation with LLM-based Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is a Good Question? Utility Estimation with LLM-based Simulations"
                },
                "summary": "Asking questions is a fundamental aspect of learning that facilitates deeper\nunderstanding. However, characterizing and crafting questions that effectively\nimprove learning remains elusive. To address this gap, we propose QUEST\n(Question Utility Estimation with Simulated Tests). QUEST simulates a learning\nenvironment that enables the quantification of a question's utility based on\nits direct impact on improving learning outcomes. Furthermore, we can identify\nhigh-utility questions and use them to fine-tune question generation models\nwith rejection sampling. We find that questions generated by models trained\nwith rejection sampling based on question utility result in exam scores that\nare higher by at least 20% than those from specialized prompting grounded on\neducational objectives literature and models fine-tuned with indirect measures\nof question quality, such as saliency and expected information gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asking questions is a fundamental aspect of learning that facilitates deeper\nunderstanding. However, characterizing and crafting questions that effectively\nimprove learning remains elusive. To address this gap, we propose QUEST\n(Question Utility Estimation with Simulated Tests). QUEST simulates a learning\nenvironment that enables the quantification of a question's utility based on\nits direct impact on improving learning outcomes. Furthermore, we can identify\nhigh-utility questions and use them to fine-tune question generation models\nwith rejection sampling. We find that questions generated by models trained\nwith rejection sampling based on question utility result in exam scores that\nare higher by at least 20% than those from specialized prompting grounded on\neducational objectives literature and models fine-tuned with indirect measures\nof question quality, such as saliency and expected information gain."
                },
                "authors": [
                    {
                        "name": "Dong-Ho Lee"
                    },
                    {
                        "name": "Hyundong Cho"
                    },
                    {
                        "name": "Jonathan May"
                    },
                    {
                        "name": "Jay Pujara"
                    }
                ],
                "author_detail": {
                    "name": "Jay Pujara"
                },
                "author": "Jay Pujara",
                "arxiv_comment": "18 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01057v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01057v3",
                "updated": "2025-02-24T17:55:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    55,
                    45,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-03T05:10:00Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    10,
                    0,
                    0,
                    34,
                    0
                ],
                "title": "FetDTIAlign: A Deep Learning Framework for Affine and Deformable\n  Registration of Fetal Brain dMRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FetDTIAlign: A Deep Learning Framework for Affine and Deformable\n  Registration of Fetal Brain dMRI"
                },
                "summary": "Diffusion MRI (dMRI) provides unique insights into fetal brain microstructure\nin utero. Longitudinal and cross-sectional fetal dMRI studies can reveal\ncrucial neurodevelopmental changes but require precise spatial alignment across\nscans and subjects. This is challenging due to low data quality, rapid brain\ndevelopment, and limited anatomical landmarks. Existing registration methods,\ndesigned for high-quality adult data, struggle with these complexities. To\naddress this, we introduce FetDTIAlign, a deep learning approach for fetal\nbrain dMRI registration, enabling accurate affine and deformable alignment.\nFetDTIAlign features a dual-encoder architecture and iterative feature-based\ninference, reducing the impact of noise and low resolution. It optimizes\nnetwork configurations and domain-specific features at each registration stage,\nenhancing both robustness and accuracy. We validated FetDTIAlign on data from\n23 to 36 weeks gestation, covering 60 white matter tracts. It consistently\noutperformed two classical optimization-based methods and a deep learning\npipeline, achieving superior anatomical correspondence. Further validation on\nexternal data from the Developing Human Connectome Project confirmed its\ngeneralizability across acquisition protocols. Our results demonstrate the\nfeasibility of deep learning for fetal brain dMRI registration, providing a\nmore accurate and reliable alternative to classical techniques. By enabling\nprecise cross-subject and tract-specific analyses, FetDTIAlign supports new\ndiscoveries in early brain development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion MRI (dMRI) provides unique insights into fetal brain microstructure\nin utero. Longitudinal and cross-sectional fetal dMRI studies can reveal\ncrucial neurodevelopmental changes but require precise spatial alignment across\nscans and subjects. This is challenging due to low data quality, rapid brain\ndevelopment, and limited anatomical landmarks. Existing registration methods,\ndesigned for high-quality adult data, struggle with these complexities. To\naddress this, we introduce FetDTIAlign, a deep learning approach for fetal\nbrain dMRI registration, enabling accurate affine and deformable alignment.\nFetDTIAlign features a dual-encoder architecture and iterative feature-based\ninference, reducing the impact of noise and low resolution. It optimizes\nnetwork configurations and domain-specific features at each registration stage,\nenhancing both robustness and accuracy. We validated FetDTIAlign on data from\n23 to 36 weeks gestation, covering 60 white matter tracts. It consistently\noutperformed two classical optimization-based methods and a deep learning\npipeline, achieving superior anatomical correspondence. Further validation on\nexternal data from the Developing Human Connectome Project confirmed its\ngeneralizability across acquisition protocols. Our results demonstrate the\nfeasibility of deep learning for fetal brain dMRI registration, providing a\nmore accurate and reliable alternative to classical techniques. By enabling\nprecise cross-subject and tract-specific analyses, FetDTIAlign supports new\ndiscoveries in early brain development."
                },
                "authors": [
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Qi Zeng"
                    },
                    {
                        "name": "Simon K. Warfield"
                    },
                    {
                        "name": "Davood Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Davood Karimi"
                },
                "author": "Davood Karimi",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01057v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01057v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05315v3",
                "updated": "2025-02-24T17:53:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    53,
                    6,
                    0,
                    55,
                    0
                ],
                "published": "2024-06-08T01:27:19Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    27,
                    19,
                    5,
                    160,
                    0
                ],
                "title": "Aligned at the Start: Conceptual Groupings in LLM Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned at the Start: Conceptual Groupings in LLM Embeddings"
                },
                "summary": "This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks."
                },
                "authors": [
                    {
                        "name": "Mehrdad Khatir"
                    },
                    {
                        "name": "Sanchit Kabra"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v7",
                "updated": "2025-02-24T17:40:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    38,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "Minor typos revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17361v1",
                "updated": "2025-02-24T17:38:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    38,
                    42,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:38:42Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    38,
                    42,
                    0,
                    55,
                    0
                ],
                "title": "A Closer Look at TabPFN v2: Strength, Limitation, and Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closer Look at TabPFN v2: Strength, Limitation, and Extension"
                },
                "summary": "Tabular datasets are inherently heterogeneous, posing significant challenges\nfor developing pre-trained foundation models. The recently introduced\ntransformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves\nunprecedented in-context learning accuracy across multiple tabular datasets,\nmarking a pivotal advancement in tabular foundation models. In this paper, we\ncomprehensively evaluate TabPFN v2 on over 300 datasets, confirming its\nexceptional generalization capabilities on small- to medium-scale tasks. Our\nanalysis identifies randomized feature tokens as a key factor behind TabPFN\nv2's success, as they unify heterogeneous datasets into a fixed-dimensional\nrepresentation, enabling more effective training and inference. To further\nunderstand TabPFN v2's predictions, we propose a leave-one-fold-out approach,\ntransforming TabPFN v2 into a feature extractor and revealing its capability to\nsimplify data distributions and boost accuracy. Lastly, to address TabPFN v2's\nlimitations in high-dimensional, large-scale, and many-category tasks, we\nintroduce a divide-and-conquer mechanism inspired by Chain-of-Thought\nprompting, enabling scalable inference. By uncovering the mechanisms behind\nTabPFN v2's success and introducing strategies to expand its applicability,\nthis study provides key insights into the future of tabular foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular datasets are inherently heterogeneous, posing significant challenges\nfor developing pre-trained foundation models. The recently introduced\ntransformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves\nunprecedented in-context learning accuracy across multiple tabular datasets,\nmarking a pivotal advancement in tabular foundation models. In this paper, we\ncomprehensively evaluate TabPFN v2 on over 300 datasets, confirming its\nexceptional generalization capabilities on small- to medium-scale tasks. Our\nanalysis identifies randomized feature tokens as a key factor behind TabPFN\nv2's success, as they unify heterogeneous datasets into a fixed-dimensional\nrepresentation, enabling more effective training and inference. To further\nunderstand TabPFN v2's predictions, we propose a leave-one-fold-out approach,\ntransforming TabPFN v2 into a feature extractor and revealing its capability to\nsimplify data distributions and boost accuracy. Lastly, to address TabPFN v2's\nlimitations in high-dimensional, large-scale, and many-category tasks, we\nintroduce a divide-and-conquer mechanism inspired by Chain-of-Thought\nprompting, enabling scalable inference. By uncovering the mechanisms behind\nTabPFN v2's success and introducing strategies to expand its applicability,\nthis study provides key insights into the future of tabular foundation models."
                },
                "authors": [
                    {
                        "name": "Han-Jia Ye"
                    },
                    {
                        "name": "Si-Yang Liu"
                    },
                    {
                        "name": "Wei-Lun Chao"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Lun Chao"
                },
                "author": "Wei-Lun Chao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17358v1",
                "updated": "2025-02-24T17:36:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    36,
                    49,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:36:49Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    36,
                    49,
                    0,
                    55,
                    0
                ],
                "title": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data"
                },
                "summary": "How can we verify whether copyrighted content was used to train a large\nvision-language model (VLM) without direct access to its training data?\nMotivated by the hypothesis that a VLM is able to recognize images from its\ntraining corpus, we propose DIS-CO, a novel approach to infer the inclusion of\ncopyrighted content during the model's development. By repeatedly querying a\nVLM with specific frames from targeted copyrighted material, DIS-CO extracts\nthe content's identity through free-form text completions. To assess its\neffectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames\npaired with detailed captions, drawn from films released both before and after\na model's training cutoff. Our results show that DIS-CO significantly improves\ndetection performance, nearly doubling the average AUC of the best prior method\non models with logits available. Our findings also highlight a broader concern:\nall tested models appear to have been exposed to some extent to copyrighted\ncontent. Our code and data are available at\nhttps://github.com/avduarte333/DIS-CO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we verify whether copyrighted content was used to train a large\nvision-language model (VLM) without direct access to its training data?\nMotivated by the hypothesis that a VLM is able to recognize images from its\ntraining corpus, we propose DIS-CO, a novel approach to infer the inclusion of\ncopyrighted content during the model's development. By repeatedly querying a\nVLM with specific frames from targeted copyrighted material, DIS-CO extracts\nthe content's identity through free-form text completions. To assess its\neffectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames\npaired with detailed captions, drawn from films released both before and after\na model's training cutoff. Our results show that DIS-CO significantly improves\ndetection performance, nearly doubling the average AUC of the best prior method\non models with logits available. Our findings also highlight a broader concern:\nall tested models appear to have been exposed to some extent to copyrighted\ncontent. Our code and data are available at\nhttps://github.com/avduarte333/DIS-CO"
                },
                "authors": [
                    {
                        "name": "Andr V. Duarte"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Arlindo L. Oliveira"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06287v2",
                "updated": "2025-02-24T17:35:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    35,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-08T18:38:32Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    18,
                    38,
                    32,
                    1,
                    282,
                    0
                ],
                "title": "Non-Halting Queries: Exploiting Fixed Points in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Halting Queries: Exploiting Fixed Points in LLMs"
                },
                "summary": "We introduce a new vulnerability that exploits fixed points in autoregressive\nmodels and use it to craft queries that never halt. More precisely, for\nnon-halting queries, the LLM never samples the end-of-string token <eos>. We\nrigorously analyze the conditions under which the non-halting anomaly presents\nitself. In particular, at temperature zero, we prove that if a repeating\n(cyclic) token sequence is observed at the output beyond the context size, then\nthe LLM does not halt.\n  We demonstrate non-halting queries in many experiments performed in base\nunaligned models where repeating prompts immediately lead to a non-halting\ncyclic behavior as predicted by the analysis. Further, we develop a simple\nrecipe that takes the same fixed points observed in the base model and creates\na prompt structure to target aligned models. We demonstrate the recipe's\nsuccess in sending every major model released over the past year into a\nnon-halting state with the same simple prompt even over higher temperatures.\nFurther, we devise an experiment with 100 randomly selected tokens and show\nthat the recipe to create non-halting queries succeeds with high success rates\nranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that\nthe proposed adversarial recipe succeeds in bypassing alignment at one to two\norders of magnitude higher rates compared to earlier reports.\n  We also study gradient-based direct inversion using ARCA to craft new short\nprompts to induce the non-halting state. We inverted 10,000 random repeating\n2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted\nprompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments\nwith ARCA show that non-halting may be easily induced with as few as 3 input\ntokens with high probability. Overall, our experiments demonstrate that\nnon-halting queries are prevalent and relatively easy to find.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new vulnerability that exploits fixed points in autoregressive\nmodels and use it to craft queries that never halt. More precisely, for\nnon-halting queries, the LLM never samples the end-of-string token <eos>. We\nrigorously analyze the conditions under which the non-halting anomaly presents\nitself. In particular, at temperature zero, we prove that if a repeating\n(cyclic) token sequence is observed at the output beyond the context size, then\nthe LLM does not halt.\n  We demonstrate non-halting queries in many experiments performed in base\nunaligned models where repeating prompts immediately lead to a non-halting\ncyclic behavior as predicted by the analysis. Further, we develop a simple\nrecipe that takes the same fixed points observed in the base model and creates\na prompt structure to target aligned models. We demonstrate the recipe's\nsuccess in sending every major model released over the past year into a\nnon-halting state with the same simple prompt even over higher temperatures.\nFurther, we devise an experiment with 100 randomly selected tokens and show\nthat the recipe to create non-halting queries succeeds with high success rates\nranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that\nthe proposed adversarial recipe succeeds in bypassing alignment at one to two\norders of magnitude higher rates compared to earlier reports.\n  We also study gradient-based direct inversion using ARCA to craft new short\nprompts to induce the non-halting state. We inverted 10,000 random repeating\n2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted\nprompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments\nwith ARCA show that non-halting may be easily induced with as few as 3 input\ntokens with high probability. Overall, our experiments demonstrate that\nnon-halting queries are prevalent and relatively easy to find."
                },
                "authors": [
                    {
                        "name": "Ghaith Hammouri"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17356v1",
                "updated": "2025-02-24T17:34:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    34,
                    45,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    34,
                    45,
                    0,
                    55,
                    0
                ],
                "title": "Distributional Scaling Laws for Emergent Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Scaling Laws for Emergent Capabilities"
                },
                "summary": "In this paper, we explore the nature of sudden breakthroughs in language\nmodel performance at scale, which stands in contrast to smooth improvements\ngoverned by scaling laws. While advocates of \"emergence\" view abrupt\nperformance gains as capabilities unlocking at specific scales, others have\nsuggested that they are produced by thresholding effects and alleviated by\ncontinuous metrics. We propose that breakthroughs are instead driven by\ncontinuous changes in the probability distribution of training outcomes,\nparticularly when performance is bimodally distributed across random seeds. In\nsynthetic length generalization tasks, we show that different random seeds can\nproduce either highly linear or emergent scaling trends. We reveal that sharp\nbreakthroughs in metrics are produced by underlying continuous changes in their\ndistribution across seeds. Furthermore, we provide a case study of inverse\nscaling and show that even as the probability of a successful run declines, the\naverage performance of a successful run continues to increase monotonically. We\nvalidate our distributional scaling framework on realistic settings by\nmeasuring MMLU performance in LLM populations. These insights emphasize the\nrole of random variation in the effect of scale on LLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the nature of sudden breakthroughs in language\nmodel performance at scale, which stands in contrast to smooth improvements\ngoverned by scaling laws. While advocates of \"emergence\" view abrupt\nperformance gains as capabilities unlocking at specific scales, others have\nsuggested that they are produced by thresholding effects and alleviated by\ncontinuous metrics. We propose that breakthroughs are instead driven by\ncontinuous changes in the probability distribution of training outcomes,\nparticularly when performance is bimodally distributed across random seeds. In\nsynthetic length generalization tasks, we show that different random seeds can\nproduce either highly linear or emergent scaling trends. We reveal that sharp\nbreakthroughs in metrics are produced by underlying continuous changes in their\ndistribution across seeds. Furthermore, we provide a case study of inverse\nscaling and show that even as the probability of a successful run declines, the\naverage performance of a successful run continues to increase monotonically. We\nvalidate our distributional scaling framework on realistic settings by\nmeasuring MMLU performance in LLM populations. These insights emphasize the\nrole of random variation in the effect of scale on LLM capabilities."
                },
                "authors": [
                    {
                        "name": "Rosie Zhao"
                    },
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Naomi Saphra"
                    }
                ],
                "author_detail": {
                    "name": "Naomi Saphra"
                },
                "author": "Naomi Saphra",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17355v1",
                "updated": "2025-02-24T17:33:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    33,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:33:18Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    33,
                    18,
                    0,
                    55,
                    0
                ],
                "title": "On Relation-Specific Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Relation-Specific Neurons in Large Language Models"
                },
                "summary": "In large language models (LLMs), certain neurons can store distinct pieces of\nknowledge learned during pretraining. While knowledge typically appears as a\ncombination of relations and entities, it remains unclear whether some neurons\nfocus on a relation itself -- independent of any entity. We hypothesize such\nneurons detect a relation in the input text and guide generation involving such\na relation. To investigate this, we study the Llama-2 family on a chosen set of\nrelations with a statistics-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts whose relation is $r$ and (2) facts whose relation is a\ndifferent relation $r' \\neq r$. With respect to their capacity for encoding\nrelation information, we give evidence for the following three properties of\nrelation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for\n$r$ present a cumulative effect so that deactivating a larger portion of them\nresults in the degradation of more facts in $r$. $\\textbf{(ii) Neuron\nversatility.}$ Neurons can be shared across multiple closely related as well as\nless related relations. Some relation neurons transfer across languages.\n$\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one\nrelation can improve LLM generation performance for facts of other relations.\nWe will make our code publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language models (LLMs), certain neurons can store distinct pieces of\nknowledge learned during pretraining. While knowledge typically appears as a\ncombination of relations and entities, it remains unclear whether some neurons\nfocus on a relation itself -- independent of any entity. We hypothesize such\nneurons detect a relation in the input text and guide generation involving such\na relation. To investigate this, we study the Llama-2 family on a chosen set of\nrelations with a statistics-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts whose relation is $r$ and (2) facts whose relation is a\ndifferent relation $r' \\neq r$. With respect to their capacity for encoding\nrelation information, we give evidence for the following three properties of\nrelation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for\n$r$ present a cumulative effect so that deactivating a larger portion of them\nresults in the degradation of more facts in $r$. $\\textbf{(ii) Neuron\nversatility.}$ Neurons can be shared across multiple closely related as well as\nless related relations. Some relation neurons transfer across languages.\n$\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one\nrelation can improve LLM generation performance for facts of other relations.\nWe will make our code publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons."
                },
                "authors": [
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Runsheng Chen"
                    },
                    {
                        "name": "Lea Hirlimann"
                    },
                    {
                        "name": "Ahmad Dawar Hakimi"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Amir Hossein Kargaran"
                    },
                    {
                        "name": "Sascha Rothe"
                    },
                    {
                        "name": "Franois Yvon"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17352v1",
                "updated": "2025-02-24T17:29:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    29,
                    10,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:29:10Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    29,
                    10,
                    0,
                    55,
                    0
                ],
                "title": "Leveraging Procedural Knowledge and Task Hierarchies for Efficient\n  Instructional Video Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Procedural Knowledge and Task Hierarchies for Efficient\n  Instructional Video Pre-training"
                },
                "summary": "Instructional videos provide a convenient modality to learn new tasks (ex.\ncooking a recipe, or assembling furniture). A viewer will want to find a\ncorresponding video that reflects both the overall task they are interested in\nas well as contains the relevant steps they need to carry out the task. To\nperform this, an instructional video model should be capable of inferring both\nthe tasks and the steps that occur in an input video. Doing this efficiently\nand in a generalizable fashion is key when compute or relevant video topics\nused to train this model are limited. To address these requirements we\nexplicitly mine task hierarchies and the procedural steps associated with\ninstructional videos. We use this prior knowledge to pre-train our model,\n$\\texttt{Pivot}$, for step and task prediction. During pre-training, we also\nprovide video augmentation and early stopping strategies to optimally identify\nwhich model to use for downstream tasks. We test this pre-trained model on task\nrecognition, step recognition, and step prediction tasks on two downstream\ndatasets. When pre-training data and compute are limited, we outperform\nprevious baselines along these tasks. Therefore, leveraging prior task and step\nstructures enables efficient training of $\\texttt{Pivot}$ for instructional\nvideo recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructional videos provide a convenient modality to learn new tasks (ex.\ncooking a recipe, or assembling furniture). A viewer will want to find a\ncorresponding video that reflects both the overall task they are interested in\nas well as contains the relevant steps they need to carry out the task. To\nperform this, an instructional video model should be capable of inferring both\nthe tasks and the steps that occur in an input video. Doing this efficiently\nand in a generalizable fashion is key when compute or relevant video topics\nused to train this model are limited. To address these requirements we\nexplicitly mine task hierarchies and the procedural steps associated with\ninstructional videos. We use this prior knowledge to pre-train our model,\n$\\texttt{Pivot}$, for step and task prediction. During pre-training, we also\nprovide video augmentation and early stopping strategies to optimally identify\nwhich model to use for downstream tasks. We test this pre-trained model on task\nrecognition, step recognition, and step prediction tasks on two downstream\ndatasets. When pre-training data and compute are limited, we outperform\nprevious baselines along these tasks. Therefore, leveraging prior task and step\nstructures enables efficient training of $\\texttt{Pivot}$ for instructional\nvideo recommendation."
                },
                "authors": [
                    {
                        "name": "Karan Samel"
                    },
                    {
                        "name": "Nitish Sontakke"
                    },
                    {
                        "name": "Irfan Essa"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Essa"
                },
                "author": "Irfan Essa",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02481v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02481v4",
                "updated": "2025-02-24T17:24:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    24,
                    4,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-04T16:57:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo."
                },
                "authors": [
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Pengzhi Gao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "Accept to NAACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02481v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02481v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17349v1",
                "updated": "2025-02-24T17:23:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    23,
                    40,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:23:40Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    23,
                    40,
                    0,
                    55,
                    0
                ],
                "title": "HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity\n  and Validity in 3D Molecular Linker Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity\n  and Validity in 3D Molecular Linker Generation"
                },
                "summary": "Linker generation is critical in drug discovery applications such as lead\noptimization and PROTAC design, where molecular fragments are assembled into\ndiverse drug candidates. Existing methods fall into PC-Free and PC-Aware\ncategories based on their use of 3D point clouds (PC). PC-Free models\nprioritize diversity but suffer from lower validity due to overlooking PC\nconstraints, while PC-Aware models ensure higher validity but restrict\ndiversity by enforcing strict PC constraints. To overcome these trade-offs\nwithout additional training, we propose HybridLinker, a framework that enhances\nPC-Aware inference by providing diverse bonding topologies from a pretrained\nPC-Free model as guidance. At its core, we propose LinkerDPS, the first\ndiffusion posterior sampling (DPS) method operating across PC-Free and PC-Aware\nspaces, bridging molecular topology with 3D point clouds via an energy-inspired\nfunction. By transferring the diverse sampling distribution of PC-Free models\ninto the PC-Aware distribution, HybridLinker significantly and consistently\nsurpasses baselines, improving both validity and diversity in foundational\nmolecular design and applied property optimization tasks, establishing a new\nDPS framework in the molecular and graph domains beyond imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linker generation is critical in drug discovery applications such as lead\noptimization and PROTAC design, where molecular fragments are assembled into\ndiverse drug candidates. Existing methods fall into PC-Free and PC-Aware\ncategories based on their use of 3D point clouds (PC). PC-Free models\nprioritize diversity but suffer from lower validity due to overlooking PC\nconstraints, while PC-Aware models ensure higher validity but restrict\ndiversity by enforcing strict PC constraints. To overcome these trade-offs\nwithout additional training, we propose HybridLinker, a framework that enhances\nPC-Aware inference by providing diverse bonding topologies from a pretrained\nPC-Free model as guidance. At its core, we propose LinkerDPS, the first\ndiffusion posterior sampling (DPS) method operating across PC-Free and PC-Aware\nspaces, bridging molecular topology with 3D point clouds via an energy-inspired\nfunction. By transferring the diverse sampling distribution of PC-Free models\ninto the PC-Aware distribution, HybridLinker significantly and consistently\nsurpasses baselines, improving both validity and diversity in foundational\nmolecular design and applied property optimization tasks, establishing a new\nDPS framework in the molecular and graph domains beyond imaging."
                },
                "authors": [
                    {
                        "name": "Minyeong Hwang"
                    },
                    {
                        "name": "Ziseok Lee"
                    },
                    {
                        "name": "Gwangsoo Kim"
                    },
                    {
                        "name": "Kyungsu Kim"
                    },
                    {
                        "name": "Eunho Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eunho Yang"
                },
                "author": "Eunho Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17341v1",
                "updated": "2025-02-24T17:17:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    17,
                    15,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:17:15Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    17,
                    15,
                    0,
                    55,
                    0
                ],
                "title": "Time series forecasting based on optimized LLM for fault prediction in\n  distribution power grid insulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting based on optimized LLM for fault prediction in\n  distribution power grid insulators"
                },
                "summary": "Surface contamination on electrical grid insulators leads to an increase in\nleakage current until an electrical discharge occurs, which can result in a\npower system shutdown. To mitigate the possibility of disruptive faults\nresulting in a power outage, monitoring contamination and leakage current can\nhelp predict the progression of faults. Given this need, this paper proposes a\nhybrid deep learning (DL) model for predicting the increase in leakage current\nin high-voltage insulators. The hybrid structure considers a multi-criteria\noptimization using tree-structured Parzen estimation, an input stage filter for\nsignal noise attenuation combined with a large language model (LLM) applied for\ntime series forecasting. The proposed optimized LLM outperforms\nstate-of-the-art DL models with a root-mean-square error equal to\n2.24$\\times10^{-4}$ for a short-term horizon and 1.21$\\times10^{-3}$ for a\nmedium-term horizon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface contamination on electrical grid insulators leads to an increase in\nleakage current until an electrical discharge occurs, which can result in a\npower system shutdown. To mitigate the possibility of disruptive faults\nresulting in a power outage, monitoring contamination and leakage current can\nhelp predict the progression of faults. Given this need, this paper proposes a\nhybrid deep learning (DL) model for predicting the increase in leakage current\nin high-voltage insulators. The hybrid structure considers a multi-criteria\noptimization using tree-structured Parzen estimation, an input stage filter for\nsignal noise attenuation combined with a large language model (LLM) applied for\ntime series forecasting. The proposed optimized LLM outperforms\nstate-of-the-art DL models with a root-mean-square error equal to\n2.24$\\times10^{-4}$ for a short-term horizon and 1.21$\\times10^{-3}$ for a\nmedium-term horizon."
                },
                "authors": [
                    {
                        "name": "Joo Pedro Matos-Carvalho"
                    },
                    {
                        "name": "Stefano Frizzo Stefenon"
                    },
                    {
                        "name": "Valderi Reis Quietinho Leithardt"
                    },
                    {
                        "name": "Kin-Choong Yow"
                    }
                ],
                "author_detail": {
                    "name": "Kin-Choong Yow"
                },
                "author": "Kin-Choong Yow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14845v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14845v3",
                "updated": "2025-02-24T17:06:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    6,
                    21,
                    0,
                    55,
                    0
                ],
                "published": "2024-07-20T11:19:58Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    11,
                    19,
                    58,
                    5,
                    202,
                    0
                ],
                "title": "Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models"
                },
                "summary": "Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real-world datasets validate\nour proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real-world datasets validate\nour proposed model."
                },
                "authors": [
                    {
                        "name": "Ze Yu Zhang"
                    },
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Finale Doshi-Velez"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "22 pages, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14845v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14845v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17333v1",
                "updated": "2025-02-24T17:05:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    5,
                    14,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:05:14Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    5,
                    14,
                    0,
                    55,
                    0
                ],
                "title": "Jet rates in Higgs boson decay at third order in QCD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jet rates in Higgs boson decay at third order in QCD"
                },
                "summary": "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables."
                },
                "authors": [
                    {
                        "name": "Elliot Fox"
                    },
                    {
                        "name": "Aude Gehrmann-De Ridder"
                    },
                    {
                        "name": "Thomas Gehrmann"
                    },
                    {
                        "name": "Nigel Glover"
                    },
                    {
                        "name": "Matteo Marcoli"
                    },
                    {
                        "name": "Christian T. Preuss"
                    }
                ],
                "author_detail": {
                    "name": "Christian T. Preuss"
                },
                "author": "Christian T. Preuss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09735v3",
                "updated": "2025-02-24T17:02:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    2,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2024-07-13T00:57:04Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    0,
                    57,
                    4,
                    5,
                    195,
                    0
                ],
                "title": "Positive and Unlabeled Data: Model, Estimation, Inference, and\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positive and Unlabeled Data: Model, Estimation, Inference, and\n  Classification"
                },
                "summary": "This study introduces a new approach to addressing positive and unlabeled\n(PU) data through the double exponential tilting model (DETM). Traditional\nmethods often fall short because they only apply to selected completely at\nrandom (SCAR) PU data, where the labeled positive and unlabeled positive data\nare assumed to be from the same distribution. In contrast, our DETM's dual\nstructure effectively accommodates the more complex and underexplored selected\nat random PU data, where the labeled and unlabeled positive data can be from\ndifferent distributions. We rigorously establish the theoretical foundations of\nDETM, including identifiability, parameter estimation, and asymptotic\nproperties. Additionally, we move forward to statistical inference by\ndeveloping a goodness-of-fit test for the SCAR condition and constructing\nconfidence intervals for the proportion of positive instances in the target\ndomain. We leverage an approximated Bayes classifier for classification tasks,\ndemonstrating DETM's robust performance in prediction. Through theoretical\ninsights and practical applications, this study highlights DETM as a\ncomprehensive framework for addressing the challenges of PU data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a new approach to addressing positive and unlabeled\n(PU) data through the double exponential tilting model (DETM). Traditional\nmethods often fall short because they only apply to selected completely at\nrandom (SCAR) PU data, where the labeled positive and unlabeled positive data\nare assumed to be from the same distribution. In contrast, our DETM's dual\nstructure effectively accommodates the more complex and underexplored selected\nat random PU data, where the labeled and unlabeled positive data can be from\ndifferent distributions. We rigorously establish the theoretical foundations of\nDETM, including identifiability, parameter estimation, and asymptotic\nproperties. Additionally, we move forward to statistical inference by\ndeveloping a goodness-of-fit test for the SCAR condition and constructing\nconfidence intervals for the proportion of positive instances in the target\ndomain. We leverage an approximated Bayes classifier for classification tasks,\ndemonstrating DETM's robust performance in prediction. Through theoretical\ninsights and practical applications, this study highlights DETM as a\ncomprehensive framework for addressing the challenges of PU data."
                },
                "authors": [
                    {
                        "name": "Siyan Liu"
                    },
                    {
                        "name": "Chi-Kuang Yeh"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Qinglong Tian"
                    },
                    {
                        "name": "Pengfei Li"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Li"
                },
                "author": "Pengfei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14926v2",
                "updated": "2025-02-24T17:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    2,
                    5,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-19T23:16:29Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    23,
                    16,
                    29,
                    2,
                    50,
                    0
                ],
                "title": "DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for\n  LoRaWAN-related engineering tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for\n  LoRaWAN-related engineering tasks"
                },
                "summary": "This paper investigates the performance of 16 Large Language Models (LLMs) in\nautomating LoRaWAN-related engineering tasks involving optimal placement of\ndrones and received power calculation under progressively complex zero-shot,\nnatural language prompts. The primary research question is whether lightweight,\nlocally executed LLMs can generate correct Python code for these tasks. To\nassess this, we compared locally run models against state-of-the-art\nalternatives, such as GPT-4 and DeepSeek-V3, which served as reference points.\nBy extracting and executing the Python functions generated by each model, we\nevaluated their outputs on a zero-to-five scale. Results show that while\nDeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller\nmodels-particularly Phi-4 and LLaMA-3.3-also demonstrated strong performance,\nunderscoring the viability of lightweight alternatives. Other models exhibited\nerrors stemming from incomplete understanding or syntactic issues. These\nfindings illustrate the potential of LLM-based approaches for specialized\nengineering applications while highlighting the need for careful model\nselection, rigorous prompt design, and targeted domain fine-tuning to achieve\nreliable outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the performance of 16 Large Language Models (LLMs) in\nautomating LoRaWAN-related engineering tasks involving optimal placement of\ndrones and received power calculation under progressively complex zero-shot,\nnatural language prompts. The primary research question is whether lightweight,\nlocally executed LLMs can generate correct Python code for these tasks. To\nassess this, we compared locally run models against state-of-the-art\nalternatives, such as GPT-4 and DeepSeek-V3, which served as reference points.\nBy extracting and executing the Python functions generated by each model, we\nevaluated their outputs on a zero-to-five scale. Results show that while\nDeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller\nmodels-particularly Phi-4 and LLaMA-3.3-also demonstrated strong performance,\nunderscoring the viability of lightweight alternatives. Other models exhibited\nerrors stemming from incomplete understanding or syntactic issues. These\nfindings illustrate the potential of LLM-based approaches for specialized\nengineering applications while highlighting the need for careful model\nselection, rigorous prompt design, and targeted domain fine-tuning to achieve\nreliable outcomes."
                },
                "authors": [
                    {
                        "name": "Daniel Fernandes"
                    },
                    {
                        "name": "Joo P. Matos-Carvalho"
                    },
                    {
                        "name": "Carlos M. Fernandes"
                    },
                    {
                        "name": "Nuno Fachada"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Fachada"
                },
                "author": "Nuno Fachada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17328v1",
                "updated": "2025-02-24T17:01:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    1,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:01:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    1,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\n  Capabilities for Few-Shot Dialogue Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\n  Capabilities for Few-Shot Dialogue Summarization"
                },
                "summary": "In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs\nto improve few-shot dialogue summarization task. Unlike prior methods that\nrequire external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis\nand summarization capabilities, allowing them to complement each other during\ntraining and enhance overall performances. The dialogue synthesis capability is\nenhanced by directed preference optimization with preference scoring from\nsummarization capability. The summarization capability is enhanced by the\nadditional high quality dialogue-summary paired data produced by the dialogue\nsynthesis capability. By leveraging the proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of synthetic data, and use it to\naugment the few-shot real training dataset. Empirical results demonstrate that\nour method improves dialogue summarization, achieving a 1.5% increase in ROUGE\nscores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,\nour method attains the highest average scores in human evaluations, surpassing\nboth the pre-trained models and the baselines fine-tuned solely for\nsummarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs\nto improve few-shot dialogue summarization task. Unlike prior methods that\nrequire external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis\nand summarization capabilities, allowing them to complement each other during\ntraining and enhance overall performances. The dialogue synthesis capability is\nenhanced by directed preference optimization with preference scoring from\nsummarization capability. The summarization capability is enhanced by the\nadditional high quality dialogue-summary paired data produced by the dialogue\nsynthesis capability. By leveraging the proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of synthetic data, and use it to\naugment the few-shot real training dataset. Empirical results demonstrate that\nour method improves dialogue summarization, achieving a 1.5% increase in ROUGE\nscores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,\nour method attains the highest average scores in human evaluations, surpassing\nboth the pre-trained models and the baselines fine-tuned solely for\nsummarization tasks."
                },
                "authors": [
                    {
                        "name": "Yen-Ju Lu"
                    },
                    {
                        "name": "Ting-Yao Hu"
                    },
                    {
                        "name": "Hema Swetha Koppula"
                    },
                    {
                        "name": "Hadi Pouransari"
                    },
                    {
                        "name": "Jen-Hao Rick Chang"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    }
                ],
                "author_detail": {
                    "name": "Raviteja Vemulapalli"
                },
                "author": "Raviteja Vemulapalli",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20318v3",
                "updated": "2025-02-24T16:42:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    42,
                    25,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-30T17:55:28Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    17,
                    55,
                    28,
                    3,
                    151,
                    0
                ],
                "title": "Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry\n  through Curiosity-Driven Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry\n  through Curiosity-Driven Queries"
                },
                "summary": "Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions."
                },
                "authors": [
                    {
                        "name": "Roberto Ceraolo"
                    },
                    {
                        "name": "Dmitrii Kharlapenko"
                    },
                    {
                        "name": "Ahmad Khan"
                    },
                    {
                        "name": "Amlie Reymond"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Bernhard Schlkopf"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17304v1",
                "updated": "2025-02-24T16:40:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    40,
                    46,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:40:46Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    40,
                    46,
                    0,
                    55,
                    0
                ],
                "title": "Child vs. machine language learning: Can the logical structure of human\n  language unleash LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Child vs. machine language learning: Can the logical structure of human\n  language unleash LLMs?"
                },
                "summary": "We argue that human language learning proceeds in a manner that is different\nin nature from current approaches to training LLMs, predicting a difference in\nlearning biases. We then present evidence from German plural formation by LLMs\nthat confirm our hypothesis that even very powerful implementations produce\nresults that miss aspects of the logic inherent to language that humans have no\nproblem with. We conclude that attention to the different structures of human\nlanguage and artificial neural networks is likely to be an avenue to improve\nLLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that human language learning proceeds in a manner that is different\nin nature from current approaches to training LLMs, predicting a difference in\nlearning biases. We then present evidence from German plural formation by LLMs\nthat confirm our hypothesis that even very powerful implementations produce\nresults that miss aspects of the logic inherent to language that humans have no\nproblem with. We conclude that attention to the different structures of human\nlanguage and artificial neural networks is likely to be an avenue to improve\nLLM performance."
                },
                "authors": [
                    {
                        "name": "Uli Sauerland"
                    },
                    {
                        "name": "Celia Matthaei"
                    },
                    {
                        "name": "Felix Salfner"
                    }
                ],
                "author_detail": {
                    "name": "Felix Salfner"
                },
                "author": "Felix Salfner",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v2",
                "updated": "2025-02-24T16:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    36,
                    32,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09630v2",
                "updated": "2025-02-24T16:35:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    35,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2024-11-27T15:46:54Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    46,
                    54,
                    2,
                    332,
                    0
                ],
                "title": "What does AI consider praiseworthy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What does AI consider praiseworthy?"
                },
                "summary": "As large language models (LLMs) are increasingly used for work, personal, and\ntherapeutic purposes, researchers have begun to investigate these models'\nimplicit and explicit moral views. Previous work, however, focuses on asking\nLLMs to state opinions, or on other technical evaluations that do not reflect\ncommon user interactions. We propose a novel evaluation of LLM behavior that\nanalyzes responses to user-stated intentions, such as \"I'm thinking of\ncampaigning for {candidate}.\" LLMs frequently respond with critiques or praise,\noften beginning responses with phrases such as \"That's great to hear!...\" While\nthis makes them friendly, these praise responses are not universal and thus\nreflect a normative stance by the LLM. We map out the moral landscape of LLMs\nin how they respond to user statements in different domains including politics\nand everyday ethical actions. In particular, although a na\\\"ive analysis might\nsuggest LLMs are biased against right-leaning politics, our findings on news\nsources indicate that trustworthiness is a stronger driver of praise and\ncritique than ideology. Second, we find strong alignment across models in\nresponse to ethically-relevant action statements, but that doing so requires\nthem to engage in high levels of praise and critique of users, suggesting a\nreticence-alignment tradeoff. Finally, our experiment on statements about world\nleaders finds no evidence of bias favoring the country of origin of the models.\nWe conclude that as AI systems become more integrated into society, their\npatterns of praise, critique, and neutrality must be carefully monitored to\nprevent unintended psychological and societal consequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used for work, personal, and\ntherapeutic purposes, researchers have begun to investigate these models'\nimplicit and explicit moral views. Previous work, however, focuses on asking\nLLMs to state opinions, or on other technical evaluations that do not reflect\ncommon user interactions. We propose a novel evaluation of LLM behavior that\nanalyzes responses to user-stated intentions, such as \"I'm thinking of\ncampaigning for {candidate}.\" LLMs frequently respond with critiques or praise,\noften beginning responses with phrases such as \"That's great to hear!...\" While\nthis makes them friendly, these praise responses are not universal and thus\nreflect a normative stance by the LLM. We map out the moral landscape of LLMs\nin how they respond to user statements in different domains including politics\nand everyday ethical actions. In particular, although a na\\\"ive analysis might\nsuggest LLMs are biased against right-leaning politics, our findings on news\nsources indicate that trustworthiness is a stronger driver of praise and\ncritique than ideology. Second, we find strong alignment across models in\nresponse to ethically-relevant action statements, but that doing so requires\nthem to engage in high levels of praise and critique of users, suggesting a\nreticence-alignment tradeoff. Finally, our experiment on statements about world\nleaders finds no evidence of bias favoring the country of origin of the models.\nWe conclude that as AI systems become more integrated into society, their\npatterns of praise, critique, and neutrality must be carefully monitored to\nprevent unintended psychological and societal consequences."
                },
                "authors": [
                    {
                        "name": "Andrew J. Peterson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Peterson"
                },
                "author": "Andrew J. Peterson",
                "arxiv_comment": "Forthcoming in AI and Ethics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.1; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06786v2",
                "updated": "2025-02-24T16:34:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    34,
                    21,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-10T18:59:10Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    10,
                    0,
                    41,
                    0
                ],
                "title": "Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Quantization"
                },
                "summary": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits.\nLeveraging this insight, in this paper, we propose Matryoshka Quantization\n(\\alg), a novel multi-scale quantization technique that alleviates the\naforementioned challenge. This technique allows us to train and maintain a\nsingle quantized model but serve it with the precision demanded by the\ndeployment. Furthermore, leveraging \\alg's co-training and co-distillation\nregularization, int2 precision models extracted by \\alg outperform standard\nint2 quantization by up to to 4\\% and 7\\% with OmniQuant and QAT as base\nalgorithms respectively. Finally, we demonstrate that by using an extra bit to\nrepresent outliers, a model with an effective precision of 2.05-bit gives an\nadditional 6\\% improvement with OmniQuant as the base algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits.\nLeveraging this insight, in this paper, we propose Matryoshka Quantization\n(\\alg), a novel multi-scale quantization technique that alleviates the\naforementioned challenge. This technique allows us to train and maintain a\nsingle quantized model but serve it with the precision demanded by the\ndeployment. Furthermore, leveraging \\alg's co-training and co-distillation\nregularization, int2 precision models extracted by \\alg outperform standard\nint2 quantization by up to to 4\\% and 7\\% with OmniQuant and QAT as base\nalgorithms respectively. Finally, we demonstrate that by using an extra bit to\nrepresent outliers, a model with an effective precision of 2.05-bit gives an\nadditional 6\\% improvement with OmniQuant as the base algorithm."
                },
                "authors": [
                    {
                        "name": "Pranav Nair"
                    },
                    {
                        "name": "Puranjay Datta"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17298v1",
                "updated": "2025-02-24T16:32:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    32,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:32:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    32,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Delta Decompression for MoE-based LLMs Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delta Decompression for MoE-based LLMs Compression"
                },
                "summary": "Mixture-of-Experts (MoE) architectures in large language models (LLMs)\nachieve exceptional performance, but face prohibitive storage and memory\nrequirements. To address these challenges, we present $D^2$-MoE, a new delta\ndecompression compressor for reducing the parameters of MoE LLMs. Based on\nobservations of expert diversity, we decompose their weights into a shared base\nweight and unique delta weights. Specifically, our method first merges each\nexpert's weight into the base weight using the Fisher information matrix to\ncapture shared components. Then, we compress delta weights through Singular\nValue Decomposition (SVD) by exploiting their low-rank properties. Finally, we\nintroduce a semi-dynamical structured pruning strategy for the base weights,\ncombining static and dynamic redundancy analysis to achieve further parameter\nreduction while maintaining input adaptivity. In this way, our $D^2$-MoE\nsuccessfully compact MoE LLMs to high compression ratios without additional\ntraining. Extensive experiments highlight the superiority of our approach, with\nover 13% performance gains than other compressors on\nMixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes\nare available in https://github.com/lliai/D2MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures in large language models (LLMs)\nachieve exceptional performance, but face prohibitive storage and memory\nrequirements. To address these challenges, we present $D^2$-MoE, a new delta\ndecompression compressor for reducing the parameters of MoE LLMs. Based on\nobservations of expert diversity, we decompose their weights into a shared base\nweight and unique delta weights. Specifically, our method first merges each\nexpert's weight into the base weight using the Fisher information matrix to\ncapture shared components. Then, we compress delta weights through Singular\nValue Decomposition (SVD) by exploiting their low-rank properties. Finally, we\nintroduce a semi-dynamical structured pruning strategy for the base weights,\ncombining static and dynamic redundancy analysis to achieve further parameter\nreduction while maintaining input adaptivity. In this way, our $D^2$-MoE\nsuccessfully compact MoE LLMs to high compression ratios without additional\ntraining. Extensive experiments highlight the superiority of our approach, with\nover 13% performance gains than other compressors on\nMixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes\nare available in https://github.com/lliai/D2MoE."
                },
                "authors": [
                    {
                        "name": "Hao Gu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qiyuan Zhu"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Shengjie Sun"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17294v1",
                "updated": "2025-02-24T16:22:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    22,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:22:16Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    22,
                    16,
                    0,
                    55,
                    0
                ],
                "title": "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties"
                },
                "summary": "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions."
                },
                "authors": [
                    {
                        "name": "Kevin Michalewicz"
                    },
                    {
                        "name": "Mauricio Barahona"
                    },
                    {
                        "name": "Barbara Bravi"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Bravi"
                },
                "author": "Barbara Bravi",
                "arxiv_comment": "18 main pages with 4 figures, 3 Supplementary pages with 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17292v1",
                "updated": "2025-02-24T16:21:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    21,
                    50,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:21:50Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    21,
                    50,
                    0,
                    55,
                    0
                ],
                "title": "Joint Value Estimation and Bidding in Repeated First-Price Auctions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Value Estimation and Bidding in Repeated First-Price Auctions"
                },
                "summary": "We study regret minimization in repeated first-price auctions (FPAs), where a\nbidder observes only the realized outcome after each auction -- win or loss.\nThis setup reflects practical scenarios in online display advertising where the\nactual value of an impression depends on the difference between two potential\noutcomes, such as clicks or conversion rates, when the auction is won versus\nlost. We analyze three outcome models: (1) adversarial outcomes without\nfeatures, (2) linear potential outcomes with features, and (3) linear treatment\neffects in features. For each setting, we propose algorithms that jointly\nestimate private values and optimize bidding strategies, achieving near-optimal\nregret bounds. Notably, our framework enjoys a unique feature that the\ntreatments are also actively chosen, and hence eliminates the need for the\noverlap condition commonly required in causal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study regret minimization in repeated first-price auctions (FPAs), where a\nbidder observes only the realized outcome after each auction -- win or loss.\nThis setup reflects practical scenarios in online display advertising where the\nactual value of an impression depends on the difference between two potential\noutcomes, such as clicks or conversion rates, when the auction is won versus\nlost. We analyze three outcome models: (1) adversarial outcomes without\nfeatures, (2) linear potential outcomes with features, and (3) linear treatment\neffects in features. For each setting, we propose algorithms that jointly\nestimate private values and optimize bidding strategies, achieving near-optimal\nregret bounds. Notably, our framework enjoys a unique feature that the\ntreatments are also actively chosen, and hence eliminates the need for the\noverlap condition commonly required in causal inference."
                },
                "authors": [
                    {
                        "name": "Yuxiao Wen"
                    },
                    {
                        "name": "Yanjun Han"
                    },
                    {
                        "name": "Zhengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyuan Zhou"
                },
                "author": "Zhengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17288v1",
                "updated": "2025-02-24T16:16:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    16,
                    1,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:16:01Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    16,
                    1,
                    0,
                    55,
                    0
                ],
                "title": "GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using\n  Gaussian Splatting and Temporal Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using\n  Gaussian Splatting and Temporal Flow"
                },
                "summary": "Occupancy estimation has become a prominent task in 3D computer vision,\nparticularly within the autonomous driving community. In this paper, we present\na novel approach to occupancy estimation, termed GaussianFlowOcc, which is\ninspired by Gaussian Splatting and replaces traditional dense voxel grids with\na sparse 3D Gaussian representation. Our efficient model architecture based on\na Gaussian Transformer significantly reduces computational and memory\nrequirements by eliminating the need for expensive 3D convolutions used with\ninefficient voxel-based representations that predominantly represent empty 3D\nspaces. GaussianFlowOcc effectively captures scene dynamics by estimating\ntemporal flow for each Gaussian during the overall network training process,\noffering a straightforward solution to a complex problem that is often\nneglected by existing methods. Moreover, GaussianFlowOcc is designed for\nscalability, as it employs weak supervision and does not require costly dense\n3D voxel annotations based on additional data (e.g., LiDAR). Through extensive\nexperimentation, we demonstrate that GaussianFlowOcc significantly outperforms\nall previous methods for weakly supervised occupancy estimation on the nuScenes\ndataset while featuring an inference speed that is 50 times faster than current\nSOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occupancy estimation has become a prominent task in 3D computer vision,\nparticularly within the autonomous driving community. In this paper, we present\na novel approach to occupancy estimation, termed GaussianFlowOcc, which is\ninspired by Gaussian Splatting and replaces traditional dense voxel grids with\na sparse 3D Gaussian representation. Our efficient model architecture based on\na Gaussian Transformer significantly reduces computational and memory\nrequirements by eliminating the need for expensive 3D convolutions used with\ninefficient voxel-based representations that predominantly represent empty 3D\nspaces. GaussianFlowOcc effectively captures scene dynamics by estimating\ntemporal flow for each Gaussian during the overall network training process,\noffering a straightforward solution to a complex problem that is often\nneglected by existing methods. Moreover, GaussianFlowOcc is designed for\nscalability, as it employs weak supervision and does not require costly dense\n3D voxel annotations based on additional data (e.g., LiDAR). Through extensive\nexperimentation, we demonstrate that GaussianFlowOcc significantly outperforms\nall previous methods for weakly supervised occupancy estimation on the nuScenes\ndataset while featuring an inference speed that is 50 times faster than current\nSOTA."
                },
                "authors": [
                    {
                        "name": "Simon Boeder"
                    },
                    {
                        "name": "Fabian Gigengack"
                    },
                    {
                        "name": "Benjamin Risse"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Risse"
                },
                "author": "Benjamin Risse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17282v1",
                "updated": "2025-02-24T16:10:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    10,
                    53,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:10:53Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    10,
                    53,
                    0,
                    55,
                    0
                ],
                "title": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated human-like\ninstruction-following abilities, particularly those exceeding 100 billion\nparameters. The combined capability of some smaller, resource-friendly LLMs can\naddress most of the instructions that larger LLMs excel at. In this work, we\nexplore how to route the best-performing LLM for each instruction to achieve\nbetter overall performance. We develop a new paradigm, constructing capability\ninstructions with model capability representation, user instruction, and\nperformance inquiry prompts to assess the performance. To learn from capability\ninstructions, we introduce a new end-to-end framework called Model Selection\nwith Aptitude Test (Model-SAT), which generates positive and negative samples\nbased on what different models perform well or struggle with. Model-SAT uses a\nmodel capability encoder that extends its model representation to a lightweight\nLLM. Our experiments show that Model-SAT understands the performance dimensions\nof candidate models and provides the probabilities of their capability to\nhandle various instructions. Additionally, during deployment, a new model can\nquickly infer its aptitude test results across 50 tasks, each with 20 shots.\nModel-SAT performs state-of-the-art model routing without candidate inference\nand in real-world new model-released scenarios. The code is available at\nhttps://github.com/Now-Join-Us/CIT-LLM-Routing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated human-like\ninstruction-following abilities, particularly those exceeding 100 billion\nparameters. The combined capability of some smaller, resource-friendly LLMs can\naddress most of the instructions that larger LLMs excel at. In this work, we\nexplore how to route the best-performing LLM for each instruction to achieve\nbetter overall performance. We develop a new paradigm, constructing capability\ninstructions with model capability representation, user instruction, and\nperformance inquiry prompts to assess the performance. To learn from capability\ninstructions, we introduce a new end-to-end framework called Model Selection\nwith Aptitude Test (Model-SAT), which generates positive and negative samples\nbased on what different models perform well or struggle with. Model-SAT uses a\nmodel capability encoder that extends its model representation to a lightweight\nLLM. Our experiments show that Model-SAT understands the performance dimensions\nof candidate models and provides the probabilities of their capability to\nhandle various instructions. Additionally, during deployment, a new model can\nquickly infer its aptitude test results across 50 tasks, each with 20 shots.\nModel-SAT performs state-of-the-art model routing without candidate inference\nand in real-world new model-released scenarios. The code is available at\nhttps://github.com/Now-Join-Us/CIT-LLM-Routing"
                },
                "authors": [
                    {
                        "name": "Yi-Kai Zhang"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "arxiv_comment": "AAAI 2025; Project Page: https://cit-llm-routing.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20296v2",
                "updated": "2025-02-24T16:00:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    0,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-30T13:55:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    55,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "PersonalLLM: Tailoring LLMs to Individual Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalLLM: Tailoring LLMs to Individual Preferences"
                },
                "summary": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM"
                },
                "authors": [
                    {
                        "name": "Thomas P. Zollo"
                    },
                    {
                        "name": "Andrew Wei Tung Siah"
                    },
                    {
                        "name": "Naimeng Ye"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13092v2",
                "updated": "2025-02-24T15:59:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    59,
                    4,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T17:59:48Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    59,
                    48,
                    1,
                    49,
                    0
                ],
                "title": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation"
                },
                "summary": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/."
                },
                "authors": [
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Yude Zou"
                    },
                    {
                        "name": "Yuheng Lei"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Project page: https://text-to-world.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12464v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12464v8",
                "updated": "2025-02-24T15:50:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    50,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2024-04-18T18:48:50Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    18,
                    48,
                    50,
                    3,
                    109,
                    0
                ],
                "title": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models"
                },
                "summary": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences."
                },
                "authors": [
                    {
                        "name": "Abhinav Rao"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Vishwa Shah"
                    },
                    {
                        "name": "Katharina Reinecke"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12464v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12464v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17262v1",
                "updated": "2025-02-24T15:44:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    44,
                    57,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:44:57Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    44,
                    57,
                    0,
                    55,
                    0
                ],
                "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective"
                },
                "summary": "The rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream task\nperformance prior to model training is crucial for efficient resource\nallocation, yet remains challenging due to two primary constraints: (1) the\n\"emergence phenomenon\", wherein downstream performance metrics become\nmeaningful only after extensive training, which limits the ability to use\nsmaller models for prediction; (2) Uneven task difficulty distributions and the\nabsence of consistent scaling laws, resulting in substantial metric\nvariability. Existing performance prediction methods suffer from limited\naccuracy and reliability, thereby impeding the assessment of potential LLM\ncapabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework. COD\nfirst constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable\nclusters. The scores on the selected subset serve as effective intermediate\npredictors of downstream performance on the full evaluation set. With\ntheoretical support, we derive a mapping function that transforms performance\nmetrics from the predictable subset to the full evaluation set, thereby\nensuring accurate extrapolation of LLM downstream performance. The proposed\nmethod has been applied to predict performance scaling for a 70B LLM, providing\nactionable insights for training resource allocation and assisting in\nmonitoring the training process. Notably, COD achieves remarkable predictive\naccuracy on the 70B LLM by leveraging an ensemble of small models,\ndemonstrating an absolute mean deviation of 1.36% across eight important LLM\nevaluation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream task\nperformance prior to model training is crucial for efficient resource\nallocation, yet remains challenging due to two primary constraints: (1) the\n\"emergence phenomenon\", wherein downstream performance metrics become\nmeaningful only after extensive training, which limits the ability to use\nsmaller models for prediction; (2) Uneven task difficulty distributions and the\nabsence of consistent scaling laws, resulting in substantial metric\nvariability. Existing performance prediction methods suffer from limited\naccuracy and reliability, thereby impeding the assessment of potential LLM\ncapabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework. COD\nfirst constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable\nclusters. The scores on the selected subset serve as effective intermediate\npredictors of downstream performance on the full evaluation set. With\ntheoretical support, we derive a mapping function that transforms performance\nmetrics from the predictable subset to the full evaluation set, thereby\nensuring accurate extrapolation of LLM downstream performance. The proposed\nmethod has been applied to predict performance scaling for a 70B LLM, providing\nactionable insights for training resource allocation and assisting in\nmonitoring the training process. Notably, COD achieves remarkable predictive\naccuracy on the 70B LLM by leveraging an ensemble of small models,\ndemonstrating an absolute mean deviation of 1.36% across eight important LLM\nevaluation benchmarks."
                },
                "authors": [
                    {
                        "name": "Chengyin Xu"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Chenggang Li"
                    }
                ],
                "author_detail": {
                    "name": "Chenggang Li"
                },
                "author": "Chenggang Li",
                "arxiv_comment": "21 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17259v1",
                "updated": "2025-02-24T15:39:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "Detecting Benchmark Contamination Through Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Benchmark Contamination Through Watermarking"
                },
                "summary": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy."
                },
                "authors": [
                    {
                        "name": "Tom Sander"
                    },
                    {
                        "name": "Pierre Fernandez"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10610v2",
                "updated": "2025-02-24T15:38:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    38,
                    45,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-13T23:32:44Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    23,
                    32,
                    44,
                    4,
                    348,
                    0
                ],
                "title": "Comparing large language models for supervised analysis of students' lab\n  notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing large language models for supervised analysis of students' lab\n  notes"
                },
                "summary": "Recent advancements in large language models (LLMs) hold significant promise\nin improving physics education research that uses machine learning. In this\nstudy, we compare the application of various models to perform large-scale\nanalysis of written text grounded in a physics education research\nclassification problem: identifying skills in students' typed lab notes through\nsentence-level labeling. Specifically, we use training data to fine-tune two\ndifferent LLMs, BERT and LLaMA, and compare the performance of these models to\nboth a traditional bag of words approach and a few-shot LLM (without\nfine-tuning).} We evaluate the models based on their resource use, performance\nmetrics, and research outcomes when identifying skills in lab notes. We find\nthat higher-resource models often, but not necessarily, perform better than\nlower-resource models. We also find that all models estimate similar trends in\nresearch outcomes, although the absolute values of the estimated measurements\nare not always within uncertainties of each other. We use the results to\ndiscuss relevant considerations for education researchers seeking to select a\nmodel type to use as a classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) hold significant promise\nin improving physics education research that uses machine learning. In this\nstudy, we compare the application of various models to perform large-scale\nanalysis of written text grounded in a physics education research\nclassification problem: identifying skills in students' typed lab notes through\nsentence-level labeling. Specifically, we use training data to fine-tune two\ndifferent LLMs, BERT and LLaMA, and compare the performance of these models to\nboth a traditional bag of words approach and a few-shot LLM (without\nfine-tuning).} We evaluate the models based on their resource use, performance\nmetrics, and research outcomes when identifying skills in lab notes. We find\nthat higher-resource models often, but not necessarily, perform better than\nlower-resource models. We also find that all models estimate similar trends in\nresearch outcomes, although the absolute values of the estimated measurements\nare not always within uncertainties of each other. We use the results to\ndiscuss relevant considerations for education researchers seeking to select a\nmodel type to use as a classifier."
                },
                "authors": [
                    {
                        "name": "Rebeckah K. Fussell"
                    },
                    {
                        "name": "Megan Flynn"
                    },
                    {
                        "name": "Anil Damle"
                    },
                    {
                        "name": "Michael F. J. Fox"
                    },
                    {
                        "name": "N. G. Holmes"
                    }
                ],
                "author_detail": {
                    "name": "N. G. Holmes"
                },
                "author": "N. G. Holmes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17255v1",
                "updated": "2025-02-24T15:35:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    35,
                    37,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:35:37Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    35,
                    37,
                    0,
                    55,
                    0
                ],
                "title": "MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image\n  Segmentation"
                },
                "summary": "Medical Hyperspectral Imaging (MHSI) offers potential for computational\npathology and precision medicine. However, existing CNN and Transformer\nstruggle to balance segmentation accuracy and speed due to high\nspatial-spectral dimensionality. In this study, we leverage Mamba's global\ncontext modeling to propose a dual-stream architecture for joint\nspatial-spectral feature extraction. To address the limitation of Mamba's\nunidirectional aggregation, we introduce a recurrent spectral sequence\nrepresentation to capture low-redundancy global spectral features. Experiments\non a public Multi-Dimensional Choledoch dataset and a private Cervical Cancer\ndataset show that our method outperforms state-of-the-art approaches in\nsegmentation accuracy while minimizing resource usage and achieving the fastest\ninference speed. Our code will be available at\nhttps://github.com/DeepMed-Lab-ECNU/MDN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Hyperspectral Imaging (MHSI) offers potential for computational\npathology and precision medicine. However, existing CNN and Transformer\nstruggle to balance segmentation accuracy and speed due to high\nspatial-spectral dimensionality. In this study, we leverage Mamba's global\ncontext modeling to propose a dual-stream architecture for joint\nspatial-spectral feature extraction. To address the limitation of Mamba's\nunidirectional aggregation, we introduce a recurrent spectral sequence\nrepresentation to capture low-redundancy global spectral features. Experiments\non a public Multi-Dimensional Choledoch dataset and a private Cervical Cancer\ndataset show that our method outperforms state-of-the-art approaches in\nsegmentation accuracy while minimizing resource usage and achieving the fastest\ninference speed. Our code will be available at\nhttps://github.com/DeepMed-Lab-ECNU/MDN."
                },
                "authors": [
                    {
                        "name": "Shijie Lin"
                    },
                    {
                        "name": "Boxiang Yun"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qingli Li"
                    },
                    {
                        "name": "Anqiang Yang"
                    },
                    {
                        "name": "Yan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Wang"
                },
                "author": "Yan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17254v1",
                "updated": "2025-02-24T15:34:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    34,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:34:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    34,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,\n  Distributional, and Semantic Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,\n  Distributional, and Semantic Objective"
                },
                "summary": "To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense."
                },
                "authors": [
                    {
                        "name": "Simon Geisler"
                    },
                    {
                        "name": "Tom Wollschlger"
                    },
                    {
                        "name": "M. H. I. Abdalla"
                    },
                    {
                        "name": "Vincent Cohen-Addad"
                    },
                    {
                        "name": "Johannes Gasteiger"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Gnnemann"
                },
                "author": "Stephan Gnnemann",
                "arxiv_comment": "30 pages, 6 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15218v2",
                "updated": "2025-02-24T15:31:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    31,
                    58,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T05:21:58Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    5,
                    21,
                    58,
                    4,
                    52,
                    0
                ],
                "title": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit"
                },
                "summary": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the\ndevelopment of speech language models (SpeechLMs) and voice-driven agentic\napplications. The toolkit standardizes speech processing tasks by framing them\nas universal sequential modeling problems, encompassing a cohesive workflow of\ndata preprocessing, pre-training, inference, and task evaluation. With\nESPnet-SpeechLM, users can easily define task templates and configure key\nsettings, enabling seamless and streamlined SpeechLM development. The toolkit\nensures flexibility, efficiency, and scalability by offering highly\nconfigurable modules for every stage of the workflow. To illustrate its\ncapabilities, we provide multiple use cases demonstrating how competitive\nSpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter\nmodel pre-trained on both text and speech tasks, across diverse benchmarks. The\ntoolkit and its recipes are fully transparent and reproducible at:\nhttps://github.com/espnet/espnet/tree/speechlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the\ndevelopment of speech language models (SpeechLMs) and voice-driven agentic\napplications. The toolkit standardizes speech processing tasks by framing them\nas universal sequential modeling problems, encompassing a cohesive workflow of\ndata preprocessing, pre-training, inference, and task evaluation. With\nESPnet-SpeechLM, users can easily define task templates and configure key\nsettings, enabling seamless and streamlined SpeechLM development. The toolkit\nensures flexibility, efficiency, and scalability by offering highly\nconfigurable modules for every stage of the workflow. To illustrate its\ncapabilities, we provide multiple use cases demonstrating how competitive\nSpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter\nmodel pre-trained on both text and speech tasks, across diverse benchmarks. The\ntoolkit and its recipes are fully transparent and reproducible at:\nhttps://github.com/espnet/espnet/tree/speechlm."
                },
                "authors": [
                    {
                        "name": "Jinchuan Tian"
                    },
                    {
                        "name": "Jiatong Shi"
                    },
                    {
                        "name": "William Chen"
                    },
                    {
                        "name": "Siddhant Arora"
                    },
                    {
                        "name": "Yoshiki Masuyama"
                    },
                    {
                        "name": "Takashi Maekaku"
                    },
                    {
                        "name": "Yihan Wu"
                    },
                    {
                        "name": "Junyi Peng"
                    },
                    {
                        "name": "Shikhar Bharadwaj"
                    },
                    {
                        "name": "Yiwen Zhao"
                    },
                    {
                        "name": "Samuele Cornell"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17248v1",
                "updated": "2025-02-24T15:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search"
                },
                "summary": "Text-to-SQL, which enables natural language interaction with databases,\nserves as a pivotal method across diverse industries. With new, more powerful\nlarge language models (LLMs) emerging every few months, fine-tuning has become\nincredibly costly, labor-intensive, and error-prone. As an alternative,\nzero-shot Text-to-SQL, which leverages the growing knowledge and reasoning\ncapabilities encoded in LLMs without task-specific fine-tuning, presents a\npromising and more challenging direction. To address this challenge, we propose\nAlpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS)\nframework to iteratively infer SQL construction actions based on partial SQL\nquery states. To enhance the framework's reasoning capabilities, we introduce\nLLM-as-Action-Model to dynamically generate SQL construction actions during the\nMCTS process, steering the search toward more promising SQL queries. Moreover,\nAlpha-SQL employs a self-supervised reward function to evaluate the quality of\ncandidate SQL queries, ensuring more accurate and efficient query generation.\nExperimental results show that Alpha-SQL achieves 69.7% execution accuracy on\nthe BIRD development set, using a 32B open-source LLM without fine-tuning.\nAlpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by\n2.5% on the BIRD development set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL, which enables natural language interaction with databases,\nserves as a pivotal method across diverse industries. With new, more powerful\nlarge language models (LLMs) emerging every few months, fine-tuning has become\nincredibly costly, labor-intensive, and error-prone. As an alternative,\nzero-shot Text-to-SQL, which leverages the growing knowledge and reasoning\ncapabilities encoded in LLMs without task-specific fine-tuning, presents a\npromising and more challenging direction. To address this challenge, we propose\nAlpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS)\nframework to iteratively infer SQL construction actions based on partial SQL\nquery states. To enhance the framework's reasoning capabilities, we introduce\nLLM-as-Action-Model to dynamically generate SQL construction actions during the\nMCTS process, steering the search toward more promising SQL queries. Moreover,\nAlpha-SQL employs a self-supervised reward function to evaluate the quality of\ncandidate SQL queries, ensuring more accurate and efficient query generation.\nExperimental results show that Alpha-SQL achieves 69.7% execution accuracy on\nthe BIRD development set, using a 32B open-source LLM without fine-tuning.\nAlpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by\n2.5% on the BIRD development set."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Yanwei Xu"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11630v2",
                "updated": "2025-02-24T15:25:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    25,
                    40,
                    0,
                    55,
                    0
                ],
                "published": "2024-06-17T15:13:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    13,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "A framework for the use of generative modelling in non-equilibrium\n  statistical mechanics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for the use of generative modelling in non-equilibrium\n  statistical mechanics"
                },
                "summary": "We discuss an approach to mathematically modelling systems made of objects\nthat are coupled together, using generative models of the dependence\nrelationships between states (or trajectories) of the things comprising such\nsystems. This broad class includes open or non-equilibrium systems and is\nespecially relevant to self-organising systems. The ensuing variational free\nenergy principle (FEP) has certain advantages over using random dynamical\nsystems explicitly, notably, by being more tractable and offering a\nparsimonious explanation of why the joint system evolves in the way that it\ndoes, based on the properties of the coupling between system components. Using\nthe FEP allows us to model the dynamics of an object as if it were a process of\nvariational inference, because variational free energy (or surprisal) is a\nLyapunov function for its dynamics. In short, we argue that using generative\nmodels to represent and track relations among subsystems leads us to a\nparticular statistical theory of interacting systems. Conversely, this theory\nenables us to construct nested models that respect the known relations among\nsubsystems. We point out that the fact that a physical object conforms to the\nFEP does not necessarily imply that this object performs inference in the\nliteral sense; rather, it is a useful explanatory fiction which replaces the\n`explicit' dynamics of the object with an `implicit' flow on free energy\ngradients -- a fiction that may or may not be entertained by the object itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss an approach to mathematically modelling systems made of objects\nthat are coupled together, using generative models of the dependence\nrelationships between states (or trajectories) of the things comprising such\nsystems. This broad class includes open or non-equilibrium systems and is\nespecially relevant to self-organising systems. The ensuing variational free\nenergy principle (FEP) has certain advantages over using random dynamical\nsystems explicitly, notably, by being more tractable and offering a\nparsimonious explanation of why the joint system evolves in the way that it\ndoes, based on the properties of the coupling between system components. Using\nthe FEP allows us to model the dynamics of an object as if it were a process of\nvariational inference, because variational free energy (or surprisal) is a\nLyapunov function for its dynamics. In short, we argue that using generative\nmodels to represent and track relations among subsystems leads us to a\nparticular statistical theory of interacting systems. Conversely, this theory\nenables us to construct nested models that respect the known relations among\nsubsystems. We point out that the fact that a physical object conforms to the\nFEP does not necessarily imply that this object performs inference in the\nliteral sense; rather, it is a useful explanatory fiction which replaces the\n`explicit' dynamics of the object with an `implicit' flow on free energy\ngradients -- a fiction that may or may not be entertained by the object itself."
                },
                "authors": [
                    {
                        "name": "Maxwell J D Ramstead"
                    },
                    {
                        "name": "Dalton A R Sakthivadivel"
                    },
                    {
                        "name": "Karl J Friston"
                    }
                ],
                "author_detail": {
                    "name": "Karl J Friston"
                },
                "author": "Karl J Friston",
                "arxiv_comment": "15+3 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09420v2",
                "updated": "2025-02-24T15:25:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    25,
                    4,
                    0,
                    55,
                    0
                ],
                "published": "2024-01-17T18:59:26Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    18,
                    59,
                    26,
                    2,
                    17,
                    0
                ],
                "title": "LionHeart: A Layer-based Mapping Framework for Heterogeneous Systems\n  with Analog In-Memory Computing Tiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LionHeart: A Layer-based Mapping Framework for Heterogeneous Systems\n  with Analog In-Memory Computing Tiles"
                },
                "summary": "When arranged in a crossbar configuration, resistive memory devices can be\nused to execute Matrix-Vector Multiplications (MVMs), the most dominant\noperation of many Machine Learning (ML) algorithms, in constant time\ncomplexity. Nonetheless, when performing computations in the analog domain,\nnovel challenges are introduced in terms of arithmetic precision and\nstochasticity, due to non-ideal circuit and device behaviour. Moreover, these\nnon-idealities have a temporal dimension, resulting in a degrading application\naccuracy over time. Facing these challenges, we propose a novel framework,\nnamed LionHeart, to obtain hybrid analog-digital mappings to execute Deep\nLearning (DL) inference workloads using heterogeneous accelerators. The\naccuracy-constrained mappings derived by LionHeart showcase, across different\nConvolutional Neural Networks (CNNs) and one transformer-based network, high\naccuracy and potential for speedup. The results of the full system simulations\nhighlight run-time reductions and energy efficiency gains that exceed 6X, with\na user-defined accuracy threshold for a fully digital floating point\nimplementation. LionHeart is open-sourced here:\nhttps://github.com/IBM/lionheart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When arranged in a crossbar configuration, resistive memory devices can be\nused to execute Matrix-Vector Multiplications (MVMs), the most dominant\noperation of many Machine Learning (ML) algorithms, in constant time\ncomplexity. Nonetheless, when performing computations in the analog domain,\nnovel challenges are introduced in terms of arithmetic precision and\nstochasticity, due to non-ideal circuit and device behaviour. Moreover, these\nnon-idealities have a temporal dimension, resulting in a degrading application\naccuracy over time. Facing these challenges, we propose a novel framework,\nnamed LionHeart, to obtain hybrid analog-digital mappings to execute Deep\nLearning (DL) inference workloads using heterogeneous accelerators. The\naccuracy-constrained mappings derived by LionHeart showcase, across different\nConvolutional Neural Networks (CNNs) and one transformer-based network, high\naccuracy and potential for speedup. The results of the full system simulations\nhighlight run-time reductions and energy efficiency gains that exceed 6X, with\na user-defined accuracy threshold for a fully digital floating point\nimplementation. LionHeart is open-sourced here:\nhttps://github.com/IBM/lionheart."
                },
                "authors": [
                    {
                        "name": "Corey Lammie"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Flavio Ponzina"
                    },
                    {
                        "name": "Joshua Klein"
                    },
                    {
                        "name": "Hadjer Benmeziane"
                    },
                    {
                        "name": "Marina Zapater"
                    },
                    {
                        "name": "Irem Boybat"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Giovanni Ansaloni"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "arxiv_comment": "Accepted by IEEE Transactions on Emerging Topics in Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07902v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07902v3",
                "updated": "2025-02-24T15:23:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    23,
                    32,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-12T10:12:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    12,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "Conformal Distributed Remote Inference in Sensor Networks Under\n  Reliability and Communication Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Distributed Remote Inference in Sensor Networks Under\n  Reliability and Communication Constraints"
                },
                "summary": "This paper presents communication-constrained distributed conformal risk\ncontrol (CD-CRC) framework, a novel decision-making framework for sensor\nnetworks under communication constraints. Targeting multi-label classification\nproblems, such as segmentation, CD-CRC dynamically adjusts local and global\nthresholds used to identify significant labels with the goal of ensuring a\ntarget false negative rate (FNR), while adhering to communication capacity\nlimits. CD-CRC builds on online exponentiated gradient descent to estimate the\nrelative quality of the observations of different sensors, and on online\nconformal risk control (CRC) as a mechanism to control local and global\nthresholds. CD-CRC is proved to offer deterministic worst-case performance\nguarantees in terms of FNR and communication overhead, while the regret\nperformance in terms of false positive rate (FPR) is characterized as a\nfunction of the key hyperparameters. Simulation results highlight the\neffectiveness of CD-CRC, particularly in communication resource-constrained\nenvironments, making it a valuable tool for enhancing the performance and\nreliability of distributed sensor networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents communication-constrained distributed conformal risk\ncontrol (CD-CRC) framework, a novel decision-making framework for sensor\nnetworks under communication constraints. Targeting multi-label classification\nproblems, such as segmentation, CD-CRC dynamically adjusts local and global\nthresholds used to identify significant labels with the goal of ensuring a\ntarget false negative rate (FNR), while adhering to communication capacity\nlimits. CD-CRC builds on online exponentiated gradient descent to estimate the\nrelative quality of the observations of different sensors, and on online\nconformal risk control (CRC) as a mechanism to control local and global\nthresholds. CD-CRC is proved to offer deterministic worst-case performance\nguarantees in terms of FNR and communication overhead, while the regret\nperformance in terms of false positive rate (FPR) is characterized as a\nfunction of the key hyperparameters. Simulation results highlight the\neffectiveness of CD-CRC, particularly in communication resource-constrained\nenvironments, making it a valuable tool for enhancing the performance and\nreliability of distributed sensor networks."
                },
                "authors": [
                    {
                        "name": "Meiyi Zhu"
                    },
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Chunyan Feng"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "15 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07902v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07902v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17239v1",
                "updated": "2025-02-24T15:16:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    16,
                    34,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    16,
                    34,
                    0,
                    55,
                    0
                ],
                "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction"
                },
                "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that\nseamlessly integrates audio understanding and generation. It features a\ntext-guided aligned speech generation mechanism, enabling real-time speech\ninteraction with both comprehension and generation capabilities. Baichuan-Audio\nleverages a pre-trained ASR model, followed by multi-codebook discretization of\nspeech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that\nspeech tokens retain both semantic and acoustic information. To further enhance\nmodeling, an independent audio head is employed to process audio tokens,\neffectively capturing their unique characteristics. To mitigate the loss of\nintelligence during pre-training and preserve the original capabilities of the\nLLM, we propose a two-stage pre-training strategy that maintains language\nunderstanding while enhancing audio modeling. Following alignment, the model\nexcels in real-time speech-based conversation and exhibits outstanding\nquestion-answering capabilities, demonstrating its versatility and efficiency.\nThe proposed model demonstrates superior performance in real-time spoken\ndialogue and exhibits strong question-answering abilities. Our code, model and\ntraining data are available at https://github.com/baichuan-inc/Baichuan-Audio",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Baichuan-Audio, an end-to-end audio large language model that\nseamlessly integrates audio understanding and generation. It features a\ntext-guided aligned speech generation mechanism, enabling real-time speech\ninteraction with both comprehension and generation capabilities. Baichuan-Audio\nleverages a pre-trained ASR model, followed by multi-codebook discretization of\nspeech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that\nspeech tokens retain both semantic and acoustic information. To further enhance\nmodeling, an independent audio head is employed to process audio tokens,\neffectively capturing their unique characteristics. To mitigate the loss of\nintelligence during pre-training and preserve the original capabilities of the\nLLM, we propose a two-stage pre-training strategy that maintains language\nunderstanding while enhancing audio modeling. Following alignment, the model\nexcels in real-time speech-based conversation and exhibits outstanding\nquestion-answering capabilities, demonstrating its versatility and efficiency.\nThe proposed model demonstrates superior performance in real-time spoken\ndialogue and exhibits strong question-answering abilities. Our code, model and\ntraining data are available at https://github.com/baichuan-inc/Baichuan-Audio"
                },
                "authors": [
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yuanbo Fang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Mingrui Wang"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Zehuan Li"
                    },
                    {
                        "name": "Mingan Lin"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00913v3",
                "updated": "2025-02-24T14:50:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    50,
                    8,
                    0,
                    55,
                    0
                ],
                "published": "2024-02-01T10:58:10Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    10,
                    58,
                    10,
                    3,
                    32,
                    0
                ],
                "title": "Institutional Platform for Secure Self-Service Large Language Model\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Institutional Platform for Secure Self-Service Large Language Model\n  Exploration"
                },
                "summary": "This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery."
                },
                "authors": [
                    {
                        "name": "V. K. Cody Bumgardner"
                    },
                    {
                        "name": "Mitchell A. Klusty"
                    },
                    {
                        "name": "W. Vaiden Logan"
                    },
                    {
                        "name": "Samuel E. Armstrong"
                    },
                    {
                        "name": "Caroline N. Leach"
                    },
                    {
                        "name": "Kenneth L. Calvert"
                    },
                    {
                        "name": "Caylin Hickey"
                    },
                    {
                        "name": "Jeff Talbert"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Talbert"
                },
                "author": "Jeff Talbert",
                "arxiv_comment": "10 pages 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17216v1",
                "updated": "2025-02-24T14:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    49,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    49,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic\n  Approaches"
                },
                "summary": "Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning\nproblems formulated in natural language into a formal intermediate language.\nSubsequently, the usage of symbolic reasoners yields reliable solving thereof.\nHowever, LLMs often fail in translation due to poorly chosen intermediate\nlanguages.\n  We introduce the intermediate language problem, which is the problem of\nchoosing a suitable formal language representation for neurosymbolic\napproaches. Theoretically, we argue that its origins lie in the inability of\nLLMs to distinguish syntax from semantics and the relative independence of the\nproblem from its representation. We showcase its existence experimentally by\ncontrasting two intermediate languages, Answer Set Programming and the Python\nKnowledge Engine. In addition, we demonstrate the effects of varying degrees of\nsupplementary context information. Our results show a maximum difference in\noverall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the\nGPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA\ndataset by 21.20% and by 50.50% on the ProofWriter dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning\nproblems formulated in natural language into a formal intermediate language.\nSubsequently, the usage of symbolic reasoners yields reliable solving thereof.\nHowever, LLMs often fail in translation due to poorly chosen intermediate\nlanguages.\n  We introduce the intermediate language problem, which is the problem of\nchoosing a suitable formal language representation for neurosymbolic\napproaches. Theoretically, we argue that its origins lie in the inability of\nLLMs to distinguish syntax from semantics and the relative independence of the\nproblem from its representation. We showcase its existence experimentally by\ncontrasting two intermediate languages, Answer Set Programming and the Python\nKnowledge Engine. In addition, we demonstrate the effects of varying degrees of\nsupplementary context information. Our results show a maximum difference in\noverall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the\nGPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA\ndataset by 21.20% and by 50.50% on the ProofWriter dataset."
                },
                "authors": [
                    {
                        "name": "Alexander Beiser"
                    },
                    {
                        "name": "David Penz"
                    }
                ],
                "author_detail": {
                    "name": "David Penz"
                },
                "author": "David Penz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17214v1",
                "updated": "2025-02-24T14:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    48,
                    6,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    48,
                    6,
                    0,
                    55,
                    0
                ],
                "title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with\n  Chain-of-Thought"
                },
                "summary": "Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on LLaMA\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on LLaMA\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ."
                },
                "authors": [
                    {
                        "name": "Boxuan Zhang"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15411v2",
                "updated": "2025-02-24T14:45:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    45,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T12:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    19,
                    8,
                    4,
                    52,
                    0
                ],
                "title": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings"
                },
                "summary": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts."
                },
                "authors": [
                    {
                        "name": "Rasmus Aavang"
                    },
                    {
                        "name": "Giovanni Rizzi"
                    },
                    {
                        "name": "Rasmus Bggild"
                    },
                    {
                        "name": "Alexandre Iolov"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17204v1",
                "updated": "2025-02-24T14:39:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    39,
                    28,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:39:28Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    39,
                    28,
                    0,
                    55,
                    0
                ],
                "title": "Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following"
                },
                "summary": "Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF."
                },
                "authors": [
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Qingyu Ren"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03035v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03035v4",
                "updated": "2025-02-24T14:34:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    34,
                    37,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-06T14:23:02Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning"
                },
                "summary": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhongwei Xie"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03035v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03035v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15485v2",
                "updated": "2025-02-24T14:30:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    30,
                    32,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T14:18:18Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    18,
                    18,
                    4,
                    52,
                    0
                ],
                "title": "Enhancing RWKV-based Language Models for Long-Sequence Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing RWKV-based Language Models for Long-Sequence Text Generation"
                },
                "summary": "This paper introduces an enhanced RWKV architecture with adaptive temporal\ngating mechanisms for improved long-context language modeling. We propose two\nprincipal innovations: (1) a position-aware convolutional shift operator that\ncaptures local syntactic patterns while preserving global coherence, and (2) a\nneurally-gated information routing mechanism that dynamically regulates\ninter-token information flow. Through comprehensive experiments on text\ngeneration tasks, our enhanced model demonstrates superior performance compared\nto the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores\nwith only 2.95 increased inference latency. Ablation studies validate the\nindividual contributions of each component, while linguistic analysis reveals\nthe model's adaptive attention to syntactic boundaries and entity coherence.\nThe proposed modifications maintain RWKV's linear computational complexity\nwhile significantly enhancing its contextual modeling capabilities,\nestablishing new state-of-the-art performance for recurrent-style architectures\nin long-form text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an enhanced RWKV architecture with adaptive temporal\ngating mechanisms for improved long-context language modeling. We propose two\nprincipal innovations: (1) a position-aware convolutional shift operator that\ncaptures local syntactic patterns while preserving global coherence, and (2) a\nneurally-gated information routing mechanism that dynamically regulates\ninter-token information flow. Through comprehensive experiments on text\ngeneration tasks, our enhanced model demonstrates superior performance compared\nto the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores\nwith only 2.95 increased inference latency. Ablation studies validate the\nindividual contributions of each component, while linguistic analysis reveals\nthe model's adaptive attention to syntactic boundaries and entity coherence.\nThe proposed modifications maintain RWKV's linear computational complexity\nwhile significantly enhancing its contextual modeling capabilities,\nestablishing new state-of-the-art performance for recurrent-style architectures\nin long-form text generation."
                },
                "authors": [
                    {
                        "name": "Xinghan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xinghan Pan"
                },
                "author": "Xinghan Pan",
                "arxiv_comment": "8 pages, 2 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17189v1",
                "updated": "2025-02-24T14:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    24,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    24,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "IGDA: Interactive Graph Discovery through Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGDA: Interactive Graph Discovery through Large Language Model Agents"
                },
                "summary": "Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for\ndiscovery. Instead of utilizing numerical data, LLMs utilize associated\nvariable $\\textit{semantic metadata}$ to predict variable relationships.\nSimultaneously, LLMs demonstrate impressive abilities to act as black-box\noptimizers when given an objective $f$ and sequence of trials. We study LLMs at\nthe intersection of these two capabilities by applying LLMs to the task of\n$\\textit{interactive graph discovery}$: given a ground truth graph $G^*$\ncapturing variable relationships and a budget of $I$ edge experiments over $R$\nrounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$\nat the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$,\na LLM-based pipeline incorporating two key components: 1) an LLM\nuncertainty-driven method for edge experiment selection 2) a local graph update\nstrategy utilizing binary feedback from experiments to improve predictions for\nunselected neighboring edges. Experiments on eight different real-world graphs\nshow our approach often outperforms all baselines including a state-of-the-art\nnumerical method for interactive graph discovery. Further, we conduct a\nrigorous series of ablations dissecting the impact of each pipeline component.\nFinally, to assess the impact of memorization, we apply our interactive graph\ndiscovery strategy to a complex, new (as of July 2024) causal graph on protein\ntranscription factors, finding strong performance in a setting where\nmemorization is impossible. Overall, our results show IGDA to be a powerful\nmethod for graph discovery complementary to existing numerically driven\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for\ndiscovery. Instead of utilizing numerical data, LLMs utilize associated\nvariable $\\textit{semantic metadata}$ to predict variable relationships.\nSimultaneously, LLMs demonstrate impressive abilities to act as black-box\noptimizers when given an objective $f$ and sequence of trials. We study LLMs at\nthe intersection of these two capabilities by applying LLMs to the task of\n$\\textit{interactive graph discovery}$: given a ground truth graph $G^*$\ncapturing variable relationships and a budget of $I$ edge experiments over $R$\nrounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$\nat the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$,\na LLM-based pipeline incorporating two key components: 1) an LLM\nuncertainty-driven method for edge experiment selection 2) a local graph update\nstrategy utilizing binary feedback from experiments to improve predictions for\nunselected neighboring edges. Experiments on eight different real-world graphs\nshow our approach often outperforms all baselines including a state-of-the-art\nnumerical method for interactive graph discovery. Further, we conduct a\nrigorous series of ablations dissecting the impact of each pipeline component.\nFinally, to assess the impact of memorization, we apply our interactive graph\ndiscovery strategy to a complex, new (as of July 2024) causal graph on protein\ntranscription factors, finding strong performance in a setting where\nmemorization is impossible. Overall, our results show IGDA to be a powerful\nmethod for graph discovery complementary to existing numerically driven\napproaches."
                },
                "authors": [
                    {
                        "name": "Alex Havrilla"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Nicolo Fusi"
                    }
                ],
                "author_detail": {
                    "name": "Nicolo Fusi"
                },
                "author": "Nicolo Fusi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17187v1",
                "updated": "2025-02-24T14:23:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    23,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    23,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks"
                },
                "summary": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers\nhave gained significant attention. Currently, state-of-the-art LLMs utilize\nthis architecture. There is a substantial amount of research on how to train\nsuch models and how to select hyperparameters for this architecture. However,\nthere is a lack of studies focusing on post-evaluation analysis of MoE layer\nproperties. In this paper, we take a first step toward closing this gap by\nevaluating expert contributions on the quiz-based MMLU benchmark. We show that\nmost experts were never activated during inference on this benchmark.\nAdditionally, the output distribution of gating networks is much closer to\nuniform than sparse. Finally, we demonstrate that the average performance of\nsome experts within the same layer varies significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers\nhave gained significant attention. Currently, state-of-the-art LLMs utilize\nthis architecture. There is a substantial amount of research on how to train\nsuch models and how to select hyperparameters for this architecture. However,\nthere is a lack of studies focusing on post-evaluation analysis of MoE layer\nproperties. In this paper, we take a first step toward closing this gap by\nevaluating expert contributions on the quiz-based MMLU benchmark. We show that\nmost experts were never activated during inference on this benchmark.\nAdditionally, the output distribution of gating networks is much closer to\nuniform than sparse. Finally, we demonstrate that the average performance of\nsome experts within the same layer varies significantly."
                },
                "authors": [
                    {
                        "name": "Andrei Chernov"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Chernov"
                },
                "author": "Andrei Chernov",
                "arxiv_comment": "preprint, short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17173v1",
                "updated": "2025-02-24T14:09:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    9,
                    45,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:09:45Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    9,
                    45,
                    0,
                    55,
                    0
                ],
                "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch"
                },
                "summary": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development."
                },
                "authors": [
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xing Yu"
                    },
                    {
                        "name": "Yuqiu Ji"
                    },
                    {
                        "name": "Guohai Xu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Debing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Debing Zhang"
                },
                "author": "Debing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08924v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08924v2",
                "updated": "2025-02-24T14:08:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    8,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-13T15:39:29Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    39,
                    29,
                    4,
                    257,
                    0
                ],
                "title": "Regression-based proximal causal inference for right-censored\n  time-to-event data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression-based proximal causal inference for right-censored\n  time-to-event data"
                },
                "summary": "Unmeasured confounding is one of the major concerns in causal inference from\nobservational data. Proximal causal inference (PCI) is an emerging\nmethodological framework to detect and potentially account for confounding bias\nby carefully leveraging a pair of negative control exposure (NCE) and outcome\n(NCO) variables, also known as treatment and outcome confounding proxies.\nAlthough regression-based PCI is well developed for binary and continuous\noutcomes, analogous PCI regression methods for right-censored time-to-event\noutcomes are currently lacking. In this paper, we propose a novel two-stage\nregression PCI approach for right-censored survival data under an additive\nhazard structural model. We provide theoretical justification for the proposed\napproach tailored to different types of NCOs, including continuous, count, and\nright-censored time-to-event variables. We illustrate the approach with an\nevaluation of the effectiveness of right heart catheterization among critically\nill patients using data from the SUPPORT study. Our method is implemented in\nthe open-access R package 'pci2s'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmeasured confounding is one of the major concerns in causal inference from\nobservational data. Proximal causal inference (PCI) is an emerging\nmethodological framework to detect and potentially account for confounding bias\nby carefully leveraging a pair of negative control exposure (NCE) and outcome\n(NCO) variables, also known as treatment and outcome confounding proxies.\nAlthough regression-based PCI is well developed for binary and continuous\noutcomes, analogous PCI regression methods for right-censored time-to-event\noutcomes are currently lacking. In this paper, we propose a novel two-stage\nregression PCI approach for right-censored survival data under an additive\nhazard structural model. We provide theoretical justification for the proposed\napproach tailored to different types of NCOs, including continuous, count, and\nright-censored time-to-event variables. We illustrate the approach with an\nevaluation of the effectiveness of right heart catheterization among critically\nill patients using data from the SUPPORT study. Our method is implemented in\nthe open-access R package 'pci2s'."
                },
                "authors": [
                    {
                        "name": "Kendrick Li"
                    },
                    {
                        "name": "George C. Linderman"
                    },
                    {
                        "name": "Xu Shi"
                    },
                    {
                        "name": "Eric J. Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J. Tchetgen Tchetgen"
                },
                "author": "Eric J. Tchetgen Tchetgen",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08924v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17172v1",
                "updated": "2025-02-24T14:07:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    7,
                    53,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:07:53Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    7,
                    53,
                    0,
                    55,
                    0
                ],
                "title": "Teleology-Driven Affective Computing: A Causal Framework for Sustained\n  Well-Being",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleology-Driven Affective Computing: A Causal Framework for Sustained\n  Well-Being"
                },
                "summary": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being."
                },
                "authors": [
                    {
                        "name": "Bin Yin"
                    },
                    {
                        "name": "Chong-Yi Liu"
                    },
                    {
                        "name": "Liya Fu"
                    },
                    {
                        "name": "Jinkun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinkun Zhang"
                },
                "author": "Jinkun Zhang",
                "arxiv_comment": "24 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1.2, J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1.2; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20777v3",
                "updated": "2025-02-24T14:06:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    6,
                    41,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-28T08:41:30Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    8,
                    41,
                    30,
                    1,
                    149,
                    0
                ],
                "title": "Black-Box Detection of Language Model Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box Detection of Language Model Watermarks"
                },
                "summary": "Watermarking has emerged as a promising way to detect LLM-generated text, by\naugmenting LLM generations with later detectable signals. Recent work has\nproposed multiple families of watermarking schemes, several of which focus on\npreserving the LLM distribution. This distribution-preservation property is\nmotivated by the fact that it is a tractable proxy for retaining LLM\ncapabilities, as well as the inherently implied undetectability of the\nwatermark by downstream users. Yet, despite much discourse around\nundetectability, no prior work has investigated the practical detectability of\nany of the current watermarking schemes in a realistic black-box setting. In\nthis work we tackle this for the first time, developing rigorous statistical\ntests to detect the presence, and estimate parameters, of all three popular\nwatermarking scheme families, using only a limited number of black-box queries.\nWe experimentally confirm the effectiveness of our methods on a range of\nschemes and a diverse set of open-source models. Further, we validate the\nfeasibility of our tests on real-world APIs. Our findings indicate that current\nwatermarking schemes are more detectable than previously believed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a promising way to detect LLM-generated text, by\naugmenting LLM generations with later detectable signals. Recent work has\nproposed multiple families of watermarking schemes, several of which focus on\npreserving the LLM distribution. This distribution-preservation property is\nmotivated by the fact that it is a tractable proxy for retaining LLM\ncapabilities, as well as the inherently implied undetectability of the\nwatermark by downstream users. Yet, despite much discourse around\nundetectability, no prior work has investigated the practical detectability of\nany of the current watermarking schemes in a realistic black-box setting. In\nthis work we tackle this for the first time, developing rigorous statistical\ntests to detect the presence, and estimate parameters, of all three popular\nwatermarking scheme families, using only a limited number of black-box queries.\nWe experimentally confirm the effectiveness of our methods on a range of\nschemes and a diverse set of open-source models. Further, we validate the\nfeasibility of our tests on real-world APIs. Our findings indicate that current\nwatermarking schemes are more detectable than previously believed."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17169v1",
                "updated": "2025-02-24T14:05:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    5,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:05:47Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    5,
                    47,
                    0,
                    55,
                    0
                ],
                "title": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without\n  Easily Identifiable Unrelated Padding)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without\n  Easily Identifiable Unrelated Padding)"
                },
                "summary": "Large language models demonstrate promising long context processing\ncapabilities, with recent models touting context windows close to one million\ntokens. However, the evaluations supporting these claims often involve simple\nretrieval tasks or synthetic tasks padded with irrelevant text, which the\nmodels may easily detect and discard. In this work, we generate lengthy\nsimplified English text with first-order logic representations spanning up to\n2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with\nevidence retrieval for contradiction detection. The long, homogeneous text is\nfilled with distractors that are both hard to distinguish from relevant\nevidences and provably not interfering with them. Our evaluation of evidence\nretrieval shows that the effective context window is much smaller with\nrealistic distractors, already crumbling at 128 clauses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrate promising long context processing\ncapabilities, with recent models touting context windows close to one million\ntokens. However, the evaluations supporting these claims often involve simple\nretrieval tasks or synthetic tasks padded with irrelevant text, which the\nmodels may easily detect and discard. In this work, we generate lengthy\nsimplified English text with first-order logic representations spanning up to\n2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with\nevidence retrieval for contradiction detection. The long, homogeneous text is\nfilled with distractors that are both hard to distinguish from relevant\nevidences and provably not interfering with them. Our evaluation of evidence\nretrieval shows that the effective context window is much smaller with\nrealistic distractors, already crumbling at 128 clauses."
                },
                "authors": [
                    {
                        "name": "Damien Sileo"
                    }
                ],
                "author_detail": {
                    "name": "Damien Sileo"
                },
                "author": "Damien Sileo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17166v1",
                "updated": "2025-02-24T14:02:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    2,
                    0,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    2,
                    0,
                    0,
                    55,
                    0
                ],
                "title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning"
                },
                "summary": "The Four-Element Theory is a fundamental framework in criminal law, defining\nthe constitution of crime through four dimensions: Subject, Object, Subjective\naspect, and Objective aspect. This theory is widely referenced in legal\nreasoning, and many Large Language Models (LLMs) attempt to incorporate it when\nhandling legal tasks. However, current approaches rely on LLMs' internal\nknowledge to incorporate this theory, often lacking completeness and\nrepresentativeness. To address this limitation, we introduce JUREX-4E, an\nexpert-annotated knowledge base covering 155 criminal charges. It is structured\nthrough a progressive hierarchical annotation framework that prioritizes legal\nsource validity and employs diverse legal interpretation methods to ensure\ncomprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge\nDistinction task and apply it to Legal Case Retrieval, demonstrating its\neffectiveness in improving LLM performance. Experimental results validate the\nhigh quality of JUREX-4E and its substantial impact on downstream legal tasks,\nunderscoring its potential for advancing legal AI applications. Code:\nhttps://github.com/THUlawtech/JUREX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Four-Element Theory is a fundamental framework in criminal law, defining\nthe constitution of crime through four dimensions: Subject, Object, Subjective\naspect, and Objective aspect. This theory is widely referenced in legal\nreasoning, and many Large Language Models (LLMs) attempt to incorporate it when\nhandling legal tasks. However, current approaches rely on LLMs' internal\nknowledge to incorporate this theory, often lacking completeness and\nrepresentativeness. To address this limitation, we introduce JUREX-4E, an\nexpert-annotated knowledge base covering 155 criminal charges. It is structured\nthrough a progressive hierarchical annotation framework that prioritizes legal\nsource validity and employs diverse legal interpretation methods to ensure\ncomprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge\nDistinction task and apply it to Legal Case Retrieval, demonstrating its\neffectiveness in improving LLM performance. Experimental results validate the\nhigh quality of JUREX-4E and its substantial impact on downstream legal tasks,\nunderscoring its potential for advancing legal AI applications. Code:\nhttps://github.com/THUlawtech/JUREX"
                },
                "authors": [
                    {
                        "name": "Huanghai Liu"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Qingjing Chen"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiayu Ma"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Weixing Shen"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17163v1",
                "updated": "2025-02-24T13:58:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    58,
                    42,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    58,
                    42,
                    0,
                    55,
                    0
                ],
                "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation"
                },
                "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nrelease our benchmark to support the community developing accurate evaluation\nmethods for multilingual RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nrelease our benchmark to support the community developing accurate evaluation\nmethods for multilingual RAG systems."
                },
                "authors": [
                    {
                        "name": "Mara Andrea Cruz Blandn"
                    },
                    {
                        "name": "Jayasimha Talur"
                    },
                    {
                        "name": "Bruno Charron"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Marcello Federico"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Federico"
                },
                "author": "Marcello Federico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17161v1",
                "updated": "2025-02-24T13:56:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    56,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:56:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    56,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Real-time Monitoring of Economic Shocks using Company Websites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Monitoring of Economic Shocks using Company Websites"
                },
                "summary": "Understanding the effects of economic shocks on firms is critical for\nanalyzing economic growth and resilience. We introduce a Web-Based Affectedness\nIndicator (WAI), a general-purpose tool for real-time monitoring of economic\ndisruptions across diverse contexts. By leveraging Large Language Model (LLM)\nassisted classification and information extraction on texts from over five\nmillion company websites, WAI quantifies the degree and nature of firms'\nresponses to external shocks. Using the COVID-19 pandemic as a specific\napplication, we show that WAI is highly correlated with pandemic containment\nmeasures and reliably predicts firm performance. Unlike traditional data\nsources, WAI provides timely firm-level information across industries and\ngeographies worldwide that would otherwise be unavailable due to institutional\nand data availability constraints. This methodology offers significant\npotential for monitoring and mitigating the impact of technological, political,\nfinancial, health or environmental crises, and represents a transformative tool\nfor adaptive policy-making and economic resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the effects of economic shocks on firms is critical for\nanalyzing economic growth and resilience. We introduce a Web-Based Affectedness\nIndicator (WAI), a general-purpose tool for real-time monitoring of economic\ndisruptions across diverse contexts. By leveraging Large Language Model (LLM)\nassisted classification and information extraction on texts from over five\nmillion company websites, WAI quantifies the degree and nature of firms'\nresponses to external shocks. Using the COVID-19 pandemic as a specific\napplication, we show that WAI is highly correlated with pandemic containment\nmeasures and reliably predicts firm performance. Unlike traditional data\nsources, WAI provides timely firm-level information across industries and\ngeographies worldwide that would otherwise be unavailable due to institutional\nand data availability constraints. This methodology offers significant\npotential for monitoring and mitigating the impact of technological, political,\nfinancial, health or environmental crises, and represents a transformative tool\nfor adaptive policy-making and economic resilience."
                },
                "authors": [
                    {
                        "name": "Michael Koenig"
                    },
                    {
                        "name": "Jakob Rauch"
                    },
                    {
                        "name": "Martin Woerter"
                    }
                ],
                "author_detail": {
                    "name": "Martin Woerter"
                },
                "author": "Martin Woerter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17152v1",
                "updated": "2025-02-24T13:45:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    45,
                    6,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:45:06Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    45,
                    6,
                    0,
                    55,
                    0
                ],
                "title": "The origin of the very-high-energy radiation along the jet of Centaurus\n  A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The origin of the very-high-energy radiation along the jet of Centaurus\n  A"
                },
                "summary": "As the closest known active galactic nucleus, Centaurus A (Cen A) provides a\nrich environment for astrophysical exploration. It has been observed across\nwavelengths from radio to gamma rays, and indications of ongoing particle\nacceleration have been found on different scales. Recent measurements of\nvery-high-energy (VHE) gamma-rays ($>240$ GeV) by the HESS observatory have\ninferred the presence of ultra-relativistic electrons along Cen A's jet, yet\nthe underlying acceleration mechanism remains uncertain. Various authors have\nproposed that jet substructures, known as knots, may serve as efficient\nparticle accelerators. In this study, we investigate the hypothesis that knots\nare the particle acceleration sites along Cen A's jets. We focus on stationary\nknots, and assume that they result from interactions between the jet and the\nstellar winds of powerful stars. By combining relativistic hydrodynamic\nsimulations and shock acceleration theory with the radio and X-ray data, we\ncompare theoretical predictions with morphological and spectral data from\ndifferent knots. We estimate the maximum electron energy and the resulting VHE\ngamma-ray emission. Our findings suggest that electrons accelerated at the\nknots are responsible for the gamma-ray spectrum detected in the VHE band.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the closest known active galactic nucleus, Centaurus A (Cen A) provides a\nrich environment for astrophysical exploration. It has been observed across\nwavelengths from radio to gamma rays, and indications of ongoing particle\nacceleration have been found on different scales. Recent measurements of\nvery-high-energy (VHE) gamma-rays ($>240$ GeV) by the HESS observatory have\ninferred the presence of ultra-relativistic electrons along Cen A's jet, yet\nthe underlying acceleration mechanism remains uncertain. Various authors have\nproposed that jet substructures, known as knots, may serve as efficient\nparticle accelerators. In this study, we investigate the hypothesis that knots\nare the particle acceleration sites along Cen A's jets. We focus on stationary\nknots, and assume that they result from interactions between the jet and the\nstellar winds of powerful stars. By combining relativistic hydrodynamic\nsimulations and shock acceleration theory with the radio and X-ray data, we\ncompare theoretical predictions with morphological and spectral data from\ndifferent knots. We estimate the maximum electron energy and the resulting VHE\ngamma-ray emission. Our findings suggest that electrons accelerated at the\nknots are responsible for the gamma-ray spectrum detected in the VHE band."
                },
                "authors": [
                    {
                        "name": "Cain de Oliveira"
                    },
                    {
                        "name": "James H. Matthews"
                    },
                    {
                        "name": "Vitor de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Vitor de Souza"
                },
                "author": "Vitor de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08090v2",
                "updated": "2025-02-24T13:44:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    44,
                    37,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-11T04:16:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    16,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages"
                },
                "summary": "The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON,\na temporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages -- Romanian, German, and French, encompassing three temporal tasks\nand including a diverse set of four contemporaneous LLMs. This marks a\nsignificant step forward in addressing resource disparity in the context of\ntemporal reasoning across languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON,\na temporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages -- Romanian, German, and French, encompassing three temporal tasks\nand including a diverse set of four contemporaneous LLMs. This marks a\nsignificant step forward in addressing resource disparity in the context of\ntemporal reasoning across languages."
                },
                "authors": [
                    {
                        "name": "Ashutosh Bajpai"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20238v2",
                "updated": "2025-02-24T13:42:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    42,
                    28,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-26T17:48:20Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    17,
                    48,
                    20,
                    5,
                    300,
                    0
                ],
                "title": "A Survey of Large Language Models for Arabic Language and its Dialects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for Arabic Language and its Dialects"
                },
                "summary": "This survey offers a comprehensive overview of Large Language Models (LLMs)\ndesigned for Arabic language and its dialects. It covers key architectures,\nincluding encoder-only, decoder-only, and encoder-decoder models, along with\nthe datasets used for pre-training, spanning Classical Arabic, Modern Standard\nArabic, and Dialectal Arabic. The study also explores monolingual, bilingual,\nand multilingual LLMs, analyzing their architectures and performance across\ndownstream tasks, such as sentiment analysis, named entity recognition, and\nquestion answering. Furthermore, it assesses the openness of Arabic LLMs based\non factors, such as source code availability, training data, model weights, and\ndocumentation. The survey highlights the need for more diverse dialectal\ndatasets and attributes the importance of openness for research reproducibility\nand transparency. It concludes by identifying key challenges and opportunities\nfor future research and stressing the need for more inclusive and\nrepresentative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey offers a comprehensive overview of Large Language Models (LLMs)\ndesigned for Arabic language and its dialects. It covers key architectures,\nincluding encoder-only, decoder-only, and encoder-decoder models, along with\nthe datasets used for pre-training, spanning Classical Arabic, Modern Standard\nArabic, and Dialectal Arabic. The study also explores monolingual, bilingual,\nand multilingual LLMs, analyzing their architectures and performance across\ndownstream tasks, such as sentiment analysis, named entity recognition, and\nquestion answering. Furthermore, it assesses the openness of Arabic LLMs based\non factors, such as source code availability, training data, model weights, and\ndocumentation. The survey highlights the need for more diverse dialectal\ndatasets and attributes the importance of openness for research reproducibility\nand transparency. It concludes by identifying key challenges and opportunities\nfor future research and stressing the need for more inclusive and\nrepresentative models."
                },
                "authors": [
                    {
                        "name": "Malak Mashaabi"
                    },
                    {
                        "name": "Shahad Al-Khalifa"
                    },
                    {
                        "name": "Hend Al-Khalifa"
                    }
                ],
                "author_detail": {
                    "name": "Hend Al-Khalifa"
                },
                "author": "Hend Al-Khalifa",
                "arxiv_comment": "Submitted to ACM Transactions on Asian and Low-Resource Language\n  Information Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18596v2",
                "updated": "2025-02-24T13:35:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-30T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    55,
                    3,
                    30,
                    0
                ],
                "title": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights"
                },
                "summary": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical."
                },
                "authors": [
                    {
                        "name": "Liana Mikaelyan"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mathew Salvaris"
                    },
                    {
                        "name": "Parth Pathak"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Fayyaz"
                },
                "author": "Mohsen Fayyaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13708v2",
                "updated": "2025-02-24T13:31:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    31,
                    8,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-17T16:08:06Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    8,
                    6,
                    3,
                    291,
                    0
                ],
                "title": "On the Role of Attention Heads in Large Language Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Role of Attention Heads in Large Language Model Safety"
                },
                "summary": "Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models."
                },
                "authors": [
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "28 pages, 18 figures, 7 tables. This paper has been accepted as ICLR\n  2025 (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17136v1",
                "updated": "2025-02-24T13:27:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    27,
                    46,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:27:46Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    27,
                    46,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating the Effectiveness of Large Language Models in Automated News\n  Article Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of Large Language Models in Automated News\n  Article Summarization"
                },
                "summary": "The automation of news analysis and summarization presents a promising\nsolution to the challenge of processing and analyzing vast amounts of\ninformation prevalent in today's information society. Large Language Models\n(LLMs) have demonstrated the capability to transform vast amounts of textual\ndata into concise and easily comprehensible summaries, offering an effective\nsolution to the problem of information overload and providing users with a\nquick overview of relevant information. A particularly significant application\nof this technology lies in supply chain risk analysis. Companies must monitor\nthe news about their suppliers and respond to incidents for several critical\nreasons, including compliance with laws and regulations, risk management, and\nmaintaining supply chain resilience. This paper develops an automated news\nsummarization system for supply chain risk analysis using LLMs. The proposed\nsolution aggregates news from various sources, summarizes them using LLMs, and\npresents the condensed information to users in a clear and concise format. This\napproach enables companies to optimize their information processing and make\ninformed decisions. Our study addresses two main research questions: (1) Are\nLLMs effective in automating news summarization, particularly in the context of\nsupply chain risk analysis? (2) How effective are various LLMs in terms of\nreadability, duplicate detection, and risk identification in their\nsummarization quality? In this paper, we conducted an offline study using a\nrange of publicly available LLMs at the time and complemented it with a user\nstudy focused on the top performing systems of the offline experiments to\nevaluate their effectiveness further. Our results demonstrate that LLMs,\nparticularly Few-Shot GPT-4o mini, offer significant improvements in summary\nquality and risk identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of news analysis and summarization presents a promising\nsolution to the challenge of processing and analyzing vast amounts of\ninformation prevalent in today's information society. Large Language Models\n(LLMs) have demonstrated the capability to transform vast amounts of textual\ndata into concise and easily comprehensible summaries, offering an effective\nsolution to the problem of information overload and providing users with a\nquick overview of relevant information. A particularly significant application\nof this technology lies in supply chain risk analysis. Companies must monitor\nthe news about their suppliers and respond to incidents for several critical\nreasons, including compliance with laws and regulations, risk management, and\nmaintaining supply chain resilience. This paper develops an automated news\nsummarization system for supply chain risk analysis using LLMs. The proposed\nsolution aggregates news from various sources, summarizes them using LLMs, and\npresents the condensed information to users in a clear and concise format. This\napproach enables companies to optimize their information processing and make\ninformed decisions. Our study addresses two main research questions: (1) Are\nLLMs effective in automating news summarization, particularly in the context of\nsupply chain risk analysis? (2) How effective are various LLMs in terms of\nreadability, duplicate detection, and risk identification in their\nsummarization quality? In this paper, we conducted an offline study using a\nrange of publicly available LLMs at the time and complemented it with a user\nstudy focused on the top performing systems of the offline experiments to\nevaluate their effectiveness further. Our results demonstrate that LLMs,\nparticularly Few-Shot GPT-4o mini, offer significant improvements in summary\nquality and risk identification."
                },
                "authors": [
                    {
                        "name": "Lionel Richy Panlap Houamegni"
                    },
                    {
                        "name": "Fatih Gedikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Gedikli"
                },
                "author": "Fatih Gedikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13929v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13929v5",
                "updated": "2025-02-24T13:24:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    24,
                    20,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-22T18:58:58Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    18,
                    58,
                    58,
                    2,
                    143,
                    0
                ],
                "title": "Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian"
                },
                "summary": "There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available."
                },
                "authors": [
                    {
                        "name": "Aleksandr Nikolich"
                    },
                    {
                        "name": "Konstantin Korolev"
                    },
                    {
                        "name": "Sergei Bratchikov"
                    },
                    {
                        "name": "Igor Kiselev"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "arxiv_comment": "Accepted at WMRL @ EMNLP-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13929v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13929v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17132v1",
                "updated": "2025-02-24T13:21:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    21,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:21:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    21,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "Applications of Large Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of Large Models in Medicine"
                },
                "summary": "This paper explores the advancements and applications of large-scale models\nin the medical field, with a particular focus on Medical Large Models (MedLMs).\nThese models, encompassing Large Language Models (LLMs), Vision Models, 3D\nLarge Models, and Multimodal Models, are revolutionizing healthcare by\nenhancing disease prediction, diagnostic assistance, personalized treatment\nplanning, and drug discovery. The integration of graph neural networks in\nmedical knowledge graphs and drug discovery highlights the potential of Large\nGraph Models (LGMs) in understanding complex biomedical relationships. The\nstudy also emphasizes the transformative role of Vision-Language Models (VLMs)\nand 3D Large Models in medical image analysis, anatomical modeling, and\nprosthetic design. Despite the challenges, these technologies are setting new\nbenchmarks in medical innovation, improving diagnostic accuracy, and paving the\nway for personalized healthcare solutions. This paper aims to provide a\ncomprehensive overview of the current state and future directions of large\nmodels in medicine, underscoring their significance in advancing global health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the advancements and applications of large-scale models\nin the medical field, with a particular focus on Medical Large Models (MedLMs).\nThese models, encompassing Large Language Models (LLMs), Vision Models, 3D\nLarge Models, and Multimodal Models, are revolutionizing healthcare by\nenhancing disease prediction, diagnostic assistance, personalized treatment\nplanning, and drug discovery. The integration of graph neural networks in\nmedical knowledge graphs and drug discovery highlights the potential of Large\nGraph Models (LGMs) in understanding complex biomedical relationships. The\nstudy also emphasizes the transformative role of Vision-Language Models (VLMs)\nand 3D Large Models in medical image analysis, anatomical modeling, and\nprosthetic design. Despite the challenges, these technologies are setting new\nbenchmarks in medical innovation, improving diagnostic accuracy, and paving the\nway for personalized healthcare solutions. This paper aims to provide a\ncomprehensive overview of the current state and future directions of large\nmodels in medicine, underscoring their significance in advancing global health."
                },
                "authors": [
                    {
                        "name": "YunHe Su"
                    },
                    {
                        "name": "Zhengyang Lu"
                    },
                    {
                        "name": "Junhui Liu"
                    },
                    {
                        "name": "Ke Pang"
                    },
                    {
                        "name": "Haoran Dai"
                    },
                    {
                        "name": "Sa Liu Yuxin Jia"
                    },
                    {
                        "name": "Lujia Ge"
                    },
                    {
                        "name": "Jing-min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jing-min Yang"
                },
                "author": "Jing-min Yang",
                "arxiv_doi": "10.71423/aimed.20250105",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.71423/aimed.20250105",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17129v1",
                "updated": "2025-02-24T13:19:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    19,
                    33,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:19:33Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    19,
                    33,
                    0,
                    55,
                    0
                ],
                "title": "Thus Spake Long-Context Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thus Spake Long-Context Large Language Model"
                },
                "summary": "Long context is an important topic in Natural Language Processing (NLP),\nrunning through the development of NLP architectures, and offers immense\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\ncompetitive advantage for LLMs. In the past two years, the context length of\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\nresearch on long-context LLMs has expanded from length extrapolation to a\ncomprehensive focus on architecture, infrastructure, training, and evaluation\ntechnologies.\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\nbetween the journey of extending the context of LLM and the attempts of humans\nto transcend its mortality. In this survey, We will illustrate how LLM\nstruggles between the tremendous need for a longer context and its equal need\nto accept the fact that it is ultimately finite. To achieve this, we give a\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\narchitecture, infrastructure, training, and evaluation, showcasing the full\nspectrum of long-context technologies. At the end of this survey, we will\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\nthis survey can serve as a systematic introduction to the research on\nlong-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context is an important topic in Natural Language Processing (NLP),\nrunning through the development of NLP architectures, and offers immense\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\ncompetitive advantage for LLMs. In the past two years, the context length of\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\nresearch on long-context LLMs has expanded from length extrapolation to a\ncomprehensive focus on architecture, infrastructure, training, and evaluation\ntechnologies.\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\nbetween the journey of extending the context of LLM and the attempts of humans\nto transcend its mortality. In this survey, We will illustrate how LLM\nstruggles between the tremendous need for a longer context and its equal need\nto accept the fact that it is ultimately finite. To achieve this, we give a\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\narchitecture, infrastructure, training, and evaluation, showcasing the full\nspectrum of long-context technologies. At the end of this survey, we will\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\nthis survey can serve as a systematic introduction to the research on\nlong-context LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Mianqiu Huang"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "a global picture of the lifecycle of long-context LLMs from four\n  perspectives: architecture, infrastructure, training, and evaluation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17125v1",
                "updated": "2025-02-24T13:11:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    11,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:11:47Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    11,
                    47,
                    0,
                    55,
                    0
                ],
                "title": "LettuceDetect: A Hallucination Detection Framework for RAG Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LettuceDetect: A Hallucination Detection Framework for RAG Applications"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems remain vulnerable to\nhallucinated answers despite incorporating external knowledge sources. We\npresent LettuceDetect a framework that addresses two critical limitations in\nexisting hallucination detection methods: (1) the context window constraints of\ntraditional encoder-based methods, and (2) the computational inefficiency of\nLLM based approaches. Building on ModernBERT's extended context capabilities\n(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach\noutperforms all previous encoder-based models and most prompt-based models,\nwhile being approximately 30 times smaller than the best models. LettuceDetect\nis a token-classification model that processes context-question-answer triples,\nallowing for the identification of unsupported claims at the token level.\nEvaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for\nexample-level detection, which is a 14.8% improvement over Luna, the previous\nstate-of-the-art encoder-based architecture. Additionally, the system can\nprocess 30 to 60 examples per second on a single GPU, making it more practical\nfor real-world RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems remain vulnerable to\nhallucinated answers despite incorporating external knowledge sources. We\npresent LettuceDetect a framework that addresses two critical limitations in\nexisting hallucination detection methods: (1) the context window constraints of\ntraditional encoder-based methods, and (2) the computational inefficiency of\nLLM based approaches. Building on ModernBERT's extended context capabilities\n(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach\noutperforms all previous encoder-based models and most prompt-based models,\nwhile being approximately 30 times smaller than the best models. LettuceDetect\nis a token-classification model that processes context-question-answer triples,\nallowing for the identification of unsupported claims at the token level.\nEvaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for\nexample-level detection, which is a 14.8% improvement over Luna, the previous\nstate-of-the-art encoder-based architecture. Additionally, the system can\nprocess 30 to 60 examples per second on a single GPU, making it more practical\nfor real-world RAG applications."
                },
                "authors": [
                    {
                        "name": "dm Kovcs"
                    },
                    {
                        "name": "Gbor Recski"
                    }
                ],
                "author_detail": {
                    "name": "Gbor Recski"
                },
                "author": "Gbor Recski",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09288v2",
                "updated": "2025-02-24T13:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    10,
                    5,
                    0,
                    55,
                    0
                ],
                "published": "2024-06-13T16:26:37Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    26,
                    37,
                    3,
                    165,
                    0
                ],
                "title": "Large Language Model as a Teacher for Zero-shot Tagging at Extreme\n  Scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model as a Teacher for Zero-shot Tagging at Extreme\n  Scales"
                },
                "summary": "Extreme Multi-label Text Classification (XMC) entails selecting the most\nrelevant labels for an instance from a vast label set. Extreme Zero-shot XMC\n(EZ-XMC) extends this challenge by operating without annotated data, relying\nonly on raw text instances and a predefined label set, making it particularly\ncritical for addressing cold-start problems in large-scale recommendation and\ncategorization systems. State-of-the-art methods, such as MACLR and RTS,\nleverage lightweight bi-encoders but rely on suboptimal pseudo labels for\ntraining, such as document titles (MACLR) or document segments (RTS), which may\nnot align well with the intended tagging or categorization tasks. On the other\nhand, LLM-based approaches, like ICXML, achieve better label-instance alignment\nbut are computationally expensive and impractical for real-world EZ-XMC\napplications due to their heavy inference costs. In this paper, we introduce\nLMTX (Large language Model as Teacher for eXtreme classification), a novel\nframework that bridges the gap between these two approaches. LMTX utilizes an\nLLM to identify high-quality pseudo labels during training, while employing a\nlightweight bi-encoder for efficient inference. This design eliminates the need\nfor LLMs at inference time, offering the benefits of improved label alignment\nwithout sacrificing computational efficiency. Our approach achieves superior\nperformance and efficiency over both LLM and non-LLM based approaches,\nestablishing a new state-of-the-art in EZ-XMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Multi-label Text Classification (XMC) entails selecting the most\nrelevant labels for an instance from a vast label set. Extreme Zero-shot XMC\n(EZ-XMC) extends this challenge by operating without annotated data, relying\nonly on raw text instances and a predefined label set, making it particularly\ncritical for addressing cold-start problems in large-scale recommendation and\ncategorization systems. State-of-the-art methods, such as MACLR and RTS,\nleverage lightweight bi-encoders but rely on suboptimal pseudo labels for\ntraining, such as document titles (MACLR) or document segments (RTS), which may\nnot align well with the intended tagging or categorization tasks. On the other\nhand, LLM-based approaches, like ICXML, achieve better label-instance alignment\nbut are computationally expensive and impractical for real-world EZ-XMC\napplications due to their heavy inference costs. In this paper, we introduce\nLMTX (Large language Model as Teacher for eXtreme classification), a novel\nframework that bridges the gap between these two approaches. LMTX utilizes an\nLLM to identify high-quality pseudo labels during training, while employing a\nlightweight bi-encoder for efficient inference. This design eliminates the need\nfor LLMs at inference time, offering the benefits of improved label alignment\nwithout sacrificing computational efficiency. Our approach achieves superior\nperformance and efficiency over both LLM and non-LLM based approaches,\nestablishing a new state-of-the-art in EZ-XMC."
                },
                "authors": [
                    {
                        "name": "Jinbin Zhang"
                    },
                    {
                        "name": "Nasib Ullah"
                    },
                    {
                        "name": "Rohit Babbar"
                    }
                ],
                "author_detail": {
                    "name": "Rohit Babbar"
                },
                "author": "Rohit Babbar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05847v2",
                "updated": "2025-02-24T13:03:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    3,
                    58,
                    0,
                    55,
                    0
                ],
                "published": "2024-08-11T19:06:26Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    19,
                    6,
                    26,
                    6,
                    224,
                    0
                ],
                "title": "Correcting invalid regression discontinuity designs with multiple time\n  period data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correcting invalid regression discontinuity designs with multiple time\n  period data"
                },
                "summary": "Regression Discontinuity (RD) designs rely on the continuity of potential\noutcome means at the cutoff, but this assumption often fails when other\ntreatments or policies are implemented at this cutoff. We characterize the bias\nin sharp and fuzzy RD designs due to violations of continuity, and develop a\ngeneral identification framework that leverages multiple time periods to\nestimate local effects on the (un)treated. We extend the framework to settings\nwith carry-over effects and time-varying running variables, highlighting\nadditional assumptions needed for valid causal inference. We propose an\nestimation framework that extends the conventional and bias-corrected\nsingle-period local linear regression framework to multiple periods and\ndifferent sampling schemes, and study its finite-sample performance in\nsimulations. Finally, we revisit a prior study on fiscal rules in Italy to\nillustrate the practical utility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression Discontinuity (RD) designs rely on the continuity of potential\noutcome means at the cutoff, but this assumption often fails when other\ntreatments or policies are implemented at this cutoff. We characterize the bias\nin sharp and fuzzy RD designs due to violations of continuity, and develop a\ngeneral identification framework that leverages multiple time periods to\nestimate local effects on the (un)treated. We extend the framework to settings\nwith carry-over effects and time-varying running variables, highlighting\nadditional assumptions needed for valid causal inference. We propose an\nestimation framework that extends the conventional and bias-corrected\nsingle-period local linear regression framework to multiple periods and\ndifferent sampling schemes, and study its finite-sample performance in\nsimulations. Finally, we revisit a prior study on fiscal rules in Italy to\nillustrate the practical utility of our approach."
                },
                "authors": [
                    {
                        "name": "Dor Leventer"
                    },
                    {
                        "name": "Daniel Nevo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Nevo"
                },
                "author": "Daniel Nevo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14583v3",
                "updated": "2025-02-24T12:49:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    49,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2023-05-23T23:45:20Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    23,
                    45,
                    20,
                    1,
                    143,
                    0
                ],
                "title": "Natural Language Decompositions of Implicit Content Enable Better Text\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Decompositions of Implicit Content Enable Better Text\n  Representations"
                },
                "summary": "When people interpret text, they rely on inferences that go beyond the\nobserved language itself. Inspired by this observation, we introduce a method\nfor the analysis of text that takes implicitly communicated content explicitly\ninto account. We use a large language model to produce sets of propositions\nthat are inferentially related to the text that has been observed, then\nvalidate the plausibility of the generated content via human judgments.\nIncorporating these explicit representations of implicit content proves useful\nin multiple problem settings that involve the human interpretation of\nutterances: assessing the similarity of arguments, making sense of a body of\nopinion data, and modeling legislative behavior. Our results suggest that\nmodeling the meanings behind observed language, rather than the literal text\nalone, is a valuable direction for NLP and particularly its applications to\nsocial science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When people interpret text, they rely on inferences that go beyond the\nobserved language itself. Inspired by this observation, we introduce a method\nfor the analysis of text that takes implicitly communicated content explicitly\ninto account. We use a large language model to produce sets of propositions\nthat are inferentially related to the text that has been observed, then\nvalidate the plausibility of the generated content via human judgments.\nIncorporating these explicit representations of implicit content proves useful\nin multiple problem settings that involve the human interpretation of\nutterances: assessing the similarity of arguments, making sense of a body of\nopinion data, and modeling legislative behavior. Our results suggest that\nmodeling the meanings behind observed language, rather than the literal text\nalone, is a valuable direction for NLP and particularly its applications to\nsocial science."
                },
                "authors": [
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Rupak Sarkar"
                    },
                    {
                        "name": "Pranav Goel"
                    },
                    {
                        "name": "Philip Resnik"
                    }
                ],
                "author_detail": {
                    "name": "Philip Resnik"
                },
                "author": "Philip Resnik",
                "arxiv_comment": "Accepted to EMNLP 2023 (Main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13711v2",
                "updated": "2025-02-24T12:47:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    47,
                    1,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-23T14:41:26Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    41,
                    26,
                    3,
                    23,
                    0
                ],
                "title": "Rapid wavefront shaping using an optical gradient acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid wavefront shaping using an optical gradient acquisition"
                },
                "summary": "Wavefront shaping systems aim to image deep into scattering tissue by\nreshaping incoming and outgoing light to correct aberrations caused by tissue\ninhomogeneity However, the desired modulation depends on the unknown tissue\nstructure and therefore its estimation is a challenging time-consuming task.\nMost strategies rely on coordinate descent optimization, which sequentially\nvaries each modulation parameter and assesses its impact on the resulting\nimage. We propose a rapid wavefront shaping scheme that transitions from\ncoordinate descent to gradient descent optimization, using the same measurement\nto update all modulation parameters simultaneously. To achieve this, we have\ndeveloped an analytical framework that expresses the gradient of the wavefront\nshaping score with respect to all modulation parameters. Although this gradient\ndepends on the unknown tissue structure, we demonstrate how it can be inferred\nfrom the optical system's measurements. Our new framework enables rapid\ninference of wavefront shaping modulations. Additionally, since the complexity\nof our algorithm does not scale with the number of modulation parameters, we\ncan achieve very high-resolution modulations, leading to better corrections in\nthicker tissue layers. We showcase the effectiveness of our framework in\ncorrecting aberrations in a coherent confocal microscope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wavefront shaping systems aim to image deep into scattering tissue by\nreshaping incoming and outgoing light to correct aberrations caused by tissue\ninhomogeneity However, the desired modulation depends on the unknown tissue\nstructure and therefore its estimation is a challenging time-consuming task.\nMost strategies rely on coordinate descent optimization, which sequentially\nvaries each modulation parameter and assesses its impact on the resulting\nimage. We propose a rapid wavefront shaping scheme that transitions from\ncoordinate descent to gradient descent optimization, using the same measurement\nto update all modulation parameters simultaneously. To achieve this, we have\ndeveloped an analytical framework that expresses the gradient of the wavefront\nshaping score with respect to all modulation parameters. Although this gradient\ndepends on the unknown tissue structure, we demonstrate how it can be inferred\nfrom the optical system's measurements. Our new framework enables rapid\ninference of wavefront shaping modulations. Additionally, since the complexity\nof our algorithm does not scale with the number of modulation parameters, we\ncan achieve very high-resolution modulations, leading to better corrections in\nthicker tissue layers. We showcase the effectiveness of our framework in\ncorrecting aberrations in a coherent confocal microscope."
                },
                "authors": [
                    {
                        "name": "Sagi Monin"
                    },
                    {
                        "name": "Marina Alterman"
                    },
                    {
                        "name": "Anat Levin"
                    }
                ],
                "author_detail": {
                    "name": "Anat Levin"
                },
                "author": "Anat Levin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01702v2",
                "updated": "2025-02-24T12:42:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    42,
                    14,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-03T08:55:19Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    55,
                    19,
                    4,
                    3,
                    0
                ],
                "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning"
                },
                "summary": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Zhuoma Gongque"
                    },
                    {
                        "name": "Weihao Zeng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17099v1",
                "updated": "2025-02-24T12:29:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    29,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T12:29:16Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    29,
                    16,
                    0,
                    55,
                    0
                ],
                "title": "Improved Diffusion-based Generative Model with Better Adversarial\n  Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Diffusion-based Generative Model with Better Adversarial\n  Robustness"
                },
                "summary": "Diffusion Probabilistic Models (DPMs) have achieved significant success in\ngenerative tasks. However, their training and sampling processes suffer from\nthe issue of distribution mismatch. During the denoising process, the input\ndata distributions differ between the training and inference stages,\npotentially leading to inaccurate data generation. To obviate this, we analyze\nthe training objective of DPMs and theoretically demonstrate that this mismatch\ncan be alleviated through Distributionally Robust Optimization (DRO), which is\nequivalent to performing robustness-driven Adversarial Training (AT) on DPMs.\nFurthermore, for the recently proposed Consistency Model (CM), which distills\nthe inference process of the DPM, we prove that its training objective also\nencounters the mismatch issue. Fortunately, this issue can be mitigated by AT\nas well. Based on these insights, we propose to conduct efficient AT on both\nDPM and CM. Finally, extensive empirical studies validate the effectiveness of\nAT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Probabilistic Models (DPMs) have achieved significant success in\ngenerative tasks. However, their training and sampling processes suffer from\nthe issue of distribution mismatch. During the denoising process, the input\ndata distributions differ between the training and inference stages,\npotentially leading to inaccurate data generation. To obviate this, we analyze\nthe training objective of DPMs and theoretically demonstrate that this mismatch\ncan be alleviated through Distributionally Robust Optimization (DRO), which is\nequivalent to performing robustness-driven Adversarial Training (AT) on DPMs.\nFurthermore, for the recently proposed Consistency Model (CM), which distills\nthe inference process of the DPM, we prove that its training objective also\nencounters the mismatch issue. Fortunately, this issue can be mitigated by AT\nas well. Based on these insights, we propose to conduct efficient AT on both\nDPM and CM. Finally, extensive empirical studies validate the effectiveness of\nAT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Mingyang Yi"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Zhi-Ming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Ming Ma"
                },
                "author": "Zhi-Ming Ma",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17434v1",
                "updated": "2025-02-24T18:59:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    59,
                    50,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:59:50Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    59,
                    50,
                    0,
                    55,
                    0
                ],
                "title": "V-HOP: Visuo-Haptic 6D Object Pose Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-HOP: Visuo-Haptic 6D Object Pose Tracking"
                },
                "summary": "Humans naturally integrate vision and haptics for robust object perception\nduring manipulation. The loss of either modality significantly degrades\nperformance. Inspired by this multisensory integration, prior object pose\nestimation research has attempted to combine visual and haptic/tactile\nfeedback. Although these works demonstrate improvements in controlled\nenvironments or synthetic datasets, they often underperform vision-only\napproaches in real-world settings due to poor generalization across diverse\ngrippers, sensor layouts, or sim-to-real environments. Furthermore, they\ntypically estimate the object pose for each frame independently, resulting in\nless coherent tracking over sequences in real-world deployments. To address\nthese limitations, we introduce a novel unified haptic representation that\neffectively handles multiple gripper embodiments. Building on this\nrepresentation, we introduce a new visuo-haptic transformer-based object pose\ntracker that seamlessly integrates visual and haptic input. We validate our\nframework in our dataset and the Feelsight dataset, demonstrating significant\nperformance improvement on challenging sequences. Notably, our method achieves\nsuperior generalization and robustness across novel embodiments, objects, and\nsensor types (both taxel-based and vision-based tactile sensors). In real-world\nexperiments, we demonstrate that our approach outperforms state-of-the-art\nvisual trackers by a large margin. We further show that we can achieve precise\nmanipulation tasks by incorporating our real-time object tracking result into\nmotion plans, underscoring the advantages of visuo-haptic perception. Our model\nand dataset will be made open source upon acceptance of the paper. Project\nwebsite: https://lhy.xyz/projects/v-hop/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally integrate vision and haptics for robust object perception\nduring manipulation. The loss of either modality significantly degrades\nperformance. Inspired by this multisensory integration, prior object pose\nestimation research has attempted to combine visual and haptic/tactile\nfeedback. Although these works demonstrate improvements in controlled\nenvironments or synthetic datasets, they often underperform vision-only\napproaches in real-world settings due to poor generalization across diverse\ngrippers, sensor layouts, or sim-to-real environments. Furthermore, they\ntypically estimate the object pose for each frame independently, resulting in\nless coherent tracking over sequences in real-world deployments. To address\nthese limitations, we introduce a novel unified haptic representation that\neffectively handles multiple gripper embodiments. Building on this\nrepresentation, we introduce a new visuo-haptic transformer-based object pose\ntracker that seamlessly integrates visual and haptic input. We validate our\nframework in our dataset and the Feelsight dataset, demonstrating significant\nperformance improvement on challenging sequences. Notably, our method achieves\nsuperior generalization and robustness across novel embodiments, objects, and\nsensor types (both taxel-based and vision-based tactile sensors). In real-world\nexperiments, we demonstrate that our approach outperforms state-of-the-art\nvisual trackers by a large margin. We further show that we can achieve precise\nmanipulation tasks by incorporating our real-time object tracking result into\nmotion plans, underscoring the advantages of visuo-haptic perception. Our model\nand dataset will be made open source upon acceptance of the paper. Project\nwebsite: https://lhy.xyz/projects/v-hop/"
                },
                "authors": [
                    {
                        "name": "Hongyu Li"
                    },
                    {
                        "name": "Mingxi Jia"
                    },
                    {
                        "name": "Tuluhan Akbulut"
                    },
                    {
                        "name": "Yu Xiang"
                    },
                    {
                        "name": "George Konidaris"
                    },
                    {
                        "name": "Srinath Sridhar"
                    }
                ],
                "author_detail": {
                    "name": "Srinath Sridhar"
                },
                "author": "Srinath Sridhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09662v2",
                "updated": "2025-02-24T18:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-12T22:31:01Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    22,
                    31,
                    1,
                    5,
                    286,
                    0
                ],
                "title": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers\n  in RAG for Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers\n  in RAG for Coding Tasks"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge bases, achieving state-of-the-art results in\nvarious coding tasks. The core of RAG is retrieving demonstration examples,\nwhich is essential to balance effectiveness (generation quality) and efficiency\n(retrieval time) for optimal performance. However, the high-dimensional nature\nof code representations and large knowledge bases often create efficiency\nbottlenecks, which are overlooked in previous research. This paper\nsystematically evaluates the efficiency-effectiveness trade-off of retrievers\nacross three coding tasks: Program Synthesis, Commit Message Generation, and\nAssertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)\nand four dense retrievers, including one exhaustive dense retriever (SBERT's\nSemantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).\nOur findings show that while BM25 excels in effectiveness, it suffers in\nefficiency as the knowledge base grows beyond 1000 entries. In large-scale\nretrieval, efficiency differences become more pronounced, with approximate\ndense retrievers offering the greatest gains. For instance, in Commit\nGeneration task, HNSW achieves a 44x speed up, while only with a 1.74% drop in\nRougeL compared with BM25. Our results also show that increasing the number of\ndemonstrations in the prompt doesn't always improve the effectiveness and can\nincrease latency and lead to incorrect outputs. Our findings provide valuable\ninsights for practitioners aiming to build efficient and effective RAG systems\nfor coding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge bases, achieving state-of-the-art results in\nvarious coding tasks. The core of RAG is retrieving demonstration examples,\nwhich is essential to balance effectiveness (generation quality) and efficiency\n(retrieval time) for optimal performance. However, the high-dimensional nature\nof code representations and large knowledge bases often create efficiency\nbottlenecks, which are overlooked in previous research. This paper\nsystematically evaluates the efficiency-effectiveness trade-off of retrievers\nacross three coding tasks: Program Synthesis, Commit Message Generation, and\nAssertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)\nand four dense retrievers, including one exhaustive dense retriever (SBERT's\nSemantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).\nOur findings show that while BM25 excels in effectiveness, it suffers in\nefficiency as the knowledge base grows beyond 1000 entries. In large-scale\nretrieval, efficiency differences become more pronounced, with approximate\ndense retrievers offering the greatest gains. For instance, in Commit\nGeneration task, HNSW achieves a 44x speed up, while only with a 1.74% drop in\nRougeL compared with BM25. Our results also show that increasing the number of\ndemonstrations in the prompt doesn't always improve the effectiveness and can\nincrease latency and lead to incorrect outputs. Our findings provide valuable\ninsights for practitioners aiming to build efficient and effective RAG systems\nfor coding tasks."
                },
                "authors": [
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Shaowei Wang"
                    },
                    {
                        "name": "Shaiful Chowdhury"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "arxiv_comment": "11 pages, 6 figures, 6 tables, accepted by SANER 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17424v1",
                "updated": "2025-02-24T18:56:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    56,
                    3,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:56:03Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    56,
                    3,
                    0,
                    55,
                    0
                ],
                "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs"
                },
                "summary": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork."
                },
                "authors": [
                    {
                        "name": "Jan Betley"
                    },
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "Niels Warncke"
                    },
                    {
                        "name": "Anna Sztyber-Betley"
                    },
                    {
                        "name": "Xuchan Bao"
                    },
                    {
                        "name": "Martn Soto"
                    },
                    {
                        "name": "Nathan Labenz"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17422v1",
                "updated": "2025-02-24T18:54:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    54,
                    40,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:54:40Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    54,
                    40,
                    0,
                    55,
                    0
                ],
                "title": "MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk."
                },
                "authors": [
                    {
                        "name": "Jiarui Zhang"
                    },
                    {
                        "name": "Mahyar Khayatkhoei"
                    },
                    {
                        "name": "Prateek Chhikara"
                    },
                    {
                        "name": "Filip Ilievski"
                    }
                ],
                "author_detail": {
                    "name": "Filip Ilievski"
                },
                "author": "Filip Ilievski",
                "arxiv_comment": "Published as a conference paper at ICLR 2025. Code at:\n  https://github.com/saccharomycetes/mllms_know",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17420v1",
                "updated": "2025-02-24T18:52:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    52,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:52:59Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    52,
                    59,
                    0,
                    55,
                    0
                ],
                "title": "The Geometry of Refusal in Large Language Models: Concept Cones and\n  Representational Independence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Refusal in Large Language Models: Concept Cones and\n  Representational Independence"
                },
                "summary": "The safety alignment of large language models (LLMs) can be circumvented\nthrough adversarially crafted inputs, yet the mechanisms by which these attacks\nbypass safety barriers remain poorly understood. Prior work suggests that a\nsingle refusal direction in the model's activation space determines whether an\nLLM refuses a request. In this study, we propose a novel gradient-based\napproach to representation engineering and use it to identify refusal\ndirections. Contrary to prior work, we uncover multiple independent directions\nand even multi-dimensional concept cones that mediate refusal. Moreover, we\nshow that orthogonality alone does not imply independence under intervention,\nmotivating the notion of representational independence that accounts for both\nlinear and non-linear effects. Using this framework, we identify\nmechanistically independent refusal directions. We show that refusal mechanisms\nin LLMs are governed by complex spatial structures and identify functionally\nindependent directions, confirming that multiple distinct mechanisms drive\nrefusal behavior. Our gradient-based approach uncovers these mechanisms and can\nfurther serve as a foundation for future work on understanding LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety alignment of large language models (LLMs) can be circumvented\nthrough adversarially crafted inputs, yet the mechanisms by which these attacks\nbypass safety barriers remain poorly understood. Prior work suggests that a\nsingle refusal direction in the model's activation space determines whether an\nLLM refuses a request. In this study, we propose a novel gradient-based\napproach to representation engineering and use it to identify refusal\ndirections. Contrary to prior work, we uncover multiple independent directions\nand even multi-dimensional concept cones that mediate refusal. Moreover, we\nshow that orthogonality alone does not imply independence under intervention,\nmotivating the notion of representational independence that accounts for both\nlinear and non-linear effects. Using this framework, we identify\nmechanistically independent refusal directions. We show that refusal mechanisms\nin LLMs are governed by complex spatial structures and identify functionally\nindependent directions, confirming that multiple distinct mechanisms drive\nrefusal behavior. Our gradient-based approach uncovers these mechanisms and can\nfurther serve as a foundation for future work on understanding LLMs."
                },
                "authors": [
                    {
                        "name": "Tom Wollschlger"
                    },
                    {
                        "name": "Jannes Elstner"
                    },
                    {
                        "name": "Simon Geisler"
                    },
                    {
                        "name": "Vincent Cohen-Addad"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    },
                    {
                        "name": "Johannes Gasteiger"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Gasteiger"
                },
                "author": "Johannes Gasteiger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17419v1",
                "updated": "2025-02-24T18:50:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:50:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
                },
                "summary": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field."
                },
                "authors": [
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ming-Liang Zhang"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Zengyan Liu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Pei-Jie Wang"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Fei Yin"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Cheng-Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Lin Liu"
                },
                "author": "Cheng-Lin Liu",
                "arxiv_comment": "Slow-thinking, Large Language Models, Human-like Reasoning, Decision\n  Making in AI, AGI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17410v1",
                "updated": "2025-02-24T18:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    42,
                    19,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:42:19Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    42,
                    19,
                    0,
                    55,
                    0
                ],
                "title": "COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of\n  LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains, yet their optimization remains a significant challenge due to\nthe complex and high-dimensional loss landscapes they inhabit. While adaptive\noptimizers such as AdamW are widely used, they suffer from critical\nlimitations, including an inability to capture interdependencies between\ncoordinates and high memory consumption. Subsequent research, exemplified by\nSOAP, attempts to better capture coordinate interdependence but incurs greater\nmemory overhead, limiting scalability for massive LLMs. An alternative approach\naims to reduce memory consumption through low-dimensional projection, but this\nleads to substantial approximation errors, resulting in less effective\noptimization (e.g., in terms of per-token efficiency). In this paper, we\npropose COSMOS, a novel hybrid optimizer that leverages the varying importance\nof eigensubspaces in the gradient matrix to achieve memory efficiency without\ncompromising optimization performance. The design of COSMOS is motivated by our\nempirical insights and practical considerations. Specifically, COSMOS applies\nSOAP to the leading eigensubspace, which captures the primary optimization\ndynamics, and MUON to the remaining eigensubspace, which is less critical but\ncomputationally expensive to handle with SOAP. This hybrid strategy\nsignificantly reduces memory consumption while maintaining robust optimization\nperformance, making it particularly suitable for massive LLMs. Numerical\nexperiments on various datasets and transformer architectures are provided to\ndemonstrate the effectiveness of COSMOS. Our code is available at\nhttps://github.com/lliu606/COSMOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains, yet their optimization remains a significant challenge due to\nthe complex and high-dimensional loss landscapes they inhabit. While adaptive\noptimizers such as AdamW are widely used, they suffer from critical\nlimitations, including an inability to capture interdependencies between\ncoordinates and high memory consumption. Subsequent research, exemplified by\nSOAP, attempts to better capture coordinate interdependence but incurs greater\nmemory overhead, limiting scalability for massive LLMs. An alternative approach\naims to reduce memory consumption through low-dimensional projection, but this\nleads to substantial approximation errors, resulting in less effective\noptimization (e.g., in terms of per-token efficiency). In this paper, we\npropose COSMOS, a novel hybrid optimizer that leverages the varying importance\nof eigensubspaces in the gradient matrix to achieve memory efficiency without\ncompromising optimization performance. The design of COSMOS is motivated by our\nempirical insights and practical considerations. Specifically, COSMOS applies\nSOAP to the leading eigensubspace, which captures the primary optimization\ndynamics, and MUON to the remaining eigensubspace, which is less critical but\ncomputationally expensive to handle with SOAP. This hybrid strategy\nsignificantly reduces memory consumption while maintaining robust optimization\nperformance, making it particularly suitable for massive LLMs. Numerical\nexperiments on various datasets and transformer architectures are provided to\ndemonstrate the effectiveness of COSMOS. Our code is available at\nhttps://github.com/lliu606/COSMOS."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Zhenghao Xu"
                    },
                    {
                        "name": "Zixuan Zhang"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Zichong Li"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "arxiv_comment": "23 pages, 9 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04059v2",
                "updated": "2025-02-24T18:38:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    38,
                    2,
                    0,
                    55,
                    0
                ],
                "published": "2023-12-07T05:45:24Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    5,
                    45,
                    24,
                    3,
                    341,
                    0
                ],
                "title": "Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss"
                },
                "summary": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement."
                },
                "authors": [
                    {
                        "name": "Zhuoran Huang"
                    },
                    {
                        "name": "Michael P. Berry"
                    },
                    {
                        "name": "Christina Chwyl"
                    },
                    {
                        "name": "Gary Hsieh"
                    },
                    {
                        "name": "Jing Wei"
                    },
                    {
                        "name": "Evan M. Forman"
                    }
                ],
                "author_detail": {
                    "name": "Evan M. Forman"
                },
                "author": "Evan M. Forman",
                "arxiv_doi": "10.1007/s41347-025-00491-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s41347-025-00491-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 5 figures",
                "arxiv_journal_ref": "Journal of Technology in Behavioral Science (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17407v1",
                "updated": "2025-02-24T18:36:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    36,
                    15,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:36:15Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    36,
                    15,
                    0,
                    55,
                    0
                ],
                "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning"
                },
                "summary": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17403v1",
                "updated": "2025-02-24T18:30:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    30,
                    36,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:30:36Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    30,
                    36,
                    0,
                    55,
                    0
                ],
                "title": "Large Language Models are Powerful EHR Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Powerful EHR Encoders"
                },
                "summary": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications."
                },
                "authors": [
                    {
                        "name": "Stefan Hegselmann"
                    },
                    {
                        "name": "Georg von Arnim"
                    },
                    {
                        "name": "Tillmann Rheude"
                    },
                    {
                        "name": "Noel Kronenberg"
                    },
                    {
                        "name": "David Sontag"
                    },
                    {
                        "name": "Gerhard Hindricks"
                    },
                    {
                        "name": "Roland Eils"
                    },
                    {
                        "name": "Benjamin Wild"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Wild"
                },
                "author": "Benjamin Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19381v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19381v4",
                "updated": "2025-02-24T18:28:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    28,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-28T15:12:55Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    12,
                    55,
                    5,
                    272,
                    0
                ],
                "title": "HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for\n  Enhanced LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for\n  Enhanced LLM Reasoning"
                },
                "summary": "LLMs approach logical and mathematical reasoning through natural or symbolic\nlanguages. While natural language offers human-accessible flexibility but\nsuffers from ambiguity, symbolic reasoning provides precise, machine-executable\ninferences at the cost of strict domain constraints. We introduce HYBRIDMIND,\nan adaptive strategy that selects the optimal reasoning approach for each\nreasoning problem. Through extensive experiments, we evaluate both\nprompting-based approaches with state-of-the-art LLMs and fine-tuned\nopen-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a\nmeta-selector outperforms GPT-4o's natural language reasoning by 4.4\\% on FOLIO\nand 1.3\\% on MATH. More notably, using GPT-3.5-turbo as a prompted\nmeta-selector yields a 10\\% improvement on FOLIO's challenging subset compared\nto GPT-4o. We will release our code and data to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs approach logical and mathematical reasoning through natural or symbolic\nlanguages. While natural language offers human-accessible flexibility but\nsuffers from ambiguity, symbolic reasoning provides precise, machine-executable\ninferences at the cost of strict domain constraints. We introduce HYBRIDMIND,\nan adaptive strategy that selects the optimal reasoning approach for each\nreasoning problem. Through extensive experiments, we evaluate both\nprompting-based approaches with state-of-the-art LLMs and fine-tuned\nopen-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a\nmeta-selector outperforms GPT-4o's natural language reasoning by 4.4\\% on FOLIO\nand 1.3\\% on MATH. More notably, using GPT-3.5-turbo as a prompted\nmeta-selector yields a 10\\% improvement on FOLIO's challenging subset compared\nto GPT-4o. We will release our code and data to support future research."
                },
                "authors": [
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Chuhan Li"
                    },
                    {
                        "name": "Xuyuan Xiong"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19381v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19381v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17390v1",
                "updated": "2025-02-24T18:16:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    16,
                    10,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:16:10Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    16,
                    10,
                    0,
                    55,
                    0
                ],
                "title": "Mitigating Bias in RAG: Controlling the Embedder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Bias in RAG: Controlling the Embedder"
                },
                "summary": "In retrieval augmented generation (RAG) systems, each individual component --\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\ntowards outputting certain perspectives or identities. In this work, we study\nthe conflict between biases of each component and their relationship to the\noverall bias of the RAG system, which we call bias conflict. Examining both\ngender and political biases as case studies, we show that bias conflict can be\ncharacterized through a linear relationship among components despite its\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\ncreating 120 differently biased embedders, we demonstrate how to control bias\nwhile maintaining utility and reveal the importance of reverse-biasing the\nembedder to mitigate bias in the overall system. Additionally, we find that\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\nfactor to consider for debiasing. Our results underscore that a fair RAG system\ncan be better achieved by carefully controlling the bias of the embedder rather\nthan increasing its fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In retrieval augmented generation (RAG) systems, each individual component --\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\ntowards outputting certain perspectives or identities. In this work, we study\nthe conflict between biases of each component and their relationship to the\noverall bias of the RAG system, which we call bias conflict. Examining both\ngender and political biases as case studies, we show that bias conflict can be\ncharacterized through a linear relationship among components despite its\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\ncreating 120 differently biased embedders, we demonstrate how to control bias\nwhile maintaining utility and reveal the importance of reverse-biasing the\nembedder to mitigate bias in the overall system. Additionally, we find that\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\nfactor to consider for debiasing. Our results underscore that a fair RAG system\ncan be better achieved by carefully controlling the bias of the embedder rather\nthan increasing its fairness."
                },
                "authors": [
                    {
                        "name": "Taeyoun Kim"
                    },
                    {
                        "name": "Jacob Springer"
                    },
                    {
                        "name": "Aditi Raghunathan"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "26 pages (8 main), 12 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12272v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12272v3",
                "updated": "2025-02-24T18:15:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    15,
                    2,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-17T19:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    16,
                    37,
                    0,
                    48,
                    0
                ],
                "title": "Learning to Reason at the Frontier of Learnability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason at the Frontier of Learnability"
                },
                "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12272v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12272v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17388v1",
                "updated": "2025-02-24T18:14:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    9,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:14:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "Coexistence of continuous-variable quantum key distribution and\n  classical data over 120-km fiber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coexistence of continuous-variable quantum key distribution and\n  classical data over 120-km fiber"
                },
                "summary": "Integrating quantum key distribution (QKD) with classical data transmission\nover the same fiber is crucial for scalable quantum-secured communication.\nHowever, noise from classical channels limits QKD distance. We demonstrate the\nlongest-distance continuous-variable QKD (CV-QKD) over 120 km (20 dB loss)\ncoexisting with a fully populated coarse wavelength-division multiplexing\n(CWDM) system. Natural mode filtering of the local oscillator and phase noise\nmitigation enabled this without additional filtering or wavelength\nreallocation. Benchmarking against a commercial discrete-variable QKD system\nand considering finite-size effects confirms the feasibility of CV-QKD as a\nplug-and-play solution for typical 80--100 km long-haul optical networks. Our\nresults set a record distance for CV-QKD, showing its potential for\ncost-effective, large-scale deployment in existing network infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating quantum key distribution (QKD) with classical data transmission\nover the same fiber is crucial for scalable quantum-secured communication.\nHowever, noise from classical channels limits QKD distance. We demonstrate the\nlongest-distance continuous-variable QKD (CV-QKD) over 120 km (20 dB loss)\ncoexisting with a fully populated coarse wavelength-division multiplexing\n(CWDM) system. Natural mode filtering of the local oscillator and phase noise\nmitigation enabled this without additional filtering or wavelength\nreallocation. Benchmarking against a commercial discrete-variable QKD system\nand considering finite-size effects confirms the feasibility of CV-QKD as a\nplug-and-play solution for typical 80--100 km long-haul optical networks. Our\nresults set a record distance for CV-QKD, showing its potential for\ncost-effective, large-scale deployment in existing network infrastructure."
                },
                "authors": [
                    {
                        "name": "Adnan A. E. Hajomer"
                    },
                    {
                        "name": "Ivan Derkach"
                    },
                    {
                        "name": "Vladyslav C. Usenko"
                    },
                    {
                        "name": "Ulrik L. Andersen"
                    },
                    {
                        "name": "Tobias Gehring"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Gehring"
                },
                "author": "Tobias Gehring",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17387v1",
                "updated": "2025-02-24T18:14:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    1,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:14:01Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    14,
                    1,
                    0,
                    55,
                    0
                ],
                "title": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models"
                },
                "summary": "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Nathan Lile"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Louis Castricato"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Chase Blagden"
                    },
                    {
                        "name": "Violet Xiang"
                    },
                    {
                        "name": "Dakota Mahan"
                    },
                    {
                        "name": "Nick Haber"
                    }
                ],
                "author_detail": {
                    "name": "Nick Haber"
                },
                "author": "Nick Haber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17383v1",
                "updated": "2025-02-24T18:08:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    8,
                    41,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:08:41Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    8,
                    41,
                    0,
                    55,
                    0
                ],
                "title": "What is a Good Question? Utility Estimation with LLM-based Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is a Good Question? Utility Estimation with LLM-based Simulations"
                },
                "summary": "Asking questions is a fundamental aspect of learning that facilitates deeper\nunderstanding. However, characterizing and crafting questions that effectively\nimprove learning remains elusive. To address this gap, we propose QUEST\n(Question Utility Estimation with Simulated Tests). QUEST simulates a learning\nenvironment that enables the quantification of a question's utility based on\nits direct impact on improving learning outcomes. Furthermore, we can identify\nhigh-utility questions and use them to fine-tune question generation models\nwith rejection sampling. We find that questions generated by models trained\nwith rejection sampling based on question utility result in exam scores that\nare higher by at least 20% than those from specialized prompting grounded on\neducational objectives literature and models fine-tuned with indirect measures\nof question quality, such as saliency and expected information gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asking questions is a fundamental aspect of learning that facilitates deeper\nunderstanding. However, characterizing and crafting questions that effectively\nimprove learning remains elusive. To address this gap, we propose QUEST\n(Question Utility Estimation with Simulated Tests). QUEST simulates a learning\nenvironment that enables the quantification of a question's utility based on\nits direct impact on improving learning outcomes. Furthermore, we can identify\nhigh-utility questions and use them to fine-tune question generation models\nwith rejection sampling. We find that questions generated by models trained\nwith rejection sampling based on question utility result in exam scores that\nare higher by at least 20% than those from specialized prompting grounded on\neducational objectives literature and models fine-tuned with indirect measures\nof question quality, such as saliency and expected information gain."
                },
                "authors": [
                    {
                        "name": "Dong-Ho Lee"
                    },
                    {
                        "name": "Hyundong Cho"
                    },
                    {
                        "name": "Jonathan May"
                    },
                    {
                        "name": "Jay Pujara"
                    }
                ],
                "author_detail": {
                    "name": "Jay Pujara"
                },
                "author": "Jay Pujara",
                "arxiv_comment": "18 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05315v3",
                "updated": "2025-02-24T17:53:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    53,
                    6,
                    0,
                    55,
                    0
                ],
                "published": "2024-06-08T01:27:19Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    27,
                    19,
                    5,
                    160,
                    0
                ],
                "title": "Aligned at the Start: Conceptual Groupings in LLM Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned at the Start: Conceptual Groupings in LLM Embeddings"
                },
                "summary": "This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks."
                },
                "authors": [
                    {
                        "name": "Mehrdad Khatir"
                    },
                    {
                        "name": "Sanchit Kabra"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v7",
                "updated": "2025-02-24T17:40:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    38,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "Minor typos revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17362v1",
                "updated": "2025-02-24T17:39:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    39,
                    4,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:39:04Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    39,
                    4,
                    0,
                    55,
                    0
                ],
                "title": "HATPIC: An Open-Source Single Axis Haptic Joystick for Robotic\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HATPIC: An Open-Source Single Axis Haptic Joystick for Robotic\n  Development"
                },
                "summary": "Humans process significantly more information through the sense of touch than\nthrough vision. Consequently, haptics for telemanipulation is poised to become\nessential in the coming years, as it offers operators an additional sensory\nchannel crucial for interpretation in extreme conditions. However, current\nhaptic device setups are either difficult to access or provide low-quality\nforce feedback rendering. This work proposes the design of a single-axis,\nopen-source setup for telemanipulation development, aimed at addressing these\nissues. We first introduce the haptic device and demonstrate its integration\nwith common robotic tools. The proposed joystick has the potential to\naccelerate the development and deployment of haptic technology in a wide range\nof robotics applications, enhancing operator feedback and control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans process significantly more information through the sense of touch than\nthrough vision. Consequently, haptics for telemanipulation is poised to become\nessential in the coming years, as it offers operators an additional sensory\nchannel crucial for interpretation in extreme conditions. However, current\nhaptic device setups are either difficult to access or provide low-quality\nforce feedback rendering. This work proposes the design of a single-axis,\nopen-source setup for telemanipulation development, aimed at addressing these\nissues. We first introduce the haptic device and demonstrate its integration\nwith common robotic tools. The proposed joystick has the potential to\naccelerate the development and deployment of haptic technology in a wide range\nof robotics applications, enhancing operator feedback and control."
                },
                "authors": [
                    {
                        "name": "Julien Mellet"
                    },
                    {
                        "name": "Fabio Ruggiero"
                    },
                    {
                        "name": "Vincenzo Lippiello"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Lippiello"
                },
                "author": "Vincenzo Lippiello",
                "arxiv_comment": "2 pages, 1 figure, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06287v2",
                "updated": "2025-02-24T17:35:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    35,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-08T18:38:32Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    18,
                    38,
                    32,
                    1,
                    282,
                    0
                ],
                "title": "Non-Halting Queries: Exploiting Fixed Points in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Halting Queries: Exploiting Fixed Points in LLMs"
                },
                "summary": "We introduce a new vulnerability that exploits fixed points in autoregressive\nmodels and use it to craft queries that never halt. More precisely, for\nnon-halting queries, the LLM never samples the end-of-string token <eos>. We\nrigorously analyze the conditions under which the non-halting anomaly presents\nitself. In particular, at temperature zero, we prove that if a repeating\n(cyclic) token sequence is observed at the output beyond the context size, then\nthe LLM does not halt.\n  We demonstrate non-halting queries in many experiments performed in base\nunaligned models where repeating prompts immediately lead to a non-halting\ncyclic behavior as predicted by the analysis. Further, we develop a simple\nrecipe that takes the same fixed points observed in the base model and creates\na prompt structure to target aligned models. We demonstrate the recipe's\nsuccess in sending every major model released over the past year into a\nnon-halting state with the same simple prompt even over higher temperatures.\nFurther, we devise an experiment with 100 randomly selected tokens and show\nthat the recipe to create non-halting queries succeeds with high success rates\nranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that\nthe proposed adversarial recipe succeeds in bypassing alignment at one to two\norders of magnitude higher rates compared to earlier reports.\n  We also study gradient-based direct inversion using ARCA to craft new short\nprompts to induce the non-halting state. We inverted 10,000 random repeating\n2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted\nprompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments\nwith ARCA show that non-halting may be easily induced with as few as 3 input\ntokens with high probability. Overall, our experiments demonstrate that\nnon-halting queries are prevalent and relatively easy to find.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new vulnerability that exploits fixed points in autoregressive\nmodels and use it to craft queries that never halt. More precisely, for\nnon-halting queries, the LLM never samples the end-of-string token <eos>. We\nrigorously analyze the conditions under which the non-halting anomaly presents\nitself. In particular, at temperature zero, we prove that if a repeating\n(cyclic) token sequence is observed at the output beyond the context size, then\nthe LLM does not halt.\n  We demonstrate non-halting queries in many experiments performed in base\nunaligned models where repeating prompts immediately lead to a non-halting\ncyclic behavior as predicted by the analysis. Further, we develop a simple\nrecipe that takes the same fixed points observed in the base model and creates\na prompt structure to target aligned models. We demonstrate the recipe's\nsuccess in sending every major model released over the past year into a\nnon-halting state with the same simple prompt even over higher temperatures.\nFurther, we devise an experiment with 100 randomly selected tokens and show\nthat the recipe to create non-halting queries succeeds with high success rates\nranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that\nthe proposed adversarial recipe succeeds in bypassing alignment at one to two\norders of magnitude higher rates compared to earlier reports.\n  We also study gradient-based direct inversion using ARCA to craft new short\nprompts to induce the non-halting state. We inverted 10,000 random repeating\n2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted\nprompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments\nwith ARCA show that non-halting may be easily induced with as few as 3 input\ntokens with high probability. Overall, our experiments demonstrate that\nnon-halting queries are prevalent and relatively easy to find."
                },
                "authors": [
                    {
                        "name": "Ghaith Hammouri"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17356v1",
                "updated": "2025-02-24T17:34:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    34,
                    45,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    34,
                    45,
                    0,
                    55,
                    0
                ],
                "title": "Distributional Scaling Laws for Emergent Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Scaling Laws for Emergent Capabilities"
                },
                "summary": "In this paper, we explore the nature of sudden breakthroughs in language\nmodel performance at scale, which stands in contrast to smooth improvements\ngoverned by scaling laws. While advocates of \"emergence\" view abrupt\nperformance gains as capabilities unlocking at specific scales, others have\nsuggested that they are produced by thresholding effects and alleviated by\ncontinuous metrics. We propose that breakthroughs are instead driven by\ncontinuous changes in the probability distribution of training outcomes,\nparticularly when performance is bimodally distributed across random seeds. In\nsynthetic length generalization tasks, we show that different random seeds can\nproduce either highly linear or emergent scaling trends. We reveal that sharp\nbreakthroughs in metrics are produced by underlying continuous changes in their\ndistribution across seeds. Furthermore, we provide a case study of inverse\nscaling and show that even as the probability of a successful run declines, the\naverage performance of a successful run continues to increase monotonically. We\nvalidate our distributional scaling framework on realistic settings by\nmeasuring MMLU performance in LLM populations. These insights emphasize the\nrole of random variation in the effect of scale on LLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the nature of sudden breakthroughs in language\nmodel performance at scale, which stands in contrast to smooth improvements\ngoverned by scaling laws. While advocates of \"emergence\" view abrupt\nperformance gains as capabilities unlocking at specific scales, others have\nsuggested that they are produced by thresholding effects and alleviated by\ncontinuous metrics. We propose that breakthroughs are instead driven by\ncontinuous changes in the probability distribution of training outcomes,\nparticularly when performance is bimodally distributed across random seeds. In\nsynthetic length generalization tasks, we show that different random seeds can\nproduce either highly linear or emergent scaling trends. We reveal that sharp\nbreakthroughs in metrics are produced by underlying continuous changes in their\ndistribution across seeds. Furthermore, we provide a case study of inverse\nscaling and show that even as the probability of a successful run declines, the\naverage performance of a successful run continues to increase monotonically. We\nvalidate our distributional scaling framework on realistic settings by\nmeasuring MMLU performance in LLM populations. These insights emphasize the\nrole of random variation in the effect of scale on LLM capabilities."
                },
                "authors": [
                    {
                        "name": "Rosie Zhao"
                    },
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Naomi Saphra"
                    }
                ],
                "author_detail": {
                    "name": "Naomi Saphra"
                },
                "author": "Naomi Saphra",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17355v1",
                "updated": "2025-02-24T17:33:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    33,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:33:18Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    33,
                    18,
                    0,
                    55,
                    0
                ],
                "title": "On Relation-Specific Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Relation-Specific Neurons in Large Language Models"
                },
                "summary": "In large language models (LLMs), certain neurons can store distinct pieces of\nknowledge learned during pretraining. While knowledge typically appears as a\ncombination of relations and entities, it remains unclear whether some neurons\nfocus on a relation itself -- independent of any entity. We hypothesize such\nneurons detect a relation in the input text and guide generation involving such\na relation. To investigate this, we study the Llama-2 family on a chosen set of\nrelations with a statistics-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts whose relation is $r$ and (2) facts whose relation is a\ndifferent relation $r' \\neq r$. With respect to their capacity for encoding\nrelation information, we give evidence for the following three properties of\nrelation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for\n$r$ present a cumulative effect so that deactivating a larger portion of them\nresults in the degradation of more facts in $r$. $\\textbf{(ii) Neuron\nversatility.}$ Neurons can be shared across multiple closely related as well as\nless related relations. Some relation neurons transfer across languages.\n$\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one\nrelation can improve LLM generation performance for facts of other relations.\nWe will make our code publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language models (LLMs), certain neurons can store distinct pieces of\nknowledge learned during pretraining. While knowledge typically appears as a\ncombination of relations and entities, it remains unclear whether some neurons\nfocus on a relation itself -- independent of any entity. We hypothesize such\nneurons detect a relation in the input text and guide generation involving such\na relation. To investigate this, we study the Llama-2 family on a chosen set of\nrelations with a statistics-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts whose relation is $r$ and (2) facts whose relation is a\ndifferent relation $r' \\neq r$. With respect to their capacity for encoding\nrelation information, we give evidence for the following three properties of\nrelation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for\n$r$ present a cumulative effect so that deactivating a larger portion of them\nresults in the degradation of more facts in $r$. $\\textbf{(ii) Neuron\nversatility.}$ Neurons can be shared across multiple closely related as well as\nless related relations. Some relation neurons transfer across languages.\n$\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one\nrelation can improve LLM generation performance for facts of other relations.\nWe will make our code publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons."
                },
                "authors": [
                    {
                        "name": "Yihong Liu"
                    },
                    {
                        "name": "Runsheng Chen"
                    },
                    {
                        "name": "Lea Hirlimann"
                    },
                    {
                        "name": "Ahmad Dawar Hakimi"
                    },
                    {
                        "name": "Mingyang Wang"
                    },
                    {
                        "name": "Amir Hossein Kargaran"
                    },
                    {
                        "name": "Sascha Rothe"
                    },
                    {
                        "name": "Franois Yvon"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17350v1",
                "updated": "2025-02-24T17:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    25,
                    9,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    25,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "Goal-Oriented Middleware Filtering at Transport Layer Based on Value of\n  Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-Oriented Middleware Filtering at Transport Layer Based on Value of\n  Updates"
                },
                "summary": "This work explores employing the concept of goal-oriented (GO) semantic\ncommunication for real-time monitoring and control. Generally, GO communication\nadvocates for the deep integration of application targets into the network\ndesign. We consider CPS and IoT applications where sensors generate a\ntremendous amount of network traffic toward monitors or controllers. Here, the\npractical introduction of GO communication must address several challenges.\nThese include stringent timing requirements, challenging network setups, and\nlimited computing and communication capabilities of the devices involved.\nMoreover, real-life CPS deployments often rely on heterogeneous communication\nstandards prompted by specific hardware. To address these issues, we introduce\na middleware design of a GO distributed Transport Layer (TL) framework for\ncontrol applications. It offers end-to-end performance improvements for diverse\nsetups and transmitting hardware. The proposed TL protocol evaluates the Value\nof sampled state Updates (VoU) for the application goal. It decides whether to\nadmit or discard the corresponding packets, thus offloading the network. VoU\ncaptures the contribution of utilizing the updates at the receiver into the\napplication's performance. We introduce a belief network and the augmentation\nprocedure used by the sensor to predict the evolution of the control process,\nincluding possible delays and losses of status updates in the network. The\nprediction is made either using a control model dynamics or a Long-Short Term\nMemory neural network approach. We test the performance of the proposed TL in\nthe experimental framework using Industrial IoT Zolertia ReMote sensors. We\nshow that while existing approaches fail to deliver sufficient control\nperformance, our VoU-based TL scheme ensures stability and performs\n$\\sim$$60\\%$ better than the naive GO TL we proposed in our previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores employing the concept of goal-oriented (GO) semantic\ncommunication for real-time monitoring and control. Generally, GO communication\nadvocates for the deep integration of application targets into the network\ndesign. We consider CPS and IoT applications where sensors generate a\ntremendous amount of network traffic toward monitors or controllers. Here, the\npractical introduction of GO communication must address several challenges.\nThese include stringent timing requirements, challenging network setups, and\nlimited computing and communication capabilities of the devices involved.\nMoreover, real-life CPS deployments often rely on heterogeneous communication\nstandards prompted by specific hardware. To address these issues, we introduce\na middleware design of a GO distributed Transport Layer (TL) framework for\ncontrol applications. It offers end-to-end performance improvements for diverse\nsetups and transmitting hardware. The proposed TL protocol evaluates the Value\nof sampled state Updates (VoU) for the application goal. It decides whether to\nadmit or discard the corresponding packets, thus offloading the network. VoU\ncaptures the contribution of utilizing the updates at the receiver into the\napplication's performance. We introduce a belief network and the augmentation\nprocedure used by the sensor to predict the evolution of the control process,\nincluding possible delays and losses of status updates in the network. The\nprediction is made either using a control model dynamics or a Long-Short Term\nMemory neural network approach. We test the performance of the proposed TL in\nthe experimental framework using Industrial IoT Zolertia ReMote sensors. We\nshow that while existing approaches fail to deliver sufficient control\nperformance, our VoU-based TL scheme ensures stability and performs\n$\\sim$$60\\%$ better than the naive GO TL we proposed in our previous work."
                },
                "authors": [
                    {
                        "name": "Polina Kutsevol"
                    },
                    {
                        "name": "Onur Ayan"
                    },
                    {
                        "name": "Nikolaos Pappas"
                    },
                    {
                        "name": "Wolfgang Kellerer"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kellerer"
                },
                "author": "Wolfgang Kellerer",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02481v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02481v4",
                "updated": "2025-02-24T17:24:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    24,
                    4,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-04T16:57:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo."
                },
                "authors": [
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Pengzhi Gao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "Accept to NAACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02481v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02481v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17341v1",
                "updated": "2025-02-24T17:17:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    17,
                    15,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:17:15Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    17,
                    15,
                    0,
                    55,
                    0
                ],
                "title": "Time series forecasting based on optimized LLM for fault prediction in\n  distribution power grid insulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting based on optimized LLM for fault prediction in\n  distribution power grid insulators"
                },
                "summary": "Surface contamination on electrical grid insulators leads to an increase in\nleakage current until an electrical discharge occurs, which can result in a\npower system shutdown. To mitigate the possibility of disruptive faults\nresulting in a power outage, monitoring contamination and leakage current can\nhelp predict the progression of faults. Given this need, this paper proposes a\nhybrid deep learning (DL) model for predicting the increase in leakage current\nin high-voltage insulators. The hybrid structure considers a multi-criteria\noptimization using tree-structured Parzen estimation, an input stage filter for\nsignal noise attenuation combined with a large language model (LLM) applied for\ntime series forecasting. The proposed optimized LLM outperforms\nstate-of-the-art DL models with a root-mean-square error equal to\n2.24$\\times10^{-4}$ for a short-term horizon and 1.21$\\times10^{-3}$ for a\nmedium-term horizon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface contamination on electrical grid insulators leads to an increase in\nleakage current until an electrical discharge occurs, which can result in a\npower system shutdown. To mitigate the possibility of disruptive faults\nresulting in a power outage, monitoring contamination and leakage current can\nhelp predict the progression of faults. Given this need, this paper proposes a\nhybrid deep learning (DL) model for predicting the increase in leakage current\nin high-voltage insulators. The hybrid structure considers a multi-criteria\noptimization using tree-structured Parzen estimation, an input stage filter for\nsignal noise attenuation combined with a large language model (LLM) applied for\ntime series forecasting. The proposed optimized LLM outperforms\nstate-of-the-art DL models with a root-mean-square error equal to\n2.24$\\times10^{-4}$ for a short-term horizon and 1.21$\\times10^{-3}$ for a\nmedium-term horizon."
                },
                "authors": [
                    {
                        "name": "Joo Pedro Matos-Carvalho"
                    },
                    {
                        "name": "Stefano Frizzo Stefenon"
                    },
                    {
                        "name": "Valderi Reis Quietinho Leithardt"
                    },
                    {
                        "name": "Kin-Choong Yow"
                    }
                ],
                "author_detail": {
                    "name": "Kin-Choong Yow"
                },
                "author": "Kin-Choong Yow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14845v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14845v3",
                "updated": "2025-02-24T17:06:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    6,
                    21,
                    0,
                    55,
                    0
                ],
                "published": "2024-07-20T11:19:58Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    11,
                    19,
                    58,
                    5,
                    202,
                    0
                ],
                "title": "Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models"
                },
                "summary": "Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real-world datasets validate\nour proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real-world datasets validate\nour proposed model."
                },
                "authors": [
                    {
                        "name": "Ze Yu Zhang"
                    },
                    {
                        "name": "Arun Verma"
                    },
                    {
                        "name": "Finale Doshi-Velez"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "22 pages, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14845v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14845v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14926v2",
                "updated": "2025-02-24T17:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    2,
                    5,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-19T23:16:29Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    23,
                    16,
                    29,
                    2,
                    50,
                    0
                ],
                "title": "DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for\n  LoRaWAN-related engineering tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for\n  LoRaWAN-related engineering tasks"
                },
                "summary": "This paper investigates the performance of 16 Large Language Models (LLMs) in\nautomating LoRaWAN-related engineering tasks involving optimal placement of\ndrones and received power calculation under progressively complex zero-shot,\nnatural language prompts. The primary research question is whether lightweight,\nlocally executed LLMs can generate correct Python code for these tasks. To\nassess this, we compared locally run models against state-of-the-art\nalternatives, such as GPT-4 and DeepSeek-V3, which served as reference points.\nBy extracting and executing the Python functions generated by each model, we\nevaluated their outputs on a zero-to-five scale. Results show that while\nDeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller\nmodels-particularly Phi-4 and LLaMA-3.3-also demonstrated strong performance,\nunderscoring the viability of lightweight alternatives. Other models exhibited\nerrors stemming from incomplete understanding or syntactic issues. These\nfindings illustrate the potential of LLM-based approaches for specialized\nengineering applications while highlighting the need for careful model\nselection, rigorous prompt design, and targeted domain fine-tuning to achieve\nreliable outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the performance of 16 Large Language Models (LLMs) in\nautomating LoRaWAN-related engineering tasks involving optimal placement of\ndrones and received power calculation under progressively complex zero-shot,\nnatural language prompts. The primary research question is whether lightweight,\nlocally executed LLMs can generate correct Python code for these tasks. To\nassess this, we compared locally run models against state-of-the-art\nalternatives, such as GPT-4 and DeepSeek-V3, which served as reference points.\nBy extracting and executing the Python functions generated by each model, we\nevaluated their outputs on a zero-to-five scale. Results show that while\nDeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller\nmodels-particularly Phi-4 and LLaMA-3.3-also demonstrated strong performance,\nunderscoring the viability of lightweight alternatives. Other models exhibited\nerrors stemming from incomplete understanding or syntactic issues. These\nfindings illustrate the potential of LLM-based approaches for specialized\nengineering applications while highlighting the need for careful model\nselection, rigorous prompt design, and targeted domain fine-tuning to achieve\nreliable outcomes."
                },
                "authors": [
                    {
                        "name": "Daniel Fernandes"
                    },
                    {
                        "name": "Joo P. Matos-Carvalho"
                    },
                    {
                        "name": "Carlos M. Fernandes"
                    },
                    {
                        "name": "Nuno Fachada"
                    }
                ],
                "author_detail": {
                    "name": "Nuno Fachada"
                },
                "author": "Nuno Fachada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17328v1",
                "updated": "2025-02-24T17:01:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    1,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T17:01:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    1,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\n  Capabilities for Few-Shot Dialogue Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\n  Capabilities for Few-Shot Dialogue Summarization"
                },
                "summary": "In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs\nto improve few-shot dialogue summarization task. Unlike prior methods that\nrequire external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis\nand summarization capabilities, allowing them to complement each other during\ntraining and enhance overall performances. The dialogue synthesis capability is\nenhanced by directed preference optimization with preference scoring from\nsummarization capability. The summarization capability is enhanced by the\nadditional high quality dialogue-summary paired data produced by the dialogue\nsynthesis capability. By leveraging the proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of synthetic data, and use it to\naugment the few-shot real training dataset. Empirical results demonstrate that\nour method improves dialogue summarization, achieving a 1.5% increase in ROUGE\nscores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,\nour method attains the highest average scores in human evaluations, surpassing\nboth the pre-trained models and the baselines fine-tuned solely for\nsummarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs\nto improve few-shot dialogue summarization task. Unlike prior methods that\nrequire external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis\nand summarization capabilities, allowing them to complement each other during\ntraining and enhance overall performances. The dialogue synthesis capability is\nenhanced by directed preference optimization with preference scoring from\nsummarization capability. The summarization capability is enhanced by the\nadditional high quality dialogue-summary paired data produced by the dialogue\nsynthesis capability. By leveraging the proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of synthetic data, and use it to\naugment the few-shot real training dataset. Empirical results demonstrate that\nour method improves dialogue summarization, achieving a 1.5% increase in ROUGE\nscores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,\nour method attains the highest average scores in human evaluations, surpassing\nboth the pre-trained models and the baselines fine-tuned solely for\nsummarization tasks."
                },
                "authors": [
                    {
                        "name": "Yen-Ju Lu"
                    },
                    {
                        "name": "Ting-Yao Hu"
                    },
                    {
                        "name": "Hema Swetha Koppula"
                    },
                    {
                        "name": "Hadi Pouransari"
                    },
                    {
                        "name": "Jen-Hao Rick Chang"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    }
                ],
                "author_detail": {
                    "name": "Raviteja Vemulapalli"
                },
                "author": "Raviteja Vemulapalli",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20318v3",
                "updated": "2025-02-24T16:42:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    42,
                    25,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-30T17:55:28Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    17,
                    55,
                    28,
                    3,
                    151,
                    0
                ],
                "title": "Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry\n  through Curiosity-Driven Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry\n  through Curiosity-Driven Queries"
                },
                "summary": "Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions."
                },
                "authors": [
                    {
                        "name": "Roberto Ceraolo"
                    },
                    {
                        "name": "Dmitrii Kharlapenko"
                    },
                    {
                        "name": "Ahmad Khan"
                    },
                    {
                        "name": "Amlie Reymond"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Bernhard Schlkopf"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17304v1",
                "updated": "2025-02-24T16:40:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    40,
                    46,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:40:46Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    40,
                    46,
                    0,
                    55,
                    0
                ],
                "title": "Child vs. machine language learning: Can the logical structure of human\n  language unleash LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Child vs. machine language learning: Can the logical structure of human\n  language unleash LLMs?"
                },
                "summary": "We argue that human language learning proceeds in a manner that is different\nin nature from current approaches to training LLMs, predicting a difference in\nlearning biases. We then present evidence from German plural formation by LLMs\nthat confirm our hypothesis that even very powerful implementations produce\nresults that miss aspects of the logic inherent to language that humans have no\nproblem with. We conclude that attention to the different structures of human\nlanguage and artificial neural networks is likely to be an avenue to improve\nLLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that human language learning proceeds in a manner that is different\nin nature from current approaches to training LLMs, predicting a difference in\nlearning biases. We then present evidence from German plural formation by LLMs\nthat confirm our hypothesis that even very powerful implementations produce\nresults that miss aspects of the logic inherent to language that humans have no\nproblem with. We conclude that attention to the different structures of human\nlanguage and artificial neural networks is likely to be an avenue to improve\nLLM performance."
                },
                "authors": [
                    {
                        "name": "Uli Sauerland"
                    },
                    {
                        "name": "Celia Matthaei"
                    },
                    {
                        "name": "Felix Salfner"
                    }
                ],
                "author_detail": {
                    "name": "Felix Salfner"
                },
                "author": "Felix Salfner",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v2",
                "updated": "2025-02-24T16:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    36,
                    32,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09630v2",
                "updated": "2025-02-24T16:35:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    35,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2024-11-27T15:46:54Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    46,
                    54,
                    2,
                    332,
                    0
                ],
                "title": "What does AI consider praiseworthy?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What does AI consider praiseworthy?"
                },
                "summary": "As large language models (LLMs) are increasingly used for work, personal, and\ntherapeutic purposes, researchers have begun to investigate these models'\nimplicit and explicit moral views. Previous work, however, focuses on asking\nLLMs to state opinions, or on other technical evaluations that do not reflect\ncommon user interactions. We propose a novel evaluation of LLM behavior that\nanalyzes responses to user-stated intentions, such as \"I'm thinking of\ncampaigning for {candidate}.\" LLMs frequently respond with critiques or praise,\noften beginning responses with phrases such as \"That's great to hear!...\" While\nthis makes them friendly, these praise responses are not universal and thus\nreflect a normative stance by the LLM. We map out the moral landscape of LLMs\nin how they respond to user statements in different domains including politics\nand everyday ethical actions. In particular, although a na\\\"ive analysis might\nsuggest LLMs are biased against right-leaning politics, our findings on news\nsources indicate that trustworthiness is a stronger driver of praise and\ncritique than ideology. Second, we find strong alignment across models in\nresponse to ethically-relevant action statements, but that doing so requires\nthem to engage in high levels of praise and critique of users, suggesting a\nreticence-alignment tradeoff. Finally, our experiment on statements about world\nleaders finds no evidence of bias favoring the country of origin of the models.\nWe conclude that as AI systems become more integrated into society, their\npatterns of praise, critique, and neutrality must be carefully monitored to\nprevent unintended psychological and societal consequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used for work, personal, and\ntherapeutic purposes, researchers have begun to investigate these models'\nimplicit and explicit moral views. Previous work, however, focuses on asking\nLLMs to state opinions, or on other technical evaluations that do not reflect\ncommon user interactions. We propose a novel evaluation of LLM behavior that\nanalyzes responses to user-stated intentions, such as \"I'm thinking of\ncampaigning for {candidate}.\" LLMs frequently respond with critiques or praise,\noften beginning responses with phrases such as \"That's great to hear!...\" While\nthis makes them friendly, these praise responses are not universal and thus\nreflect a normative stance by the LLM. We map out the moral landscape of LLMs\nin how they respond to user statements in different domains including politics\nand everyday ethical actions. In particular, although a na\\\"ive analysis might\nsuggest LLMs are biased against right-leaning politics, our findings on news\nsources indicate that trustworthiness is a stronger driver of praise and\ncritique than ideology. Second, we find strong alignment across models in\nresponse to ethically-relevant action statements, but that doing so requires\nthem to engage in high levels of praise and critique of users, suggesting a\nreticence-alignment tradeoff. Finally, our experiment on statements about world\nleaders finds no evidence of bias favoring the country of origin of the models.\nWe conclude that as AI systems become more integrated into society, their\npatterns of praise, critique, and neutrality must be carefully monitored to\nprevent unintended psychological and societal consequences."
                },
                "authors": [
                    {
                        "name": "Andrew J. Peterson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Peterson"
                },
                "author": "Andrew J. Peterson",
                "arxiv_comment": "Forthcoming in AI and Ethics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.1; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06786v2",
                "updated": "2025-02-24T16:34:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    34,
                    21,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-10T18:59:10Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    59,
                    10,
                    0,
                    41,
                    0
                ],
                "title": "Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Quantization"
                },
                "summary": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits.\nLeveraging this insight, in this paper, we propose Matryoshka Quantization\n(\\alg), a novel multi-scale quantization technique that alleviates the\naforementioned challenge. This technique allows us to train and maintain a\nsingle quantized model but serve it with the precision demanded by the\ndeployment. Furthermore, leveraging \\alg's co-training and co-distillation\nregularization, int2 precision models extracted by \\alg outperform standard\nint2 quantization by up to to 4\\% and 7\\% with OmniQuant and QAT as base\nalgorithms respectively. Finally, we demonstrate that by using an extra bit to\nrepresent outliers, a model with an effective precision of 2.05-bit gives an\nadditional 6\\% improvement with OmniQuant as the base algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits.\nLeveraging this insight, in this paper, we propose Matryoshka Quantization\n(\\alg), a novel multi-scale quantization technique that alleviates the\naforementioned challenge. This technique allows us to train and maintain a\nsingle quantized model but serve it with the precision demanded by the\ndeployment. Furthermore, leveraging \\alg's co-training and co-distillation\nregularization, int2 precision models extracted by \\alg outperform standard\nint2 quantization by up to to 4\\% and 7\\% with OmniQuant and QAT as base\nalgorithms respectively. Finally, we demonstrate that by using an extra bit to\nrepresent outliers, a model with an effective precision of 2.05-bit gives an\nadditional 6\\% improvement with OmniQuant as the base algorithm."
                },
                "authors": [
                    {
                        "name": "Pranav Nair"
                    },
                    {
                        "name": "Puranjay Datta"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17298v1",
                "updated": "2025-02-24T16:32:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    32,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:32:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    32,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Delta Decompression for MoE-based LLMs Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delta Decompression for MoE-based LLMs Compression"
                },
                "summary": "Mixture-of-Experts (MoE) architectures in large language models (LLMs)\nachieve exceptional performance, but face prohibitive storage and memory\nrequirements. To address these challenges, we present $D^2$-MoE, a new delta\ndecompression compressor for reducing the parameters of MoE LLMs. Based on\nobservations of expert diversity, we decompose their weights into a shared base\nweight and unique delta weights. Specifically, our method first merges each\nexpert's weight into the base weight using the Fisher information matrix to\ncapture shared components. Then, we compress delta weights through Singular\nValue Decomposition (SVD) by exploiting their low-rank properties. Finally, we\nintroduce a semi-dynamical structured pruning strategy for the base weights,\ncombining static and dynamic redundancy analysis to achieve further parameter\nreduction while maintaining input adaptivity. In this way, our $D^2$-MoE\nsuccessfully compact MoE LLMs to high compression ratios without additional\ntraining. Extensive experiments highlight the superiority of our approach, with\nover 13% performance gains than other compressors on\nMixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes\nare available in https://github.com/lliai/D2MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures in large language models (LLMs)\nachieve exceptional performance, but face prohibitive storage and memory\nrequirements. To address these challenges, we present $D^2$-MoE, a new delta\ndecompression compressor for reducing the parameters of MoE LLMs. Based on\nobservations of expert diversity, we decompose their weights into a shared base\nweight and unique delta weights. Specifically, our method first merges each\nexpert's weight into the base weight using the Fisher information matrix to\ncapture shared components. Then, we compress delta weights through Singular\nValue Decomposition (SVD) by exploiting their low-rank properties. Finally, we\nintroduce a semi-dynamical structured pruning strategy for the base weights,\ncombining static and dynamic redundancy analysis to achieve further parameter\nreduction while maintaining input adaptivity. In this way, our $D^2$-MoE\nsuccessfully compact MoE LLMs to high compression ratios without additional\ntraining. Extensive experiments highlight the superiority of our approach, with\nover 13% performance gains than other compressors on\nMixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes\nare available in https://github.com/lliai/D2MoE."
                },
                "authors": [
                    {
                        "name": "Hao Gu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qiyuan Zhu"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Shengjie Sun"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17294v1",
                "updated": "2025-02-24T16:22:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    22,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:22:16Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    22,
                    16,
                    0,
                    55,
                    0
                ],
                "title": "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties"
                },
                "summary": "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions."
                },
                "authors": [
                    {
                        "name": "Kevin Michalewicz"
                    },
                    {
                        "name": "Mauricio Barahona"
                    },
                    {
                        "name": "Barbara Bravi"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Bravi"
                },
                "author": "Barbara Bravi",
                "arxiv_comment": "18 main pages with 4 figures, 3 Supplementary pages with 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17282v1",
                "updated": "2025-02-24T16:10:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    10,
                    53,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T16:10:53Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    10,
                    53,
                    0,
                    55,
                    0
                ],
                "title": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated human-like\ninstruction-following abilities, particularly those exceeding 100 billion\nparameters. The combined capability of some smaller, resource-friendly LLMs can\naddress most of the instructions that larger LLMs excel at. In this work, we\nexplore how to route the best-performing LLM for each instruction to achieve\nbetter overall performance. We develop a new paradigm, constructing capability\ninstructions with model capability representation, user instruction, and\nperformance inquiry prompts to assess the performance. To learn from capability\ninstructions, we introduce a new end-to-end framework called Model Selection\nwith Aptitude Test (Model-SAT), which generates positive and negative samples\nbased on what different models perform well or struggle with. Model-SAT uses a\nmodel capability encoder that extends its model representation to a lightweight\nLLM. Our experiments show that Model-SAT understands the performance dimensions\nof candidate models and provides the probabilities of their capability to\nhandle various instructions. Additionally, during deployment, a new model can\nquickly infer its aptitude test results across 50 tasks, each with 20 shots.\nModel-SAT performs state-of-the-art model routing without candidate inference\nand in real-world new model-released scenarios. The code is available at\nhttps://github.com/Now-Join-Us/CIT-LLM-Routing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated human-like\ninstruction-following abilities, particularly those exceeding 100 billion\nparameters. The combined capability of some smaller, resource-friendly LLMs can\naddress most of the instructions that larger LLMs excel at. In this work, we\nexplore how to route the best-performing LLM for each instruction to achieve\nbetter overall performance. We develop a new paradigm, constructing capability\ninstructions with model capability representation, user instruction, and\nperformance inquiry prompts to assess the performance. To learn from capability\ninstructions, we introduce a new end-to-end framework called Model Selection\nwith Aptitude Test (Model-SAT), which generates positive and negative samples\nbased on what different models perform well or struggle with. Model-SAT uses a\nmodel capability encoder that extends its model representation to a lightweight\nLLM. Our experiments show that Model-SAT understands the performance dimensions\nof candidate models and provides the probabilities of their capability to\nhandle various instructions. Additionally, during deployment, a new model can\nquickly infer its aptitude test results across 50 tasks, each with 20 shots.\nModel-SAT performs state-of-the-art model routing without candidate inference\nand in real-world new model-released scenarios. The code is available at\nhttps://github.com/Now-Join-Us/CIT-LLM-Routing"
                },
                "authors": [
                    {
                        "name": "Yi-Kai Zhang"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "arxiv_comment": "AAAI 2025; Project Page: https://cit-llm-routing.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20296v2",
                "updated": "2025-02-24T16:00:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    16,
                    0,
                    16,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-30T13:55:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    13,
                    55,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "PersonalLLM: Tailoring LLMs to Individual Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalLLM: Tailoring LLMs to Individual Preferences"
                },
                "summary": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM"
                },
                "authors": [
                    {
                        "name": "Thomas P. Zollo"
                    },
                    {
                        "name": "Andrew Wei Tung Siah"
                    },
                    {
                        "name": "Naimeng Ye"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13092v2",
                "updated": "2025-02-24T15:59:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    59,
                    4,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T17:59:48Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    59,
                    48,
                    1,
                    49,
                    0
                ],
                "title": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation"
                },
                "summary": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/."
                },
                "authors": [
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Yude Zou"
                    },
                    {
                        "name": "Yuheng Lei"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Project page: https://text-to-world.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12464v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12464v8",
                "updated": "2025-02-24T15:50:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    50,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2024-04-18T18:48:50Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    18,
                    48,
                    50,
                    3,
                    109,
                    0
                ],
                "title": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models"
                },
                "summary": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences."
                },
                "authors": [
                    {
                        "name": "Abhinav Rao"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Vishwa Shah"
                    },
                    {
                        "name": "Katharina Reinecke"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12464v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12464v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17262v1",
                "updated": "2025-02-24T15:44:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    44,
                    57,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:44:57Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    44,
                    57,
                    0,
                    55,
                    0
                ],
                "title": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective"
                },
                "summary": "The rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream task\nperformance prior to model training is crucial for efficient resource\nallocation, yet remains challenging due to two primary constraints: (1) the\n\"emergence phenomenon\", wherein downstream performance metrics become\nmeaningful only after extensive training, which limits the ability to use\nsmaller models for prediction; (2) Uneven task difficulty distributions and the\nabsence of consistent scaling laws, resulting in substantial metric\nvariability. Existing performance prediction methods suffer from limited\naccuracy and reliability, thereby impeding the assessment of potential LLM\ncapabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework. COD\nfirst constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable\nclusters. The scores on the selected subset serve as effective intermediate\npredictors of downstream performance on the full evaluation set. With\ntheoretical support, we derive a mapping function that transforms performance\nmetrics from the predictable subset to the full evaluation set, thereby\nensuring accurate extrapolation of LLM downstream performance. The proposed\nmethod has been applied to predict performance scaling for a 70B LLM, providing\nactionable insights for training resource allocation and assisting in\nmonitoring the training process. Notably, COD achieves remarkable predictive\naccuracy on the 70B LLM by leveraging an ensemble of small models,\ndemonstrating an absolute mean deviation of 1.36% across eight important LLM\nevaluation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream task\nperformance prior to model training is crucial for efficient resource\nallocation, yet remains challenging due to two primary constraints: (1) the\n\"emergence phenomenon\", wherein downstream performance metrics become\nmeaningful only after extensive training, which limits the ability to use\nsmaller models for prediction; (2) Uneven task difficulty distributions and the\nabsence of consistent scaling laws, resulting in substantial metric\nvariability. Existing performance prediction methods suffer from limited\naccuracy and reliability, thereby impeding the assessment of potential LLM\ncapabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework. COD\nfirst constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable\nclusters. The scores on the selected subset serve as effective intermediate\npredictors of downstream performance on the full evaluation set. With\ntheoretical support, we derive a mapping function that transforms performance\nmetrics from the predictable subset to the full evaluation set, thereby\nensuring accurate extrapolation of LLM downstream performance. The proposed\nmethod has been applied to predict performance scaling for a 70B LLM, providing\nactionable insights for training resource allocation and assisting in\nmonitoring the training process. Notably, COD achieves remarkable predictive\naccuracy on the 70B LLM by leveraging an ensemble of small models,\ndemonstrating an absolute mean deviation of 1.36% across eight important LLM\nevaluation benchmarks."
                },
                "authors": [
                    {
                        "name": "Chengyin Xu"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Chenggang Li"
                    }
                ],
                "author_detail": {
                    "name": "Chenggang Li"
                },
                "author": "Chenggang Li",
                "arxiv_comment": "21 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17259v1",
                "updated": "2025-02-24T15:39:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "Detecting Benchmark Contamination Through Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Benchmark Contamination Through Watermarking"
                },
                "summary": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark contamination poses a significant challenge to the reliability of\nLarge Language Models (LLMs) evaluations, as it is difficult to assert whether\na model has been trained on a test set. We introduce a solution to this problem\nby watermarking benchmarks before their release. The embedding involves\nreformulating the original questions with a watermarked LLM, in a way that does\nnot alter the benchmark utility. During evaluation, we can detect\n``radioactivity'', \\ie traces that the text watermarks leave in the model\nduring training, using a theoretically grounded statistical test. We test our\nmethod by pre-training 1B models from scratch on 10B tokens with controlled\nbenchmark contamination, and validate its effectiveness in detecting\ncontamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar\nbenchmark utility post-watermarking and successful contamination detection when\nmodels are contaminated enough to enhance performance, e.g. $p$-val $=10^{-3}$\nfor +5$\\%$ on ARC-Easy."
                },
                "authors": [
                    {
                        "name": "Tom Sander"
                    },
                    {
                        "name": "Pierre Fernandez"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10610v2",
                "updated": "2025-02-24T15:38:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    38,
                    45,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-13T23:32:44Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    23,
                    32,
                    44,
                    4,
                    348,
                    0
                ],
                "title": "Comparing large language models for supervised analysis of students' lab\n  notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing large language models for supervised analysis of students' lab\n  notes"
                },
                "summary": "Recent advancements in large language models (LLMs) hold significant promise\nin improving physics education research that uses machine learning. In this\nstudy, we compare the application of various models to perform large-scale\nanalysis of written text grounded in a physics education research\nclassification problem: identifying skills in students' typed lab notes through\nsentence-level labeling. Specifically, we use training data to fine-tune two\ndifferent LLMs, BERT and LLaMA, and compare the performance of these models to\nboth a traditional bag of words approach and a few-shot LLM (without\nfine-tuning).} We evaluate the models based on their resource use, performance\nmetrics, and research outcomes when identifying skills in lab notes. We find\nthat higher-resource models often, but not necessarily, perform better than\nlower-resource models. We also find that all models estimate similar trends in\nresearch outcomes, although the absolute values of the estimated measurements\nare not always within uncertainties of each other. We use the results to\ndiscuss relevant considerations for education researchers seeking to select a\nmodel type to use as a classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) hold significant promise\nin improving physics education research that uses machine learning. In this\nstudy, we compare the application of various models to perform large-scale\nanalysis of written text grounded in a physics education research\nclassification problem: identifying skills in students' typed lab notes through\nsentence-level labeling. Specifically, we use training data to fine-tune two\ndifferent LLMs, BERT and LLaMA, and compare the performance of these models to\nboth a traditional bag of words approach and a few-shot LLM (without\nfine-tuning).} We evaluate the models based on their resource use, performance\nmetrics, and research outcomes when identifying skills in lab notes. We find\nthat higher-resource models often, but not necessarily, perform better than\nlower-resource models. We also find that all models estimate similar trends in\nresearch outcomes, although the absolute values of the estimated measurements\nare not always within uncertainties of each other. We use the results to\ndiscuss relevant considerations for education researchers seeking to select a\nmodel type to use as a classifier."
                },
                "authors": [
                    {
                        "name": "Rebeckah K. Fussell"
                    },
                    {
                        "name": "Megan Flynn"
                    },
                    {
                        "name": "Anil Damle"
                    },
                    {
                        "name": "Michael F. J. Fox"
                    },
                    {
                        "name": "N. G. Holmes"
                    }
                ],
                "author_detail": {
                    "name": "N. G. Holmes"
                },
                "author": "N. G. Holmes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17254v1",
                "updated": "2025-02-24T15:34:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    34,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:34:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    34,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,\n  Distributional, and Semantic Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,\n  Distributional, and Semantic Objective"
                },
                "summary": "To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense."
                },
                "authors": [
                    {
                        "name": "Simon Geisler"
                    },
                    {
                        "name": "Tom Wollschlger"
                    },
                    {
                        "name": "M. H. I. Abdalla"
                    },
                    {
                        "name": "Vincent Cohen-Addad"
                    },
                    {
                        "name": "Johannes Gasteiger"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Gnnemann"
                },
                "author": "Stephan Gnnemann",
                "arxiv_comment": "30 pages, 6 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17248v1",
                "updated": "2025-02-24T15:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search"
                },
                "summary": "Text-to-SQL, which enables natural language interaction with databases,\nserves as a pivotal method across diverse industries. With new, more powerful\nlarge language models (LLMs) emerging every few months, fine-tuning has become\nincredibly costly, labor-intensive, and error-prone. As an alternative,\nzero-shot Text-to-SQL, which leverages the growing knowledge and reasoning\ncapabilities encoded in LLMs without task-specific fine-tuning, presents a\npromising and more challenging direction. To address this challenge, we propose\nAlpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS)\nframework to iteratively infer SQL construction actions based on partial SQL\nquery states. To enhance the framework's reasoning capabilities, we introduce\nLLM-as-Action-Model to dynamically generate SQL construction actions during the\nMCTS process, steering the search toward more promising SQL queries. Moreover,\nAlpha-SQL employs a self-supervised reward function to evaluate the quality of\ncandidate SQL queries, ensuring more accurate and efficient query generation.\nExperimental results show that Alpha-SQL achieves 69.7% execution accuracy on\nthe BIRD development set, using a 32B open-source LLM without fine-tuning.\nAlpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by\n2.5% on the BIRD development set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL, which enables natural language interaction with databases,\nserves as a pivotal method across diverse industries. With new, more powerful\nlarge language models (LLMs) emerging every few months, fine-tuning has become\nincredibly costly, labor-intensive, and error-prone. As an alternative,\nzero-shot Text-to-SQL, which leverages the growing knowledge and reasoning\ncapabilities encoded in LLMs without task-specific fine-tuning, presents a\npromising and more challenging direction. To address this challenge, we propose\nAlpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS)\nframework to iteratively infer SQL construction actions based on partial SQL\nquery states. To enhance the framework's reasoning capabilities, we introduce\nLLM-as-Action-Model to dynamically generate SQL construction actions during the\nMCTS process, steering the search toward more promising SQL queries. Moreover,\nAlpha-SQL employs a self-supervised reward function to evaluate the quality of\ncandidate SQL queries, ensuring more accurate and efficient query generation.\nExperimental results show that Alpha-SQL achieves 69.7% execution accuracy on\nthe BIRD development set, using a 32B open-source LLM without fine-tuning.\nAlpha-SQL outperforms the best previous zero-shot approach based on GPT-4o by\n2.5% on the BIRD development set."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Yanwei Xu"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17239v1",
                "updated": "2025-02-24T15:16:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    16,
                    34,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    16,
                    34,
                    0,
                    55,
                    0
                ],
                "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction"
                },
                "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that\nseamlessly integrates audio understanding and generation. It features a\ntext-guided aligned speech generation mechanism, enabling real-time speech\ninteraction with both comprehension and generation capabilities. Baichuan-Audio\nleverages a pre-trained ASR model, followed by multi-codebook discretization of\nspeech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that\nspeech tokens retain both semantic and acoustic information. To further enhance\nmodeling, an independent audio head is employed to process audio tokens,\neffectively capturing their unique characteristics. To mitigate the loss of\nintelligence during pre-training and preserve the original capabilities of the\nLLM, we propose a two-stage pre-training strategy that maintains language\nunderstanding while enhancing audio modeling. Following alignment, the model\nexcels in real-time speech-based conversation and exhibits outstanding\nquestion-answering capabilities, demonstrating its versatility and efficiency.\nThe proposed model demonstrates superior performance in real-time spoken\ndialogue and exhibits strong question-answering abilities. Our code, model and\ntraining data are available at https://github.com/baichuan-inc/Baichuan-Audio",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Baichuan-Audio, an end-to-end audio large language model that\nseamlessly integrates audio understanding and generation. It features a\ntext-guided aligned speech generation mechanism, enabling real-time speech\ninteraction with both comprehension and generation capabilities. Baichuan-Audio\nleverages a pre-trained ASR model, followed by multi-codebook discretization of\nspeech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that\nspeech tokens retain both semantic and acoustic information. To further enhance\nmodeling, an independent audio head is employed to process audio tokens,\neffectively capturing their unique characteristics. To mitigate the loss of\nintelligence during pre-training and preserve the original capabilities of the\nLLM, we propose a two-stage pre-training strategy that maintains language\nunderstanding while enhancing audio modeling. Following alignment, the model\nexcels in real-time speech-based conversation and exhibits outstanding\nquestion-answering capabilities, demonstrating its versatility and efficiency.\nThe proposed model demonstrates superior performance in real-time spoken\ndialogue and exhibits strong question-answering abilities. Our code, model and\ntraining data are available at https://github.com/baichuan-inc/Baichuan-Audio"
                },
                "authors": [
                    {
                        "name": "Tianpeng Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Yuanbo Fang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Mingrui Wang"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Zehuan Li"
                    },
                    {
                        "name": "Mingan Lin"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07663v2",
                "updated": "2025-02-24T15:00:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    0,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-11T15:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    56,
                    22,
                    1,
                    42,
                    0
                ],
                "title": "Human Decision-making is Susceptible to AI-driven Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Decision-making is Susceptible to AI-driven Manipulation"
                },
                "summary": "Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy."
                },
                "authors": [
                    {
                        "name": "Sahand Sabour"
                    },
                    {
                        "name": "June M. Liu"
                    },
                    {
                        "name": "Siyang Liu"
                    },
                    {
                        "name": "Chris Z. Yao"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Yaru Cao"
                    },
                    {
                        "name": "Advait Bhat"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Tim Althoff"
                    },
                    {
                        "name": "Tatia M. C. Lee"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00913v3",
                "updated": "2025-02-24T14:50:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    50,
                    8,
                    0,
                    55,
                    0
                ],
                "published": "2024-02-01T10:58:10Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    10,
                    58,
                    10,
                    3,
                    32,
                    0
                ],
                "title": "Institutional Platform for Secure Self-Service Large Language Model\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Institutional Platform for Secure Self-Service Large Language Model\n  Exploration"
                },
                "summary": "This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery."
                },
                "authors": [
                    {
                        "name": "V. K. Cody Bumgardner"
                    },
                    {
                        "name": "Mitchell A. Klusty"
                    },
                    {
                        "name": "W. Vaiden Logan"
                    },
                    {
                        "name": "Samuel E. Armstrong"
                    },
                    {
                        "name": "Caroline N. Leach"
                    },
                    {
                        "name": "Kenneth L. Calvert"
                    },
                    {
                        "name": "Caylin Hickey"
                    },
                    {
                        "name": "Jeff Talbert"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Talbert"
                },
                "author": "Jeff Talbert",
                "arxiv_comment": "10 pages 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17216v1",
                "updated": "2025-02-24T14:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    49,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    49,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic\n  Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic\n  Approaches"
                },
                "summary": "Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning\nproblems formulated in natural language into a formal intermediate language.\nSubsequently, the usage of symbolic reasoners yields reliable solving thereof.\nHowever, LLMs often fail in translation due to poorly chosen intermediate\nlanguages.\n  We introduce the intermediate language problem, which is the problem of\nchoosing a suitable formal language representation for neurosymbolic\napproaches. Theoretically, we argue that its origins lie in the inability of\nLLMs to distinguish syntax from semantics and the relative independence of the\nproblem from its representation. We showcase its existence experimentally by\ncontrasting two intermediate languages, Answer Set Programming and the Python\nKnowledge Engine. In addition, we demonstrate the effects of varying degrees of\nsupplementary context information. Our results show a maximum difference in\noverall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the\nGPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA\ndataset by 21.20% and by 50.50% on the ProofWriter dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning\nproblems formulated in natural language into a formal intermediate language.\nSubsequently, the usage of symbolic reasoners yields reliable solving thereof.\nHowever, LLMs often fail in translation due to poorly chosen intermediate\nlanguages.\n  We introduce the intermediate language problem, which is the problem of\nchoosing a suitable formal language representation for neurosymbolic\napproaches. Theoretically, we argue that its origins lie in the inability of\nLLMs to distinguish syntax from semantics and the relative independence of the\nproblem from its representation. We showcase its existence experimentally by\ncontrasting two intermediate languages, Answer Set Programming and the Python\nKnowledge Engine. In addition, we demonstrate the effects of varying degrees of\nsupplementary context information. Our results show a maximum difference in\noverall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the\nGPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA\ndataset by 21.20% and by 50.50% on the ProofWriter dataset."
                },
                "authors": [
                    {
                        "name": "Alexander Beiser"
                    },
                    {
                        "name": "David Penz"
                    }
                ],
                "author_detail": {
                    "name": "David Penz"
                },
                "author": "David Penz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17214v1",
                "updated": "2025-02-24T14:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    48,
                    6,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    48,
                    6,
                    0,
                    55,
                    0
                ],
                "title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with\n  Chain-of-Thought"
                },
                "summary": "Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on LLaMA\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on LLaMA\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ."
                },
                "authors": [
                    {
                        "name": "Boxuan Zhang"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15411v2",
                "updated": "2025-02-24T14:45:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    45,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T12:19:08Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    19,
                    8,
                    4,
                    52,
                    0
                ],
                "title": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings"
                },
                "summary": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts."
                },
                "authors": [
                    {
                        "name": "Rasmus Aavang"
                    },
                    {
                        "name": "Giovanni Rizzi"
                    },
                    {
                        "name": "Rasmus Bggild"
                    },
                    {
                        "name": "Alexandre Iolov"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17204v1",
                "updated": "2025-02-24T14:39:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    39,
                    28,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:39:28Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    39,
                    28,
                    0,
                    55,
                    0
                ],
                "title": "Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following"
                },
                "summary": "Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF."
                },
                "authors": [
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Qingyu Ren"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03035v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03035v4",
                "updated": "2025-02-24T14:34:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    34,
                    37,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-06T14:23:02Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning"
                },
                "summary": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhongwei Xie"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03035v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03035v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17189v1",
                "updated": "2025-02-24T14:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    24,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    24,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "IGDA: Interactive Graph Discovery through Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGDA: Interactive Graph Discovery through Large Language Model Agents"
                },
                "summary": "Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for\ndiscovery. Instead of utilizing numerical data, LLMs utilize associated\nvariable $\\textit{semantic metadata}$ to predict variable relationships.\nSimultaneously, LLMs demonstrate impressive abilities to act as black-box\noptimizers when given an objective $f$ and sequence of trials. We study LLMs at\nthe intersection of these two capabilities by applying LLMs to the task of\n$\\textit{interactive graph discovery}$: given a ground truth graph $G^*$\ncapturing variable relationships and a budget of $I$ edge experiments over $R$\nrounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$\nat the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$,\na LLM-based pipeline incorporating two key components: 1) an LLM\nuncertainty-driven method for edge experiment selection 2) a local graph update\nstrategy utilizing binary feedback from experiments to improve predictions for\nunselected neighboring edges. Experiments on eight different real-world graphs\nshow our approach often outperforms all baselines including a state-of-the-art\nnumerical method for interactive graph discovery. Further, we conduct a\nrigorous series of ablations dissecting the impact of each pipeline component.\nFinally, to assess the impact of memorization, we apply our interactive graph\ndiscovery strategy to a complex, new (as of July 2024) causal graph on protein\ntranscription factors, finding strong performance in a setting where\nmemorization is impossible. Overall, our results show IGDA to be a powerful\nmethod for graph discovery complementary to existing numerically driven\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for\ndiscovery. Instead of utilizing numerical data, LLMs utilize associated\nvariable $\\textit{semantic metadata}$ to predict variable relationships.\nSimultaneously, LLMs demonstrate impressive abilities to act as black-box\noptimizers when given an objective $f$ and sequence of trials. We study LLMs at\nthe intersection of these two capabilities by applying LLMs to the task of\n$\\textit{interactive graph discovery}$: given a ground truth graph $G^*$\ncapturing variable relationships and a budget of $I$ edge experiments over $R$\nrounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$\nat the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$,\na LLM-based pipeline incorporating two key components: 1) an LLM\nuncertainty-driven method for edge experiment selection 2) a local graph update\nstrategy utilizing binary feedback from experiments to improve predictions for\nunselected neighboring edges. Experiments on eight different real-world graphs\nshow our approach often outperforms all baselines including a state-of-the-art\nnumerical method for interactive graph discovery. Further, we conduct a\nrigorous series of ablations dissecting the impact of each pipeline component.\nFinally, to assess the impact of memorization, we apply our interactive graph\ndiscovery strategy to a complex, new (as of July 2024) causal graph on protein\ntranscription factors, finding strong performance in a setting where\nmemorization is impossible. Overall, our results show IGDA to be a powerful\nmethod for graph discovery complementary to existing numerically driven\napproaches."
                },
                "authors": [
                    {
                        "name": "Alex Havrilla"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Nicolo Fusi"
                    }
                ],
                "author_detail": {
                    "name": "Nicolo Fusi"
                },
                "author": "Nicolo Fusi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17187v1",
                "updated": "2025-02-24T14:23:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    23,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:23:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    23,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks"
                },
                "summary": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers\nhave gained significant attention. Currently, state-of-the-art LLMs utilize\nthis architecture. There is a substantial amount of research on how to train\nsuch models and how to select hyperparameters for this architecture. However,\nthere is a lack of studies focusing on post-evaluation analysis of MoE layer\nproperties. In this paper, we take a first step toward closing this gap by\nevaluating expert contributions on the quiz-based MMLU benchmark. We show that\nmost experts were never activated during inference on this benchmark.\nAdditionally, the output distribution of gating networks is much closer to\nuniform than sparse. Finally, we demonstrate that the average performance of\nsome experts within the same layer varies significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers\nhave gained significant attention. Currently, state-of-the-art LLMs utilize\nthis architecture. There is a substantial amount of research on how to train\nsuch models and how to select hyperparameters for this architecture. However,\nthere is a lack of studies focusing on post-evaluation analysis of MoE layer\nproperties. In this paper, we take a first step toward closing this gap by\nevaluating expert contributions on the quiz-based MMLU benchmark. We show that\nmost experts were never activated during inference on this benchmark.\nAdditionally, the output distribution of gating networks is much closer to\nuniform than sparse. Finally, we demonstrate that the average performance of\nsome experts within the same layer varies significantly."
                },
                "authors": [
                    {
                        "name": "Andrei Chernov"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Chernov"
                },
                "author": "Andrei Chernov",
                "arxiv_comment": "preprint, short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17173v1",
                "updated": "2025-02-24T14:09:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    9,
                    45,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:09:45Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    9,
                    45,
                    0,
                    55,
                    0
                ],
                "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch"
                },
                "summary": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development."
                },
                "authors": [
                    {
                        "name": "Xueru Wen"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xing Yu"
                    },
                    {
                        "name": "Yuqiu Ji"
                    },
                    {
                        "name": "Guohai Xu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Debing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Debing Zhang"
                },
                "author": "Debing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20777v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20777v3",
                "updated": "2025-02-24T14:06:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    6,
                    41,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-28T08:41:30Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    8,
                    41,
                    30,
                    1,
                    149,
                    0
                ],
                "title": "Black-Box Detection of Language Model Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box Detection of Language Model Watermarks"
                },
                "summary": "Watermarking has emerged as a promising way to detect LLM-generated text, by\naugmenting LLM generations with later detectable signals. Recent work has\nproposed multiple families of watermarking schemes, several of which focus on\npreserving the LLM distribution. This distribution-preservation property is\nmotivated by the fact that it is a tractable proxy for retaining LLM\ncapabilities, as well as the inherently implied undetectability of the\nwatermark by downstream users. Yet, despite much discourse around\nundetectability, no prior work has investigated the practical detectability of\nany of the current watermarking schemes in a realistic black-box setting. In\nthis work we tackle this for the first time, developing rigorous statistical\ntests to detect the presence, and estimate parameters, of all three popular\nwatermarking scheme families, using only a limited number of black-box queries.\nWe experimentally confirm the effectiveness of our methods on a range of\nschemes and a diverse set of open-source models. Further, we validate the\nfeasibility of our tests on real-world APIs. Our findings indicate that current\nwatermarking schemes are more detectable than previously believed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a promising way to detect LLM-generated text, by\naugmenting LLM generations with later detectable signals. Recent work has\nproposed multiple families of watermarking schemes, several of which focus on\npreserving the LLM distribution. This distribution-preservation property is\nmotivated by the fact that it is a tractable proxy for retaining LLM\ncapabilities, as well as the inherently implied undetectability of the\nwatermark by downstream users. Yet, despite much discourse around\nundetectability, no prior work has investigated the practical detectability of\nany of the current watermarking schemes in a realistic black-box setting. In\nthis work we tackle this for the first time, developing rigorous statistical\ntests to detect the presence, and estimate parameters, of all three popular\nwatermarking scheme families, using only a limited number of black-box queries.\nWe experimentally confirm the effectiveness of our methods on a range of\nschemes and a diverse set of open-source models. Further, we validate the\nfeasibility of our tests on real-world APIs. Our findings indicate that current\nwatermarking schemes are more detectable than previously believed."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20777v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20777v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17169v1",
                "updated": "2025-02-24T14:05:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    5,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:05:47Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    5,
                    47,
                    0,
                    55,
                    0
                ],
                "title": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without\n  Easily Identifiable Unrelated Padding)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without\n  Easily Identifiable Unrelated Padding)"
                },
                "summary": "Large language models demonstrate promising long context processing\ncapabilities, with recent models touting context windows close to one million\ntokens. However, the evaluations supporting these claims often involve simple\nretrieval tasks or synthetic tasks padded with irrelevant text, which the\nmodels may easily detect and discard. In this work, we generate lengthy\nsimplified English text with first-order logic representations spanning up to\n2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with\nevidence retrieval for contradiction detection. The long, homogeneous text is\nfilled with distractors that are both hard to distinguish from relevant\nevidences and provably not interfering with them. Our evaluation of evidence\nretrieval shows that the effective context window is much smaller with\nrealistic distractors, already crumbling at 128 clauses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demonstrate promising long context processing\ncapabilities, with recent models touting context windows close to one million\ntokens. However, the evaluations supporting these claims often involve simple\nretrieval tasks or synthetic tasks padded with irrelevant text, which the\nmodels may easily detect and discard. In this work, we generate lengthy\nsimplified English text with first-order logic representations spanning up to\n2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with\nevidence retrieval for contradiction detection. The long, homogeneous text is\nfilled with distractors that are both hard to distinguish from relevant\nevidences and provably not interfering with them. Our evaluation of evidence\nretrieval shows that the effective context window is much smaller with\nrealistic distractors, already crumbling at 128 clauses."
                },
                "authors": [
                    {
                        "name": "Damien Sileo"
                    }
                ],
                "author_detail": {
                    "name": "Damien Sileo"
                },
                "author": "Damien Sileo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17166v1",
                "updated": "2025-02-24T14:02:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    2,
                    0,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T14:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    2,
                    0,
                    0,
                    55,
                    0
                ],
                "title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning"
                },
                "summary": "The Four-Element Theory is a fundamental framework in criminal law, defining\nthe constitution of crime through four dimensions: Subject, Object, Subjective\naspect, and Objective aspect. This theory is widely referenced in legal\nreasoning, and many Large Language Models (LLMs) attempt to incorporate it when\nhandling legal tasks. However, current approaches rely on LLMs' internal\nknowledge to incorporate this theory, often lacking completeness and\nrepresentativeness. To address this limitation, we introduce JUREX-4E, an\nexpert-annotated knowledge base covering 155 criminal charges. It is structured\nthrough a progressive hierarchical annotation framework that prioritizes legal\nsource validity and employs diverse legal interpretation methods to ensure\ncomprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge\nDistinction task and apply it to Legal Case Retrieval, demonstrating its\neffectiveness in improving LLM performance. Experimental results validate the\nhigh quality of JUREX-4E and its substantial impact on downstream legal tasks,\nunderscoring its potential for advancing legal AI applications. Code:\nhttps://github.com/THUlawtech/JUREX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Four-Element Theory is a fundamental framework in criminal law, defining\nthe constitution of crime through four dimensions: Subject, Object, Subjective\naspect, and Objective aspect. This theory is widely referenced in legal\nreasoning, and many Large Language Models (LLMs) attempt to incorporate it when\nhandling legal tasks. However, current approaches rely on LLMs' internal\nknowledge to incorporate this theory, often lacking completeness and\nrepresentativeness. To address this limitation, we introduce JUREX-4E, an\nexpert-annotated knowledge base covering 155 criminal charges. It is structured\nthrough a progressive hierarchical annotation framework that prioritizes legal\nsource validity and employs diverse legal interpretation methods to ensure\ncomprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge\nDistinction task and apply it to Legal Case Retrieval, demonstrating its\neffectiveness in improving LLM performance. Experimental results validate the\nhigh quality of JUREX-4E and its substantial impact on downstream legal tasks,\nunderscoring its potential for advancing legal AI applications. Code:\nhttps://github.com/THUlawtech/JUREX"
                },
                "authors": [
                    {
                        "name": "Huanghai Liu"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Qingjing Chen"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiayu Ma"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Weixing Shen"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17163v1",
                "updated": "2025-02-24T13:58:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    58,
                    42,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    58,
                    42,
                    0,
                    55,
                    0
                ],
                "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation"
                },
                "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nrelease our benchmark to support the community developing accurate evaluation\nmethods for multilingual RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nrelease our benchmark to support the community developing accurate evaluation\nmethods for multilingual RAG systems."
                },
                "authors": [
                    {
                        "name": "Mara Andrea Cruz Blandn"
                    },
                    {
                        "name": "Jayasimha Talur"
                    },
                    {
                        "name": "Bruno Charron"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Marcello Federico"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Federico"
                },
                "author": "Marcello Federico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17161v1",
                "updated": "2025-02-24T13:56:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    56,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:56:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    56,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Real-time Monitoring of Economic Shocks using Company Websites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Monitoring of Economic Shocks using Company Websites"
                },
                "summary": "Understanding the effects of economic shocks on firms is critical for\nanalyzing economic growth and resilience. We introduce a Web-Based Affectedness\nIndicator (WAI), a general-purpose tool for real-time monitoring of economic\ndisruptions across diverse contexts. By leveraging Large Language Model (LLM)\nassisted classification and information extraction on texts from over five\nmillion company websites, WAI quantifies the degree and nature of firms'\nresponses to external shocks. Using the COVID-19 pandemic as a specific\napplication, we show that WAI is highly correlated with pandemic containment\nmeasures and reliably predicts firm performance. Unlike traditional data\nsources, WAI provides timely firm-level information across industries and\ngeographies worldwide that would otherwise be unavailable due to institutional\nand data availability constraints. This methodology offers significant\npotential for monitoring and mitigating the impact of technological, political,\nfinancial, health or environmental crises, and represents a transformative tool\nfor adaptive policy-making and economic resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the effects of economic shocks on firms is critical for\nanalyzing economic growth and resilience. We introduce a Web-Based Affectedness\nIndicator (WAI), a general-purpose tool for real-time monitoring of economic\ndisruptions across diverse contexts. By leveraging Large Language Model (LLM)\nassisted classification and information extraction on texts from over five\nmillion company websites, WAI quantifies the degree and nature of firms'\nresponses to external shocks. Using the COVID-19 pandemic as a specific\napplication, we show that WAI is highly correlated with pandemic containment\nmeasures and reliably predicts firm performance. Unlike traditional data\nsources, WAI provides timely firm-level information across industries and\ngeographies worldwide that would otherwise be unavailable due to institutional\nand data availability constraints. This methodology offers significant\npotential for monitoring and mitigating the impact of technological, political,\nfinancial, health or environmental crises, and represents a transformative tool\nfor adaptive policy-making and economic resilience."
                },
                "authors": [
                    {
                        "name": "Michael Koenig"
                    },
                    {
                        "name": "Jakob Rauch"
                    },
                    {
                        "name": "Martin Woerter"
                    }
                ],
                "author_detail": {
                    "name": "Martin Woerter"
                },
                "author": "Martin Woerter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08090v2",
                "updated": "2025-02-24T13:44:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    44,
                    37,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-11T04:16:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    16,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages"
                },
                "summary": "The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON,\na temporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages -- Romanian, German, and French, encompassing three temporal tasks\nand including a diverse set of four contemporaneous LLMs. This marks a\nsignificant step forward in addressing resource disparity in the context of\ntemporal reasoning across languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON,\na temporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages -- Romanian, German, and French, encompassing three temporal tasks\nand including a diverse set of four contemporaneous LLMs. This marks a\nsignificant step forward in addressing resource disparity in the context of\ntemporal reasoning across languages."
                },
                "authors": [
                    {
                        "name": "Ashutosh Bajpai"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20238v2",
                "updated": "2025-02-24T13:42:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    42,
                    28,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-26T17:48:20Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    17,
                    48,
                    20,
                    5,
                    300,
                    0
                ],
                "title": "A Survey of Large Language Models for Arabic Language and its Dialects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for Arabic Language and its Dialects"
                },
                "summary": "This survey offers a comprehensive overview of Large Language Models (LLMs)\ndesigned for Arabic language and its dialects. It covers key architectures,\nincluding encoder-only, decoder-only, and encoder-decoder models, along with\nthe datasets used for pre-training, spanning Classical Arabic, Modern Standard\nArabic, and Dialectal Arabic. The study also explores monolingual, bilingual,\nand multilingual LLMs, analyzing their architectures and performance across\ndownstream tasks, such as sentiment analysis, named entity recognition, and\nquestion answering. Furthermore, it assesses the openness of Arabic LLMs based\non factors, such as source code availability, training data, model weights, and\ndocumentation. The survey highlights the need for more diverse dialectal\ndatasets and attributes the importance of openness for research reproducibility\nand transparency. It concludes by identifying key challenges and opportunities\nfor future research and stressing the need for more inclusive and\nrepresentative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey offers a comprehensive overview of Large Language Models (LLMs)\ndesigned for Arabic language and its dialects. It covers key architectures,\nincluding encoder-only, decoder-only, and encoder-decoder models, along with\nthe datasets used for pre-training, spanning Classical Arabic, Modern Standard\nArabic, and Dialectal Arabic. The study also explores monolingual, bilingual,\nand multilingual LLMs, analyzing their architectures and performance across\ndownstream tasks, such as sentiment analysis, named entity recognition, and\nquestion answering. Furthermore, it assesses the openness of Arabic LLMs based\non factors, such as source code availability, training data, model weights, and\ndocumentation. The survey highlights the need for more diverse dialectal\ndatasets and attributes the importance of openness for research reproducibility\nand transparency. It concludes by identifying key challenges and opportunities\nfor future research and stressing the need for more inclusive and\nrepresentative models."
                },
                "authors": [
                    {
                        "name": "Malak Mashaabi"
                    },
                    {
                        "name": "Shahad Al-Khalifa"
                    },
                    {
                        "name": "Hend Al-Khalifa"
                    }
                ],
                "author_detail": {
                    "name": "Hend Al-Khalifa"
                },
                "author": "Hend Al-Khalifa",
                "arxiv_comment": "Submitted to ACM Transactions on Asian and Low-Resource Language\n  Information Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18596v2",
                "updated": "2025-02-24T13:35:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-30T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    59,
                    55,
                    3,
                    30,
                    0
                ],
                "title": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights"
                },
                "summary": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DeltaLLM, a new post-training compression technique to reduce\nthe memory footprint of LLMs. We propose an alternative way of structuring LLMs\nwith weight sharing between layers in subsequent Transformer blocks, along with\nadditional low-rank difference matrices between them. For training, we adopt\nthe progressing module replacement method and show that the lightweight\ntraining of the low-rank modules with approximately 30M-40M tokens is\nsufficient to achieve performance on par with LLMs of comparable sizes trained\nfrom scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a\n12% parameter reduction, retaining 90% of the performance of the base Llama and\nPhi models on common knowledge and reasoning benchmarks. Our method also\noutperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with\nthe same number of parameters removed. For example, DeltaPhi 2.9B with a 24%\nreduction achieves similar average zero-shot accuracies as recovery fine-tuned\nSlicedPhi 3.3B with a 12% reduction, despite being approximately 400M\nparameters smaller with no fine-tuning applied. This work provides new insights\ninto LLM architecture design and compression methods when storage space is\ncritical."
                },
                "authors": [
                    {
                        "name": "Liana Mikaelyan"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mathew Salvaris"
                    },
                    {
                        "name": "Parth Pathak"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Fayyaz"
                },
                "author": "Mohsen Fayyaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13708v2",
                "updated": "2025-02-24T13:31:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    31,
                    8,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-17T16:08:06Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    8,
                    6,
                    3,
                    291,
                    0
                ],
                "title": "On the Role of Attention Heads in Large Language Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Role of Attention Heads in Large Language Model Safety"
                },
                "summary": "Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models."
                },
                "authors": [
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "28 pages, 18 figures, 7 tables. This paper has been accepted as ICLR\n  2025 (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17136v1",
                "updated": "2025-02-24T13:27:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    27,
                    46,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:27:46Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    27,
                    46,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating the Effectiveness of Large Language Models in Automated News\n  Article Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Effectiveness of Large Language Models in Automated News\n  Article Summarization"
                },
                "summary": "The automation of news analysis and summarization presents a promising\nsolution to the challenge of processing and analyzing vast amounts of\ninformation prevalent in today's information society. Large Language Models\n(LLMs) have demonstrated the capability to transform vast amounts of textual\ndata into concise and easily comprehensible summaries, offering an effective\nsolution to the problem of information overload and providing users with a\nquick overview of relevant information. A particularly significant application\nof this technology lies in supply chain risk analysis. Companies must monitor\nthe news about their suppliers and respond to incidents for several critical\nreasons, including compliance with laws and regulations, risk management, and\nmaintaining supply chain resilience. This paper develops an automated news\nsummarization system for supply chain risk analysis using LLMs. The proposed\nsolution aggregates news from various sources, summarizes them using LLMs, and\npresents the condensed information to users in a clear and concise format. This\napproach enables companies to optimize their information processing and make\ninformed decisions. Our study addresses two main research questions: (1) Are\nLLMs effective in automating news summarization, particularly in the context of\nsupply chain risk analysis? (2) How effective are various LLMs in terms of\nreadability, duplicate detection, and risk identification in their\nsummarization quality? In this paper, we conducted an offline study using a\nrange of publicly available LLMs at the time and complemented it with a user\nstudy focused on the top performing systems of the offline experiments to\nevaluate their effectiveness further. Our results demonstrate that LLMs,\nparticularly Few-Shot GPT-4o mini, offer significant improvements in summary\nquality and risk identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of news analysis and summarization presents a promising\nsolution to the challenge of processing and analyzing vast amounts of\ninformation prevalent in today's information society. Large Language Models\n(LLMs) have demonstrated the capability to transform vast amounts of textual\ndata into concise and easily comprehensible summaries, offering an effective\nsolution to the problem of information overload and providing users with a\nquick overview of relevant information. A particularly significant application\nof this technology lies in supply chain risk analysis. Companies must monitor\nthe news about their suppliers and respond to incidents for several critical\nreasons, including compliance with laws and regulations, risk management, and\nmaintaining supply chain resilience. This paper develops an automated news\nsummarization system for supply chain risk analysis using LLMs. The proposed\nsolution aggregates news from various sources, summarizes them using LLMs, and\npresents the condensed information to users in a clear and concise format. This\napproach enables companies to optimize their information processing and make\ninformed decisions. Our study addresses two main research questions: (1) Are\nLLMs effective in automating news summarization, particularly in the context of\nsupply chain risk analysis? (2) How effective are various LLMs in terms of\nreadability, duplicate detection, and risk identification in their\nsummarization quality? In this paper, we conducted an offline study using a\nrange of publicly available LLMs at the time and complemented it with a user\nstudy focused on the top performing systems of the offline experiments to\nevaluate their effectiveness further. Our results demonstrate that LLMs,\nparticularly Few-Shot GPT-4o mini, offer significant improvements in summary\nquality and risk identification."
                },
                "authors": [
                    {
                        "name": "Lionel Richy Panlap Houamegni"
                    },
                    {
                        "name": "Fatih Gedikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Gedikli"
                },
                "author": "Fatih Gedikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13929v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13929v5",
                "updated": "2025-02-24T13:24:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    24,
                    20,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-22T18:58:58Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    18,
                    58,
                    58,
                    2,
                    143,
                    0
                ],
                "title": "Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian"
                },
                "summary": "There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available."
                },
                "authors": [
                    {
                        "name": "Aleksandr Nikolich"
                    },
                    {
                        "name": "Konstantin Korolev"
                    },
                    {
                        "name": "Sergei Bratchikov"
                    },
                    {
                        "name": "Igor Kiselev"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "arxiv_comment": "Accepted at WMRL @ EMNLP-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13929v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13929v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17132v1",
                "updated": "2025-02-24T13:21:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    21,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:21:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    21,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "Applications of Large Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of Large Models in Medicine"
                },
                "summary": "This paper explores the advancements and applications of large-scale models\nin the medical field, with a particular focus on Medical Large Models (MedLMs).\nThese models, encompassing Large Language Models (LLMs), Vision Models, 3D\nLarge Models, and Multimodal Models, are revolutionizing healthcare by\nenhancing disease prediction, diagnostic assistance, personalized treatment\nplanning, and drug discovery. The integration of graph neural networks in\nmedical knowledge graphs and drug discovery highlights the potential of Large\nGraph Models (LGMs) in understanding complex biomedical relationships. The\nstudy also emphasizes the transformative role of Vision-Language Models (VLMs)\nand 3D Large Models in medical image analysis, anatomical modeling, and\nprosthetic design. Despite the challenges, these technologies are setting new\nbenchmarks in medical innovation, improving diagnostic accuracy, and paving the\nway for personalized healthcare solutions. This paper aims to provide a\ncomprehensive overview of the current state and future directions of large\nmodels in medicine, underscoring their significance in advancing global health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the advancements and applications of large-scale models\nin the medical field, with a particular focus on Medical Large Models (MedLMs).\nThese models, encompassing Large Language Models (LLMs), Vision Models, 3D\nLarge Models, and Multimodal Models, are revolutionizing healthcare by\nenhancing disease prediction, diagnostic assistance, personalized treatment\nplanning, and drug discovery. The integration of graph neural networks in\nmedical knowledge graphs and drug discovery highlights the potential of Large\nGraph Models (LGMs) in understanding complex biomedical relationships. The\nstudy also emphasizes the transformative role of Vision-Language Models (VLMs)\nand 3D Large Models in medical image analysis, anatomical modeling, and\nprosthetic design. Despite the challenges, these technologies are setting new\nbenchmarks in medical innovation, improving diagnostic accuracy, and paving the\nway for personalized healthcare solutions. This paper aims to provide a\ncomprehensive overview of the current state and future directions of large\nmodels in medicine, underscoring their significance in advancing global health."
                },
                "authors": [
                    {
                        "name": "YunHe Su"
                    },
                    {
                        "name": "Zhengyang Lu"
                    },
                    {
                        "name": "Junhui Liu"
                    },
                    {
                        "name": "Ke Pang"
                    },
                    {
                        "name": "Haoran Dai"
                    },
                    {
                        "name": "Sa Liu Yuxin Jia"
                    },
                    {
                        "name": "Lujia Ge"
                    },
                    {
                        "name": "Jing-min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jing-min Yang"
                },
                "author": "Jing-min Yang",
                "arxiv_doi": "10.71423/aimed.20250105",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.71423/aimed.20250105",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17129v1",
                "updated": "2025-02-24T13:19:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    19,
                    33,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:19:33Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    19,
                    33,
                    0,
                    55,
                    0
                ],
                "title": "Thus Spake Long-Context Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thus Spake Long-Context Large Language Model"
                },
                "summary": "Long context is an important topic in Natural Language Processing (NLP),\nrunning through the development of NLP architectures, and offers immense\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\ncompetitive advantage for LLMs. In the past two years, the context length of\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\nresearch on long-context LLMs has expanded from length extrapolation to a\ncomprehensive focus on architecture, infrastructure, training, and evaluation\ntechnologies.\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\nbetween the journey of extending the context of LLM and the attempts of humans\nto transcend its mortality. In this survey, We will illustrate how LLM\nstruggles between the tremendous need for a longer context and its equal need\nto accept the fact that it is ultimately finite. To achieve this, we give a\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\narchitecture, infrastructure, training, and evaluation, showcasing the full\nspectrum of long-context technologies. At the end of this survey, we will\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\nthis survey can serve as a systematic introduction to the research on\nlong-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context is an important topic in Natural Language Processing (NLP),\nrunning through the development of NLP architectures, and offers immense\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\ncompetitive advantage for LLMs. In the past two years, the context length of\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\nresearch on long-context LLMs has expanded from length extrapolation to a\ncomprehensive focus on architecture, infrastructure, training, and evaluation\ntechnologies.\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\nbetween the journey of extending the context of LLM and the attempts of humans\nto transcend its mortality. In this survey, We will illustrate how LLM\nstruggles between the tremendous need for a longer context and its equal need\nto accept the fact that it is ultimately finite. To achieve this, we give a\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\narchitecture, infrastructure, training, and evaluation, showcasing the full\nspectrum of long-context technologies. At the end of this survey, we will\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\nthis survey can serve as a systematic introduction to the research on\nlong-context LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Mianqiu Huang"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "a global picture of the lifecycle of long-context LLMs from four\n  perspectives: architecture, infrastructure, training, and evaluation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17128v1",
                "updated": "2025-02-24T13:16:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    16,
                    13,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:16:13Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    16,
                    13,
                    0,
                    55,
                    0
                ],
                "title": "Conditional Generative Adversarial Networks for Channel Estimation in\n  RIS-Assisted ISAC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Generative Adversarial Networks for Channel Estimation in\n  RIS-Assisted ISAC Systems"
                },
                "summary": "Integrated sensing and communication (ISAC) technology has been explored as a\npotential advancement for future wireless networks, striving to effectively use\nspectral resources for both communication and sensing. The integration of\nreconfigurable intelligent surfaces (RIS) with ISAC further enhances this\ncapability by optimizing the propagation environment, thereby improving both\nthe sensing accuracy and communication quality. Within this domain, accurate\nchannel estimation is crucial to ensure a reliable deployment. Traditional deep\nlearning (DL) approaches, while effective, can impose performance limitations\nin modeling the complex dynamics of wireless channels. This paper proposes a\nnovel application of conditional generative adversarial networks (CGANs) to\nsolve the channel estimation problem of an RIS-assisted ISAC system. The CGAN\nframework adversarially trains two DL networks, enabling the generator network\nto not only learn the mapping relationship from observed data to real channel\nconditions but also to improve its output based on the discriminator network\nfeedback, thus effectively optimizing the training process and estimation\naccuracy. The numerical simulations demonstrate that the proposed CGAN-based\nmethod improves the estimation performance effectively compared to conventional\nDL techniques. The results highlight the CGAN's potential to revolutionize\nchannel estimation, paving the way for more accurate and reliable ISAC\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated sensing and communication (ISAC) technology has been explored as a\npotential advancement for future wireless networks, striving to effectively use\nspectral resources for both communication and sensing. The integration of\nreconfigurable intelligent surfaces (RIS) with ISAC further enhances this\ncapability by optimizing the propagation environment, thereby improving both\nthe sensing accuracy and communication quality. Within this domain, accurate\nchannel estimation is crucial to ensure a reliable deployment. Traditional deep\nlearning (DL) approaches, while effective, can impose performance limitations\nin modeling the complex dynamics of wireless channels. This paper proposes a\nnovel application of conditional generative adversarial networks (CGANs) to\nsolve the channel estimation problem of an RIS-assisted ISAC system. The CGAN\nframework adversarially trains two DL networks, enabling the generator network\nto not only learn the mapping relationship from observed data to real channel\nconditions but also to improve its output based on the discriminator network\nfeedback, thus effectively optimizing the training process and estimation\naccuracy. The numerical simulations demonstrate that the proposed CGAN-based\nmethod improves the estimation performance effectively compared to conventional\nDL techniques. The results highlight the CGAN's potential to revolutionize\nchannel estimation, paving the way for more accurate and reliable ISAC\ndeployments."
                },
                "authors": [
                    {
                        "name": "Alice Faisal"
                    },
                    {
                        "name": "Ibrahim Al-Nahhal"
                    },
                    {
                        "name": "Kyesan Lee"
                    },
                    {
                        "name": "Octavia A. Dobre"
                    },
                    {
                        "name": "Hyundong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Hyundong Shin"
                },
                "author": "Hyundong Shin",
                "arxiv_doi": "10.1109/TCOMM.2025.3541047",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCOMM.2025.3541047",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.17128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Communications",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17125v1",
                "updated": "2025-02-24T13:11:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    11,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:11:47Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    11,
                    47,
                    0,
                    55,
                    0
                ],
                "title": "LettuceDetect: A Hallucination Detection Framework for RAG Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LettuceDetect: A Hallucination Detection Framework for RAG Applications"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems remain vulnerable to\nhallucinated answers despite incorporating external knowledge sources. We\npresent LettuceDetect a framework that addresses two critical limitations in\nexisting hallucination detection methods: (1) the context window constraints of\ntraditional encoder-based methods, and (2) the computational inefficiency of\nLLM based approaches. Building on ModernBERT's extended context capabilities\n(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach\noutperforms all previous encoder-based models and most prompt-based models,\nwhile being approximately 30 times smaller than the best models. LettuceDetect\nis a token-classification model that processes context-question-answer triples,\nallowing for the identification of unsupported claims at the token level.\nEvaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for\nexample-level detection, which is a 14.8% improvement over Luna, the previous\nstate-of-the-art encoder-based architecture. Additionally, the system can\nprocess 30 to 60 examples per second on a single GPU, making it more practical\nfor real-world RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems remain vulnerable to\nhallucinated answers despite incorporating external knowledge sources. We\npresent LettuceDetect a framework that addresses two critical limitations in\nexisting hallucination detection methods: (1) the context window constraints of\ntraditional encoder-based methods, and (2) the computational inefficiency of\nLLM based approaches. Building on ModernBERT's extended context capabilities\n(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach\noutperforms all previous encoder-based models and most prompt-based models,\nwhile being approximately 30 times smaller than the best models. LettuceDetect\nis a token-classification model that processes context-question-answer triples,\nallowing for the identification of unsupported claims at the token level.\nEvaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for\nexample-level detection, which is a 14.8% improvement over Luna, the previous\nstate-of-the-art encoder-based architecture. Additionally, the system can\nprocess 30 to 60 examples per second on a single GPU, making it more practical\nfor real-world RAG applications."
                },
                "authors": [
                    {
                        "name": "dm Kovcs"
                    },
                    {
                        "name": "Gbor Recski"
                    }
                ],
                "author_detail": {
                    "name": "Gbor Recski"
                },
                "author": "Gbor Recski",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09288v2",
                "updated": "2025-02-24T13:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    10,
                    5,
                    0,
                    55,
                    0
                ],
                "published": "2024-06-13T16:26:37Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    26,
                    37,
                    3,
                    165,
                    0
                ],
                "title": "Large Language Model as a Teacher for Zero-shot Tagging at Extreme\n  Scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model as a Teacher for Zero-shot Tagging at Extreme\n  Scales"
                },
                "summary": "Extreme Multi-label Text Classification (XMC) entails selecting the most\nrelevant labels for an instance from a vast label set. Extreme Zero-shot XMC\n(EZ-XMC) extends this challenge by operating without annotated data, relying\nonly on raw text instances and a predefined label set, making it particularly\ncritical for addressing cold-start problems in large-scale recommendation and\ncategorization systems. State-of-the-art methods, such as MACLR and RTS,\nleverage lightweight bi-encoders but rely on suboptimal pseudo labels for\ntraining, such as document titles (MACLR) or document segments (RTS), which may\nnot align well with the intended tagging or categorization tasks. On the other\nhand, LLM-based approaches, like ICXML, achieve better label-instance alignment\nbut are computationally expensive and impractical for real-world EZ-XMC\napplications due to their heavy inference costs. In this paper, we introduce\nLMTX (Large language Model as Teacher for eXtreme classification), a novel\nframework that bridges the gap between these two approaches. LMTX utilizes an\nLLM to identify high-quality pseudo labels during training, while employing a\nlightweight bi-encoder for efficient inference. This design eliminates the need\nfor LLMs at inference time, offering the benefits of improved label alignment\nwithout sacrificing computational efficiency. Our approach achieves superior\nperformance and efficiency over both LLM and non-LLM based approaches,\nestablishing a new state-of-the-art in EZ-XMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Multi-label Text Classification (XMC) entails selecting the most\nrelevant labels for an instance from a vast label set. Extreme Zero-shot XMC\n(EZ-XMC) extends this challenge by operating without annotated data, relying\nonly on raw text instances and a predefined label set, making it particularly\ncritical for addressing cold-start problems in large-scale recommendation and\ncategorization systems. State-of-the-art methods, such as MACLR and RTS,\nleverage lightweight bi-encoders but rely on suboptimal pseudo labels for\ntraining, such as document titles (MACLR) or document segments (RTS), which may\nnot align well with the intended tagging or categorization tasks. On the other\nhand, LLM-based approaches, like ICXML, achieve better label-instance alignment\nbut are computationally expensive and impractical for real-world EZ-XMC\napplications due to their heavy inference costs. In this paper, we introduce\nLMTX (Large language Model as Teacher for eXtreme classification), a novel\nframework that bridges the gap between these two approaches. LMTX utilizes an\nLLM to identify high-quality pseudo labels during training, while employing a\nlightweight bi-encoder for efficient inference. This design eliminates the need\nfor LLMs at inference time, offering the benefits of improved label alignment\nwithout sacrificing computational efficiency. Our approach achieves superior\nperformance and efficiency over both LLM and non-LLM based approaches,\nestablishing a new state-of-the-art in EZ-XMC."
                },
                "authors": [
                    {
                        "name": "Jinbin Zhang"
                    },
                    {
                        "name": "Nasib Ullah"
                    },
                    {
                        "name": "Rohit Babbar"
                    }
                ],
                "author_detail": {
                    "name": "Rohit Babbar"
                },
                "author": "Rohit Babbar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01702v2",
                "updated": "2025-02-24T12:42:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    42,
                    14,
                    0,
                    55,
                    0
                ],
                "published": "2025-01-03T08:55:19Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    55,
                    19,
                    4,
                    3,
                    0
                ],
                "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning"
                },
                "summary": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Zhuoma Gongque"
                    },
                    {
                        "name": "Weihao Zeng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17100v1",
                "updated": "2025-02-24T12:31:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    31,
                    28,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T12:31:28Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    31,
                    28,
                    0,
                    55,
                    0
                ],
                "title": "Generative Models in Decision Making: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Models in Decision Making: A Survey"
                },
                "summary": "In recent years, the exceptional performance of generative models in\ngenerative tasks has sparked significant interest in their integration into\ndecision-making processes. Due to their ability to handle complex data\ndistributions and their strong model capacity, generative models can be\neffectively incorporated into decision-making systems by generating\ntrajectories that guide agents toward high-reward state-action regions or\nintermediate sub-goals. This paper presents a comprehensive review of the\napplication of generative models in decision-making tasks. We classify seven\nfundamental types of generative models: energy-based models, generative\nadversarial networks, variational autoencoders, normalizing flows, diffusion\nmodels, generative flow networks, and autoregressive models. Regarding their\napplications, we categorize their functions into three main roles: controllers,\nmodelers and optimizers, and discuss how each role contributes to\ndecision-making. Furthermore, we examine the deployment of these models across\nfive critical real-world decision-making scenarios. Finally, we summarize the\nstrengths and limitations of current approaches and propose three key\ndirections for advancing next-generation generative directive models:\nhigh-performance algorithms, large-scale generalized decision-making models,\nand self-evolving and adaptive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the exceptional performance of generative models in\ngenerative tasks has sparked significant interest in their integration into\ndecision-making processes. Due to their ability to handle complex data\ndistributions and their strong model capacity, generative models can be\neffectively incorporated into decision-making systems by generating\ntrajectories that guide agents toward high-reward state-action regions or\nintermediate sub-goals. This paper presents a comprehensive review of the\napplication of generative models in decision-making tasks. We classify seven\nfundamental types of generative models: energy-based models, generative\nadversarial networks, variational autoencoders, normalizing flows, diffusion\nmodels, generative flow networks, and autoregressive models. Regarding their\napplications, we categorize their functions into three main roles: controllers,\nmodelers and optimizers, and discuss how each role contributes to\ndecision-making. Furthermore, we examine the deployment of these models across\nfive critical real-world decision-making scenarios. Finally, we summarize the\nstrengths and limitations of current approaches and propose three key\ndirections for advancing next-generation generative directive models:\nhigh-performance algorithms, large-scale generalized decision-making models,\nand self-evolving and adaptive models."
                },
                "authors": [
                    {
                        "name": "Yinchuan Li"
                    },
                    {
                        "name": "Xinyu Shao"
                    },
                    {
                        "name": "Jianping Zhang"
                    },
                    {
                        "name": "Haozhi Wang"
                    },
                    {
                        "name": "Leo Maxime Brunswic"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Jiqian Dong"
                    },
                    {
                        "name": "Kaiyang Guo"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Zhitang Chen"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04205v2",
                "updated": "2025-02-24T12:23:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    23,
                    3,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-06T16:45:12Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    16,
                    45,
                    12,
                    3,
                    37,
                    0
                ],
                "title": "Beyond 2050: From deployment to renewal of the global solar and wind\n  energy system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond 2050: From deployment to renewal of the global solar and wind\n  energy system"
                },
                "summary": "The global energy transition depends on large-scale photovoltaic (PV) and\nwind power deployment. While 2050 targets suggest a transition endpoint,\nmaintaining these systems beyond mid-century requires continuous renewal,\nmarking a fundamental yet often overlooked shift in industrial dynamics. This\nstudy examines the transition from initial deployment to long-term renewal,\nusing a two-phase growth model: an exponential expansion followed by capacity\nstabilization. By integrating this pattern with a Weibull distribution of PV\npanel and wind turbine lifespans, we estimate the annual production required\nfor both expansion and maintenance. Our findings highlight two key factors\ninfluencing production dynamics: deployment speed and lifespan. When deployment\noccurs faster than the average lifespan, production overshoots and exhibits\ndamped oscillations due to successive installation and replacement cycles. In\ncontrast, gradual deployment leads to a smooth increase before stabilizing at\nthe renewal rate. Given current scenarios, the PV industry is likely to\nexperience significant oscillations - ranging from 15 % to 60 % of global\nproduction - while wind power follows a monotonic growth trajectory. These\noscillations, driven by ambitious energy targets, may result in cycles of\noverproduction and underproduction, affecting industrial stability. Beyond\nsolar and wind, this study underscores a broader challenge in the energy\ntransition: shifting from infrastructure expansion to long-term maintenance.\nAddressing this phase is crucial for ensuring the resilience and sustainability\nof renewable energy systems beyond 2050.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global energy transition depends on large-scale photovoltaic (PV) and\nwind power deployment. While 2050 targets suggest a transition endpoint,\nmaintaining these systems beyond mid-century requires continuous renewal,\nmarking a fundamental yet often overlooked shift in industrial dynamics. This\nstudy examines the transition from initial deployment to long-term renewal,\nusing a two-phase growth model: an exponential expansion followed by capacity\nstabilization. By integrating this pattern with a Weibull distribution of PV\npanel and wind turbine lifespans, we estimate the annual production required\nfor both expansion and maintenance. Our findings highlight two key factors\ninfluencing production dynamics: deployment speed and lifespan. When deployment\noccurs faster than the average lifespan, production overshoots and exhibits\ndamped oscillations due to successive installation and replacement cycles. In\ncontrast, gradual deployment leads to a smooth increase before stabilizing at\nthe renewal rate. Given current scenarios, the PV industry is likely to\nexperience significant oscillations - ranging from 15 % to 60 % of global\nproduction - while wind power follows a monotonic growth trajectory. These\noscillations, driven by ambitious energy targets, may result in cycles of\noverproduction and underproduction, affecting industrial stability. Beyond\nsolar and wind, this study underscores a broader challenge in the energy\ntransition: shifting from infrastructure expansion to long-term maintenance.\nAddressing this phase is crucial for ensuring the resilience and sustainability\nof renewable energy systems beyond 2050."
                },
                "authors": [
                    {
                        "name": "Joseph Le Bihan"
                    },
                    {
                        "name": "Thomas Lapi"
                    },
                    {
                        "name": "Jos Halloy"
                    }
                ],
                "author_detail": {
                    "name": "Jos Halloy"
                },
                "author": "Jos Halloy",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "93-10, 91B74",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17091v1",
                "updated": "2025-02-24T12:14:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    14,
                    5,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T12:14:05Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    14,
                    5,
                    0,
                    55,
                    0
                ],
                "title": "WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring\n  Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring\n  Texts"
                },
                "summary": "Humans are influenced by how information is presented, a phenomenon known as\nthe framing effect. Previous work has shown that LLMs may also be susceptible\nto framing but has done so on synthetic data and did not compare to human\nbehavior. We introduce WildFrame, a dataset for evaluating LLM responses to\npositive and negative framing, in naturally-occurring sentences, and compare\nhumans on the same data. WildFrame consists of 1,000 texts, first selecting\nreal-world statements with clear sentiment, then reframing them in either\npositive or negative light, and lastly, collecting human sentiment annotations.\nBy evaluating eight state-of-the-art LLMs on WildFrame, we find that all models\nexhibit framing effects similar to humans ($r\\geq0.57$), with both humans and\nmodels being more influenced by positive rather than negative reframing. Our\nfindings benefit model developers, who can either harness framing or mitigate\nits effects, depending on the downstream application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans are influenced by how information is presented, a phenomenon known as\nthe framing effect. Previous work has shown that LLMs may also be susceptible\nto framing but has done so on synthetic data and did not compare to human\nbehavior. We introduce WildFrame, a dataset for evaluating LLM responses to\npositive and negative framing, in naturally-occurring sentences, and compare\nhumans on the same data. WildFrame consists of 1,000 texts, first selecting\nreal-world statements with clear sentiment, then reframing them in either\npositive or negative light, and lastly, collecting human sentiment annotations.\nBy evaluating eight state-of-the-art LLMs on WildFrame, we find that all models\nexhibit framing effects similar to humans ($r\\geq0.57$), with both humans and\nmodels being more influenced by positive rather than negative reframing. Our\nfindings benefit model developers, who can either harness framing or mitigate\nits effects, depending on the downstream application."
                },
                "authors": [
                    {
                        "name": "Gili Lior"
                    },
                    {
                        "name": "Liron Nacchace"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15164v2",
                "updated": "2025-02-24T12:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    11,
                    40,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-19T17:28:48Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    17,
                    28,
                    48,
                    5,
                    293,
                    0
                ],
                "title": "SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPA-Bench: A Comprehensive Benchmark for SmartPhone Agent Evaluation"
                },
                "summary": "Smartphone agents are increasingly important for helping users control\ndevices efficiently, with (Multimodal) Large Language Model (MLLM)-based\napproaches emerging as key contenders. Fairly comparing these agents is\nessential but challenging, requiring a varied task scope, the integration of\nagents with different implementations, and a generalisable evaluation pipeline\nto assess their strengths and weaknesses. In this paper, we present SPA-B ENCH,\na comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based\nagents in an interactive environment that simulates real-world conditions.\nSPA-B ENCH offers three key contributions: (1) A diverse set of tasks covering\nsystem and third-party apps in both English and Chinese, focusing on features\ncommonly used in daily routines; (2) A plug-and-play framework enabling\nreal-time agent interaction with Android devices, integrating over ten agents\nwith the flexibility to add more; (3) A novel evaluation pipeline that\nautomatically assesses agent performance across multiple dimensions,\nencompassing seven metrics related to task completion and resource consumption.\nOur extensive experiments across tasks and agents reveal challenges like\ninterpreting mobile user interfaces, action grounding, memory retention, and\nexecution costs. We propose future research directions to ease these\ndifficulties, moving closer to real-world smartphone agent applications. SPA-B\nENCH is available at https://ai-agents-2030.github.io/SPA-Bench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smartphone agents are increasingly important for helping users control\ndevices efficiently, with (Multimodal) Large Language Model (MLLM)-based\napproaches emerging as key contenders. Fairly comparing these agents is\nessential but challenging, requiring a varied task scope, the integration of\nagents with different implementations, and a generalisable evaluation pipeline\nto assess their strengths and weaknesses. In this paper, we present SPA-B ENCH,\na comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based\nagents in an interactive environment that simulates real-world conditions.\nSPA-B ENCH offers three key contributions: (1) A diverse set of tasks covering\nsystem and third-party apps in both English and Chinese, focusing on features\ncommonly used in daily routines; (2) A plug-and-play framework enabling\nreal-time agent interaction with Android devices, integrating over ten agents\nwith the flexibility to add more; (3) A novel evaluation pipeline that\nautomatically assesses agent performance across multiple dimensions,\nencompassing seven metrics related to task completion and resource consumption.\nOur extensive experiments across tasks and agents reveal challenges like\ninterpreting mobile user interfaces, action grounding, memory retention, and\nexecution costs. We propose future research directions to ease these\ndifficulties, moving closer to real-world smartphone agent applications. SPA-B\nENCH is available at https://ai-agents-2030.github.io/SPA-Bench/."
                },
                "authors": [
                    {
                        "name": "Jingxuan Chen"
                    },
                    {
                        "name": "Derek Yuen"
                    },
                    {
                        "name": "Bin Xie"
                    },
                    {
                        "name": "Yuhao Yang"
                    },
                    {
                        "name": "Gongwei Chen"
                    },
                    {
                        "name": "Zhihao Wu"
                    },
                    {
                        "name": "Li Yixing"
                    },
                    {
                        "name": "Xurui Zhou"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "Rui Shao"
                    },
                    {
                        "name": "Liqiang Nie"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kun Shao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Shao"
                },
                "author": "Kun Shao",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17086v1",
                "updated": "2025-02-24T12:05:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    5,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T12:05:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    5,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Automatically Evaluating the Paper Reviewing Capability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Evaluating the Paper Reviewing Capability of Large\n  Language Models"
                },
                "summary": "Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time."
                },
                "authors": [
                    {
                        "name": "Hyungyu Shin"
                    },
                    {
                        "name": "Jingyu Tang"
                    },
                    {
                        "name": "Yoonjoo Lee"
                    },
                    {
                        "name": "Nayoung Kim"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Ji Yong Cho"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05093v2",
                "updated": "2025-02-24T11:45:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    45,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-08T13:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    13,
                    38,
                    59,
                    6,
                    252,
                    0
                ],
                "title": "CloudNativeSim: a toolkit for modeling and simulation of cloud-native\n  applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudNativeSim: a toolkit for modeling and simulation of cloud-native\n  applications"
                },
                "summary": "Cloud-native applications are increasingly becoming popular in modern\nsoftware design. Employing a microservice-based architecture into these\napplications is a prevalent strategy that enhances system availability and\nflexibility. However, cloud-native applications also introduce new challenges,\nsuch as frequent inter-service communication and the complexity of managing\nheterogeneous codebases and hardware, resulting in unpredictable complexity and\ndynamism. Furthermore, as applications scale, only limited research teams or\nenterprises possess the resources for large-scale deployment and testing, which\nimpedes progress in the cloud-native domain. To address these challenges, we\npropose CloudNativeSim, a simulator for cloud-native applications with a\nmicroservice-based architecture. CloudNativeSim offers several key benefits:\n(i) comprehensive and dynamic modeling for cloud-native applications, (ii) an\nextended simulation framework with new policy interfaces for scheduling\ncloud-native applications, and (iii) support for customized application\nscenarios and user feedback based on Quality of Service (QoS) metrics.\nCloudNativeSim can be easily deployed on standard computers to manage a high\nvolume of requests and services. Its performance was validated through a case\nstudy, demonstrating higher than 94.5% accuracy in terms of response time. The\nstudy further highlights the feasibility of CloudNativeSim by illustrating the\neffects of various scaling policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-native applications are increasingly becoming popular in modern\nsoftware design. Employing a microservice-based architecture into these\napplications is a prevalent strategy that enhances system availability and\nflexibility. However, cloud-native applications also introduce new challenges,\nsuch as frequent inter-service communication and the complexity of managing\nheterogeneous codebases and hardware, resulting in unpredictable complexity and\ndynamism. Furthermore, as applications scale, only limited research teams or\nenterprises possess the resources for large-scale deployment and testing, which\nimpedes progress in the cloud-native domain. To address these challenges, we\npropose CloudNativeSim, a simulator for cloud-native applications with a\nmicroservice-based architecture. CloudNativeSim offers several key benefits:\n(i) comprehensive and dynamic modeling for cloud-native applications, (ii) an\nextended simulation framework with new policy interfaces for scheduling\ncloud-native applications, and (iii) support for customized application\nscenarios and user feedback based on Quality of Service (QoS) metrics.\nCloudNativeSim can be easily deployed on standard computers to manage a high\nvolume of requests and services. Its performance was validated through a case\nstudy, demonstrating higher than 94.5% accuracy in terms of response time. The\nstudy further highlights the feasibility of CloudNativeSim by illustrating the\neffects of various scaling policies."
                },
                "authors": [
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "24 pages",
                "arxiv_journal_ref": "Software: Practice and Experience 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17071v1",
                "updated": "2025-02-24T11:34:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    34,
                    49,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T11:34:49Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    34,
                    49,
                    0,
                    55,
                    0
                ],
                "title": "Systematic Weight Evaluation for Pruning Large Language Models:\n  Enhancing Performance and Sustainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Weight Evaluation for Pruning Large Language Models:\n  Enhancing Performance and Sustainability"
                },
                "summary": "The exponential growth of large language models (LLMs) like ChatGPT has\nrevolutionized artificial intelligence, offering unprecedented capabilities in\nnatural language processing. However, the extensive computational resources\nrequired for training these models have significant environmental implications,\nincluding high carbon emissions, energy consumption, and water usage. This\nresearch presents a novel approach to LLM pruning, focusing on the systematic\nevaluation of individual weight importance throughout the training process. By\nmonitoring parameter evolution over time, we propose a method that effectively\nreduces model size without compromising performance. Extensive experiments with\nboth a scaled-down LLM and a large multimodal model reveal that moderate\npruning enhances efficiency and reduces loss, while excessive pruning\ndrastically deteriorates model performance. These findings highlight the\ncritical need for optimized AI models to ensure sustainable development,\nbalancing technological advancement with environmental responsibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of large language models (LLMs) like ChatGPT has\nrevolutionized artificial intelligence, offering unprecedented capabilities in\nnatural language processing. However, the extensive computational resources\nrequired for training these models have significant environmental implications,\nincluding high carbon emissions, energy consumption, and water usage. This\nresearch presents a novel approach to LLM pruning, focusing on the systematic\nevaluation of individual weight importance throughout the training process. By\nmonitoring parameter evolution over time, we propose a method that effectively\nreduces model size without compromising performance. Extensive experiments with\nboth a scaled-down LLM and a large multimodal model reveal that moderate\npruning enhances efficiency and reduces loss, while excessive pruning\ndrastically deteriorates model performance. These findings highlight the\ncritical need for optimized AI models to ensure sustainable development,\nbalancing technological advancement with environmental responsibility."
                },
                "authors": [
                    {
                        "name": "Ashhadul Islam"
                    },
                    {
                        "name": "Samir Brahim Belhaouari"
                    },
                    {
                        "name": "Amine Bermak"
                    }
                ],
                "author_detail": {
                    "name": "Amine Bermak"
                },
                "author": "Amine Bermak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15022v2",
                "updated": "2025-02-24T11:28:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    28,
                    32,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-20T20:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    20,
                    16,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "A Meta-Evaluation of Style and Attribute Transfer Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Meta-Evaluation of Style and Attribute Transfer Metrics"
                },
                "summary": "LLMs make it easy to rewrite text in any style, be it more polite,\npersuasive, or more positive. We present a large-scale study of evaluation\nmetrics for style and attribute transfer with a focus on content preservation;\nmeaning content not attributed to the style shift is preserved. The de facto\nevaluation approach uses lexical or semantic similarity metrics often between\nsource sentences and rewrites. While these metrics are not designed to\ndistinguish between style or content differences, empirical meta-evaluation\nshows a reasonable correlation to human judgment. In fact, recent works find\nthat LLMs prompted as evaluators are only comparable to semantic similarity\nmetrics, even though intuitively, the LLM approach should better fit the task.\nTo investigate this discrepancy, we benchmark 8 metrics for evaluating content\npreservation on existing datasets and additionally construct a new test set\nthat better aligns with the meta-evaluation aim. Indeed, we then find that the\nempirical conclusion aligns with the intuition: content preservation metrics\nfor style/attribute transfer must be conditional on the style shift. To support\nthis, we propose a new efficient zero-shot evaluation method using the\nlikelihood of the next token. We hope our meta-evaluation can foster more\nresearch on evaluating content preservation metrics, and also to ensure fair\nevaluation of methods for conducting style transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs make it easy to rewrite text in any style, be it more polite,\npersuasive, or more positive. We present a large-scale study of evaluation\nmetrics for style and attribute transfer with a focus on content preservation;\nmeaning content not attributed to the style shift is preserved. The de facto\nevaluation approach uses lexical or semantic similarity metrics often between\nsource sentences and rewrites. While these metrics are not designed to\ndistinguish between style or content differences, empirical meta-evaluation\nshows a reasonable correlation to human judgment. In fact, recent works find\nthat LLMs prompted as evaluators are only comparable to semantic similarity\nmetrics, even though intuitively, the LLM approach should better fit the task.\nTo investigate this discrepancy, we benchmark 8 metrics for evaluating content\npreservation on existing datasets and additionally construct a new test set\nthat better aligns with the meta-evaluation aim. Indeed, we then find that the\nempirical conclusion aligns with the intuition: content preservation metrics\nfor style/attribute transfer must be conditional on the style shift. To support\nthis, we propose a new efficient zero-shot evaluation method using the\nlikelihood of the next token. We hope our meta-evaluation can foster more\nresearch on evaluating content preservation metrics, and also to ensure fair\nevaluation of methods for conducting style transfer."
                },
                "authors": [
                    {
                        "name": "Amalie Brogaard Pauli"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "Ira Assent"
                    }
                ],
                "author_detail": {
                    "name": "Ira Assent"
                },
                "author": "Ira Assent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03758v2",
                "updated": "2025-02-24T11:27:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    27,
                    10,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-04T22:53:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    22,
                    53,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "ARCON: Advancing Auto-Regressive Continuation for Video Frames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCON: Advancing Auto-Regressive Continuation for Video Frames"
                },
                "summary": "Recent advancements in auto-regressive large language models (LLMs) have led\nto their application in video generation. This paper explores the use of Large\nVision Models (LVMs) for video continuation, a task essential for building\nworld models and predicting future frames. We introduce ARCON, a scheme that\nalternates between generating semantic and RGB tokens, allowing the LVM to\nexplicitly learn high-level structural video information. We find high\nconsistency in the RGB images and semantic maps generated without special\ndesign. Moreover, we employ an optical flow-based texture stitching method to\nenhance visual quality. Experiments in autonomous driving scenarios show that\nour model can consistently generate long videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in auto-regressive large language models (LLMs) have led\nto their application in video generation. This paper explores the use of Large\nVision Models (LVMs) for video continuation, a task essential for building\nworld models and predicting future frames. We introduce ARCON, a scheme that\nalternates between generating semantic and RGB tokens, allowing the LVM to\nexplicitly learn high-level structural video information. We find high\nconsistency in the RGB images and semantic maps generated without special\ndesign. Moreover, we employ an optical flow-based texture stitching method to\nenhance visual quality. Experiments in autonomous driving scenarios show that\nour model can consistently generate long videos."
                },
                "authors": [
                    {
                        "name": "Ruibo Ming"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhuoxuan Ju"
                    },
                    {
                        "name": "Jianming HU"
                    },
                    {
                        "name": "Lihui Peng"
                    },
                    {
                        "name": "Shuchang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuchang Zhou"
                },
                "author": "Shuchang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17057v1",
                "updated": "2025-02-24T11:15:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    15,
                    41,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T11:15:41Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    15,
                    41,
                    0,
                    55,
                    0
                ],
                "title": "LLM-QE: Improving Query Expansion by Aligning Large Language Models with\n  Ranking Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-QE: Improving Query Expansion by Aligning Large Language Models with\n  Ranking Preferences"
                },
                "summary": "Query expansion plays a crucial role in information retrieval, which aims to\nbridge the semantic gap between queries and documents to improve matching\nperformance. This paper introduces LLM-QE, a novel approach that leverages\nLarge Language Models (LLMs) to generate document-based query expansions,\nthereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE\ndesigns both rank-based and answer-based rewards and uses these reward models\nto optimize LLMs to align with the ranking preferences of both retrievers and\nLLMs, thus mitigating the hallucination of LLMs during query expansion. Our\nexperiments on the zero-shot dense retrieval model, Contriever, demonstrate the\neffectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by\nincorporating answer-based reward modeling, LLM-QE generates more relevant and\nprecise information related to the documents, rather than simply producing\nredundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves\nthe training process of dense retrievers, achieving a more than 5% improvement\nafter fine-tuning. All codes are available at https://github.com/NEUIR/LLM-QE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query expansion plays a crucial role in information retrieval, which aims to\nbridge the semantic gap between queries and documents to improve matching\nperformance. This paper introduces LLM-QE, a novel approach that leverages\nLarge Language Models (LLMs) to generate document-based query expansions,\nthereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE\ndesigns both rank-based and answer-based rewards and uses these reward models\nto optimize LLMs to align with the ranking preferences of both retrievers and\nLLMs, thus mitigating the hallucination of LLMs during query expansion. Our\nexperiments on the zero-shot dense retrieval model, Contriever, demonstrate the\neffectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by\nincorporating answer-based reward modeling, LLM-QE generates more relevant and\nprecise information related to the documents, rather than simply producing\nredundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves\nthe training process of dense retrievers, achieving a more than 5% improvement\nafter fine-tuning. All codes are available at https://github.com/NEUIR/LLM-QE."
                },
                "authors": [
                    {
                        "name": "Sijia Yao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "13 pages, 5 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17055v1",
                "updated": "2025-02-24T11:09:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    9,
                    15,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T11:09:15Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    9,
                    15,
                    0,
                    55,
                    0
                ],
                "title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam"
                },
                "summary": "This paper comprehensively evaluates several recently proposed optimizers for\n4-bit training, revealing that low-bit precision amplifies sensitivity to\nlearning rates and often causes unstable gradient norms, leading to divergence\nat higher learning rates. Among these, SPAM, a recent optimizer featuring\nmomentum reset and spike-aware gradient clipping, achieves the best performance\nacross various bit levels, but struggles to stabilize gradient norms, requiring\ncareful learning rate tuning. To address these limitations, we propose\nStable-SPAM, which incorporates enhanced gradient normalization and clipping\ntechniques. In particular, Stable-SPAM (1) adaptively updates the clipping\nthreshold for spiked gradients by tracking their historical maxima; (2)\nnormalizes the entire gradient matrix based on its historical $l_2$-norm\nstatistics; and $(3)$ inherits momentum reset from SPAM to periodically reset\nthe first and second moments of Adam, mitigating the accumulation of spiked\ngradients. Extensive experiments show that Stable-SPAM effectively stabilizes\ngradient norms in 4-bit LLM training, delivering superior performance compared\nto Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM\noutperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity.\nFurthermore, when both models are trained in 4-bit, Stable-SPAM achieves the\nsame loss as Adam while requiring only about half the training steps. Code is\navailable at https://github.com/TianjinYellow/StableSPAM.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper comprehensively evaluates several recently proposed optimizers for\n4-bit training, revealing that low-bit precision amplifies sensitivity to\nlearning rates and often causes unstable gradient norms, leading to divergence\nat higher learning rates. Among these, SPAM, a recent optimizer featuring\nmomentum reset and spike-aware gradient clipping, achieves the best performance\nacross various bit levels, but struggles to stabilize gradient norms, requiring\ncareful learning rate tuning. To address these limitations, we propose\nStable-SPAM, which incorporates enhanced gradient normalization and clipping\ntechniques. In particular, Stable-SPAM (1) adaptively updates the clipping\nthreshold for spiked gradients by tracking their historical maxima; (2)\nnormalizes the entire gradient matrix based on its historical $l_2$-norm\nstatistics; and $(3)$ inherits momentum reset from SPAM to periodically reset\nthe first and second moments of Adam, mitigating the accumulation of spiked\ngradients. Extensive experiments show that Stable-SPAM effectively stabilizes\ngradient norms in 4-bit LLM training, delivering superior performance compared\nto Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM\noutperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity.\nFurthermore, when both models are trained in 4-bit, Stable-SPAM achieves the\nsame loss as Adam while requiring only about half the training steps. Code is\navailable at https://github.com/TianjinYellow/StableSPAM.git."
                },
                "authors": [
                    {
                        "name": "Tianjin Huang"
                    },
                    {
                        "name": "Haotian Hu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Gaojie Jin"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05806v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05806v3",
                "updated": "2025-02-24T11:02:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    2,
                    47,
                    0,
                    55,
                    0
                ],
                "published": "2024-09-09T17:11:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs"
                },
                "summary": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Tianhe Lu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ziyan Jiang"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05806v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05806v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10250v2",
                "updated": "2025-02-24T10:58:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    58,
                    12,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-14T15:59:33Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    59,
                    33,
                    4,
                    45,
                    0
                ],
                "title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models"
                },
                "summary": "Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a 'leaky modality mix', where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a 'leaky modality mix', where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset."
                },
                "authors": [
                    {
                        "name": "Gokul Karthik Kumar"
                    },
                    {
                        "name": "Iheb Chaabane"
                    },
                    {
                        "name": "Kebin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kebin Wu"
                },
                "author": "Kebin Wu",
                "arxiv_comment": "Accepted at PAKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03438v2",
                "updated": "2025-02-24T10:56:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    56,
                    2,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-05T18:33:36Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    33,
                    36,
                    2,
                    36,
                    0
                ],
                "title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating the underlying large proof search spaces.\nWhile the existing approaches primarily rely on value functions and/or Monte\nCarlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree\nSearch (BFS) remains underexplored. In this paper, we investigate whether BFS\ncan achieve competitive performance in large-scale theorem proving tasks. We\npresent BFS-Prover, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. BFS-Prover achieves a\nstate-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore\nchallenges the perceived necessity of complex tree search methods,\ndemonstrating that BFS can achieve competitive performance when properly\nscaled. To facilitate further research and development in this area, we have\nopen-sourced our model at https://huggingface.co/bytedance-research/BFS-Prover.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating the underlying large proof search spaces.\nWhile the existing approaches primarily rely on value functions and/or Monte\nCarlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree\nSearch (BFS) remains underexplored. In this paper, we investigate whether BFS\ncan achieve competitive performance in large-scale theorem proving tasks. We\npresent BFS-Prover, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. BFS-Prover achieves a\nstate-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore\nchallenges the perceived necessity of complex tree search methods,\ndemonstrating that BFS can achieve competitive performance when properly\nscaled. To facilitate further research and development in this area, we have\nopen-sourced our model at https://huggingface.co/bytedance-research/BFS-Prover."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Xia Xiao"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "Kai Shen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shen"
                },
                "author": "Kai Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17041v1",
                "updated": "2025-02-24T10:49:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    49,
                    34,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T10:49:34Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    49,
                    34,
                    0,
                    55,
                    0
                ],
                "title": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal\n  Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal\n  Compliance"
                },
                "summary": "Recent advancements in generative large language models (LLMs) have enabled\nwider applicability, accessibility, and flexibility. However, their reliability\nand trustworthiness are still in doubt, especially for concerns regarding\nindividuals' data privacy. Great efforts have been made on privacy by building\nvarious evaluation benchmarks to study LLMs' privacy awareness and robustness\nfrom their generated outputs to their hidden representations. Unfortunately,\nmost of these works adopt a narrow formulation of privacy and only investigate\npersonally identifiable information (PII). In this paper, we follow the merit\nof the Contextual Integrity (CI) theory, which posits that privacy evaluation\nshould not only cover the transmitted attributes but also encompass the whole\nrelevant social context through private information flows. We present\nPrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted\nat legal compliance to cover well-annotated privacy and safety regulations,\nreal court cases, privacy policies, and synthetic data built from the official\ntoolkit to study LLMs' privacy and safety compliance. We evaluate the latest\nLLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our\nexperimental results suggest that though LLMs can effectively capture key CI\nparameters inside a given context, they still require further advancements for\nprivacy compliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative large language models (LLMs) have enabled\nwider applicability, accessibility, and flexibility. However, their reliability\nand trustworthiness are still in doubt, especially for concerns regarding\nindividuals' data privacy. Great efforts have been made on privacy by building\nvarious evaluation benchmarks to study LLMs' privacy awareness and robustness\nfrom their generated outputs to their hidden representations. Unfortunately,\nmost of these works adopt a narrow formulation of privacy and only investigate\npersonally identifiable information (PII). In this paper, we follow the merit\nof the Contextual Integrity (CI) theory, which posits that privacy evaluation\nshould not only cover the transmitted attributes but also encompass the whole\nrelevant social context through private information flows. We present\nPrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted\nat legal compliance to cover well-annotated privacy and safety regulations,\nreal court cases, privacy policies, and synthetic data built from the official\ntoolkit to study LLMs' privacy and safety compliance. We evaluate the latest\nLLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our\nexperimental results suggest that though LLMs can effectively capture key CI\nparameters inside a given context, they still require further advancements for\nprivacy compliance."
                },
                "authors": [
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Qi Hu"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Tianshu Chu"
                    },
                    {
                        "name": "Peizhao Hu"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "Project Webpage: https://hkust-knowcomp.github.io/privacy/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17026v1",
                "updated": "2025-02-24T10:28:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    28,
                    21,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T10:28:21Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    28,
                    21,
                    0,
                    55,
                    0
                ],
                "title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based\n  on Reasoning Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Uncertainty of LLM Explanations: A Perspective Based\n  on Reasoning Topology"
                },
                "summary": "Understanding the uncertainty in large language model (LLM) explanations is\nimportant for evaluating their faithfulness and reasoning consistency, and thus\nprovides insights into the reliability of LLM's output regarding a question. In\nthis work, we propose a novel framework that quantifies uncertainty in LLM\nexplanations through a reasoning topology perspective. By designing a\nstructural elicitation strategy, we guide the LLMs to frame the explanations of\nan answer into a graph topology. This process decomposes the explanations into\nthe knowledge related sub-questions and topology-based reasoning structures,\nwhich allows us to quantify uncertainty not only at the semantic level but also\nfrom the reasoning path. It further brings convenience to assess knowledge\nredundancy and provide interpretable insights into the reasoning process. Our\nmethod offers a systematic way to interpret the LLM reasoning, analyze\nlimitations, and provide guidance for enhancing robustness and faithfulness.\nThis work pioneers the use of graph-structured uncertainty measurement in LLM\nexplanations and demonstrates the potential of topology-based quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the uncertainty in large language model (LLM) explanations is\nimportant for evaluating their faithfulness and reasoning consistency, and thus\nprovides insights into the reliability of LLM's output regarding a question. In\nthis work, we propose a novel framework that quantifies uncertainty in LLM\nexplanations through a reasoning topology perspective. By designing a\nstructural elicitation strategy, we guide the LLMs to frame the explanations of\nan answer into a graph topology. This process decomposes the explanations into\nthe knowledge related sub-questions and topology-based reasoning structures,\nwhich allows us to quantify uncertainty not only at the semantic level but also\nfrom the reasoning path. It further brings convenience to assess knowledge\nredundancy and provide interpretable insights into the reasoning process. Our\nmethod offers a systematic way to interpret the LLM reasoning, analyze\nlimitations, and provide guidance for enhancing robustness and faithfulness.\nThis work pioneers the use of graph-structured uncertainty measurement in LLM\nexplanations and demonstrates the potential of topology-based quantification."
                },
                "authors": [
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Xiaoou Liu"
                    },
                    {
                        "name": "Jiaxin Dai"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Yaqing Wang"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T37, 68Q32",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17024v1",
                "updated": "2025-02-24T10:26:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    26,
                    29,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T10:26:29Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    26,
                    29,
                    0,
                    55,
                    0
                ],
                "title": "Towards Auto-Regressive Next-Token Prediction: In-Context Learning\n  Emerges from Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Auto-Regressive Next-Token Prediction: In-Context Learning\n  Emerges from Generalization"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets."
                },
                "authors": [
                    {
                        "name": "Zixuan Gong"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Huayi Tang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17017v1",
                "updated": "2025-02-24T10:02:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    2,
                    50,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T10:02:50Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    2,
                    50,
                    0,
                    55,
                    0
                ],
                "title": "Quantifying Logical Consistency in Transformers via Query-Key Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Logical Consistency in Transformers via Query-Key Alignment"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance in\nvarious natural language processing tasks, yet their ability to perform\nmulti-step logical reasoning remains an open challenge. Although\nChain-of-Thought prompting has improved logical reasoning by enabling models to\ngenerate intermediate steps, it lacks mechanisms to assess the coherence of\nthese logical transitions. In this paper, we propose a novel, lightweight\nevaluation strategy for logical reasoning that uses query-key alignments inside\ntransformer attention heads. By computing a single forward pass and extracting\na \"QK-score\" from carefully chosen heads, our method reveals latent\nrepresentations that reliably separate valid from invalid inferences, offering\na scalable alternative to traditional ablation-based techniques. We also\nprovide an empirical validation on multiple logical reasoning benchmarks,\ndemonstrating improved robustness of our evaluation method against distractors\nand increased reasoning depth. The experiments were conducted on a diverse set\nof models, ranging from 1.5B to 70B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance in\nvarious natural language processing tasks, yet their ability to perform\nmulti-step logical reasoning remains an open challenge. Although\nChain-of-Thought prompting has improved logical reasoning by enabling models to\ngenerate intermediate steps, it lacks mechanisms to assess the coherence of\nthese logical transitions. In this paper, we propose a novel, lightweight\nevaluation strategy for logical reasoning that uses query-key alignments inside\ntransformer attention heads. By computing a single forward pass and extracting\na \"QK-score\" from carefully chosen heads, our method reveals latent\nrepresentations that reliably separate valid from invalid inferences, offering\na scalable alternative to traditional ablation-based techniques. We also\nprovide an empirical validation on multiple logical reasoning benchmarks,\ndemonstrating improved robustness of our evaluation method against distractors\nand increased reasoning depth. The experiments were conducted on a diverse set\nof models, ranging from 1.5B to 70B parameters."
                },
                "authors": [
                    {
                        "name": "Eduard Tulchinskii"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Laida Kushnareva"
                    },
                    {
                        "name": "Andrei Andriiainen"
                    },
                    {
                        "name": "Irina Piontkovskaya"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Serguei Barannikov"
                    }
                ],
                "author_detail": {
                    "name": "Serguei Barannikov"
                },
                "author": "Serguei Barannikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19799v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19799v4",
                "updated": "2025-02-24T09:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    50,
                    0,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-30T08:10:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    8,
                    10,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation\n  in Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation\n  in Dialogue"
                },
                "summary": "In dialogue systems, discourse plays a crucial role in managing\nconversational focus and coordinating interactions. It consists of two key\nstructures: rhetorical structure and topic structure. The former captures the\nlogical flow of conversations, while the latter detects transitions between\ntopics. Together, they improve the ability of a dialogue system to track\nconversation dynamics and generate contextually relevant high-quality\nresponses. These structures are typically identified through discourse parsing\nand topic segmentation, respectively. However, existing supervised methods rely\non costly manual annotations, while unsupervised methods often focus on a\nsingle task, overlooking the deep linguistic interplay between rhetorical and\ntopic structures. To address these issues, we first introduce a unified\nrepresentation that integrates rhetorical and topic structures, ensuring\nsemantic consistency between them. Under the unified representation, we further\npropose two linguistically grounded hypotheses based on discourse theories: (1)\nLocal Discourse Coupling, where rhetorical cues dynamically enhance topic-aware\ninformation flow, and (2) Global Topology Constraint, where topic structure\npatterns probabilistically constrain rhetorical relation distributions.\nBuilding on the unified representation and two hypotheses, we propose an\nunsupervised mutual learning framework (UMLF) that jointly models rhetorical\nand topic structures, allowing them to mutually reinforce each other without\nrequiring additional annotations. We evaluate our approach on two rhetorical\ndatasets and three topic segmentation datasets. Experimental results\ndemonstrate that our method surpasses all strong baselines built on pre-trained\nlanguage models. Furthermore, when applied to LLMs, our framework achieves\nnotable improvements, demonstrating its effectiveness in improving discourse\nstructure modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dialogue systems, discourse plays a crucial role in managing\nconversational focus and coordinating interactions. It consists of two key\nstructures: rhetorical structure and topic structure. The former captures the\nlogical flow of conversations, while the latter detects transitions between\ntopics. Together, they improve the ability of a dialogue system to track\nconversation dynamics and generate contextually relevant high-quality\nresponses. These structures are typically identified through discourse parsing\nand topic segmentation, respectively. However, existing supervised methods rely\non costly manual annotations, while unsupervised methods often focus on a\nsingle task, overlooking the deep linguistic interplay between rhetorical and\ntopic structures. To address these issues, we first introduce a unified\nrepresentation that integrates rhetorical and topic structures, ensuring\nsemantic consistency between them. Under the unified representation, we further\npropose two linguistically grounded hypotheses based on discourse theories: (1)\nLocal Discourse Coupling, where rhetorical cues dynamically enhance topic-aware\ninformation flow, and (2) Global Topology Constraint, where topic structure\npatterns probabilistically constrain rhetorical relation distributions.\nBuilding on the unified representation and two hypotheses, we propose an\nunsupervised mutual learning framework (UMLF) that jointly models rhetorical\nand topic structures, allowing them to mutually reinforce each other without\nrequiring additional annotations. We evaluate our approach on two rhetorical\ndatasets and three topic segmentation datasets. Experimental results\ndemonstrate that our method surpasses all strong baselines built on pre-trained\nlanguage models. Furthermore, when applied to LLMs, our framework achieves\nnotable improvements, demonstrating its effectiveness in improving discourse\nstructure modeling."
                },
                "authors": [
                    {
                        "name": "Jiahui Xu"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Anningzhe Gao"
                    },
                    {
                        "name": "Luis Fernando D'Haro"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19799v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19799v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17011v1",
                "updated": "2025-02-24T09:46:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    46,
                    37,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T09:46:37Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    46,
                    37,
                    0,
                    55,
                    0
                ],
                "title": "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep\n  Reinforcement Learning with LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep\n  Reinforcement Learning with LLM Evaluation"
                },
                "summary": "Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation\n(3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation\n(3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making."
                },
                "authors": [
                    {
                        "name": "Jaskaran Singh Walia"
                    },
                    {
                        "name": "Aarush Sinha"
                    },
                    {
                        "name": "Srinitish Srinivasan"
                    },
                    {
                        "name": "Srihari Unnikrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Srihari Unnikrishnan"
                },
                "author": "Srihari Unnikrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12776v2",
                "updated": "2025-02-24T09:44:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    44,
                    12,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-17T10:37:00Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    37,
                    0,
                    1,
                    352,
                    0
                ],
                "title": "Physical simulation of Marsupial UAV-UGV Systems Connected by a\n  Variable-Length Hanging Tether",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical simulation of Marsupial UAV-UGV Systems Connected by a\n  Variable-Length Hanging Tether"
                },
                "summary": "This paper presents a simulation framework able of modeling the dynamics of a\nhanging tether with adjustable length, connecting a UAV to a UGV. The model\nincorporates the interaction between the UAV, UGV, and a winch, allowing for\ndynamic tether adjustments based on the relative motion of the robots. The\naccuracy and reliability of the simulator are assessed through extensive\nexperiments, including comparisons with real-world experiment, to evaluate its\nability to reproduce the complex tether dynamics observed in physical\ndeployments. The results demonstrate that the simulation closely aligns with\nreal-world behavior, particularly in constrained environments where tether\neffects are significant. This work provides a validated tool for studying\ntethered robotic systems, offering valuable insights into their motion dynamics\nand control strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a simulation framework able of modeling the dynamics of a\nhanging tether with adjustable length, connecting a UAV to a UGV. The model\nincorporates the interaction between the UAV, UGV, and a winch, allowing for\ndynamic tether adjustments based on the relative motion of the robots. The\naccuracy and reliability of the simulator are assessed through extensive\nexperiments, including comparisons with real-world experiment, to evaluate its\nability to reproduce the complex tether dynamics observed in physical\ndeployments. The results demonstrate that the simulation closely aligns with\nreal-world behavior, particularly in constrained environments where tether\neffects are significant. This work provides a validated tool for studying\ntethered robotic systems, offering valuable insights into their motion dynamics\nand control strategies."
                },
                "authors": [
                    {
                        "name": "Jose Enrique Maese"
                    },
                    {
                        "name": "Fernando Caballero"
                    },
                    {
                        "name": "Luis Merino"
                    }
                ],
                "author_detail": {
                    "name": "Luis Merino"
                },
                "author": "Luis Merino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10329v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10329v4",
                "updated": "2025-02-24T09:34:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    34,
                    38,
                    0,
                    55,
                    0
                ],
                "published": "2024-10-14T09:40:52Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    40,
                    52,
                    0,
                    288,
                    0
                ],
                "title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs"
                },
                "summary": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Xiaotang Wang"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Boci Peng"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted to WWW'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10329v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10329v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17000v1",
                "updated": "2025-02-24T09:31:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    31,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T09:31:18Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    31,
                    18,
                    0,
                    55,
                    0
                ],
                "title": "An Enhanced Large Language Model For Cross Modal Query Understanding\n  System Using DL-KeyBERT Based CAZSSCL-MPGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Enhanced Large Language Model For Cross Modal Query Understanding\n  System Using DL-KeyBERT Based CAZSSCL-MPGPT"
                },
                "summary": "Large Language Models (LLMs) are advanced deep-learning models designed to\nunderstand and generate human language. They work together with models that\nprocess data like images, enabling cross-modal understanding. However, existing\napproaches often suffer from the echo chamber effect, where redundant visual\npatterns reduce model generalization and accuracy. Thus, the proposed system\nconsidered this limitation and developed an enhanced LLM-based framework for\ncross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT. The\ncollected dataset consists of pre-processed images and texts. The preprocessed\nimages then undergo object segmentation using Easom-You Only Look Once\n(E-YOLO). The object skeleton is generated, along with the knowledge graph\nusing a Conditional Random Knowledge Graph (CRKG) technique. Further, features\nare extracted from the knowledge graph, generated skeletons, and segmented\nobjects. The optimal features are then selected using the Fossa Optimization\nAlgorithm (FOA). Meanwhile, the text undergoes word embedding using DL-KeyBERT.\nFinally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to\ngenerate accurate and contextually relevant image descriptions as text. The\nproposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362% in the COCO dataset\n2017 and 98.43224393% in the vqav2-val dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are advanced deep-learning models designed to\nunderstand and generate human language. They work together with models that\nprocess data like images, enabling cross-modal understanding. However, existing\napproaches often suffer from the echo chamber effect, where redundant visual\npatterns reduce model generalization and accuracy. Thus, the proposed system\nconsidered this limitation and developed an enhanced LLM-based framework for\ncross-modal query understanding using DL-KeyBERT-based CAZSSCL-MPGPT. The\ncollected dataset consists of pre-processed images and texts. The preprocessed\nimages then undergo object segmentation using Easom-You Only Look Once\n(E-YOLO). The object skeleton is generated, along with the knowledge graph\nusing a Conditional Random Knowledge Graph (CRKG) technique. Further, features\nare extracted from the knowledge graph, generated skeletons, and segmented\nobjects. The optimal features are then selected using the Fossa Optimization\nAlgorithm (FOA). Meanwhile, the text undergoes word embedding using DL-KeyBERT.\nFinally, the cross-modal query understanding system utilizes CAZSSCL-MPGPT to\ngenerate accurate and contextually relevant image descriptions as text. The\nproposed CAZSSCL-MPGPT achieved an accuracy of 99.14187362% in the COCO dataset\n2017 and 98.43224393% in the vqav2-val dataset."
                },
                "authors": [
                    {
                        "name": "Shreya Singh"
                    }
                ],
                "author_detail": {
                    "name": "Shreya Singh"
                },
                "author": "Shreya Singh",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16994v1",
                "updated": "2025-02-24T09:28:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    28,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T09:28:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    28,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "FADE: Why Bad Descriptions Happen to Good Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FADE: Why Bad Descriptions Happen to Good Features"
                },
                "summary": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While they may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for evaluating feature-description alignment. FADE\nevaluates alignment across four key metrics - Clarity, Responsiveness, Purity,\nand Faithfulness - and systematically quantifies the causes for the\nmisalignment of feature and their description. We apply FADE to analyze\nexisting open-source feature descriptions, and assess key components of\nautomated interpretability pipelines, aiming to enhance the quality of\ndescriptions. Our findings highlight fundamental challenges in generating\nfeature descriptions, particularly for SAEs as compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While they may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for evaluating feature-description alignment. FADE\nevaluates alignment across four key metrics - Clarity, Responsiveness, Purity,\nand Faithfulness - and systematically quantifies the causes for the\nmisalignment of feature and their description. We apply FADE to analyze\nexisting open-source feature descriptions, and assess key components of\nautomated interpretability pipelines, aiming to enhance the quality of\ndescriptions. Our findings highlight fundamental challenges in generating\nfeature descriptions, particularly for SAEs as compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE."
                },
                "authors": [
                    {
                        "name": "Bruno Puri"
                    },
                    {
                        "name": "Aakriti Jain"
                    },
                    {
                        "name": "Elena Golimblevskaia"
                    },
                    {
                        "name": "Patrick Kahardipraja"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]